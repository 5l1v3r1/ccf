 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="light" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/light.css">
<link id="dark" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/dark.css" disabled/>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/skin.js"></script>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/classie.js"></script>

<!-- This is for Mathjax -->

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["$","$"] ],
			displayMath: [ ['$$','$$'], ["$$","$$"] ],
			processEscapes: true
			},
		TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
		"HTML-CSS": {linebreaks: {automatic: true}},
		SVG: {linebreaks: {automatic: true}}
	});
</script>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#9. WSDM 2016:San Francisco, CA, USA">9. WSDM 2016:San Francisco, CA, USA</a><ul>
<li><a href="#Paper Num: 93 || Session Num: 17">Paper Num: 93 || Session Num: 17</a></li>
<li><a href="#Keynote Address    1">Keynote Address    1</a><ul>
<li><a href="#1. Large-Scale Deep Learning For Building Intelligent Computer Systems.">1. Large-Scale Deep Learning For Building Intelligent Computer Systems.</a></li>
</ul>
</li>
<li><a href="#Communities and Social Interaction    10">Communities and Social Interaction    10</a><ul>
<li><a href="#2. Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions.">2. Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions.</a></li>
<li><a href="#3. Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia.">3. Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia.</a></li>
<li><a href="#4. How Relevant is the Irrelevant Data: Leveraging the Tagging Data for a Learning-to-Rank Model.">4. How Relevant is the Irrelevant Data: Leveraging the Tagging Data for a Learning-to-Rank Model.</a></li>
<li><a href="#5. Quantifying Controversy in Social Media.">5. Quantifying Controversy in Social Media.</a></li>
<li><a href="#6. Understanding and Identifying Advocates for Political Campaigns on Social Media.">6. Understanding and Identifying Advocates for Political Campaigns on Social Media.</a></li>
<li><a href="#7. Exploiting New Sentiment-Based Meta-level Features for Effective Sentiment Analysis.">7. Exploiting New Sentiment-Based Meta-level Features for Effective Sentiment Analysis.</a></li>
<li><a href="#8. Mobile App Tagging.">8. Mobile App Tagging.</a></li>
<li><a href="#9. CCCF: Improving Collaborative Filtering via Scalable User-Item Co-Clustering.">9. CCCF: Improving Collaborative Filtering via Scalable User-Item Co-Clustering.</a></li>
<li><a href="#10. On the Efficiency of the Information Networks in Social Media.">10. On the Efficiency of the Information Networks in Social Media.</a></li>
<li><a href="#11. Modeling and Predicting Learning Behavior in MOOCs.">11. Modeling and Predicting Learning Behavior in MOOCs.</a></li>
</ul>
</li>
<li><a href="#Search and Semantics    10">Search and Semantics    10</a><ul>
<li><a href="#12. Beyond Ranking: Optimizing Whole-Page Presentation.">12. Beyond Ranking: Optimizing Whole-Page Presentation.</a></li>
<li><a href="#13. Understanding User Attention and Engagement in Online News Reading.">13. Understanding User Attention and Engagement in Online News Reading.</a></li>
<li><a href="#14. Publication Date Prediction through Reverse Engineering of the Web.">14. Publication Date Prediction through Reverse Engineering of the Web.</a></li>
<li><a href="#15. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation.">15. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation.</a></li>
<li><a href="#16. Term-by-Term Query Auto-Completion for Mobile Search.">16. Term-by-Term Query Auto-Completion for Mobile Search.</a></li>
<li><a href="#17. Collaborative Denoising Auto-Encoders for Top-N Recommender Systems.">17. Collaborative Denoising Auto-Encoders for Top-N Recommender Systems.</a></li>
<li><a href="#18. Personalized PageRank Estimation and Search: A Bidirectional Approach.">18. Personalized PageRank Estimation and Search: A Bidirectional Approach.</a></li>
<li><a href="#19. Your Cart tells You: Inferring Demographic Attributes from Purchase Data.">19. Your Cart tells You: Inferring Demographic Attributes from Purchase Data.</a></li>
<li><a href="#20. Reducing Click and Skip Errors in Search Result Ranking.">20. Reducing Click and Skip Errors in Search Result Ranking.</a></li>
<li><a href="#21. Hierarchical Semi-supervised Classification with Incomplete Class Hierarchies.">21. Hierarchical Semi-supervised Classification with Incomplete Class Hierarchies.</a></li>
</ul>
</li>
<li><a href="#Practice & Experience Track    1">Practice &amp; Experience Track    1</a><ul>
<li><a href="#22. Is Mail The Next Frontier In Search And Data Mining?">22. Is Mail The Next Frontier In Search And Data Mining?</a></li>
</ul>
</li>
<li><a href="#Observing Users    2">Observing Users    2</a><ul>
<li><a href="#23. Portrait of an Online Shopper: Understanding and Predicting Consumer Behavior.">23. Portrait of an Online Shopper: Understanding and Predicting Consumer Behavior.</a></li>
<li><a href="#24. Evolution of Privacy Loss in Wikipedia.">24. Evolution of Privacy Loss in Wikipedia.</a></li>
</ul>
</li>
<li><a href="#Keynote Address    1">Keynote Address    1</a><ul>
<li><a href="#25. Keynote Speaker Bio.">25. Keynote Speaker Bio.</a></li>
</ul>
</li>
<li><a href="#Leveraging Users    14">Leveraging Users    14</a><ul>
<li><a href="#26. Modeling Intransitivity in Matchup and Comparison Data.">26. Modeling Intransitivity in Matchup and Comparison Data.</a></li>
<li><a href="#27. Crowdsourcing High Quality Labels with a Tight Budget.">27. Crowdsourcing High Quality Labels with a Tight Budget.</a></li>
<li><a href="#28. Project Success Prediction in Crowdfunding Environments.">28. Project Success Prediction in Crowdfunding Environments.</a></li>
<li><a href="#29. Probabilistic Group Recommendation Model for Crowdfunding Domains.">29. Probabilistic Group Recommendation Model for Crowdfunding Domains.</a></li>
<li><a href="#30. Quality Management in Crowdsourcing using Gold Judges Behavior.">30. Quality Management in Crowdsourcing using Gold Judges Behavior.</a></li>
<li><a href="#31. On Obtaining Effort Based Judgements for Information Retrieval.">31. On Obtaining Effort Based Judgements for Information Retrieval.</a></li>
<li><a href="#32. A Semantic Graph based Topic Model for Question Retrieval in Community Question Answering.">32. A Semantic Graph based Topic Model for Question Retrieval in Community Question Answering.</a></li>
<li><a href="#33. Modeling Check-in Preferences with Multidimensional Knowledge: A Minimax Entropy Approach.">33. Modeling Check-in Preferences with Multidimensional Knowledge: A Minimax Entropy Approach.</a></li>
<li><a href="#34. You've got Mail, and Here is What you Could do With It!: Analyzing and Predicting Actions on Email Messages.">34. You've got Mail, and Here is What you Could do With It!: Analyzing and Predicting Actions on Email Messages.</a></li>
<li><a href="#35. Hierarchical Label Propagation and Discovery for Machine Generated Email.">35. Hierarchical Label Propagation and Discovery for Machine Generated Email.</a></li>
<li><a href="#36. Enforcing k-anonymity in Web Mail Auditing.">36. Enforcing k-anonymity in Web Mail Auditing.</a></li>
<li><a href="#37. An Information-Theoretic Approach to Individual Sequential Data Sanitization.">37. An Information-Theoretic Approach to Individual Sequential Data Sanitization.</a></li>
<li><a href="#38. Improving IP Geolocation using Query Logs.">38. Improving IP Geolocation using Query Logs.</a></li>
<li><a href="#39. Geographic Segmentation via Latent Poisson Factor Model.">39. Geographic Segmentation via Latent Poisson Factor Model.</a></li>
</ul>
</li>
<li><a href="#Big Data Algorithms    10">Big Data Algorithms    10</a><ul>
<li><a href="#40. Scaling up Link Prediction with Ensembles.">40. Scaling up Link Prediction with Ensembles.</a></li>
<li><a href="#41. DiFacto: Distributed Factorization Machines.">41. DiFacto: Distributed Factorization Machines.</a></li>
<li><a href="#42. Distributed Balanced Partitioning via Linear Embedding.">42. Distributed Balanced Partitioning via Linear Embedding.</a></li>
<li><a href="#43. Kangaroo: Workload-Aware Processing of Range Data and Range Queries in Hadoop.">43. Kangaroo: Workload-Aware Processing of Range Data and Range Queries in Hadoop.</a></li>
<li><a href="#44. Feedback Control of Real-Time Display Advertising.">44. Feedback Control of Real-Time Display Advertising.</a></li>
<li><a href="#45. Multi-Score Position Auctions.">45. Multi-Score Position Auctions.</a></li>
<li><a href="#46. Multi-view Machines.">46. Multi-view Machines.</a></li>
<li><a href="#47. Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization.">47. Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization.</a></li>
<li><a href="#48. The Troll-Trust Model for Ranking in Signed Networks.">48. The Troll-Trust Model for Ranking in Signed Networks.</a></li>
<li><a href="#49. Multileave Gradient Descent for Fast Online Learning to Rank.">49. Multileave Gradient Descent for Fast Online Learning to Rank.</a></li>
</ul>
</li>
<li><a href="#Practice & Experience Track    2">Practice &amp; Experience Track    2</a><ul>
<li><a href="#50. AMiner: Toward Understanding Big Scholar Data.">50. AMiner: Toward Understanding Big Scholar Data.</a></li>
<li><a href="#51. Serving a Billion Personalized News Feeds.">51. Serving a Billion Personalized News Feeds.</a></li>
</ul>
</li>
<li><a href="#Keynote Address    1">Keynote Address    1</a><ul>
<li><a href="#52. The Predictive Power of Massive Data about our Fine-Grained Behavior.">52. The Predictive Power of Massive Data about our Fine-Grained Behavior.</a></li>
</ul>
</li>
<li><a href="#Social Networks    12">Social Networks    12</a><ul>
<li><a href="#53. Information Evolution in Social Networks.">53. Information Evolution in Social Networks.</a></li>
<li><a href="#54. Nonlinear Laplacian for Digraphs and its Applications to Network Analysis.">54. Nonlinear Laplacian for Digraphs and its Applications to Network Analysis.</a></li>
<li><a href="#55. Querying and Tracking Influencers in Social Streams.">55. Querying and Tracking Influencers in Social Streams.</a></li>
<li><a href="#56. Centrality-Aware Link Recommendations.">56. Centrality-Aware Link Recommendations.</a></li>
<li><a href="#57. Relational Learning with Social Status Analysis.">57. Relational Learning with Social Status Analysis.</a></li>
<li><a href="#58. Equality and Social Mobility in Twitter Discussion Groups.">58. Equality and Social Mobility in Twitter Discussion Groups.</a></li>
<li><a href="#59. Learning Distributed Representations of Data in Community Question Answering for Question Retrieval.">59. Learning Distributed Representations of Data in Community Question Answering for Question Retrieval.</a></li>
<li><a href="#60. Inferring Latent Triggers of Purchases with Consideration of Social Effects and Media Advertisements.">60. Inferring Latent Triggers of Purchases with Consideration of Social Effects and Media Advertisements.</a></li>
<li><a href="#61. Towards Modelling Language Innovation Acceptance in Online Social Networks.">61. Towards Modelling Language Innovation Acceptance in Online Social Networks.</a></li>
<li><a href="#62. Discriminative Learning of Infection Models.">62. Discriminative Learning of Infection Models.</a></li>
<li><a href="#63. Representation Learning for Information Diffusion through Social Networks: an Embedded Cascade Model.">63. Representation Learning for Information Diffusion through Social Networks: an Embedded Cascade Model.</a></li>
<li><a href="#64. Ensemble Models for Data-driven Prediction of Malware Infections.">64. Ensemble Models for Data-driven Prediction of Malware Infections.</a></li>
</ul>
</li>
<li><a href="#Entities and Structure    8">Entities and Structure    8</a><ul>
<li><a href="#65. WSDM Cup 2016: Entity Ranking Challenge.">65. WSDM Cup 2016: Entity Ranking Challenge.</a></li>
<li><a href="#66. Dynamic Collective Entity Representations for Entity Ranking.">66. Dynamic Collective Entity Representations for Entity Ranking.</a></li>
<li><a href="#67. Relationship Queries on Extended Knowledge Graphs.">67. Relationship Queries on Extended Knowledge Graphs.</a></li>
<li><a href="#68. Improving Website Hyperlink Structure Using Server Logs.">68. Improving Website Hyperlink Structure Using Server Logs.</a></li>
<li><a href="#69. Long-tail Vocabulary Dictionary Extraction from the Web.">69. Long-tail Vocabulary Dictionary Extraction from the Web.</a></li>
<li><a href="#70. Semantic Documents Relatedness using Concept Graph Representation.">70. Semantic Documents Relatedness using Concept Graph Representation.</a></li>
<li><a href="#71. EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion.">71. EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion.</a></li>
<li><a href="#72. Extracting Search Query Patterns via the Pairwise Coupled Topic Model.">72. Extracting Search Query Patterns via the Pairwise Coupled Topic Model.</a></li>
</ul>
</li>
<li><a href="#Practice & Experience Track    1">Practice &amp; Experience Track    1</a><ul>
<li><a href="#73. The Past and Future of Systems for Current Events.">73. The Past and Future of Systems for Current Events.</a></li>
</ul>
</li>
<li><a href="#Social Events    2">Social Events    2</a><ul>
<li><a href="#74. Barbara Made the News: Mining the Behavior of Crowds for Time-Aware Learning to Rank.">74. Barbara Made the News: Mining the Behavior of Crowds for Time-Aware Learning to Rank.</a></li>
<li><a href="#75. Wiggins: Detecting Valuable Information in Dynamic Networks Using Limited Resources.">75. Wiggins: Detecting Valuable Information in Dynamic Networks Using Limited Resources.</a></li>
</ul>
</li>
<li><a href="#Tutorials    2">Tutorials    2</a><ul>
<li><a href="#76. Understanding Offline Political Systems by Mining Online Political Data.">76. Understanding Offline Political Systems by Mining Online Political Data.</a></li>
<li><a href="#77. Click Models for Web Search and their Applications to IR: WSDM 2016 Tutorial.">77. Click Models for Web Search and their Applications to IR: WSDM 2016 Tutorial.</a></li>
</ul>
</li>
<li><a href="#Workshop Summaries    4">Workshop Summaries    4</a><ul>
<li><a href="#78. Query Understanding for Search on All Devices at WSDM 2016.">78. Query Understanding for Search on All Devices at WSDM 2016.</a></li>
<li><a href="#79. TargetAd2016: 2nd International Workshop on Ad Targeting at Scale.">79. TargetAd2016: 2nd International Workshop on Ad Targeting at Scale.</a></li>
<li><a href="#80. WSDM 2016 Workshop on the Ethics of Online Experimentation.">80. WSDM 2016 Workshop on the Ethics of Online Experimentation.</a></li>
<li><a href="#81. Second Workshop on Search and Exploration of X-Rated Information (SEXI'16">81. Second Workshop on Search and Exploration of X-Rated Information (SEXI'16): WSDM Workshop Summary.</a>: WSDM Workshop Summary.)</li>
</ul>
</li>
<li><a href="#Doctoral Consortium    12">Doctoral Consortium    12</a><ul>
<li><a href="#82. Mining Complaints to Improve a Product: a Study about Problem Phrase Extraction from User Reviews.">82. Mining Complaints to Improve a Product: a Study about Problem Phrase Extraction from User Reviews.</a></li>
<li><a href="#83. Web-scale Multimedia Search for Internet Video Content.">83. Web-scale Multimedia Search for Internet Video Content.</a></li>
<li><a href="#84. Affective Computing of Image Emotion Perceptions.">84. Affective Computing of Image Emotion Perceptions.</a></li>
<li><a href="#85. Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search & Analytics.">85. Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search &amp; Analytics.</a></li>
<li><a href="#86. Understanding Diffusion Processes: Inference and Theory.">86. Understanding Diffusion Processes: Inference and Theory.</a></li>
<li><a href="#87. E-commerce Product Recommendation by Personalized Promotion and Total Surplus Maximization.">87. E-commerce Product Recommendation by Personalized Promotion and Total Surplus Maximization.</a></li>
<li><a href="#88. Detecting Social Media Icebergs by Their Tips: Rumors, Persuasion Campaigns, and Information Needs.">88. Detecting Social Media Icebergs by Their Tips: Rumors, Persuasion Campaigns, and Information Needs.</a></li>
<li><a href="#89. User Modeling in Large Social Networks.">89. User Modeling in Large Social Networks.</a></li>
<li><a href="#90. Feature Generation and Selection on the Heterogeneous Graph for Music Recommendation.">90. Feature Generation and Selection on the Heterogeneous Graph for Music Recommendation.</a></li>
<li><a href="#91. Temporal Formation and Evolution of Online Communities.">91. Temporal Formation and Evolution of Online Communities.</a></li>
<li><a href="#92. Mining the Web for Intelligent Problem Solving for Programmers.">92. Mining the Web for Intelligent Problem Solving for Programmers.</a></li>
<li><a href="#93. Optimizing Search Interactions within Professional Social Networks.">93. Optimizing Search Interactions within Professional Social Networks.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="9. WSDM 2016:San Francisco, CA, USA">9. WSDM 2016:San Francisco, CA, USA</h1>
<p><a href="http://dl.acm.org/citation.cfm?id=2835776">Proceedings of the Ninth ACM International Conference on Web Search and Data Mining, San Francisco, CA, USA, February 22-25, 2016.</a> ACM
【<a href="http://dblp.uni-trier.de/db/conf/wsdm/wsdm2016.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 93 || Session Num: 17">Paper Num: 93 || Session Num: 17</h2>
<h2 id="Keynote Address    1">Keynote Address    1</h2>
<h3 id="1. Large-Scale Deep Learning For Building Intelligent Computer Systems.">1. Large-Scale Deep Learning For Building Intelligent Computer Systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835844">Paper Link</a>】    【Pages】:1</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dean:Jeffrey">Jeffrey Dean</a></p>
<p>【Abstract】:
For the past five years, the Google Brain team has focused on conducting research in difficult problems in artificial intelligence, on building large-scale computer systems for machine learning research, and, in collaboration with many teams at Google, on applying our research and systems to dozens of Google products. Our group has recently open-sourced the TensorFlow system (tensorflow.org), a system designed to easily express machine ideas, and to quickly train, evaluate and deploy machine learning systems. In this talk, I'll highlight some of the design decisions we made in building TensorFlow, discuss research results produced within our group, and describe ways in which these ideas have been applied to a variety of problems in Google's products, usually in close collaboration with other teams. This talk describes joint work with many people at Google.</p>
<p>【Keywords】:
computer vision; deep learning; distributed systems; language understanding; machine learning; neural networks; speech recognition</p>
<h2 id="Communities and Social Interaction    10">Communities and Social Interaction    10</h2>
<h3 id="2. Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions.">2. Who Will Reply to/Retweet This Tweet?: The Dynamics of Intimacy from Online Social Interactions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835800">Paper Link</a>】    【Pages】:3-12</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Nicholas_Jing">Nicholas Jing Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhong:Yuan">Yuan Zhong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Fuzheng">Fuzheng Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Xing">Xing Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rui:Yong">Yong Rui</a></p>
<p>【Abstract】:
Friendships are dynamic. Previous studies have converged to suggest that social interactions, in both online and offline social networks, are diagnostic reflections of friendship relations (also called social ties). However, most existing approaches consider a social tie as either a binary relation, or a fixed value (named tie strength). In this paper, we investigate the dynamics of dyadic friend relationships through online social interactions, in terms of a variety of aspects, such as reciprocity, temporality, and contextuality. In turn, we propose a model to predict repliers and retweeters given a particular tweet posted at a certain time in a microblog-based social network. More specifically, we have devised a learning-to-rank approach to train a ranker that considers elaborate user-level and tweet-level features (like sentiment, self-disclosure, and responsiveness) to address these dynamics. In the prediction phase, a tweet posted by a user is deemed a query and the predicted repliers/retweeters are retrieved using the learned ranker. We have collected a large dataset containing 73.3 million dyadic relationships with their interactions (replies and retweets). Extensive experimental results based on this dataset show that by incorporating the dynamics of friendship relations, our approach significantly outperforms state-of-the-art models in terms of multiple evaluation metrics, such as MAP, NDCG and Topmost Accuracy. In particular, the advantage of our model is even more promising in predicting the exact sequence of repliers/retweeters considering their orders. Furthermore, the proposed approach provides emerging implications for many high-value applications in online social networks.</p>
<p>【Keywords】:
dynamic tie strength; friendship relation; online social interaction</p>
<h3 id="3. Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia.">3. Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835779">Paper Link</a>】    【Pages】:13-22</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/You:Quanzeng">Quanzeng You</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Jiebo">Jiebo Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Hailin">Hailin Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Jianchao">Jianchao Yang</a></p>
<p>【Abstract】:
Sentiment analysis of online user generated content is important for many social media analytics tasks. Researchers have largely relied on textual sentiment analysis to develop systems to predict political elections, measure economic indicators, and so on. Recently, social media users are increasingly using additional images and videos to express their opinions and share their experiences. Sentiment analysis of such large-scale textual and visual content can help better extract user sentiments toward events or topics. Motivated by the needs to leverage large-scale social multimedia content for sentiment analysis, we propose a cross-modality consistent regression (CCR) model, which is able to utilize both the state-of-the-art visual and textual sentiment analysis techniques. We first fine-tune a convolutional neural network (CNN) for image sentiment analysis and train a paragraph vector model for textual sentiment analysis. On top of them, we train our multi-modality regression model. We use sentimental queries to obtain half a million training samples from Getty Images. We have conducted extensive experiments on both machine weakly labeled and manually labeled image tweets. The results show that the proposed model can achieve better performance than the state-of-the-art textual and visual sentiment analysis algorithms alone.</p>
<p>【Keywords】:
cross-modality regression; multimodality analysis; sentiment analysis</p>
<h3 id="4. How Relevant is the Irrelevant Data: Leveraging the Tagging Data for a Learning-to-Rank Model.">4. How Relevant is the Irrelevant Data: Leveraging the Tagging Data for a Learning-to-Rank Model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835790">Paper Link</a>】    【Pages】:23-32</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Ifada:Noor">Noor Ifada</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nayak:Richi">Richi Nayak</a></p>
<p>【Abstract】:
For the task of tag-based item recommendations, the underlying tensor model faces several challenges such as high data sparsity and inferring latent factors effectively. To overcome the inherent sparsity issue of tensor models, we propose the graded-relevance interpretation scheme that leverages the tagging data effectively. Unlike the existing schemes, the graded-relevance scheme interprets the tagging data richly, differentiates the non-observed tagging data insightfully, and annotates each entry as one of the "relevant", "likely relevant", "irrelevant", or "indecisive" labels. To infer the latent factors of tensor models correctly to produce the high quality recommendation, we develop a novel learning-to-rank method, Go-Rank, that optimizes Graded Average Precision (GAP). Evaluating the proposed method on real-world datasets, we show that the proposed interpretation scheme produces a denser tensor model by revealing "relevant" entries from the previously assumed "irrelevant" entries. Optimizing GAP as the ranking metric, the quality of the recommendations generated by Go-Rank is found superior against the benchmarking methods.</p>
<p>【Keywords】:
graded average precision; graded-relevance scheme; item recommendation; tagging data</p>
<h3 id="5. Quantifying Controversy in Social Media.">5. Quantifying Controversy in Social Media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835792">Paper Link</a>】    【Pages】:33-42</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Garimella:Kiran">Kiran Garimella</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Morales:Gianmarco_De_Francisci">Gianmarco De Francisci Morales</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gionis:Aristides">Aristides Gionis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mathioudakis:Michael">Michael Mathioudakis</a></p>
<p>【Abstract】:
Which topics spark the most heated debates in social media? Identifying these topics is a first step towards creating systems which pierce echo chambers. In this paper, we perform a systematic methodological study of controversy detection using social media network structure and content. Unlike previous work, rather than identifying controversy in a single hand-picked topic and use domain-specific knowledge, we focus on comparing topics in any domain. Our approach to quantifying controversy is a graph-based three-stage pipeline, which involves (i) building a conversation graph about a topic, which represents alignment of opinion among users; (ii) partitioning the conversation graph to identify potential sides of the controversy; and (iii)measuring the amount of controversy from characteristics of the~graph. We perform an extensive comparison of controversy measures, as well as graph building approaches and data sources. We use both controversial and non-controversial topics on Twitter, as well as other external datasets. We find that our new random-walk-based measure outperforms existing ones in capturing the intuitive notion of controversy, and show that content features are vastly less helpful in this task.</p>
<p>【Keywords】:
controversy; random walks; social media; twitter</p>
<h3 id="6. Understanding and Identifying Advocates for Political Campaigns on Social Media.">6. Understanding and Identifying Advocates for Political Campaigns on Social Media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835807">Paper Link</a>】    【Pages】:43-52</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ranganath:Suhas">Suhas Ranganath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xia">Xia Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jiliang">Jiliang Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Huan">Huan Liu</a></p>
<p>【Abstract】:
Social media is increasingly being used to access and disseminate information on sociopolitical issues like gun rights and general elections. The popularity and openness of social media makes it conducive for some individuals, known as advocates, who use social media to push their agendas on these issues strategically. Identifying these advocates will caution social media users before reading their information and also enable campaign managers to identify advocates for their digital political campaigns. A significant challenge in identifying advocates is that they employ nuanced strategies to shape user opinion and increase the spread of their messages, making it difficult to distinguish them from random users posting on the campaign. In this paper, we draw from social movement theories and design a quantitative framework to study the nuanced message strategies, propagation strategies, and community structure adopted by advocates for political campaigns in social media. Based on observations of their social media activities manifesting from these strategies, we investigate how to model these strategies for identifying them. We evaluate the framework using two datasets from Twitter, and our experiments demonstrate its effectiveness in identifying advocates for political campaigns with ramifications of this work directed towards assisting users as they navigate through social media spaces.</p>
<p>【Keywords】:
advocacy; political campaigns; user interactions</p>
<h3 id="7. Exploiting New Sentiment-Based Meta-level Features for Effective Sentiment Analysis.">7. Exploiting New Sentiment-Based Meta-level Features for Effective Sentiment Analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835821">Paper Link</a>】    【Pages】:53-62</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Canuto:S=eacute=rgio_D=">Sérgio D. Canuto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gon=ccedil=alves:Marcos_Andr=eacute=">Marcos André Gonçalves</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Benevenuto:Fabr=iacute=cio">Fabrício Benevenuto</a></p>
<p>【Abstract】:
In this paper we address the problem of automatically learning to classify the sentiment of short messages/reviews by exploiting information derived from meta-level features i.e., features derived primarily from the original bag-of-words representation. We propose new meta-level features especially designed for the sentiment analysis of short messages such as: (i) information derived from the sentiment distribution among the k nearest neighbors of a given short test document x, (ii) the distribution of distances of x to their neighbors and (iii) the document polarity of these neighbors given by unsupervised lexical-based methods. Our approach is also capable of exploiting information from the neighborhood of document x regarding (highly noisy) data obtained from 1.6 million Twitter messages with emoticons. The set of proposed features is capable of transforming the original feature space into a new one, potentially smaller and more informed. Experiments performed with a substantial number of datasets (nineteen) demonstrate that the effectiveness of the proposed sentiment-based meta-level features is not only superior to the traditional bag-of-word representation (by up to 16%) but is also superior in most cases to state-of-art meta-level features previously proposed in the literature for text classification tasks that do not take into account some idiosyncrasies of sentiment analysis. Our proposal is also largely superior to the best lexicon-based methods as well as to supervised combinations of them. In fact, the proposed approach is the only one to produce the best results in all tested datasets in all scenarios.</p>
<p>【Keywords】:
meta features; sentiment analysis</p>
<h3 id="8. Mobile App Tagging.">8. Mobile App Tagging.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835812">Paper Link</a>】    【Pages】:63-72</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ning">Ning Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hoi:Steven_C=_H=">Steven C. H. Hoi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Shaohua">Shaohua Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiao:Xiaokui">Xiaokui Xiao</a></p>
<p>【Abstract】:
Mobile app tagging aims to assign a list of keywords indicating core functionalities, main contents, key features or concepts of a mobile app. Mobile app tags can be potentially useful for app ecosystem stakeholders or other parties to improve app search, browsing, categorization, and advertising, etc. However, most mainstream app markets, e.g., Google Play, Apple App Store, etc., currently do not explicitly support such tags for apps. To address this problem, we propose a novel auto mobile app tagging framework for annotating a given mobile app automatically, which is based on a search-based annotation paradigm powered by machine learning techniques. Specifically, given a novel query app without tags, our proposed framework (i) first explores online kernel learning techniques to retrieve a set of top-N similar apps that are semantically most similar to the query app from a large app repository; and (ii) then mines the text data of both the query app and the top-N similar apps to discover the most relevant tags for annotating the query app. To evaluate the efficacy of our proposed framework, we conduct an extensive set of experiments on a large real-world dataset crawled from Google Play. The encouraging results demonstrate that our technique is effective and promising.</p>
<p>【Keywords】:
app tagging; mobile app markets; online kernel learning</p>
<h3 id="9. CCCF: Improving Collaborative Filtering via Scalable User-Item Co-Clustering.">9. CCCF: Improving Collaborative Filtering via Scalable User-Item Co-Clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835836">Paper Link</a>】    【Pages】:73-82</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Yao">Yao Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xudong">Xudong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Min">Min Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ester:Martin">Martin Ester</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Qing">Qing Yang</a></p>
<p>【Abstract】:
Collaborative Filtering (CF) is the most popular method for recommender systems. The principal idea of CF is that users might be interested in items that are favorited by similar users, and most of the existing CF methods measure users' preferences by their behaviours over all the items. However, users might have different interests over different topics, thus might share similar preferences with different groups of users over different sets of items. In this paper, we propose a novel and scalable method CCCF which improves the performance of CF methods via user-item co-clustering. CCCF first clusters users and items into several subgroups, where each subgroup includes a set of like-minded users and a set of items in which these users share their interests. Then, traditional CF methods can be easily applied to each subgroup, and the recommendation results from all the subgroups can be easily aggregated. Compared with previous works, CCCF has several advantages including scalability, flexibility, interpretability and extensibility. Experimental results on four real world data sets demonstrate that the proposed method significantly improves the performance of several state-of-the-art recommendation algorithms.</p>
<p>【Keywords】:
co-clustering; collaborative filtering; recommender systems</p>
<h3 id="10. On the Efficiency of the Information Networks in Social Media.">10. On the Efficiency of the Information Networks in Social Media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835826">Paper Link</a>】    【Pages】:83-92</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Babaei:Mahmoudreza">Mahmoudreza Babaei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grabowicz:Przemyslaw_A=">Przemyslaw A. Grabowicz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Valera:Isabel">Isabel Valera</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gummadi:Krishna_P=">Krishna P. Gummadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gomez=Rodriguez:Manuel">Manuel Gomez-Rodriguez</a></p>
<p>【Abstract】:
Social media sites are information marketplaces, where users produce and consume a wide variety of information and ideas. In these sites, users typically choose their information sources, which in turn determine what specific information they receive, how much information they receive and how quickly this information is shown to them. In this context, a natural question that arises is how efficient are social media users at selecting their information sources. In this work, we propose a computational framework to quantify users' efficiency at selecting information sources. Our framework is based on the assumption that the goal of users is to acquire a set of unique pieces of information. To quantify user's efficiency, we ask if the user could have acquired the same pieces of information from another set of sources more efficiently. We define three different notions of efficiency -- link, in-flow, and delay -- corresponding to the number of sources the user follows, the amount of (redundant) information she acquires and the delay with which she receives the information. Our definitions of efficiency are general and applicable to any social media system with an underlying in- formation network, in which every user follows others to receive the information they produce. In our experiments, we measure the efficiency of Twitter users at acquiring different types of information. We find that Twitter users exhibit sub-optimal efficiency across the three notions of efficiency, although they tend to be more efficient at acquiring non- popular pieces of information than they are at acquiring popular pieces of information. We then show that this lack of efficiency is a consequence of the triadic closure mechanism by which users typically discover and follow other users in social media. Thus, our study reveals a tradeoff between the efficiency and discoverability of information sources. Finally, we develop a heuristic algorithm that enables users to be significantly more efficient at acquiring the same unique pieces of information.</p>
<p>【Keywords】:
cover set; efficiency; information; information network; lossless; optimization; rewiring algorithm; social media</p>
<h3 id="11. Modeling and Predicting Learning Behavior in MOOCs.">11. Modeling and Predicting Learning Behavior in MOOCs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835842">Paper Link</a>】    【Pages】:93-102</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qiu:Jiezhong">Jiezhong Qiu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tracy_Xiao">Tracy Xiao Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gong:Jie">Jie Gong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Chenhui">Chenhui Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Qian">Qian Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Yufei">Yufei Xue</a></p>
<p>【Abstract】:
Massive Open Online Courses (MOOCs), which collect complete records of all student interactions in an online learning environment, offer us an unprecedented opportunity to analyze students' learning behavior at a very fine granularity than ever before. Using dataset from xuetangX, one of the largest MOOCs from China, we analyze key factors that influence students' engagement in MOOCs and study to what extent we could infer a student's learning effectiveness. We observe significant behavioral heterogeneity in students' course selection as well as their learning patterns. For example, students who exert higher effort and ask more questions are not necessarily more likely to get certificates. Additionally, the probability that a student obtains the course certificate increases dramatically (3 x higher) when she has one or more "certificate friends". Moreover, we develop a unified model to predict students' learning effectiveness, by incorporating user demographics, forum activities, and learning behavior. We demonstrate that the proposed model significantly outperforms (+2.03-9.03% by F1-score) several alternative methods in predicting students' performance on assignments and course certificates. The model is flexible and can be applied to various settings. For example, we are deploying a new feature into xuetangX to help teachers dynamically optimize the teaching process.</p>
<p>【Keywords】:
moocs; online engagement; predictive model; user behavior</p>
<h2 id="Search and Semantics    10">Search and Semantics    10</h2>
<h3 id="12. Beyond Ranking: Optimizing Whole-Page Presentation.">12. Beyond Ranking: Optimizing Whole-Page Presentation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835824">Paper Link</a>】    【Pages】:103-112</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yue">Yue Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Dawei">Dawei Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jie:Luo">Luo Jie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Pengyuan">Pengyuan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yamada:Makoto">Makoto Yamada</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mei:Qiaozhu">Qiaozhu Mei</a></p>
<p>【Abstract】:
Modern search engines aggregate results from different verticals: webpages, news, images, video, shopping, knowledge cards, local maps, etc. Unlike "ten blue links", these search results are heterogeneous in nature and not even arranged in a list on the page. This revolution directly challenges the conventional "ranked list" formulation in ad hoc search. Therefore, finding proper presentation for a gallery of heterogeneous results is critical for modern search engines. We propose a novel framework that learns the optimal page presentation to render heterogeneous results onto search result page (SERP). Page presentation is broadly defined as the strategy to present a set of items on SERP, much more expressive than a ranked list. It can specify item positions, image sizes, text fonts, and any other styles as long as variations are within business and design constraints. The learned presentation is content-aware, i.e. tailored to specific queries and returned results. Simulation experiments show that the framework automatically learns eye-catchy presentations for relevant results. Experiments on real data show that simple instantiations of the framework already outperform leading algorithm in federated search result presentation. It means the framework can learn its own result presentation strategy purely from data, without even knowing the "probability ranking principle".</p>
<p>【Keywords】:
federated search; user feedback; whole-page presentation</p>
<h3 id="13. Understanding User Attention and Engagement in Online News Reading.">13. Understanding User Attention and Engagement in Online News Reading.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835833">Paper Link</a>】    【Pages】:113-122</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lagun:Dmitry">Dmitry Lagun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lalmas:Mounia">Mounia Lalmas</a></p>
<p>【Abstract】:
Prior work on user engagement with online media identified web page dwell time as a key metric reflecting level of user engagement with online news articles. While on average, dwell time gives a reasonable estimate of user experience with a news article, it is not able to capture important aspects of user interaction with the page, such as how much time a user spends reading the article vs. viewing the comment posted by other users, or the actual proportion of article read by the user. In this paper, we propose a set of user engagement classes along with new user engagement metrics that, unlike dwell time, more accurately reflect user experience with the content. Our user engagement classes provide clear and interpretable taxonomy of user engagement with online news, and are defined based on amount of time user spends on the page, proportion of the article user actually reads and the amount of interaction users performs with the comments. Moreover, we demonstrate that our metrics are relatively easier to predict from the news article content, compared to the dwell time, making optimization of user engagement more attainable goal.</p>
<p>【Keywords】:
attention; engagement; large scale; news reading; topic modeling; user modeling; viewport</p>
<h3 id="14. Publication Date Prediction through Reverse Engineering of the Web.">14. Publication Date Prediction through Reverse Engineering of the Web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835796">Paper Link</a>】    【Pages】:123-132</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Prokhorenkova:Liudmila_Ostroumova">Liudmila Ostroumova Prokhorenkova</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Prokhorenkov:Petr">Petr Prokhorenkov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Samosvat:Egor">Egor Samosvat</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a></p>
<p>【Abstract】:
In this paper, we focus on one of the most challenging tasks in temporal information retrieval: detection of a web page publication date. The natural approach to this problem is to find the publication date in the HTML body of a page. However, there are two fundamental problems with this approach. First, not all web pages contain the publication dates in their texts. Second, it is hard to distinguish the publication date among all the dates found in the page's text. The approach we suggest in this paper supplements methods of date extraction from the page's text with novel link-based methods of dating. Some of our link-based methods are based on a probabilistic model of the Web graph structure evolution, which relies on the publication dates of web pages as on its parameters. We use this model to estimate the publication dates of web pages: based only on the link structure currently observed, we perform a ``reverse engineering'' to reveal the whole process of the Web's evolution.</p>
<p>【Keywords】:
likelihood optimization; link-based method; publication dates; web pages</p>
<h3 id="15. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation.">15. To Suggest, or Not to Suggest for Queries with Diverse Intents: Optimizing Search Result Presentation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835805">Paper Link</a>】    【Pages】:133-142</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kato:Makoto_P=">Makoto P. Kato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tanaka:Katsumi">Katsumi Tanaka</a></p>
<p>【Abstract】:
We propose a method of optimizing search result presentation for queries with diverse intents, by selectively presenting query suggestions for leading users to more relevant search results. The optimization is based on a probabilistic model of users who click on query suggestions in accordance with their intents, and modified versions of intent-aware evaluation metrics that take into account the co-occurrence between intents. Showing many query suggestions simply increases a chance to satisfy users with diverse intents in this model, while it in fact requires users to spend additional time for scanning and selecting suggestions, and may result in low satisfaction for some users. Therefore, we measured the loss of time caused by query suggestion presentation by conducting a user study in different settings, and included its negative effects in our optimization problem. Our experiments revealed that the optimization of search result presentation significantly improved that of a single ranked list, and was beneficial especially for patient users. Moreover, experimental results showed that our optimization was effective particularly when intents of a query often co-occur with a small subset of intents.</p>
<p>【Keywords】:
optimization; query suggestion; search result diversification</p>
<h3 id="16. Term-by-Term Query Auto-Completion for Mobile Search.">16. Term-by-Term Query Auto-Completion for Mobile Search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835813">Paper Link</a>】    【Pages】:143-152</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vargas:Sa=uacute=l">Saúl Vargas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mika:Peter">Peter Mika</a></p>
<p>【Abstract】:
With the ever increasing usage of mobile search, where text input is typically slow and error-prone, assisting users to formulate their queries contributes to a more satisfactory search experience. Query auto-completion (QAC) techniques, which predict possible completions for user queries, are the archetypal example of query assistance and are present in most search engines. We argue, however, that classic QAC, which operates by suggesting whole-query completions, may be sub-optimal for the case of mobile search as the available screen real estate to show suggestions is limited and editing is typically slower than in desktop search. In this paper we propose the idea of term-by-term QAC, which is a new technique inspired by predictive keyboards that suggests to the user one term at a time, instead of whole-query completions. We describe an efficient mechanism to implement this technique and an adaptation of a prior user model to evaluate the effectiveness of both standard and term-by-term QAC approaches using query log data. Our experiments with a mobile query log from a commercial search engine show the validity of our approach according to this user model with respect to saved characters, saved terms and examination effort. Finally, a user study provides further insights about our term-by-term technique compared with standard QAC with respect to the variables analyzed in the query log-based evaluation and additional variables related to the successfulness, the speed of the interactions and the properties of the submitted queries.</p>
<p>【Keywords】:
query auto completion; query logs; user models; word prediction</p>
<h3 id="17. Collaborative Denoising Auto-Encoders for Top-N Recommender Systems.">17. Collaborative Denoising Auto-Encoders for Top-N Recommender Systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835837">Paper Link</a>】    【Pages】:153-162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Yao">Yao Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/DuBois:Christopher">Christopher DuBois</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Alice_X=">Alice X. Zheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ester:Martin">Martin Ester</a></p>
<p>【Abstract】:
Most real-world recommender services measure their performance based on the top-N results shown to the end users. Thus, advances in top-N recommendation have far-ranging consequences in practical applications. In this paper, we present a novel method, called Collaborative Denoising Auto-Encoder (CDAE), for top-N recommendation that utilizes the idea of Denoising Auto-Encoders. We demonstrate that the proposed model is a generalization of several well-known collaborative filtering models but with more flexible components. Thorough experiments are conducted to understand the performance of CDAE under various component settings. Furthermore, experimental results on several public datasets demonstrate that CDAE consistently outperforms state-of-the-art top-N recommendation methods on a variety of common evaluation metrics.</p>
<p>【Keywords】:
collaborative filtering; denoising auto- encoders; recommender systems</p>
<h3 id="18. Personalized PageRank Estimation and Search: A Bidirectional Approach.">18. Personalized PageRank Estimation and Search: A Bidirectional Approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835823">Paper Link</a>】    【Pages】:163-172</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lofgren:Peter">Peter Lofgren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Banerjee:Siddhartha">Siddhartha Banerjee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goel:Ashish">Ashish Goel</a></p>
<p>【Abstract】:
We present new algorithms for Personalized PageRank estimation and Personalized PageRank search. First, for the problem of estimating Personalized PageRank (PPR) from a source distribution to a target node, we present a new bidirectional estimator with simple yet strong guarantees on correctness and performance, and 3x to 8x speedup over existing estimators in experiments on a diverse set of networks. Moreover, it has a clean algebraic structure which enables it to be used as a primitive for the Personalized PageRank Search problem: Given a network like Facebook, a query like "people named John," and a searching user, return the top nodes in the network ranked by PPR from the perspective of the searching user. Previous solutions either score all nodes or score candidate nodes one at a time, which is prohibitively slow for large candidate sets. We develop a new algorithm based on our bidirectional PPR estimator which identifies the most relevant results by sampling candidates based on their PPR; this is the first solution to PPR search that can find the best results without iterating through the set of all candidate results. Finally, by combining PPR sampling with sequential PPR estimation and Monte Carlo, we develop practical algorithms for PPR search, and we show via experiments that our algorithms are efficient on networks with billions of edges.</p>
<p>【Keywords】:
personalized pagerank; personalized search; social network analysis</p>
<h3 id="19. Your Cart tells You: Inferring Demographic Attributes from Purchase Data.">19. Your Cart tells You: Inferring Demographic Attributes from Purchase Data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835783">Paper Link</a>】    【Pages】:173-182</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Pengfei">Pengfei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lan:Yanyan">Yanyan Lan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Jun">Jun Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
Demographic attributes play an important role in retail market to characterize different types of users. Such signals however are often only available for a small fraction of users in practice due to the difficulty in manual collection process by retailers. In this paper, we aim to harness the power of big data to automatically infer users' demographic attributes based on their purchase data. Typically, demographic prediction can be formalized as a multi-task multi-class prediction problem, i.e., multiple demographic attributes (e.g., gender, age and income) are to be inferred for each user where each attribute may belong to one of N possible classes (N-2). Most previous work on this problem explores different types of features and usually predicts different attributes independently. However, modeling the tasks separately may lose the ability to leverage the correlations among different attributes. Meanwhile, manually defined features require professional knowledge and often suffer from under specification. To address these problems, we propose a novel Structured Neural Embedding (SNE) model to automatically learn the representations from users' purchase data for predicting multiple demographic attributes simultaneously. Experiments are conducted on a real-world retail dataset where five attributes (gender, marital status, income, age, and education level) are to be predicted. The empirical results show that our SNE model can improve the performance significantly compared with state-of-the-art baselines.</p>
<p>【Keywords】:
demographic attribute; multitask multi-class prediction; structured neural embedding</p>
<h3 id="20. Reducing Click and Skip Errors in Search Result Ranking.">20. Reducing Click and Skip Errors in Search Result Ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835838">Paper Link</a>】    【Pages】:183-192</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Jiepu">Jiepu Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Allan:James">James Allan</a></p>
<p>【Abstract】:
Search engines provide result summaries to help users quickly identify whether or not it is worthwhile to click on a result and read in detail. However, users may visit non-relevant results and/or skip relevant ones. These actions are usually harmful to the user experience, but few considered this problem in search result ranking. This paper optimizes relevance of results and user click and skip activities at the same time. Comparing two equally relevant results, our approach learns to rank the one that users are more likely to click on at a higher position. Similarly, it demotes non-relevant web pages with high click probabilities. Experimental results show this approach reduces about 10%-20% of the click and skip errors with a trade off of 2.1% decline in nDCG@10.</p>
<p>【Keywords】:
click; interactive search; search result ranking; web search</p>
<h3 id="21. Hierarchical Semi-supervised Classification with Incomplete Class Hierarchies.">21. Hierarchical Semi-supervised Classification with Incomplete Class Hierarchies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835810">Paper Link</a>】    【Pages】:193-202</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dalvi:Bhavana">Bhavana Dalvi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mishra:Aditya_Kumar">Aditya Kumar Mishra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a></p>
<p>【Abstract】:
In an entity classification task, topic or concept hierarchies are often incomplete. Previous work by Dalvi et al. [12] has showed that in non-hierarchical semi-supervised classification tasks, the presence of such unanticipated classes can cause semantic drift for seeded classes. The Exploratory learning [12] method was proposed to solve this problem; however it is limited to the flat classification task. This paper builds such exploratory learning methods for hierarchical classification tasks. We experimented with subsets of the NELL [8] ontology and text, and HTML table datasets derived from the ClueWeb09 corpus. Our method (OptDAC-ExploreEM) outperforms the existing Exploratory EM method, and its naive extension (DAC-ExploreEM), in terms of seed class F1 on average by 10% and 7% respectively.</p>
<p>【Keywords】:
concept discovery; hierarchical classification; ontology extension; semi-supervised learning</p>
<h2 id="Practice & Experience Track    1">Practice &amp; Experience Track    1</h2>
<h3 id="22. Is Mail The Next Frontier In Search And Data Mining?">22. Is Mail The Next Frontier In Search And Data Mining?</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835847">Paper Link</a>】    【Pages】:203</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maarek:Yoelle">Yoelle Maarek</a></p>
<p>【Abstract】:
The nature of Web mail traffic has significantly evolved in the last two decades, and consequently the behavior of Web mail users has also changed. For instance a recent study conducted by Yahoo Labs showed that today 90% of Web mail traffic is machine-generated. This partly explains why email traffic continues to grow even if a significant amount of personal communications has moved towards social media. Most users today are receiving in their inbox important invoices, receipts, and travel itineraries, together with non-malicious junk mail such as hotel newsletters or shopping promotions that could safely ignore. This is one of the reasons that a majority of messages remain unread, and many are deleted without being read. In that sense, Web mail has become quite similar to traditional snail mail. In spite of this drastic change in nature, many mail features remain unchanged. While 70% of mail users do not define even a single folder, folders are still predominant in the left trail of many Web mail clients. Mail search results are still mostly ranked by date, which makes the retrieving of older messages extremely challenging. This is even more painful to users, as unlike in Web search, they will know when a relevant previously read message has not been returned. In this talk, I present the results of multiple large-scale studies that have been conducted at Yahoo Labs in the last few years. I highlight the inherent challenges associated with such studies, especially around privacy concerns. I will discuss the new nature of consumer Web mail, which is dominated by machine-generated messages of highly heterogeneous forms and value. I will show how the change has not been fully recognized yet by my most email clients. As an example, why should there still be a reply option associated with a message coming from a "do-not-reply@" address?. I will introduce some approaches for large-scale mail mining specifically tailored to machine-generated email. I will conclude by discussing possible applications and research directions.</p>
<p>【Keywords】:
machine-generated email; mail search; web mail</p>
<h2 id="Observing Users    2">Observing Users    2</h2>
<h3 id="23. Portrait of an Online Shopper: Understanding and Predicting Consumer Behavior.">23. Portrait of an Online Shopper: Understanding and Predicting Consumer Behavior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835831">Paper Link</a>】    【Pages】:205-214</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kooti:Farshad">Farshad Kooti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lerman:Kristina">Kristina Lerman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aiello:Luca_Maria">Luca Maria Aiello</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grbovic:Mihajlo">Mihajlo Grbovic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Djuric:Nemanja">Nemanja Djuric</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Radosavljevic:Vladan">Vladan Radosavljevic</a></p>
<p>【Abstract】:
Consumer spending accounts for a large fraction of economic footprint of modern countries. Increasingly, consumer activity is moving to the web, where digital receipts of online purchases provide valuable data sources detailing consumer behavior. We consider such data extracted from emails and combined with with consumers' demographic information, which we use to characterize, model, and predict purchasing behavior. We analyze such behavior of consumers in different age and gender groups, and find interesting, actionable patterns that can be used to improve ad targeting systems. For example, we found that the amount of money spent on online purchases grows sharply with age, peaking in the late 30s, while shoppers from wealthy areas tend to purchase more expensive items and buy them more frequently. Furthermore, we look at the influence of social connections on purchasing habits, as well as at the temporal dynamics of online shopping where we discovered daily and weekly behavioral patterns. Finally, we build a model to predict when shoppers are most likely to make a purchase and how much will they spend, showing improvement over baseline approaches. The presented results paint a clear picture of a modern online shopper, and allow better understanding of consumer behavior that can help improve marketing efforts and make shopping more pleasant and efficient experience for online customers.</p>
<p>【Keywords】:
demographics; online shopping; prediction; shopping</p>
<h3 id="24. Evolution of Privacy Loss in Wikipedia.">24. Evolution of Privacy Loss in Wikipedia.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835798">Paper Link</a>】    【Pages】:215-224</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rizoiu:Marian=Andrei">Marian-Andrei Rizoiu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Lexing">Lexing Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caetano:Tiberio">Tiberio Caetano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cebri=aacute=n:Manuel">Manuel Cebrián</a></p>
<p>【Abstract】:
The cumulative effect of collective online participation has an important and adverse impact on individual privacy. As an online system evolves over time, new digital traces of individual behavior may uncover previously hidden statistical links between an individual's past actions and her private traits. To quantify this effect, we analyze the evolution of individual privacy loss by studying the edit history of Wikipedia over 13 years, including more than 117,523 different users performing 188,805,088 edits. We trace each Wikipedia's contributor using apparently harmless features, such as the number of edits performed on predefined broad categories in a given time period (e.g. Mathematics, Culture or Nature). We show that even at this unspecific level of behavior description, it is possible to use off-the-shelf machine learning algorithms to uncover usually undisclosed personal traits, such as gender, religion or education. We provide empirical evidence that the prediction accuracy for almost all private traits consistently improves over time. Surprisingly, the prediction performance for users who stopped editing after a given time still improves. The activities performed by new users seem to have contributed more to this effect than additional activities from existing (but still active) users. Insights from this work should help users, system designers, and policy makers understand and make long-term design choices in online content creation systems.</p>
<p>【Keywords】:
de-anonymization; online privacy; temporal loss of privacy</p>
<h2 id="Keynote Address    1">Keynote Address    1</h2>
<h3 id="25. Keynote Speaker Bio.">25. Keynote Speaker Bio.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835845">Paper Link</a>】    【Pages】:225</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yiling">Yiling Chen</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:</p>
<h2 id="Leveraging Users    14">Leveraging Users    14</h2>
<h3 id="26. Modeling Intransitivity in Matchup and Comparison Data.">26. Modeling Intransitivity in Matchup and Comparison Data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835787">Paper Link</a>】    【Pages】:227-236</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Shuo">Shuo Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Joachims:Thorsten">Thorsten Joachims</a></p>
<p>【Abstract】:
We present a method for learning potentially intransitive preference relations from pairwise comparison and matchup data. Unlike standard preference-learning models that represent the properties of each item/player as a single number, our method infers a multi-dimensional representation for the different aspects of each item/player's strength. We show that our model can represent any pairwise stochastic preference relation and provide a comprehensive evaluation of its predictive performance on a wide range of pairwise comparison tasks and matchup problems from online video games and sports, to peer grading and election. We find that several of these task -- especially matchups in online video games -- show substantial intransitivity that our method can model effectively.</p>
<p>【Keywords】:
games; matchup; pairwise comparison; ranking; representation learning; sports</p>
<h3 id="27. Crowdsourcing High Quality Labels with a Tight Budget.">27. Crowdsourcing High Quality Labels with a Tight Budget.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835797">Paper Link</a>】    【Pages】:237-246</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Qi">Qi Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Fenglong">Fenglong Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Jing">Jing Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Lu">Lu Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Quinn:Christopher_J=">Christopher J. Quinn</a></p>
<p>【Abstract】:
In the past decade, commercial crowdsourcing platforms have revolutionized the ways of classifying and annotating data, especially for large datasets. Obtaining labels for a single instance can be inexpensive, but for large datasets, it is important to allocate budgets wisely. With limited budgets, requesters must trade-off between the quantity of labeled instances and the quality of the final results. Existing budget allocation methods can achieve good quantity but cannot guarantee high quality of individual instances under a tight budget. However, in some scenarios, requesters may be willing to label fewer instances but of higher quality. Moreover, they may have different requirements on quality for different tasks. To address these challenges, we propose a flexible budget allocation framework called Requallo. Requallo allows requesters to set their specific requirements on the labeling quality and maximizes the number of labeled instances that achieve the quality requirement under a tight budget. The budget allocation problem is modeled as a Markov decision process and a sequential labeling policy is produced. The proposed policy greedily searches for the instance to query next as the one that can provide the maximum reward for the goal. The Requallo framework is further extended to consider worker reliability so that the budget can be better allocated. Experiments on two real-world crowdsourcing tasks as well as a simulated task demonstrate that when the budget is tight, the proposed Requallo framework outperforms existing state-of-the-art budget allocation methods from both quantity and quality aspects.</p>
<p>【Keywords】:
budget allocation; crowdsourcing</p>
<h3 id="28. Project Success Prediction in Crowdfunding Environments.">28. Project Success Prediction in Crowdfunding Environments.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835791">Paper Link</a>】    【Pages】:247-256</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yan">Yan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rakesh:Vineeth">Vineeth Rakesh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Reddy:Chandan_K=">Chandan K. Reddy</a></p>
<p>【Abstract】:
Crowdfunding has gained widespread attention in recent years. Despite the huge success of crowdfunding platforms, the percentage of projects that succeed in achieving their desired goal amount is only around 40%. Moreover, many of these crowdfunding platforms follow "all-or-nothing" policy which means the pledged amount is collected only if the goal is reached within a certain predefined time duration. Hence, estimating the probability of success for a project is one of the most important research challenges in the crowdfunding domain. To predict the project success, there is a need for new prediction models that can potentially combine the power of both classification (which incorporate both successful and failed projects) and regression (for estimating the time for success). In this paper, we formulate the project success prediction as a survival analysis problem and apply the censored regression approach where one can perform regression in the presence of partial information. We rigorously study the project success time distribution of crowdfunding data and show that the logistic and log-logistic distributions are a natural choice for learning from such data. We investigate various censored regression models using comprehensive data of 18K Kickstarter (a popular crowdfunding platform) projects and 116K corresponding tweets collected from Twitter. We show that the models that take complete advantage of both the successful and failed projects during the training phase will perform significantly better at predicting the success of future projects compared to the ones that only use the successful projects. We provide a rigorous evaluation on many sets of relevant features and show that adding few temporal features that are obtained at the project's early stages can dramatically improve the performance.</p>
<p>【Keywords】:
crowdfunding.; prediction; project success; regression; survival analysis</p>
<h3 id="29. Probabilistic Group Recommendation Model for Crowdfunding Domains.">29. Probabilistic Group Recommendation Model for Crowdfunding Domains.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835793">Paper Link</a>】    【Pages】:257-266</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rakesh:Vineeth">Vineeth Rakesh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Reddy:Chandan_K=">Chandan K. Reddy</a></p>
<p>【Abstract】:
Crowdfunding has gained a widespread popularity by fueling the creative minds of entrepreneurs. Not only has it democratized the funding of startups, it has also bridged the gap between the venture capitalists and the entrepreneurs by providing a plethora of opportunities for people seeking to invest in new business ventures. Nonetheless, despite the huge success of the crowdfunding platforms, not every project reaches its funding goal. One of the main reasons for a project's failure is the difficulty in establishing a linkage between it's founders and those investors who are interested in funding such projects. A potential solution to this problem is to develop recommendation systems that suggest suitable projects to crowdfunding investors by capturing their interests. In this paper, we explore Kickstarter, a popular reward-based crowdfunding platform. Being a highly heterogeneous platform, Kickstarter is fuelled by a dynamic community of people who constantly interact with each other before investing in projects. Therefore, the decision to invest in a project depends not only on the preference of individuals, but also on the influence of groups that a person belongs and the on-going status of the projects. In this paper, we propose a probabilistic recommendation model, called CrowdRec, that recommends Kickstarter projects to a group of investors by incorporating the on-going status of projects, the personal preference of individual members, and the collective preference of the group . Using a comprehensive dataset of over 40K crowdfunding groups and 5K projects, we show that our model is effective in recommending projects to groups of Kickstarter users.</p>
<p>【Keywords】:
crowdfunding; group recommendation; probabilistic models; recommendation; topic models</p>
<h3 id="30. Quality Management in Crowdsourcing using Gold Judges Behavior.">30. Quality Management in Crowdsourcing using Gold Judges Behavior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835835">Paper Link</a>】    【Pages】:267-276</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zitouni:Imed">Imed Zitouni</a></p>
<p>【Abstract】:
Crowdsourcing relevance labels has become an accepted practice for the evaluation of IR systems, where the task of constructing a test collection is distributed over large populations of unknown users with widely varied skills and motivations. Typical methods to check and ensure the quality of the crowd's output is to inject work tasks with known answers (gold tasks) on which workers' performance can be measured. However, gold tasks are expensive to create and have limited application. A more recent trend is to monitor the workers' interactions during a task and estimate their work quality based on their behavior. In this paper, we show that without gold behavior signals that reflect trusted interaction patterns, classifiers can perform poorly, especially for complex tasks, which can lead to high quality crowd workers getting blocked while poorly performing workers remain undetected. Through a series of crowdsourcing experiments, we compare the behaviors of trained professional judges and crowd workers and then use the trained judges' behavior signals as gold behavior to train a classifier to detect poorly performing crowd workers. Our experiments show that classification accuracy almost doubles in some tasks with the use of gold behavior data.</p>
<p>【Keywords】:
experimentation; measurement</p>
<h3 id="31. On Obtaining Effort Based Judgements for Information Retrieval.">31. On Obtaining Effort Based Judgements for Information Retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835840">Paper Link</a>】    【Pages】:277-286</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Verma:Manisha">Manisha Verma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yilmaz:Emine">Emine Yilmaz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Craswell:Nick">Nick Craswell</a></p>
<p>【Abstract】:
Document relevance has been the primary focus in the design, optimization and evaluation of retrieval systems. Traditional testcollections are constructed by asking judges the relevance grade for a document with respect to an input query. Recent work of Yilmaz et al. found an evidence that effort is another important factor in determining document utility, suggesting that more thought should be given into incorporating effort into information retrieval. However, that work did not ask judges to directly assess the level of effort required to consume a document or analyse how effort judgements relate to traditional relevance judgements. In this work, focusing on three aspects associated with effort, we show that it is possible to get judgements of effort from the assessors. We further show that given documents of the same relevance grade, effort needed to find the portion of the document relevant to the query is a significant factor in determining user satisfaction as well as user preference between these documents. Our results suggest that if the end goal is to build retrieval systems that optimize user satisfaction, effort should be included as an additional factor to relevance in building and evaluating retrieval systems. We further show that new retrieval features are needed if the goal is to build retrieval systems that jointly optimize relevance and effort and propose a set of such features. Finally, we focus on the evaluation of retrieval systems and show that incorporating effort into retrieval evaluation could lead to significant differences regarding the performance of retrieval systems.</p>
<p>【Keywords】:
effort; evaluation; information retrieval; judgements</p>
<h3 id="32. A Semantic Graph based Topic Model for Question Retrieval in Community Question Answering.">32. A Semantic Graph based Topic Model for Question Retrieval in Community Question Answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835809">Paper Link</a>】    【Pages】:287-296</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Long">Long Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Haitao">Haitao Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Fajie">Fajie Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Dell">Dell Zhang</a></p>
<p>【Abstract】:
Community Question Answering (CQA) services, such as Yahoo! Answers and WikiAnswers, have become popular with users as one of the central paradigms for satisfying users' information needs. The task of question retrieval aims to resolve one's query directly by finding the most relevant questions (together with their answers) from an archive of past questions. However, as the text of each question is short, there is usually a lexical gap between the queried question and the past questions. To alleviate this problem, we present a hybrid approach that blends several language modelling techniques for question retrieval, namely, the classic (query-likelihood) language model, the state-of-the-art translation-based language model, and our proposed semantics-based language model. The semantics of each candidate question is given by a probabilistic topic model which makes use of local and global semantic graphs for capturing the hidden interactions among entities (e.g., people, places, and concepts) in question-answer pairs. Experiments on two real-world datasets show that our approach can significantly outperform existing ones.</p>
<p>【Keywords】:
community question answering; knowledge repository; language modelling; question retrieval; topic modelling</p>
<h3 id="33. Modeling Check-in Preferences with Multidimensional Knowledge: A Minimax Entropy Approach.">33. Modeling Check-in Preferences with Multidimensional Knowledge: A Minimax Entropy Approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835839">Paper Link</a>】    【Pages】:297-306</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jingjing">Jingjing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Min">Min Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xiaolong">Xiaolong Wang</a></p>
<p>【Abstract】:
We propose a single unified minimax entropy approach for user preference modeling with multidimensional knowledge. Our approach provides a discriminative learning protocol which is able to simultaneously a) leverage explicit human knowledge, which are encoded as explicit features, and b) model the more ambiguous hidden intent, which are encoded as latent features. A latent feature can be carved by any parametric form, which allows it to accommodate arbitrary underlying assumptions. We present our approach in the scenario of check-in preference learning and demonstrate it is capable of modeling user preference in an optimized manner. Check-in preference is a fundamental component of Point-of-Interest (POI) prediction and recommendation. A user's check-in can be affected at multiple dimensions, such as the particular time, popularity of the place, his/her category and geographic preference, etc. With the geographic preferences modeled as latent features and the rest as explicit features, our approach provides an in-depth understanding of users' time-varying preferences over different POIs, as well as a reasonable representation of the hidden geographic clusters in a joint manner. Experimental results based on the task of POI prediction/recommendation with two real-world check-in datasets demonstrate that our approach can accurately model the check-in preferences and significantly outperforms the state-of-art models.</p>
<p>【Keywords】:
check-in; minimax entropy; multidimensional knowledge</p>
<h3 id="34. You've got Mail, and Here is What you Could do With It!: Analyzing and Predicting Actions on Email Messages.">34. You've got Mail, and Here is What you Could do With It!: Analyzing and Predicting Actions on Email Messages.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835811">Paper Link</a>】    【Pages】:307-316</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Castro:Dotan_Di">Dotan Di Castro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karnin:Zohar_Shay">Zohar Shay Karnin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lewin=Eytan:Liane">Liane Lewin-Eytan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maarek:Yoelle">Yoelle Maarek</a></p>
<p>【Abstract】:
With email traffic increasing, leading Web mail services have started to offer features that assist users in reading and processing their inboxes. One approach is to identify "important" messages, while a complementary one is to bundle messages, especially machine-generated ones, in pre-defined categories. We rather propose here to go back to the task at hand and consider what actions the users might conduct on received messages. We thoroughly studied, in a privacy-preserving manner, the actions of a large number of users in Yahoo mail, and found out that the most frequent actions are typically read, reply, delete and a sub-type of delete, delete-without-read. We devised a learning framework for predicting these four actions, for users with various levels of activity per action. Our framework leverages both vertical learning for personalization and horizontal learning for regularization purposes. In order to verify the quality of our predictions, we conducted a large-scale experiment involving users who had previously agreed to participate in such research studies. Our results show that, for recall values of 90%, we can predict important actions such as read or reply at precision levels up to 40% for active users, which we consider pretty encouraging for an assistance task. For less active users, we show that our regularization achieves an increase in AUC of close to 50%. To the best of our knowledge, our work is the first to provide a unified framework of this scale for predicting multiple actions on Web email, which hopefully provides a new ground for inventing new user experiences to help users process their inboxes.</p>
<p>【Keywords】:
actions; email; email message; predicting actions</p>
<h3 id="35. Hierarchical Label Propagation and Discovery for Machine Generated Email.">35. Hierarchical Label Propagation and Discovery for Machine Generated Email.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835780">Paper Link</a>】    【Pages】:317-326</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wendt:James_Bradley">James Bradley Wendt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bendersky:Michael">Michael Bendersky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pueyo:Lluis_Garcia">Lluis Garcia Pueyo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Josifovski:Vanja">Vanja Josifovski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Miklos:Balint">Balint Miklos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krka:Ivo">Ivo Krka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saikia:Amitabh">Amitabh Saikia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Jie">Jie Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cartright:Marc=Allen">Marc-Allen Cartright</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ravi:Sujith">Sujith Ravi</a></p>
<p>【Abstract】:
Machine-generated documents such as email or dynamic web pages are single instantiations of a pre-defined structural template. As such, they can be viewed as a hierarchy of template and document specific content. This hierarchical template representation has several important advantages for document clustering and classification. First, templates capture common topics among the documents, while filtering out the potentially noisy variabilities such as personal information. Second, template representations scale far better than document representations since a single template captures numerous documents. Finally, since templates group together structurally similar documents, they can propagate properties between all the documents that match the template. In this paper, we use these advantages for document classification by formulating an efficient and effective hierarchical label propagation and discovery algorithm. The labels are propagated first over a template graph (constructed based on either term-based or topic-based similarities), and then to the matching documents. We evaluate the performance of the proposed algorithm using a large donated email corpus and show that the resulting template graph is significantly more compact than the corresponding document graph and the hierarchical label propagation is both efficient and effective in increasing the coverage of the baseline document classification algorithm. We demonstrate that the template label propagation achieves more than 91% precision and 93% recall, while increasing the label coverage by more than 11%.</p>
<p>【Keywords】:
hierarchical label propagation; machine-generated email; structural template</p>
<h3 id="36. Enforcing k-anonymity in Web Mail Auditing.">36. Enforcing k-anonymity in Web Mail Auditing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835803">Paper Link</a>】    【Pages】:327-336</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Castro:Dotan_Di">Dotan Di Castro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lewin=Eytan:Liane">Liane Lewin-Eytan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maarek:Yoelle">Yoelle Maarek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wolff:Ran">Ran Wolff</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zohar:Eyal">Eyal Zohar</a></p>
<p>【Abstract】:
We study the problem of k-anonymization of mail messages in the realistic scenario of auditing mail traffic in a major commercial Web mail service. Mail auditing is necessary in various Web mail debugging and quality assurance activities, such as anti-spam or the qualitative evaluation of novel mail features. It is conducted by trained professionals, often referred to as "auditors", who are shown messages that could expose personally identifiable information. We address here the challenge of k-anonymizing such messages, focusing on machine generated mail messages that represent more than 90% of today's mail traffic. We introduce a novel message signature Mail-Hash, specifically tailored to identifying structurally-similar messages, which allows us to put such messages in a same equivalence class. We then define a process that generates, for each class, masked mail samples that can be shown to auditors, while guaranteeing the k-anonymity of users. The productivity of auditors is measured by the amount of non-hidden mail content they can see every day, while considering normal working conditions, which set a limit to the number of mail samples they can review. In addition, we consider k-anonymity over time since, by definition of k-anonymity, every new release places additional constraints on the assignment of samples. We describe in details the results we obtained over actual Yahoo mail traffic, and thus demonstrate that our methods are feasible at Web mail scale. Given the constantly growing concern of users over their email being scanned by others, we argue that it is critical to devise such algorithms that guarantee k-anonymity, and implement associated processes in order to restore the trust of mail users.</p>
<p>【Keywords】:
k-anonymization; mail auditing; mail templating; quality assurance</p>
<h3 id="37. An Information-Theoretic Approach to Individual Sequential Data Sanitization.">37. An Information-Theoretic Approach to Individual Sequential Data Sanitization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835828">Paper Link</a>】    【Pages】:337-346</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bonomi:Luca">Luca Bonomi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Liyue">Liyue Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Hongxia">Hongxia Jin</a></p>
<p>【Abstract】:
Fine-grained, personal data has been largely, continuously generated nowadays, such as location check-ins, web histories, physical activities, etc. Those data sequences are typically shared with untrusted parties for data analysis and promotional services. However, the individually-generated sequential data contains behavior patterns and may disclose sensitive information if not properly sanitized. Furthermore, the utility of the released sequence can be adversely affected by sanitization techniques. In this paper, we study the problem of individual sequence data sanitization with minimum utility loss, given user-specified sensitive patterns. We propose a privacy notion based on information theory and sanitize sequence data via generalization. We show the optimization problem is hard and develop two efficient heuristic solutions. Extensive experimental evaluations are conducted on real-world datasets and the results demonstrate the efficiency and effectiveness of our solutions.</p>
<p>【Keywords】:
data sanitization; mutual information; sequential patterns</p>
<h3 id="38. Improving IP Geolocation using Query Logs.">38. Improving IP Geolocation using Query Logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835820">Paper Link</a>】    【Pages】:347-356</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dan:Ovidiu">Ovidiu Dan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Parikh:Vaibhav">Vaibhav Parikh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davison_0001:Brian_D=">Brian D. Davison</a></p>
<p>【Abstract】:
IP geolocation databases map IP addresses to their geographical locations. These databases are important for several applications such as local search engine relevance, credit card fraud protection, geotargetted advertising, and online content delivery. While they are the most popular method of geolocation, they can have low accuracy at the city level. In this paper we evaluate and improve IP geolocation databases using data collected from search engine logs. We generate a large ground-truth dataset using real time global positioning data extracted from search engine logs. We show that incorrect geolocation information can have a negative impact on implicit user metrics. Using the dataset we measure the accuracy of three state-of-the-art commercial IP geolocation databases. We then introduce a technique to improve existing geolocation databases by mining explicit locations from query logs. We show significant accuracy gains in 44 to 49 out of the top 50 countries, depending on the IP geolocation database. Finally, we validate the approach with a large scale A/B experiment that shows improvements in several user metrics.</p>
<p>【Keywords】:
contextual relevance; geographic personalization; geographic targeting; geotargeting; ip geolocation; local search</p>
<h3 id="39. Geographic Segmentation via Latent Poisson Factor Model.">39. Geographic Segmentation via Latent Poisson Factor Model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835806">Paper Link</a>】    【Pages】:357-366</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Rose">Rose Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gelfand:Andrew">Andrew Gelfand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rajan:Suju">Suju Rajan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shahabi:Cyrus">Cyrus Shahabi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yan">Yan Liu</a></p>
<p>【Abstract】:
Discovering latent structures in spatial data is of critical importance to understanding the user behavior of location-based services. In this paper, we study the problem of geographic segmentation of spatial data, which involves dividing a collection of observations into distinct geo-spatial regions and uncovering abstract correlation structures in the data. We introduce a novel, Latent Poisson Factor (LPF) model to describe spatial count data. The model describes the spatial counts as a Poisson distribution with a mean that factors over a joint item-location latent space. The latent factors are constrained with weak labels to help uncover interesting spatial dependencies. We study the LPF model on a mobile app usage data set and a news article readership data set. We empirically demonstrate its effectiveness on a variety of prediction tasks on these two data sets.</p>
<p>【Keywords】:
geographic segmentation; mobile app usage; spatial data</p>
<h2 id="Big Data Algorithms    10">Big Data Algorithms    10</h2>
<h3 id="40. Scaling up Link Prediction with Ensembles.">40. Scaling up Link Prediction with Ensembles.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835815">Paper Link</a>】    【Pages】:367-376</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Duan:Liang">Liang Duan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Charu">Charu Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Shuai">Shuai Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Renjun">Renjun Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huai:Jinpeng">Jinpeng Huai</a></p>
<p>【Abstract】:
A network with $n$ nodes contains O(n2) possible links. Even for networks of modest size, it is often difficult to evaluate all pairwise possibilities for links in a meaningful way. Furthermore, even though link prediction is closely related to missing value estimation problems, such as collaborative filtering, it is often difficult to use sophisticated models such as latent factor methods because of their computational complexity over very large networks. Due to this computational complexity, most known link prediction methods are designed for evaluating the link propensity over a specified subset of links, rather than for performing a global search over the entire networks. In practice, however, it is essential to perform an exhaustive search over the entire networks. In this paper, we propose an ensemble enabled approach to scaling up link prediction, which is able to decompose traditional link prediction problems into subproblems of smaller size. These subproblems are each solved with the use of latent factor models, which can be effectively implemented over networks of modest size. Furthermore, the ensemble enabled approach has several advantages in terms of performance. We show the advantage of using ensemble-based latent factor models with experiments on very large networks. Experimental results demonstrate the effectiveness and scalability of our approach.</p>
<p>【Keywords】:
big data; ensembles; link prediction; networks</p>
<h3 id="41. DiFacto: Distributed Factorization Machines.">41. DiFacto: Distributed Factorization Machines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835781">Paper Link</a>】    【Pages】:377-386</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Mu">Mu Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Ziqi">Ziqi Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yu=Xiang">Yu-Xiang Wang</a></p>
<p>【Abstract】:
Factorization Machines offer good performance and useful embeddings of data. However, they are costly to scale to large amounts of data and large numbers of features. In this paper we describe DiFacto, which uses a refined Factorization Machine model with sparse memory adaptive constraints and frequency adaptive regularization. We show how to distribute DiFacto over multiple machines using the Parameter Server framework by computing distributed subgradients on minibatches asynchronously. We analyze its convergence and demonstrate its efficiency in computational advertising datasets with billions examples and features.</p>
<p>【Keywords】:
distributed systems; factorization machines; optimization; recommender systems; statistics</p>
<h3 id="42. Distributed Balanced Partitioning via Linear Embedding.">42. Distributed Balanced Partitioning via Linear Embedding.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835829">Paper Link</a>】    【Pages】:387-396</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Aydin:Kevin">Kevin Aydin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bateni:MohammadHossein">MohammadHossein Bateni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mirrokni:Vahab_S=">Vahab S. Mirrokni</a></p>
<p>【Abstract】:
Balanced partitioning is often a crucial first step in solving large-scale graph optimization problems: in some cases, a big graph is chopped into pieces that fit on one machine to be processed independently before stitching the results together, leading to certain suboptimality from the interaction among different pieces. In other cases, links between different parts may show up in the running time and/or network communications cost, hence the desire to have small cut size. We study a distributed balanced partitioning problem where the goal is to partition the vertices of a given graph into k pieces, minimizing the total cut size. Our algorithm is composed of a few steps that are easily implementable in distributed computation frameworks, e.g., MapReduce. The algorithm first embeds nodes of the graph onto a line, and then processes nodes in a distributed manner guided by the linear embedding order. We examine various ways to find the first embedding, e.g., via a hierarchical clustering or Hilbert curves. Then we apply four different techniques such as local swaps, minimum cuts on partition boundaries, as well as contraction and dynamic programming. Our empirical study compares the above techniques with each other, and to previous work in distributed algorithms, e.g., a label propagation method, FENNEL and Spinner. We report our results both on a private map graph and several public social networks, and show that our results beat previous distributed algorithms: we notice, e.g., 15-25% reduction in cut size over [UB13]. We also observe that our algorithms allow for scalable distributed implementation for any number of partitions. Finally, we apply our techniques for the Google Maps Driving Directions to minimize the number of multi-shard queries with the goal of saving in CPU usage. During live experiments, we observe an ≈ 40% drop in the number of multi-shard queries when comparing our method with a standard geography-based method.</p>
<p>【Keywords】:
cut minimization; embedding to line; imbalance; local improvement; mapreduce; maps; partitioning; social networks</p>
<h3 id="43. Kangaroo: Workload-Aware Processing of Range Data and Range Queries in Hadoop.">43. Kangaroo: Workload-Aware Processing of Range Data and Range Queries in Hadoop.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835841">Paper Link</a>】    【Pages】:397-406</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Aly:Ahmed_M=">Ahmed M. Aly</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elmeleegy:Hazem">Hazem Elmeleegy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qi_0002:Yan">Yan Qi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aref:Walid_G=">Walid G. Aref</a></p>
<p>【Abstract】:
Despite the importance and widespread use of range data, e.g., time intervals, spatial ranges, etc., little attention has been devoted to study the processing and querying of range data in the context of big data. The main challenge relies in the nature of the traditional index structures e.g., B-Tree and R-Tree, being centralized by nature, and hence are almost crippled when deployed in a distributed environment. To address this challenge, this paper presents Kangaroo, a system built on top of Hadoop to optimize the execution of range queries over range data. The main idea behind Kangaroo is to split the data into non-overlapping partitions in a way that minimizes the query execution time. Kangaroo is query workload-aware, i.e., results in partitioning layouts that minimize the query processing time of given query patterns. In this paper, we study the design challenges Kangaroo addresses in order to be deployed on top of a distributed file system, i.e., HDFS. We also study four different partitioning schemes that Kangaroo can support. With extensive experiments using real range data of more than one billion records and real query workload of more than 30,000 queries, we show that the partitioning schemes of Kangaroo can significantly reduce the I/O of range queries on range data.</p>
<p>【Keywords】:
big data; hadoop; indexing; query processing and optimization</p>
<h3 id="44. Feedback Control of Real-Time Display Advertising.">44. Feedback Control of Real-Time Display Advertising.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835843">Paper Link</a>】    【Pages】:407-416</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Weinan">Weinan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rong:Yifei">Yifei Rong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jun">Jun Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Tianchi">Tianchi Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xiaofan">Xiaofan Wang</a></p>
<p>【Abstract】:
Real-Time Bidding (RTB) is revolutionising display advertising by facilitating per-impression auctions to buy ad impressions as they are being generated. Being able to use impression-level data, such as user cookies, encourages user behaviour targeting, and hence has significantly improved the effectiveness of ad campaigns. However, a fundamental drawback of RTB is its instability because the bid decision is made per impression and there are enormous fluctuations in campaigns' key performance indicators (KPIs). As such, advertisers face great difficulty in controlling their campaign performance against the associated costs. In this paper, we propose a feedback control mechanism for RTB which helps advertisers dynamically adjust the bids to effectively control the KPIs, e.g., the auction winning ratio and the effective cost per click. We further formulate an optimisation framework to show that the proposed feedback control mechanism also has the ability of optimising campaign performance. By settling the effective cost per click at an optimal reference value, the number of campaign's ad clicks can be maximised with the budget constraint. Our empirical study based on real-world data verifies the effectiveness and robustness of our RTB control system in various situations. The proposed feedback control mechanism has also been deployed on a commercial RTB platform and the online test has shown its success in generating controllable advertising performance.</p>
<p>【Keywords】:
demand-side platform; display advertising; feedback control; real-time bidding</p>
<h3 id="45. Multi-Score Position Auctions.">45. Multi-Score Position Auctions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835822">Paper Link</a>】    【Pages】:417-425</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Charles:Denis_Xavier">Denis Xavier Charles</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Devanur:Nikhil_R=">Nikhil R. Devanur</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sivan:Balasubramanian">Balasubramanian Sivan</a></p>
<p>【Abstract】:
In this paper we propose a general family of position auctions used in paid search, which we call multi-score position auctions. These auctions contain the GSP auction and the GSP auction with squashing as special cases. We show experimentally that these auctions contain special cases that perform better than the GSP auction with squashing, in terms of revenue, and the number of clicks on ads. In particular, we study in detail the special case that squashes the first slot alone and show that this beats pure squashing (which squashes all slots uniformly). We study the equilibria that arise in this special case to examine both the first order and the second order effect of moving from the squashing-all-slots auction to the squash-only-the-top-slot auction. For studying the second order effect, we simulate auctions using the value-relevance correlated distribution suggested in Lahaie and Pennock [2007]. Since this distribution is derived from a study of value and relevance distributions in Yahoo! we believe the insights derived from this simulation to be valuable. For measuring the first order effect, in addition to the said simulation, we also conduct experiments using auction data from Bing over several weeks that includes a random sample of all auctions.</p>
<p>【Keywords】:
multi-score position auctions; squashing</p>
<h3 id="46. Multi-view Machines.">46. Multi-view Machines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835777">Paper Link</a>】    【Pages】:427-436</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Bokai">Bokai Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Hucheng">Hucheng Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoqiang">Guoqiang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Philip_S=">Philip S. Yu</a></p>
<p>【Abstract】:
With rapidly growing amount of data available on the web, it becomes increasingly likely to obtain data from different perspectives for multi-view learning. Some successive examples of web applications include recommendation and target advertising. Specifically, to predict whether a user will click an ad in a query context, there are available features extracted from user profile, ad information and query description, and each of them can only capture part of the task signals from a particular aspect/view. Different views provide complementary information to learn a practical model for these applications. Therefore, an effective integration of the multi-view information is critical to facilitate the learning performance. In this paper, we propose a general predictor, named multi-view machines (MVMs), that can effectively explore the full-order interactions between features from multiple views. A joint factorization is applied for the interaction parameters which makes parameter estimation more accurate under sparsity and renders the model with the capacity to avoid overfitting. Moreover, MVMs can work in conjunction with different loss functions for a variety of machine learning tasks. The advantages of MVMs are illustrated through comparison with other methods for multi-view prediction, including support vector machines (SVMs), support tensor machines (STMs) and factorization machines (FMs). A stochastic gradient descent method and a distributed implementation on Spark are presented to learn the MVM model. Through empirical studies on two real-world web application datasets, we demonstrate the effectiveness of MVMs on modeling feature interactions in multi-view data. A 3.51\% accuracy improvement is shown on MVMs over FMs for the problem of movie rating prediction, and 0.57\% for ad click prediction.</p>
<p>【Keywords】:
factorization; feature interaction; multi-view learning</p>
<h3 id="47. Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization.">47. Transductive Classification on Heterogeneous Information Networks with Edge Betweenness-based Normalization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835799">Paper Link</a>】    【Pages】:437-446</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bangcharoensap:Phiradet">Phiradet Bangcharoensap</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Murata:Tsuyoshi">Tsuyoshi Murata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kobayashi:Hayato">Hayato Kobayashi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shimizu:Nobuyuki">Nobuyuki Shimizu</a></p>
<p>【Abstract】:
This paper proposes a novel method for transductive classification on heterogeneous information networks composed of multiple types of vertices. Such networks naturally represent many real-world Web data such as DBLP data (author, paper, and conference). Given a network where some vertices are labeled, the classifier aims to predict labels for the remaining vertices by propagating the labels to the entire network. In the label propagation process, many studies reduce the importance of edges connecting to a high-degree vertex. The assumption is unsatisfactory when reliability of a label of a vertex cannot be implied from its degree. On the basis of our intuition that edges bridging across communities are less trustworthy, we adapt edge betweenness to imply the importance of edges. Since directly applying the conventional edge betweenness is inefficient on heterogeneous networks, we propose two additional refinements. First, the centrality utilizes the fact that networks contain multiple types of vertices. Second, the centrality ignores flows originating from endpoints of considering edges. The experimental results on real-world datasets show our proposed method is more effective than a state-of-the-art method, GNetMine. On average, our method yields 92.79 ± 1.25% accuracy on a DBLP network even if only 1.92% of vertices are labeled. Our simple weighting scheme results in more than 5 percentage points increase in accuracy compared with GNetMine.</p>
<p>【Keywords】:
edge betweenness centrality; heterogeneous information network; transductive classification</p>
<h3 id="48. The Troll-Trust Model for Ranking in Signed Networks.">48. The Troll-Trust Model for Ranking in Signed Networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835816">Paper Link</a>】    【Pages】:447-456</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Zhaoming">Zhaoming Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Charu_C=">Charu C. Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Jimeng">Jimeng Sun</a></p>
<p>【Abstract】:
Signed social networks have become increasingly important in recent years because of the ability to model trust-based relationships in review sites like Slashdot, Epinions, and Wikipedia. As a result, many traditional network mining problems have been re-visited in the context of networks in which signs are associated with the links. Examples of such problems include community detection, link prediction, and low rank approximation. In this paper, we will examine the problem of ranking nodes in signed networks. In particular, we will design a ranking model, which has a clear physical interpretation in terms of the sign of the edges in the network. Specifically, we propose the Troll-Trust model that models the probability of trustworthiness of individual data sources as an interpretation for the underlying ranking values. We will show the advantages of this approach over a variety of baselines.</p>
<p>【Keywords】:
data mining; ranking; signed networks</p>
<h3 id="49. Multileave Gradient Descent for Fast Online Learning to Rank.">49. Multileave Gradient Descent for Fast Online Learning to Rank.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835804">Paper Link</a>】    【Pages】:457-466</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Schuth:Anne">Anne Schuth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Oosterhuis:Harrie">Harrie Oosterhuis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Whiteson:Shimon">Shimon Whiteson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
Modern search systems are based on dozens or even hundreds of ranking features. The dueling bandit gradient descent (DBGD) algorithm has been shown to effectively learn combinations of these features solely from user interactions. DBGD explores the search space by comparing a possibly improved ranker to the current production ranker. To this end, it uses interleaved comparison methods, which can infer with high sensitivity a preference between two rankings based only on interaction data. A limiting factor is that it can compare only to a single exploratory ranker. We propose an online learning to rank algorithm called multileave gradient descent (MGD) that extends DBGD to learn from so-called multileaved comparison methods that can compare a set of rankings instead of merely a pair. We show experimentally that MGD allows for better selection of candidates than DBGD without the need for more comparisons involving users. An important implication of our results is that orders of magnitude less user interaction data is required to find good rankers when multileaved comparisons are used within online learning to rank. Hence, fewer users need to be exposed to possibly inferior rankers and our method allows search engines to adapt more quickly to changes in user preferences.</p>
<p>【Keywords】:
information retrieval; interleaved comparisons; learning to rank; multileaved comparisons</p>
<h2 id="Practice & Experience Track    2">Practice &amp; Experience Track    2</h2>
<h3 id="50. AMiner: Toward Understanding Big Scholar Data.">50. AMiner: Toward Understanding Big Scholar Data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835849">Paper Link</a>】    【Pages】:467</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a></p>
<p>【Abstract】:
In this talk, I will present a novel academic search and mining system, AMiner, the second generation of the ArnetMiner system. Different from traditional academic search systems that focus on document (paper) search, AMiner aims to provide a systematic modeling approach for researchers (authors), ultimately to gain a deep understanding of the big (heterogeneous) network formed by authors, papers they have published, and venues they published those papers. The system extracts researchers' profiles automatically from the Web and integrates the researcher profiles with publication papers after name disambiguation. For now, the system has collected a big scholar data with more than 130,000,000 researcher profiles and 100,000,000 papers from multiple publication databases. We also developed an approach named COSNET to connect AMiner with several professional social networks such as LinkedIn and VideoLectures, which significantly enriches the metadata of the scholarly data. Based on the integrated big scholar data, we devise a unified topic modeling approach for modeling the different entities (authors, papers, venues) simultaneously and provide a topic-level expertise search by leveraging the modeling results. In addition, AMiner offers a set of researcher-centered functions including social influence analysis, influence visualization, collaboration recommendation, relationship mining, similarity analysis and community evolution. The system has been put into operation since 2006 and has attracted more than 7,000,000 independent IP accesses from over 200 countries/regions.</p>
<p>【Keywords】:
academic search; community evolution; integration; recommendation; social influence</p>
<h3 id="51. Serving a Billion Personalized News Feeds.">51. Serving a Billion Personalized News Feeds.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835848">Paper Link</a>】    【Pages】:469</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Backstrom:Lars">Lars Backstrom</a></p>
<p>【Abstract】:
Feed ranking's goal is to provide perople with over a billion personalized experiences. We strive to provide the most compelling content to each person, personalized to them so that they are most likely to see the content that is most interesting to them. Similar to a newspaper, putting the right stories above the fold has always been critical to engaging customers and interesting them in the rest of the paper. In feed ranking, we face a similar challenge, but on a grander scale. Each time a person visits, we need to find the best piece of content out of all the available stories and put it at the top of feed where people are most likely to see it. To accomplish this, we do large-scale machine learning to model each person, figure out which friends, pages and topics they care about and pick the stories each particular person is interested in. In addition to the large-scale machine learning problems we work on, another primary area of research is understanding the value we are creating for people and making sure that our objective function is in alignment with what people want.</p>
<p>【Keywords】:
facebook; machine learning; news feed; social networks</p>
<h2 id="Keynote Address    1">Keynote Address    1</h2>
<h3 id="52. The Predictive Power of Massive Data about our Fine-Grained Behavior.">52. The Predictive Power of Massive Data about our Fine-Grained Behavior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835846">Paper Link</a>】    【Pages】:471-472</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Provost:Foster_J=">Foster J. Provost</a></p>
<p>【Abstract】:
What really is it about "big data" that makes it different from traditional data? In this talk I illustrate one important aspect: massive ultra-fine-grained data on individuals' behaviors holds remarkable predictive power. I examine several applications to marketing-related tasks, showing how machine learning methods can extract the predictive power and how the value of the data "asset" seems different from the value of traditional data used for predictive modeling. I then dig deeper into explaining the predictions made from massive numbers of fine-grained behaviors by applying a counter-factual framework for explaining model behavior based on treating the individual behaviors as evidence that is combined by the model. This analysis shows that the fine-grained behavior data incorporate various sorts of information that we traditionally have sought to capture by other means. For example, for marketing modeling the behavior data effectively incorporate demographics, psychographics, category interest, and purchase intent. Finally, I discuss the flip side of the coin: the remarkable predictive power based on fine-grained information on individuals raises new privacy concerns. In particular, I discuss privacy concerns based on inferences drawn about us (in contrast to privacy concerns stemming from violations to data confidentiality). The evidence counterfactual approach used to explain the predictions also can be used to provide online consumers with transparency into the reasons why inferences are drawn about them. In addition, it offers the possibility to design novel solutions such as a privacy-friendly "cloaking device" to inhibit inferences from being drawn based on particular behaviors.</p>
<p>【Keywords】:
keynote talk</p>
<h2 id="Social Networks    12">Social Networks    12</h2>
<h3 id="53. Information Evolution in Social Networks.">53. Information Evolution in Social Networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835827">Paper Link</a>】    【Pages】:473-482</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Adamic:Lada_A=">Lada A. Adamic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lento:Thomas_M=">Thomas M. Lento</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Adar:Eytan">Eytan Adar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Pauline_C=">Pauline C. Ng</a></p>
<p>【Abstract】:
Social networks readily transmit information, albeit with less than perfect fidelity. We present a large-scale measurement of this imperfect information copying mechanism by examining the dissemination and evolution of thousands of memes, collectively replicated hundreds of millions of times in the online social network Facebook. The information undergoes an evolutionary process that exhibits several regularities. A meme's mutation rate characterizes the population distribution of its variants, in accordance with the Yule process. Variants further apart in the diffusion cascade have greater edit distance, as would be expected in an iterative, imperfect replication process. Some text sequences can confer a replicative advantage; these sequences are abundant and transfer "laterally" between different memes. Subpopulations of the social network can preferentially transmit a specific variant of a meme if the variant matches their beliefs or culture. Understanding the mechanism driving change in diffusing information has important implications for how we interpret and harness the information that reaches us through our social networks.</p>
<p>【Keywords】:
evolution; memes; social computing; social networks</p>
<h3 id="54. Nonlinear Laplacian for Digraphs and its Applications to Network Analysis.">54. Nonlinear Laplacian for Digraphs and its Applications to Network Analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835785">Paper Link</a>】    【Pages】:483-492</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yoshida:Yuichi">Yuichi Yoshida</a></p>
<p>【Abstract】:
In this work, we introduce a new Markov operator associated with a digraph, which we refer to as a nonlinear Laplacian. Unlike previous Laplacians for digraphs, the nonlinear Laplacian does not rely on the stationary distribution of the random walk process and is well defined on digraphs that are not strongly connected. We show that the nonlinear Laplacian has nontrivial eigenvalues and give a Cheeger-like inequality, which relates the conductance of a digraph and the smallest non-zero eigenvalue of its nonlinear Laplacian. Finally, we apply the nonlinear Laplacian to the analysis of real-world networks and obtain encouraging results.</p>
<p>【Keywords】:
spectral graph theory</p>
<h3 id="55. Querying and Tracking Influencers in Social Streams.">55. Querying and Tracking Influencers in Social Streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835788">Paper Link</a>】    【Pages】:493-502</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Subbian:Karthik">Karthik Subbian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Charu_C=">Charu C. Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srivastava:Jaideep">Jaideep Srivastava</a></p>
<p>【Abstract】:
Influence analysis is an important problem in social network analysis due to its impact on viral marketing and targeted advertisements. Most of the existing influence analysis methods determine the influencers in a static network with an influence propagation model based on pre-defined edge propagation probabilities. However, none of these models can be queried to find influencers in both context and time-sensitive fashion from a streaming social data. In this paper, we propose an approach to maintain real-time influence scores of users in a social stream using a topic and time-sensitive approach, while the network and topic is constantly evolving over time. We show that our approach is efficient in terms of online maintenance and effective in terms various types of real-time context- and time-sensitive queries. We evaluate our results on both social and collaborative network data sets.</p>
<p>【Keywords】:
influence analysis; influence maximization; querying influencer; social streams; tracking influencer</p>
<h3 id="56. Centrality-Aware Link Recommendations.">56. Centrality-Aware Link Recommendations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835818">Paper Link</a>】    【Pages】:503-512</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Parotsidis:Nikos">Nikos Parotsidis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pitoura:Evaggelia">Evaggelia Pitoura</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsaparas:Panayiotis">Panayiotis Tsaparas</a></p>
<p>【Abstract】:
Link recommendations are critical for both improving the utility and expediting the growth of social networks. Most previous approaches focus on suggesting links that are highly likely to be adopted. In this paper, we add a different perspective to the problem by aiming at recommending links that also improve specific properties of the network. In particular, our goal is to recommend to users links that if adopted would improve their centrality in the network. Specifically, we introduce the centrality-aware link recommendation problem as the problem of recommending to a user u, k links from a pool of recommended links so as to maximize the expected decrease of the sum of the shortest path distances of $u$ to all other nodes in the network. We show that the problem is NP-hard, but our optimization function is monotone and sub-modular which guarantees a constant approximation ratio for the greedy algorithm. We present a fast algorithm for computing the expected decrease caused by a set of recommendations which we use as a building block in our algorithms. We provide experimental results that evaluate the performance of our algorithms with respect to both the accuracy of the prediction and the improvement in the centrality of the nodes, and we study the tradeoff between the two.</p>
<p>【Keywords】:
link recommendations; node centrality; probabilistic networks; social networks</p>
<h3 id="57. Relational Learning with Social Status Analysis.">57. Relational Learning with Social Status Analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835782">Paper Link</a>】    【Pages】:513-522</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Liang">Liang Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xia">Xia Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Huan">Huan Liu</a></p>
<p>【Abstract】:
Relational learning has been proposed to cope with the interdependency among linked instances in social network analysis, which often adopts network connectivity and social media content for prediction. A common assumption in existing relational learning methods is that data instances are equally important. The algorithms developed based on the assumption may be significantly affected by outlier data and thus less robust. In the meantime, it has been well established in social sciences that actors are naturally of different social status in a social network. Motivated by findings from social sciences, in this paper, we investigate whether social status analysis could facilitate relational learning. Particularly, we propose a novel framework RESA to model social status using the network structure. It extracts robust and intrinsic latent social dimensions for social actors, which are further exploited as features for supervised learning models. The proposed method is applicable for real-world relational learning problems where noise exists. Extensive experiments are conducted on datasets obtained from real-world social media platforms. Empirical results demonstrate the effectiveness of RESA and further experiments are conducted to help understand the effects of parameter settings to the proposed model and how local social status works.</p>
<p>【Keywords】:
relational learning; social dimensions; social media</p>
<h3 id="58. Equality and Social Mobility in Twitter Discussion Groups.">58. Equality and Social Mobility in Twitter Discussion Groups.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835814">Paper Link</a>】    【Pages】:523-532</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Ellis:Katherine">Katherine Ellis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goldszmidt:Mois=eacute=s">Moisés Goldszmidt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lanckriet:Gert_R=_G=">Gert R. G. Lanckriet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mishra:Nina">Nina Mishra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Reingold:Omer">Omer Reingold</a></p>
<p>【Abstract】:
Online groups, including chat groups and forums, are becoming important avenues for gathering and exchanging information ranging from troubleshooting devices, to sharing experiences, to finding medical information and advice. Thus, issues about the health and stability of these groups are of particular interest to both industry and academia. In this paper we conduct a large scale study with the objectives of first, characterizing essential aspects of the interactions between the participants of such groups and second, characterizing how the nature of these interactions relate to the health of the groups. Specifically, we concentrate on Twitter Discussion Groups (TDGs), self-organized groups that meet on Twitter by agreeing on a hashtag, date and time. These groups have repeated, real-time meetings and are a rising phenomenon on Twitter. We examine the interactions in these groups in terms of the social equality and mobility of the exchange of attention between participants, according to the @mention convention on Twitter. We estimate the health of a group by measuring the retention rate of participants and the change in the number of meetings over time. We find that social equality and mobility are correlated, and that equality and mobility are related to a group's health. In fact, equality and mobility are as predictive of a group's health as some prior characteristics used to predict health of other online groups. Our findings are based on studying 100 thousand sessions of over two thousand discussion groups over the period of June 2012 to June 2013. These finding are not only relevant to stakeholders interested in maintaining these groups, but to researchers and academics interested in understanding the behavior of participants in online discussions. We also find the parallel with findings on the relationship between economic mobility and equality and health indicators in real-world nations striking and thought-provoking.</p>
<p>【Keywords】:
generative model; graph analysis; social networks</p>
<h3 id="59. Learning Distributed Representations of Data in Community Question Answering for Question Retrieval.">59. Learning Distributed Representations of Data in Community Question Answering for Question Retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835786">Paper Link</a>】    【Pages】:533-542</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Kai">Kai Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Wei">Wei Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Fang">Fang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Zhoujun">Zhoujun Li</a></p>
<p>【Abstract】:
We study the problem of question retrieval in community question answering (CQA). The biggest challenge within this task is lexical gaps between questions since similar questions are usually expressed with different but semantically related words. To bridge the gaps, state-of-the-art methods incorporate extra information such as word-to-word translation and categories of questions into the traditional language models. We find that the existing language model based methods can be interpreted using a new framework, that is they represent words and question categories in a vector space and calculate question-question similarities with a linear combination of dot products of the vectors. The problem is that these methods are either heuristic on data representation or difficult to scale up. We propose a principled and efficient approach to learning representations of data in CQA. In our method, we simultaneously learn vectors of words and vectors of question categories by optimizing an objective function naturally derived from the framework. In question retrieval, we incorporate learnt representations into traditional language models in an effective and efficient way. We conduct experiments on large scale data from Yahoo! Answers and Baidu Knows, and compared our method with state-of-the-art methods on two public data sets. Experimental results show that our method can significantly improve on baseline methods for retrieval relevance. On 1 million training data, our method takes less than 50 minutes to learn a model on a single multicore machine, while the translation based language model needs more than 2 days to learn a translation table on the same machine.</p>
<p>【Keywords】:
community question answering; question retrieval; unsupervised model; word vector representation</p>
<h3 id="60. Inferring Latent Triggers of Purchases with Consideration of Social Effects and Media Advertisements.">60. Inferring Latent Triggers of Purchases with Consideration of Social Effects and Media Advertisements.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835789">Paper Link</a>】    【Pages】:543-552</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tanaka:Yusuke">Yusuke Tanaka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurashima:Takeshi">Takeshi Kurashima</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fujiwara:Yasuhiro">Yasuhiro Fujiwara</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iwata:Tomoharu">Tomoharu Iwata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sawada:Hiroshi">Hiroshi Sawada</a></p>
<p>【Abstract】:
This paper proposes a method for inferring from single-source data the factors that trigger purchases. Here, single-source data are the histories of item purchases and media advertisement views for each individual. We assume a sequence of purchase events to be a stochastic process incorporating the following three factors: (a) user preference, (b) social effects received from other users, and (c) media advertising effects. As our user-purchase model incorporates the latent relationships between users and advertisers, it can infer the latent triggers of purchases. Experiments on real single-source data show that our model can (a) achieve high prediction accuracy for purchases, (b) discover the key information, i.e., popular items, influential users, and influential advertisers, (c) estimate the relative impact of the three factors on purchases, and (d) find user segments according to the estimated factors.</p>
<p>【Keywords】:
marked point processes; purchase behavior; single-source data</p>
<h3 id="61. Towards Modelling Language Innovation Acceptance in Online Social Networks.">61. Towards Modelling Language Innovation Acceptance in Online Social Networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835784">Paper Link</a>】    【Pages】:553-562</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kershaw:Daniel">Daniel Kershaw</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rowe:Matthew">Matthew Rowe</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Stacey:Patrick">Patrick Stacey</a></p>
<p>【Abstract】:
Language change and innovation is constant in online and offline communication, and has led to new words entering people's lexicon and even entering modern day dictionaries, with recent additions of 'e-cig' and 'vape'. However the manual work required to identify these 'innovations' is both time consuming and subjective. In this work we demonstrate how such innovations in language can be identified across two different OSN's (Online Social Networks) through the operationalisation of known language acceptance models that incorporate relatively simple statistical tests. From grounding our work in language theory, we identified three statistical tests that can be applied - variation in; frequency, form and meaning. Each show different success rates across the two networks (Geo-bound Twitter sample and a sample of Reddit). These tests were also applied to different community levels within the two networks allowing for different innovations to be identified across different community structures over the two networks, for instance: identifying regional variation across Twitter, and variation across groupings of Subreddits, where identified example innovations included 'casualidad' and 'cym'.</p>
<p>【Keywords】:
language change; language evolution; osn; reddit; twitter</p>
<h3 id="62. Discriminative Learning of Infection Models.">62. Discriminative Learning of Infection Models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835802">Paper Link</a>】    【Pages】:563-572</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rosenfeld:Nir">Nir Rosenfeld</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nitzan:Mor">Mor Nitzan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Globerson:Amir">Amir Globerson</a></p>
<p>【Abstract】:
Infection and diffusion processes over networks arise in many domains. These introduce many challenging prediction tasks, such as influence estimation, trend prediction, and epidemic source localization. The standard approach to such problems is generative: assume an underlying infection model, learn its parameters, and infer the required output. In order to learn efficiently, the chosen infection models are often simple, and learning is focused on inferring the parameters of the model rather than on optimizing prediction accuracy. Here we argue that for prediction tasks, a discriminative approach is more adequate. We introduce DIMPLE, a novel discriminative learning framework for training classifiers based on dynamic infection models. We show how highly non-linear predictors based on infection models can be "linearized" by considering a larger class of prediction functions. Efficient learning over this class is performed by constructing "infection kernels" based on the outputs of infection models, and can be plugged into any kernel-supporting framework. DIMPLE can be applied to virtually any infection-related prediction task and any infection model for which the desired output can be calculated or simulated. For influence estimation in well-known infection models, we show that the kernel can either be computed in closed form, or reduces to estimating co-influence of seed pairs. We apply DIMPLE to the tasks of influence estimation on synthetic and real data from Digg, and to predicting customer network value in Polly, a viral phone-based development-related service deployed in low-literate communities. Our results show that DIMPLE outperforms strong baselines.</p>
<p>【Keywords】:
branching processes; diffusion processes; discriminative learning; improper learning; independent cascade model; infection kernels; infection processes; kernel methods; kernels; linear threshold model; social networks</p>
<h3 id="63. Representation Learning for Information Diffusion through Social Networks: an Embedded Cascade Model.">63. Representation Learning for Information Diffusion through Social Networks: an Embedded Cascade Model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835817">Paper Link</a>】    【Pages】:573-582</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bourigault:Simon">Simon Bourigault</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lamprier:Sylvain">Sylvain Lamprier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gallinari:Patrick">Patrick Gallinari</a></p>
<p>【Abstract】:
In this paper, we focus on information diffusion through social networks. Based on the well-known Independent Cascade model, we embed users of the social network in a latent space to extract more robust diffusion probabilities than those defined by classical graphical learning approaches. Better generalization abilities provided by the use of such a projection space allows our approach to present good performances on various real-world datasets, for both diffusion prediction and influence relationships inference tasks. Additionally, the use of a projection space enables our model to deal with larger social networks.</p>
<p>【Keywords】:
information diffusion; machine learning; representation learning</p>
<h3 id="64. Ensemble Models for Data-driven Prediction of Malware Infections.">64. Ensemble Models for Data-driven Prediction of Malware Infections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835834">Paper Link</a>】    【Pages】:583-592</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Chanhyun">Chanhyun Kang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Noseong">Noseong Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Prakash:B=_Aditya">B. Aditya Prakash</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serra:Edoardo">Edoardo Serra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Subrahmanian:V=_S=">V. S. Subrahmanian</a></p>
<p>【Abstract】:
Given a history of detected malware attacks, can we predict the number of malware infections in a country? Can we do this for different malware and countries? This is an important question which has numerous implications for cyber security, right from designing better anti-virus software, to designing and implementing targeted patches to more accurately measuring the economic impact of breaches. This problem is compounded by the fact that, as externals, we can only detect a fraction of actual malware infections. In this paper we address this problem using data from Symantec covering more than 1.4 million hosts and 50 malware spread across 2 years and multiple countries. We first carefully design domain-based features from both malware and machine-hosts perspectives. Secondly, inspired by epidemiological and information diffusion models, we design a novel temporal non-linear model for malware spread and detection. Finally we present ESM, an ensemble-based approach which combines both these methods to construct a more accurate algorithm. Using extensive experiments spanning multiple malware and countries, we show that ESM can effectively predict malware infection ratios over time (both the actual number and trend) upto 4 times better compared to several baselines on various metrics. Furthermore, ESM's performance is stable and robust even when the number of detected infections is low.</p>
<p>【Keywords】:
anti-virus; cyber security; information diffusion; malware attacks; prediction model</p>
<h2 id="Entities and Structure    8">Entities and Structure    8</h2>
<h3 id="65. WSDM Cup 2016: Entity Ranking Challenge.">65. WSDM Cup 2016: Entity Ranking Challenge.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855119">Paper Link</a>】    【Pages】:593-594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wade:Alex_D=">Alex D. Wade</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Kuansan">Kuansan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Yizhou">Yizhou Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gulli:Antonio">Antonio Gulli</a></p>
<p>【Abstract】:
In this paper, we describe the WSDM Cup entity ranking challenge held in conjunction with the 2016 Web Search and Data Mining conference (WSDM 2016). Participants in the challenge were provided access to the Microsoft Academic Graph (MAG), a large heterogeneous graph of academic entities, and were invited to calculate the query-independent importance of each publication in the graph. Submissions for the challenge were open from August through November 2015, and a public leaderboard displayed teams? progress against a set of training judgements. Final evaluations were performed against a separate, withheld portion of the evaluation judgements. The top eight performing teams were then invited to submit papers to the WSDM Cup workshop, held at the WSDM 2016 conference.</p>
<p>【Keywords】:
heterogeneous graphs; microsoft academic; microsoft academic graph</p>
<h3 id="66. Dynamic Collective Entity Representations for Entity Ranking.">66. Dynamic Collective Entity Representations for Entity Ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835819">Paper Link</a>】    【Pages】:595-604</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Graus:David">David Graus</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsagkias:Manos">Manos Tsagkias</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weerkamp:Wouter">Wouter Weerkamp</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meij:Edgar">Edgar Meij</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
Entity ranking, i.e., successfully positioning a relevant entity at the top of the ranking for a given query, is inherently difficult due to the potential mismatch between the entity's description in a knowledge base, and the way people refer to the entity when searching for it. To counter this issue we propose a method for constructing dynamic collective entity representations. We collect entity descriptions from a variety of sources and combine them into a single entity representation by learning to weight the content from different sources that are associated with an entity for optimal retrieval effectiveness. Our method is able to add new descriptions in real time and learn the best representation as time evolves so as to capture the dynamics of how people search entities. Incorporating dynamic description sources into dynamic collective entity representations improves retrieval effectiveness by 7% over a state-of-the-art learning to rank baseline. Periodic retraining of the ranker enables higher ranking effectiveness for dynamic collective entity representations.</p>
<p>【Keywords】:
content representation; entity ranking; entity retrieval; fielded retrieval</p>
<h3 id="67. Relationship Queries on Extended Knowledge Graphs.">67. Relationship Queries on Extended Knowledge Graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835795">Paper Link</a>】    【Pages】:605-614</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yahya:Mohamed">Mohamed Yahya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Barbosa:Denilson">Denilson Barbosa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Berberich:Klaus">Klaus Berberich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Qiuyue">Qiuyue Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
Entity search over text corpora is not geared for relationship queries where answers are tuples of related entities and where a query often requires joining cues from multiple documents. With large knowledge graphs, structured querying on their relational facts is an alternative, but often suffers from poor recall because of mismatches between user queries and the knowledge graph or because of weakly populated relations. This paper presents the TriniT search engine for querying and ranking on extended knowledge graphs that combine relational facts with textual web contents. Our query language is designed on the paradigm of SPO triple patterns, but is more expressive, supporting textual phrases for each of the SPO arguments. We present a model for automatic query relaxation to compensate for mismatches between the data and a user's query. Query answers -- tuples of entities -- are ranked by a statistical language model. We present experiments with different benchmarks, including complex relationship queries, over a combination of the Yago knowledge graph and the entity-annotated ClueWeb'09 corpus.</p>
<p>【Keywords】:
extended knowledge graphs; query relaxation; relationship queries</p>
<h3 id="68. Improving Website Hyperlink Structure Using Server Logs.">68. Improving Website Hyperlink Structure Using Server Logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835832">Paper Link</a>】    【Pages】:615-624</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Paranjape:Ashwin">Ashwin Paranjape</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/West_0001:Robert">Robert West</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zia:Leila">Leila Zia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leskovec:Jure">Jure Leskovec</a></p>
<p>【Abstract】:
Good websites should be easy to navigate via hyperlinks, yet maintaining a high-quality link structure is difficult. Identifying pairs of pages that should be linked may be hard for human editors, especially if the site is large and changes frequently. Further, given a set of useful link candidates, the task of incorporating them into the site can be expensive, since it typically involves humans editing pages. In the light of these challenges, it is desirable to develop data-driven methods for automating the link placement task. Here we develop an approach for automatically finding useful hyperlinks to add to a website. We show that passively collected server logs, beyond telling us which existing links are useful, also contain implicit signals indicating which nonexistent links would be useful if they were to be introduced. We leverage these signals to model the future usefulness of yet nonexistent links. Based on our model, we define the problem of link placement under budget constraints and propose an efficient algorithm for solving it. We demonstrate the effectiveness of our approach by evaluating it on Wikipedia, a large website for which we have access to both server logs (used for finding useful new links) and the complete revision history (containing a ground truth of new links). As our method is based exclusively on standard server logs, it may also be applied to any other website, as we show with the example of the biomedical research site Simtk.</p>
<p>【Keywords】:
browsing; link prediction; log analysis; navigation</p>
<h3 id="69. Long-tail Vocabulary Dictionary Extraction from the Web.">69. Long-tail Vocabulary Dictionary Extraction from the Web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835778">Paper Link</a>】    【Pages】:625-634</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhe">Zhe Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cafarella:Michael_J=">Michael J. Cafarella</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jagadish:H=_V=">H. V. Jagadish</a></p>
<p>【Abstract】:
A dictionary --- a set of instances belonging to the same conceptual class --- is central to information extraction and is a useful primitive for many applications, including query log analysis and document categorization. Considerable work has focused on generating accurate dictionaries given a few example seeds, but methods to date cannot obtain long-tail (rare) items with high accuracy and recall. In this paper, we develop a novel method to construct high-quality dictionaries, especially for long-tail vocabularies, using just a few user-provided seeds for each topic. Our algorithm obtains long-tail (i.e., rare) items by building and executing high-quality webpage-specific extractors. We use webpage-specific structural and textual information to build more accurate per-page extractors in order to detect the long-tail items from a single webpage. These webpage-specific extractors are obtained via a co-training procedure using distantly-supervised training data. By aggregating the page-specific dictionaries of many webpages, Lyretail is able to output a high-quality comprehensive dictionary. Our experiments demonstrate that in long-tail vocabulary settings, we obtained a 17.3% improvement on mean average precision for the dictionary generation process, and a 30.7% improvement on F1 for the page-specific extraction, when compared to previous state-of-the-art methods.</p>
<p>【Keywords】:
information extraction; long-tail dictionary; set expansion</p>
<h3 id="70. Semantic Documents Relatedness using Concept Graph Representation.">70. Semantic Documents Relatedness using Concept Graph Representation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835801">Paper Link</a>】    【Pages】:635-644</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Ni:Yuan">Yuan Ni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Qiong_Kai">Qiong Kai Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Feng">Feng Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mass:Yosi">Yosi Mass</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sheinwald:Dafna">Dafna Sheinwald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Huijia">Huijia Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Shao_Sheng">Shao Sheng Cao</a></p>
<p>【Abstract】:
We deal with the problem of document representation for the task of measuring semantic relatedness between documents. A document is represented as a compact concept graph where nodes represent concepts extracted from the document through references to entities in a knowledge base such as DBpedia. Edges represent the semantic and structural relationships among the concepts. Several methods are presented to measure the strength of those relationships. Concepts are weighted through the concept graph using closeness centrality measure which reflects their relevance to the aspects of the document. A novel similarity measure between two concept graphs is presented. The similarity measure first represents concepts as continuous vectors by means of neural networks. Second, the continuous vectors are used to accumulate pairwise similarity between pairs of concepts while considering their assigned weights. We evaluate our method on a standard benchmark for document similarity. Our method outperforms state-of-the-art methods including ESA (Explicit Semantic Annotation) while our concept graphs are much smaller than the concept vectors generated by ESA. Moreover, we show that by combining our concept graph with ESA, we obtain an even further improvement.</p>
<p>【Keywords】:
dbpedia; document representation; document semantic similarity; graph model; neural network</p>
<h3 id="71. EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion.">71. EgoSet: Exploiting Word Ego-networks and User-generated Ontology for Multifaceted Set Expansion.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835808">Paper Link</a>】    【Pages】:645-654</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rong:Xin">Xin Rong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhe">Zhe Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mei:Qiaozhu">Qiaozhu Mei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Adar:Eytan">Eytan Adar</a></p>
<p>【Abstract】:
A key challenge of entity set expansion is that multifaceted input seeds can lead to significant incoherence in the result set. In this paper, we present a novel solution to handling multifaceted seeds by combining existing user-generated ontologies with a novel word-similarity metric based on skip-grams. By blending the two resources we are able to produce sparse word ego-networks that are centered on the seed terms and are able to capture semantic equivalence among words. We demonstrate that the resulting networks possess internally-coherent clusters, which can be exploited to provide non-overlapping expansions, in order to reflect different semantic classes of the seeds. Empirical evaluation against state-of-the-art baselines shows that our solution, EgoSet, is able to not only capture multiple facets in the input query, but also generate expansions for each facet with higher precision.</p>
<p>【Keywords】:
entity set expansion; information extraction; web mining</p>
<h3 id="72. Extracting Search Query Patterns via the Pairwise Coupled Topic Model.">72. Extracting Search Query Patterns via the Pairwise Coupled Topic Model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835794">Paper Link</a>】    【Pages】:655-664</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Konishi:Takuya">Takuya Konishi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ohwa:Takuya">Takuya Ohwa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fujita:Sumio">Sumio Fujita</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ikeda:Kazushi">Kazushi Ikeda</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hayashi:Kohei">Kohei Hayashi</a></p>
<p>【Abstract】:
A fundamental yet new challenge in information retrieval is the identification of patterns behind search queries. For example, the query "NY restaurant" and "boston hotel" shares the common pattern "LOCATION SERVICE". However, because of the diversity of real queries, existing approaches require data preprocessing by humans or specifying the target query domains, which hinders their applicability. We propose a probabilistic topic model that assumes that each term (e.g., "NY") has a topic (LOCATION). The key idea is that we consider topic co-occurrence in a query rather than a topic sequence, which significantly reduces computational cost yet enables us to acquire coherent topics without the preprocessing. Using two real query datasets, we demonstrate that the obtained topics are intelligible by humans, and are highly accurate in keyword prediction and query generation tasks.</p>
<p>【Keywords】:
query intent analysis; search queries; topic model</p>
<h2 id="Practice & Experience Track    1">Practice &amp; Experience Track    1</h2>
<h3 id="73. The Past and Future of Systems for Current Events.">73. The Past and Future of Systems for Current Events.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835850">Paper Link</a>】    【Pages】:665</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Naaman:Mor">Mor Naaman</a></p>
<p>【Abstract】:
People share in social media an overwhelming amount of content from real-world events. These events range from major global events like an uprising or an earthquake, to local events and emergencies such as a fire or a parade; from media events like the Oscar's, to events that enjoy little media coverage such as a conference or a music concert. This shared media represents an important part of our society, culture and history. At the same time, this social media content is still fragmented across services, hard to find, and difficult to consume and understand.</p>
<p>【Keywords】:
events; information organization; social media; startup</p>
<h2 id="Social Events    2">Social Events    2</h2>
<h3 id="74. Barbara Made the News: Mining the Behavior of Crowds for Time-Aware Learning to Rank.">74. Barbara Made the News: Mining the Behavior of Crowds for Time-Aware Learning to Rank.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835825">Paper Link</a>】    【Pages】:667-676</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Martins:Fl=aacute=vio">Flávio Martins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Magalh=atilde=es:Jo=atilde=o">João Magalhães</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Callan:Jamie">Jamie Callan</a></p>
<p>【Abstract】:
In Twitter, and other microblogging services, the generation of new content by the crowd is often biased towards immediacy: what is happening now. Prompted by the propagation of commentary and information through multiple mediums, users on the Web interact with and produce new posts about newsworthy topics and give rise to trending topics. This paper proposes to leverage on the behavioral dynamics of users to estimate the most relevant time periods for a topic. Our hypothesis stems from the fact that when a real-world event occurs it usually has peak times on the Web: a higher volume of tweets, new visits and edits to related Wikipedia articles, and news published about the event. In this paper, we propose a novel time-aware ranking model that leverages on multiple sources of crowd signals. Our approach builds on two major novelties. First, a unifying approach that given query q, mines and represents temporal evidence from multiple sources of crowd signals. This allows us to predict the temporal relevance of documents for query q. Second, a principled retrieval model that integrates temporal signals in a learning to rank framework, to rank results according to the predicted temporal relevance. Evaluation on the TREC 2013 and 2014 Microblog track datasets demonstrates that the proposed model achieves a relative improvement of 13.2% over lexical retrieval models and 6.2% over a learning to rank baseline.</p>
<p>【Keywords】:
learning to rank; microblog search; social media; temporal information retrieval; time-aware ranking models; twitter</p>
<h3 id="75. Wiggins: Detecting Valuable Information in Dynamic Networks Using Limited Resources.">75. Wiggins: Detecting Valuable Information in Dynamic Networks Using Limited Resources.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2835830">Paper Link</a>】    【Pages】:677-686</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mahmoody:Ahmad">Ahmad Mahmoody</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Riondato:Matteo">Matteo Riondato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Upfal:Eli">Eli Upfal</a></p>
<p>【Abstract】:
Detecting new information and events in a dynamic network by probing individual nodes has many practical applications: discovering new webpages, analyzing influence properties in network, and detecting failure propagation in electronic circuits or infections in public drinkable water systems. In practice, it is infeasible for anyone but the owner of the network (if existent) to monitor all nodes at all times. In this work we study the constrained setting when the observer can only probe a small set of nodes at each time step to check whether new pieces of information (items) have reached those nodes. We formally define the problem through an infinite time generating process that places new items in subsets of nodes according to an unknown probability distribution. Items have an exponentially decaying novelty, modeling their decreasing value. The observer uses a probing schedule (i.e., a probability distribution over the set of nodes) to choose, at each time step, a small set of nodes to check for new items. The goal is to compute a schedule that minimizes the average novelty of undetected items. We present an algorithm, WIGGINS, to compute the optimal schedule through convex optimization, and then show how it can be adapted when the parameters of the problem must be learned or change over time. We also present a scalable variant of WIGGINS for the MapReduce framework. The results of our experimental evaluation on real social networks demonstrate the practicality of our approach.</p>
<p>【Keywords】:
information diffusion; rumor catching; social networks</p>
<h2 id="Tutorials    2">Tutorials    2</h2>
<h3 id="76. Understanding Offline Political Systems by Mining Online Political Data.">76. Understanding Offline Political Systems by Mining Online Political Data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855112">Paper Link</a>】    【Pages】:687-688</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lazer:David">David Lazer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsur:Oren">Oren Tsur</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eliassi=Rad:Tina">Tina Eliassi-Rad</a></p>
<p>【Abstract】:
"Man is by nature a political animal", as asserted by Aristotle. This political nature manifests itself in the data we produce and the traces we leave online. In this tutorial, we address a number of fundamental issues regarding mining of political data: What types of data could be considered political? What can we learn from such data? Can we use the data for prediction of political changes, etc? How can these prediction tasks be done efficiently? Can we use online socio-political data in order to get a better understanding of our political systems and of recent political changes? What are the pitfalls and inherent shortcomings of using online data for political analysis? In recent years, with the abundance of data, these questions, among others, have gained importance, especially in light of the global political turmoil and the upcoming 2016 US presidential election. We introduce relevant political science theory, describe the challenges within the framework of computational social science and present state of the art approaches bridging social network analysis, graph mining, and natural language processing.</p>
<p>【Keywords】:
computational social science; graph mining; political data; social and information networks</p>
<h3 id="77. Click Models for Web Search and their Applications to IR: WSDM 2016 Tutorial.">77. Click Models for Web Search and their Applications to IR: WSDM 2016 Tutorial.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855113">Paper Link</a>】    【Pages】:689-690</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chuklin:Aleksandr">Aleksandr Chuklin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Markov:Ilya">Ilya Markov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
In this tutorial we give an overview of click models for web search. We show how the framework of probabilistic graphical models helps to explain user behavior, build new evaluation metrics and perform simulations. The tutorial discusses foundational aspects alongside experimental details and applications, with live demos and discussions of publicly available resources.</p>
<p>【Keywords】:
click models; web search</p>
<h2 id="Workshop Summaries    4">Workshop Summaries    4</h2>
<h3 id="78. Query Understanding for Search on All Devices at WSDM 2016.">78. Query Understanding for Search on All Devices at WSDM 2016.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855115">Paper Link</a>】    【Pages】:691-692</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Goyal:Amit">Amit Goyal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Hongbo">Hongbo Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
mobile search; query auto completion; query reformulation; query understanding; spelling correction</p>
<h3 id="79. TargetAd2016: 2nd International Workshop on Ad Targeting at Scale.">79. TargetAd2016: 2nd International Workshop on Ad Targeting at Scale.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855116">Paper Link</a>】    【Pages】:693-694</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Grbovic:Mihajlo">Mihajlo Grbovic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Djuric:Nemanja">Nemanja Djuric</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Radosavljevic:Vladan">Vladan Radosavljevic</a></p>
<p>【Abstract】:
The 2nd International Workshop on Ad Targeting at Scale will be held in San Francisco, California, USA on February 22nd, 2016, co-located with the 9th ACM International Conference on Web Search and Data Mining (WSDM). The main objective of the workshop is to address the challenges of ad targeting in web-scale settings. The workshop brings together interdisciplinary researchers in computational advertising, recommender systems, personalization, and related areas, to share, exchange, learn, and develop preliminary results, new concepts, ideas, principles, and methodologies on applying data mining technologies to ad targeting. We have constructed an exciting program of eight refereed papers and several invited talks that will help us better understand the future of ad targeting.</p>
<p>【Keywords】:
ad targeting; audience modeling; computational advertising</p>
<h3 id="80. WSDM 2016 Workshop on the Ethics of Online Experimentation.">80. WSDM 2016 Workshop on the Ethics of Online Experimentation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855117">Paper Link</a>】    【Pages】:695-696</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Diaz:Fernando">Fernando Diaz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Barocas:Solon">Solon Barocas</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
ethics; experimentation</p>
<h3 id="81. Second Workshop on Search and Exploration of X-Rated Information (SEXI'16): WSDM Workshop Summary.">81. Second Workshop on Search and Exploration of X-Rated Information (SEXI'16): WSDM Workshop Summary.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855118">Paper Link</a>】    【Pages】:697-698</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Murdock:Vanessa">Vanessa Murdock</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clarke:Charles_L=_A=">Charles L. A. Clarke</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kamps:Jaap">Jaap Kamps</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karlgren:Jussi">Jussi Karlgren</a></p>
<p>【Abstract】:
Adult content is pervasive on the web, has been a driving factor in the adoption of the Internet medium, and is responsible for a significant fraction of traffic and revenues, yet rarely attracts attention in research. The research questions surrounding adult content access behaviors are unique, and interesting and valuable research in this area can be done ethically. WSDM 2016 features a half day workshop on Search and Exploration of X-Rated Information (SEXI) for information access tasks related to adult content. While the scope of the workshop remains broad, special attention is devoted to the privacy and security issues surrounding adult content by inviting keynote speakers with extensive experience on these topics. The recent release of the personal data belonging to customers of the adult dating site Ashley Madison provides a timely context for the focus on privacy and security.</p>
<p>【Keywords】:
adult content; privacy and security; research ethics; research practice</p>
<h2 id="Doctoral Consortium    12">Doctoral Consortium    12</h2>
<h3 id="82. Mining Complaints to Improve a Product: a Study about Problem Phrase Extraction from User Reviews.">82. Mining Complaints to Improve a Product: a Study about Problem Phrase Extraction from User Reviews.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855080">Paper Link</a>】    【Pages】:699</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tutubalina:Elena">Elena Tutubalina</a></p>
<p>【Abstract】:
The rapidly growing availability of user reviews has become an important resource for companies to detect customer dissatisfaction from textual opinions. Much research in opinion mining focuses on extracting customers' opinions from products' reviews and predicting their sentiment orientation or ratings with the aim of helping other users to make a decision on whether to buy a product. However, there have been few recent studies conducted on business-related opinion tasks to extract more refined opinions about a product's quality problems or technical failures. The focus of this study is the extraction of problem phrases, mentioned in user reviews about products. We explore main opinion mining tasks to determine whether given text from reviews contains a mention of a problem. We formulate research questions and propose knowledge-based methods and probabilistic models to classify users' phrases and extract latent problem indicators, aspects and related sentiments from online reviews.</p>
<p>【Keywords】:
opinion mining; problem phrase extraction; user reviews</p>
<h3 id="83. Web-scale Multimedia Search for Internet Video Content.">83. Web-scale Multimedia Search for Internet Video Content.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855081">Paper Link</a>】    【Pages】:701</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Lu">Lu Jiang</a></p>
<p>【Abstract】:
The Internet has been witnessing an explosion of video content. According to a Cisco study, video content is estimated to account for 80% of all the entire world's internet traffic by 2019. Video data are becoming one of the most valuable sources to assess information and knowledge. However, existing video search solutions are still based on text matching (text-to-text search), and could fail for the huge volumes of videos that have little relevant metadata or no metadata at all. The need for large-scale and intelligent video search, which bridges the gap between the user's information need and the video content, seems to be urgent. In this thesis, we propose an accurate, efficient and scalable search method for video content. As opposed to text matching, the proposed method relies on automatic video content understanding, and allows for intelligent and flexible search paradigms over the video content, including text-to-video and text&amp;video-to-video search. Suppose our goal is to search the videos about birthday party. In traditional text-to-text queries, we have to search the keywords in the user-generated metadata (titles or descriptions). In a text-to-video query, however, we might look for visual clues in the video content such as "cake", "gift" and "kids", audio clues like "birthday song" and "cheering sound", or visible text like "happy birthday". Text-to-video queries are flexible and can be further refined by Boolean and temporal operators. After watching the retrieved videos, the user may select a few interesting videos to find more relevant videos like these. This can be achieved by issuing a text&amp;video-to-video query which adds the selected video examples to the query. The proposed method provides a new dimension of looking at content-based video search, from finding a simple concept like "puppy" to searching a complex incident like "a scene in urban area where people running away after an explosion". To achieve this ambitious goal, we propose several novel methods focusing on accuracy, efficiency and scalability in the novel search paradigm. First, we introduce a novel self-paced curriculum learning theory that allows for training more accurate semantic concepts. Second, we propose a novel and scalable approach to index semantic concepts that can significantly improve the search efficiency with minimum accuracy loss. Third, we design a novel video reranking algorithm that can boost accuracy for video retrieval. The extensive experiments demonstrate that the proposed methods are able to surpass state-of-the-art accuracy on multiple datasets. In addition, our method can efficiently scale up the search to hundreds of millions videos, and only takes about 0.2 second to search a semantic query on a collection of 100 million videos, 1 second to process a hybrid query over 1 million videos. Based on the proposed methods, we implement E-Lamp Lite, the first of its kind large-scale semantic search engine for Internet videos. According to National Institute of Standards and Technology (NIST), it achieved the best accuracy in the TRECVID Multimedia Event Detection (MED) 2013, 2014 and 2015, the most representative task for content-based video search. To the best of our knowledge, E-Lamp Lite is the first content-based semantic search engine that is capable of indexing and searching a collection of 100 million videos.</p>
<p>【Keywords】:
big data; content-based retrieval; multimedia event detection; video content search; web search</p>
<h3 id="84. Affective Computing of Image Emotion Perceptions.">84. Affective Computing of Image Emotion Perceptions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855082">Paper Link</a>】    【Pages】:703</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Sicheng">Sicheng Zhao</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
image emotion; personalized perceptions; supervised learning</p>
<h3 id="85. Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search & Analytics.">85. Event Search and Analytics: Detecting Events in Semantically Annotated Corpora for Search &amp; Analytics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855083">Paper Link</a>】    【Pages】:705</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gupta_0002:Dhruv">Dhruv Gupta</a></p>
<p>【Abstract】:
In this article, I present the questions that I seek to answer in my PhD research. I posit to analyze natural language text with the help of semantic annotations and mine important events for navigating large text corpora. Semantic annotations such as named entities, geographic locations, and temporal expressions can help us mine events from the given corpora. These events thus provide us with useful means to discover the locked knowledge in them. I pose three problems that can help unlock this knowledge vault in semantically annotated text corpora: i. identifying important events; ii. semantic search; iii. and event analytics.</p>
<p>【Keywords】:
diversity &amp; novelty; information retrieval; semantic annotations; semantic search; text analytics; text summarization</p>
<h3 id="86. Understanding Diffusion Processes: Inference and Theory.">86. Understanding Diffusion Processes: Inference and Theory.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855084">Paper Link</a>】    【Pages】:707</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/He:Xinran">Xinran He</a></p>
<p>【Abstract】:
With increasing popularity of social media and social networks sites, analyzing the social networks offers great potential to shed light on human social structure and provides great marketing opportunities. Usually, social network analysis starts with extracting or learning the social network and the associated parameters. Contrary to other analytical tasks, this step is highly non-trivial due to amorphous nature of social ties and the challenges of noisy and incomplete observations. My research focuses on improving accuracy in inferring the network as well as analyzing the consequences when the extracted network is noisy or erroneous. To be more precise, I propose to study the following two questions with a special focus on analyzing diffusion behaviors: (1) How to utilize special properties of social networks to improve accuracy of the extracted network under noisy and missing data; (2) How to characterize the impact of noise in the inferred network and carry out robust analysis and optimization.</p>
<p>【Keywords】:
diffusion process; influence maximization; network inference; social network analysis</p>
<h3 id="87. E-commerce Product Recommendation by Personalized Promotion and Total Surplus Maximization.">87. E-commerce Product Recommendation by Personalized Promotion and Total Surplus Maximization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855085">Paper Link</a>】    【Pages】:709</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Qi">Qi Zhao</a></p>
<p>【Abstract】:
Existing recommendation algorithms treat recommendation problem as rating prediction and the recommendation quality is measured by RMSE or other similar metrics. However, we argued that when it comes to E-commerce product recommendation, recommendation is more than rating prediction by realizing the fact price plays a critical role in recommendation result. In this work, we propose to build E-commerce product recommender systems based on fundamental economic notions. We first proposed an incentive compatible method that can effectively elicit consumer's willingness-to-pay in a typical E-commerce setting and in a further step, we formalize the recommendation problem as maximizing total surplus. We validated the proposed WTP elicitation algorithm through crowd sourcing and the results demonstrated that the proposed approach can achieve higher seller profit by personalizing promotion. We also proposed a total surplus maximization (TSM) based recommendation framework. We specified TSM by three of the most representative settings - e-commerce where the product quantity can be viewed as infinity, P2P lending where the resource is bounded and freelancer marketing where the resource (job) can be assigned to one freelancer. The experimental results of the corresponding datasets shows that TSM exceeds existing approach in terms of total surplus.</p>
<p>【Keywords】:
e-commerce; economics; personalized promotion; recommendation; surplus maximization</p>
<h3 id="88. Detecting Social Media Icebergs by Their Tips: Rumors, Persuasion Campaigns, and Information Needs.">88. Detecting Social Media Icebergs by Their Tips: Rumors, Persuasion Campaigns, and Information Needs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855086">Paper Link</a>】    【Pages】:711</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Zhe">Zhe Zhao</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
early detection; signal activities; social media phenomena</p>
<h3 id="89. User Modeling in Large Social Networks.">89. User Modeling in Large Social Networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855087">Paper Link</a>】    【Pages】:713</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dong:Yuxiao">Yuxiao Dong</a></p>
<p>【Abstract】:
This proposal aims to harness the power of data, social, and network sciences to model user behavior in social networks. Specifically, we focus on individual users and investigate the interplay between their behavior and subsequently emergent social phenomena. Work in this proposal unveils the significant social strategies that are used by people to satisfy their social needs. We apply computational methods to address user modeling problems, including demographic inference, link recommendation, and social impact prediction. The proposed research work can be translated into applications in large social systems, such as mobile communication, online social media, and academic collaboration.</p>
<p>【Keywords】:
computational social science; social impact; user behavior</p>
<h3 id="90. Feature Generation and Selection on the Heterogeneous Graph for Music Recommendation.">90. Feature Generation and Selection on the Heterogeneous Graph for Music Recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855088">Paper Link</a>】    【Pages】:715</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Chun">Chun Guo</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
feature selection; meta-path; music recommendation</p>
<h3 id="91. Temporal Formation and Evolution of Online Communities.">91. Temporal Formation and Evolution of Online Communities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855089">Paper Link</a>】    【Pages】:717</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fani:Hossein">Hossein Fani</a></p>
<p>【Abstract】:
Researchers have already studied the identification of online communities and the possible impact or influence relationships from several perspectives. For instance, communities of users that are formed based on shared relationships and topological similarities, or communities that consist of users that share similar content. However, little work has been done on detection of communities that simultaneously share topical and temporal similarities. Furthermore, these studies have not explored the causation relationship between the communities. Causation provides systematic explanation as to why communities are formed and helps to predict future communities. This proposal will address two main research questions: i) how can communities that share topical and temporal similarities be identified, and ii) how can causation relation between different online communities be detected and modelled. We model users' behaviour towards topics of interest through multivariate time series to identify like-minded communities. Further, we employ Granger's concept of causality to infer causation between detected communities from corresponding users' time series. Granger causality is the prominent approach in time series modelling and rests on a firm statistical foundation. We assess the proposed community detection methods through comparison with the state of the art and verify the causal model through its prediction accuracy.</p>
<p>【Keywords】:
causality; community detection; online social network</p>
<h3 id="92. Mining the Web for Intelligent Problem Solving for Programmers.">92. Mining the Web for Intelligent Problem Solving for Programmers.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855091">Paper Link</a>】    【Pages】:719</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rong:Xin">Xin Rong</a></p>
<p>【Abstract】:
Programming can be hard to learn and master. Novice programmers often find themselves struggling with terminology, concepts, or different solutions to the same problem with little clue on how to choose the best one. Professional programmers often spend a considerable amount of time learning to use third-party libraries, APIs, or an unfamiliar piece of code. Although programmers can turn to search engines or question-and-answer websites for help, the problem solving process can often take multiple iterations and can be time-consuming. An integrated system that can recognize a programmer's difficulties and provide contextualized solutions is thus desirable, as it may significantly reduce the amount of manual effort required in the loop of troubleshooting. Ideally, a programmer should be able to interact with such an intelligent system using natural language, in a way similar to how they document code or communicate with peers. However, using automatic natural language processing techniques to address programming questions is very difficult, mainly due to the following reasons: (1) the terms and common expressions vary greatly across different domains and individual programmers, making it difficult to associate relevant concepts together; (2) the solution to the user's trouble in programming often requires multiple steps or different resources, which requires deep understanding of the relations or dependencies of the possible solutions, as well as the user's personal capability of handling those solutions; (3) the documents in the training data usually include a mixture of general-domain expressions with mentions of variables, functions, and classes, as well as source code, making low-level text processing difficult; (4) the evaluation of the system generally requires skilled experts to provide ground truth, which is expensive and often unreliable. We address the above difficulties and build an intelligent programming helper system by mining the massive data available online related to programming, including question-and-answer websites, tutorials, blogs, and code repositories. In specific, the study involves three important components. First, we use information extraction techniques to extract common programming tasks, issues, and solutions from the Web data, and establish connections between these extracted elements by leveraging their discrete or distributed representations (e.g., using neural embedding models). Such techniques have been shown to be useful in helping general users solve problems that require interactions with a complex computer software application through the interface of natural language. Second, we study how to handle complicated problems that require multiple steps to solve. The existing troubleshooting instances documented online are collectively modeled as a heterogeneous network, on which the random walk paths can be exploited to recommend solutions. Third, we study how to personalize the problem-solving process for users with varying levels of skills and background knowledge. In particular, each user's past adoptions of technologies and the adoption behavior in his/her social community can be jointly leveraged to provide the appropriate recommendations of technologies and may even promote innovations (e.g., new algorithms) in the process. Collectively, these three components form an integral solution to computer-assisted problem solving for programmers driven by big data, and may have impact on various different domains, including information extraction, language modeling, natural language understanding, automatic problem solving, and social network analysis.</p>
<p>【Keywords】:
human-computer interaction; language modeling; software engineering</p>
<h3 id="93. Optimizing Search Interactions within Professional Social Networks.">93. Optimizing Search Interactions within Professional Social Networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2835776.2855092">Paper Link</a>】    【Pages】:721</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Spirin:Nikita_V=">Nikita V. Spirin</a></p>
<p>【Abstract】:
To help users cope with the scale and influx of new information, professional social networks (PSNs) provide a search functionality. However, most of the search engines within PSNs today only support keyword queries and basic faceted search capabilities overlooking serendipitous network exploration and search for relationships between entities. This results in siloed information and a limited search space. My thesis is that we must redesign all major elements of a search user interface, such as input, control, and informational, to enable more effective search interactions within PSNs. I will introduce new insights and algorithms supporting the thesis.</p>
<p>【Keywords】:
entity search; filtering; job search; online social network; query log analysis; search user interaction; snippet</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
