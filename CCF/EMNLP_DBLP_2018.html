 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#EMNLP 2018:Brussels, Belgium">EMNLP 2018:Brussels, Belgium</a><ul>
<li><a href="#Paper Num: 549 || Session Num: 0">Paper Num: 549 || Session Num: 0</a><ul>
<li><a href="#1. Privacy-preserving Neural Representations of Text.">1. Privacy-preserving Neural Representations of Text.</a></li>
<li><a href="#2. Adversarial Removal of Demographic Attributes from Text Data.">2. Adversarial Removal of Demographic Attributes from Text Data.</a></li>
<li><a href="#3. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning.">3. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning.</a></li>
<li><a href="#4. It's going to be okay: Measuring Access to Support in Online Communities.">4. It's going to be okay: Measuring Access to Support in Online Communities.</a></li>
<li><a href="#5. Detecting Gang-Involved Escalation on Social Media Using Context.">5. Detecting Gang-Involved Escalation on Social Media Using Context.</a></li>
<li><a href="#6. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge.">6. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge.</a></li>
<li><a href="#7. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.">7. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.</a></li>
<li><a href="#8. Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts.">8. Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts.</a></li>
<li><a href="#9. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.">9. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.</a></li>
<li><a href="#10. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification.">10. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification.</a></li>
<li><a href="#11. Associative Multichannel Autoencoder for Multimodal Word Representation.">11. Associative Multichannel Autoencoder for Multimodal Word Representation.</a></li>
<li><a href="#12. Game-Based Video-Context Dialogue.">12. Game-Based Video-Context Dialogue.</a></li>
<li><a href="#13. simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions.">13. simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions.</a></li>
<li><a href="#14. Multimodal Language Analysis with Recurrent Multistage Fusion.">14. Multimodal Language Analysis with Recurrent Multistage Fusion.</a></li>
<li><a href="#15. Temporally Grounding Natural Sentence in Video.">15. Temporally Grounding Natural Sentence in Video.</a></li>
<li><a href="#16. PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution.">16. PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution.</a></li>
<li><a href="#17. Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism.">17. Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism.</a></li>
<li><a href="#18. Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers.">18. Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers.</a></li>
<li><a href="#19. Neural Segmental Hypergraphs for Overlapping Mention Recognition.">19. Neural Segmental Hypergraphs for Overlapping Mention Recognition.</a></li>
<li><a href="#20. Variational Sequential Labelers for Semi-Supervised Learning.">20. Variational Sequential Labelers for Semi-Supervised Learning.</a></li>
<li><a href="#21. Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision.">21. Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision.</a></li>
<li><a href="#22. Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance.">22. Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance.</a></li>
<li><a href="#23. Multi-lingual Common Semantic Space Construction via Cluster-Consistent Word Embedding.">23. Multi-lingual Common Semantic Space Construction via Cluster-Consistent Word Embedding.</a></li>
<li><a href="#24. Unsupervised Multilingual Word Embeddings.">24. Unsupervised Multilingual Word Embeddings.</a></li>
<li><a href="#25. CLUSE: Cross-Lingual Unsupervised Sense Embeddings.">25. CLUSE: Cross-Lingual Unsupervised Sense Embeddings.</a></li>
<li><a href="#26. Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization.">26. Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization.</a></li>
<li><a href="#27. Improving Cross-Lingual Word Embeddings by Meeting in the Middle.">27. Improving Cross-Lingual Word Embeddings by Meeting in the Middle.</a></li>
<li><a href="#28. WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse.">28. WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse.</a></li>
<li><a href="#29. On the Relation between Linguistic Typology and (Limitations of">29. On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling.</a> Multilingual Language Modeling.)</li>
<li><a href="#30. A Fast, Compact, Accurate Model for Language Identification of Codemixed Text.">30. A Fast, Compact, Accurate Model for Language Identification of Codemixed Text.</a></li>
<li><a href="#31. Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning.">31. Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning.</a></li>
<li><a href="#32. Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks.">32. Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks.</a></li>
<li><a href="#33. Cross-lingual Lexical Sememe Prediction.">33. Cross-lingual Lexical Sememe Prediction.</a></li>
<li><a href="#34. Neural Cross-lingual Named Entity Recognition with Minimal Resources.">34. Neural Cross-lingual Named Entity Recognition with Minimal Resources.</a></li>
<li><a href="#35. A Stable and Effective Learning Strategy for Trainable Greedy Decoding.">35. A Stable and Effective Learning Strategy for Trainable Greedy Decoding.</a></li>
<li><a href="#36. Addressing Troublesome Words in Neural Machine Translation.">36. Addressing Troublesome Words in Neural Machine Translation.</a></li>
<li><a href="#37. Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing.">37. Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing.</a></li>
<li><a href="#38. XL-NBT: A Cross-lingual Neural Belief Tracking Framework.">38. XL-NBT: A Cross-lingual Neural Belief Tracking Framework.</a></li>
<li><a href="#39. Contextual Parameter Generation for Universal Neural Machine Translation.">39. Contextual Parameter Generation for Universal Neural Machine Translation.</a></li>
<li><a href="#40. Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.">40. Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.</a></li>
<li><a href="#41. Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.">41. Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.</a></li>
<li><a href="#42. A Discriminative Latent-Variable Model for Bilingual Lexicon Induction.">42. A Discriminative Latent-Variable Model for Bilingual Lexicon Induction.</a></li>
<li><a href="#43. Non-Adversarial Unsupervised Word Translation.">43. Non-Adversarial Unsupervised Word Translation.</a></li>
<li><a href="#44. Semi-Autoregressive Neural Machine Translation.">44. Semi-Autoregressive Neural Machine Translation.</a></li>
<li><a href="#45. Understanding Back-Translation at Scale.">45. Understanding Back-Translation at Scale.</a></li>
<li><a href="#46. Bootstrapping Transliteration with Guided Discovery for Low-Resource Languages.">46. Bootstrapping Transliteration with Guided Discovery for Low-Resource Languages.</a></li>
<li><a href="#47. NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.">47. NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.</a></li>
<li><a href="#48. Adaptive Multi-pass Decoder for Neural Machine Translation.">48. Adaptive Multi-pass Decoder for Neural Machine Translation.</a></li>
<li><a href="#49. Improving the Transformer Translation Model with Document-Level Context.">49. Improving the Transformer Translation Model with Document-Level Context.</a></li>
<li><a href="#50. MTNT: A Testbed for Machine Translation of Noisy Text.">50. MTNT: A Testbed for Machine Translation of Noisy Text.</a></li>
<li><a href="#51. SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach.">51. SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach.</a></li>
<li><a href="#52. Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension.">52. Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension.</a></li>
<li><a href="#53. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering.">53. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering.</a></li>
<li><a href="#54. Cut to the Chase: A Context Zoom-in Network for Reading Comprehension.">54. Cut to the Chase: A Context Zoom-in Network for Reading Comprehension.</a></li>
<li><a href="#55. Adaptive Document Retrieval for Deep Question Answering.">55. Adaptive Document Retrieval for Deep Question Answering.</a></li>
<li><a href="#56. Why is unsupervised alignment of English embeddings from different algorithms so hard?">56. Why is unsupervised alignment of English embeddings from different algorithms so hard?</a></li>
<li><a href="#57. Quantifying Context Overlap for Training Word Embeddings.">57. Quantifying Context Overlap for Training Word Embeddings.</a></li>
<li><a href="#58. Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space.">58. Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space.</a></li>
<li><a href="#59. Generalizing Word Embeddings using Bag of Subwords.">59. Generalizing Word Embeddings using Bag of Subwords.</a></li>
<li><a href="#60. Neural Metaphor Detection in Context.">60. Neural Metaphor Detection in Context.</a></li>
<li><a href="#61. Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging.">61. Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging.</a></li>
<li><a href="#62. Unsupervised Bilingual Lexicon Induction via Latent Variable Models.">62. Unsupervised Bilingual Lexicon Induction via Latent Variable Models.</a></li>
<li><a href="#63. Learning Unsupervised Word Translations Without Adversaries.">63. Learning Unsupervised Word Translations Without Adversaries.</a></li>
<li><a href="#64. Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification.">64. Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification.</a></li>
<li><a href="#65. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.">65. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.</a></li>
<li><a href="#66. Joint Learning for Emotion Classification and Emotion Cause Detection.">66. Joint Learning for Emotion Classification and Emotion Cause Detection.</a></li>
<li><a href="#67. Exploring Optimism and Pessimism in Twitter Using Deep Learning.">67. Exploring Optimism and Pessimism in Twitter Using Deep Learning.</a></li>
<li><a href="#68. Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning.">68. Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning.</a></li>
<li><a href="#69. Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates.">69. Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates.</a></li>
<li><a href="#70. Improving Author Attribute Prediction by Retrofitting Linguistic Representations with Homophily.">70. Improving Author Attribute Prediction by Retrofitting Linguistic Representations with Homophily.</a></li>
<li><a href="#71. A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation.">71. A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation.</a></li>
<li><a href="#72. Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning.">72. Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning.</a></li>
<li><a href="#73. Extending Neural Generative Conversational Model using External Knowledge Sources.">73. Extending Neural Generative Conversational Model using External Knowledge Sources.</a></li>
<li><a href="#74. Modeling Temporality of Human Intentions by Domain Adaptation.">74. Modeling Temporality of Human Intentions by Domain Adaptation.</a></li>
<li><a href="#75. An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation.">75. An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation.</a></li>
<li><a href="#76. A Dataset for Document Grounded Conversations.">76. A Dataset for Document Grounded Conversations.</a></li>
<li><a href="#77. Out-of-domain Detection based on Generative Adversarial Network.">77. Out-of-domain Detection based on Generative Adversarial Network.</a></li>
<li><a href="#78. Listening Comprehension over Argumentative Content.">78. Listening Comprehension over Argumentative Content.</a></li>
<li><a href="#79. Using Active Learning to Expand Training Data for Implicit Discourse Relation Recognition.">79. Using Active Learning to Expand Training Data for Implicit Discourse Relation Recognition.</a></li>
<li><a href="#80. Learning To Split and Rephrase From Wikipedia Edit History.">80. Learning To Split and Rephrase From Wikipedia Edit History.</a></li>
<li><a href="#81. BLEU is Not Suitable for the Evaluation of Text Simplification.">81. BLEU is Not Suitable for the Evaluation of Text Simplification.</a></li>
<li><a href="#82. S2SPMN: A Simple and Effective Framework for Response Generation with Relevant Information.">82. S2SPMN: A Simple and Effective Framework for Response Generation with Relevant Information.</a></li>
<li><a href="#83. Improving Reinforcement Learning Based Image Captioning with Natural Language Prior.">83. Improving Reinforcement Learning Based Image Captioning with Natural Language Prior.</a></li>
<li><a href="#84. Training for Diversity in Image Paragraph Captioning.">84. Training for Diversity in Image Paragraph Captioning.</a></li>
<li><a href="#85. A Graph-Theoretic Summary Evaluation for Rouge.">85. A Graph-Theoretic Summary Evaluation for Rouge.</a></li>
<li><a href="#86. Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation.">86. Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation.</a></li>
<li><a href="#87. Evaluating Multiple System Summary Lengths: A Case Study.">87. Evaluating Multiple System Summary Lengths: A Case Study.</a></li>
<li><a href="#88. Neural Latent Extractive Document Summarization.">88. Neural Latent Extractive Document Summarization.</a></li>
<li><a href="#89. On the Abstractiveness of Neural Document Summarization.">89. On the Abstractiveness of Neural Document Summarization.</a></li>
<li><a href="#90. Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.">90. Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.</a></li>
<li><a href="#91. Identifying Well-formed Natural Language Questions.">91. Identifying Well-formed Natural Language Questions.</a></li>
<li><a href="#92. Self-Governing Neural Networks for On-Device Short Text Classification.">92. Self-Governing Neural Networks for On-Device Short Text Classification.</a></li>
<li><a href="#93. HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization.">93. HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization.</a></li>
<li><a href="#94. A Hierarchical Neural Attention-based Text Classifier.">94. A Hierarchical Neural Attention-based Text Classifier.</a></li>
<li><a href="#95. Labeled Anchors and a Scalable, Transparent, and Interactive Classifier.">95. Labeled Anchors and a Scalable, Transparent, and Interactive Classifier.</a></li>
<li><a href="#96. Coherence-Aware Neural Topic Modeling.">96. Coherence-Aware Neural Topic Modeling.</a></li>
<li><a href="#97. Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.">97. Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.</a></li>
<li><a href="#98. Topic Intrusion for Automatic Topic Model Evaluation.">98. Topic Intrusion for Automatic Topic Model Evaluation.</a></li>
<li><a href="#99. Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents.">99. Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents.</a></li>
<li><a href="#100. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation.">100. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation.</a></li>
<li><a href="#101. Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder.">101. Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder.</a></li>
<li><a href="#102. Decipherment of Substitution Ciphers with Neural Language Models.">102. Decipherment of Substitution Ciphers with Neural Language Models.</a></li>
<li><a href="#103. Rapid Adaptation of Neural Machine Translation to New Languages.">103. Rapid Adaptation of Neural Machine Translation to New Languages.</a></li>
<li><a href="#104. Compact Personalized Models for Neural Machine Translation.">104. Compact Personalized Models for Neural Machine Translation.</a></li>
<li><a href="#105. Self-Governing Neural Networks for On-Device Short Text Classification.">105. Self-Governing Neural Networks for On-Device Short Text Classification.</a></li>
<li><a href="#106. Supervised Domain Enablement Attention for Personalized Domain Classification.">106. Supervised Domain Enablement Attention for Personalized Domain Classification.</a></li>
<li><a href="#107. A Deep Neural Network Sentence Level Classification Method with Context Information.">107. A Deep Neural Network Sentence Level Classification Method with Context Information.</a></li>
<li><a href="#108. Towards Dynamic Computation Graphs via Sparse Latent Structure.">108. Towards Dynamic Computation Graphs via Sparse Latent Structure.</a></li>
<li><a href="#109. Convolutional Neural Networks with Recurrent Neural Filters.">109. Convolutional Neural Networks with Recurrent Neural Filters.</a></li>
<li><a href="#110. Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model.">110. Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model.</a></li>
<li><a href="#111. Retrieval-Based Neural Code Generation.">111. Retrieval-Based Neural Code Generation.</a></li>
<li><a href="#112. SQL-to-Text Generation with Graph-to-Sequence Model.">112. SQL-to-Text Generation with Graph-to-Sequence Model.</a></li>
<li><a href="#113. Generating Syntactic Paraphrases.">113. Generating Syntactic Paraphrases.</a></li>
<li><a href="#114. Neural-Davidsonian Semantic Proto-role Labeling.">114. Neural-Davidsonian Semantic Proto-role Labeling.</a></li>
<li><a href="#115. Conversational Decision Making Model for Predicting King's Decision in the Annals of the Joseon Dynasty.">115. Conversational Decision Making Model for Predicting King's Decision in the Annals of the Joseon Dynasty.</a></li>
<li><a href="#116. Toward Fast and Accurate Neural Discourse Segmentation.">116. Toward Fast and Accurate Neural Discourse Segmentation.</a></li>
<li><a href="#117. A Dataset for Telling the Stories of Social Media Videos.">117. A Dataset for Telling the Stories of Social Media Videos.</a></li>
<li><a href="#118. Cascaded Mutual Modulation for Visual Reasoning.">118. Cascaded Mutual Modulation for Visual Reasoning.</a></li>
<li><a href="#119. How agents see things: On visual representations in an emergent language game.">119. How agents see things: On visual representations in an emergent language game.</a></li>
<li><a href="#120. Attention-Based Capsule Network with Dynamic Routing for Relation Extraction.">120. Attention-Based Capsule Network with Dynamic Routing for Relation Extraction.</a></li>
<li><a href="#121. Put It Back: Entity Typing with Language Model Enhancement.">121. Put It Back: Entity Typing with Language Model Enhancement.</a></li>
<li><a href="#122. Event Detection with Neural Networks: A Rigorous Empirical Evaluation.">122. Event Detection with Neural Networks: A Rigorous Empirical Evaluation.</a></li>
<li><a href="#123. PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages.">123. PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages.</a></li>
<li><a href="#124. A Neural Transition-based Model for Nested Mention Recognition.">124. A Neural Transition-based Model for Nested Mention Recognition.</a></li>
<li><a href="#125. Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction.">125. Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction.</a></li>
<li><a href="#126. Effective Use of Context in Noisy Entity Linking.">126. Effective Use of Context in Noisy Entity Linking.</a></li>
<li><a href="#127. Exploiting Contextual Information via Dynamic Memory Network for Event Detection.">127. Exploiting Contextual Information via Dynamic Memory Network for Event Detection.</a></li>
<li><a href="#128. Do explanations make VQA models more predictable to a human?">128. Do explanations make VQA models more predictable to a human?</a></li>
<li><a href="#129. Facts That Matter.">129. Facts That Matter.</a></li>
<li><a href="#130. Entity Tracking Improves Cloze-style Reading Comprehension.">130. Entity Tracking Improves Cloze-style Reading Comprehension.</a></li>
<li><a href="#131. Adversarial Domain Adaptation for Duplicate Question Detection.">131. Adversarial Domain Adaptation for Duplicate Question Detection.</a></li>
<li><a href="#132. Translating Math Word Problem to Expression Tree.">132. Translating Math Word Problem to Expression Tree.</a></li>
<li><a href="#133. Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection.">133. Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection.</a></li>
<li><a href="#134. A dataset and baselines for sequential open-domain question answering.">134. A dataset and baselines for sequential open-domain question answering.</a></li>
<li><a href="#135. Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set.">135. Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set.</a></li>
<li><a href="#136. Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification.">136. Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification.</a></li>
<li><a href="#137. Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network.">137. Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network.</a></li>
<li><a href="#138. Learning Sentiment Memories for Sentiment Modification without Parallel Data.">138. Learning Sentiment Memories for Sentiment Modification without Parallel Data.</a></li>
<li><a href="#139. Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.">139. Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.</a></li>
<li><a href="#140. Representing Social Media Users for Sarcasm Detection.">140. Representing Social Media Users for Sarcasm Detection.</a></li>
<li><a href="#141. Syntactical Analysis of the Weaknesses of Sentiment Analyzers.">141. Syntactical Analysis of the Weaknesses of Sentiment Analyzers.</a></li>
<li><a href="#142. Is Nike female? Exploring the role of sound symbolism in predicting brand name gender.">142. Is Nike female? Exploring the role of sound symbolism in predicting brand name gender.</a></li>
<li><a href="#143. Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging.">143. Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging.</a></li>
<li><a href="#144. Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations.">144. Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations.</a></li>
<li><a href="#145. Identifying Locus of Control in Social Media Language.">145. Identifying Locus of Control in Social Media Language.</a></li>
<li><a href="#146. Somm: Into the Model.">146. Somm: Into the Model.</a></li>
<li><a href="#147. Fine-Grained Emotion Detection in Health-Related Online Posts.">147. Fine-Grained Emotion Detection in Health-Related Online Posts.</a></li>
<li><a href="#148. The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions.">148. The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions.</a></li>
<li><a href="#149. Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.">149. Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.</a></li>
<li><a href="#150. Large Margin Neural Language Model.">150. Large Margin Neural Language Model.</a></li>
<li><a href="#151. Targeted Syntactic Evaluation of Language Models.">151. Targeted Syntactic Evaluation of Language Models.</a></li>
<li><a href="#152. Rational Recurrences.">152. Rational Recurrences.</a></li>
<li><a href="#153. Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling.">153. Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling.</a></li>
<li><a href="#154. Automatic Event Salience Identification.">154. Automatic Event Salience Identification.</a></li>
<li><a href="#155. Temporal Information Extraction by Predicting Relative Time-lines.">155. Temporal Information Extraction by Predicting Relative Time-lines.</a></li>
<li><a href="#156. Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation.">156. Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation.</a></li>
<li><a href="#157. RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information.">157. RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information.</a></li>
<li><a href="#158. Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms.">158. Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms.</a></li>
<li><a href="#159. Valency-Augmented Dependency Parsing.">159. Valency-Augmented Dependency Parsing.</a></li>
<li><a href="#160. Unsupervised Learning of Syntactic Structure with Invertible Neural Projections.">160. Unsupervised Learning of Syntactic Structure with Invertible Neural Projections.</a></li>
<li><a href="#161. Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing.">161. Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing.</a></li>
<li><a href="#162. Constituent Parsing as Sequence Labeling.">162. Constituent Parsing as Sequence Labeling.</a></li>
<li><a href="#163. Synthetic Data Made to Order: The Case of Parsing.">163. Synthetic Data Made to Order: The Case of Parsing.</a></li>
<li><a href="#164. Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions.">164. Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions.</a></li>
<li><a href="#165. Learning a Policy for Opportunistic Active Learning.">165. Learning a Policy for Opportunistic Active Learning.</a></li>
<li><a href="#166. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.">166. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.</a></li>
<li><a href="#167. TVQA: Localized, Compositional Video Question Answering.">167. TVQA: Localized, Compositional Video Question Answering.</a></li>
<li><a href="#168. Localizing Moments in Video with Temporal Language.">168. Localizing Moments in Video with Temporal Language.</a></li>
<li><a href="#169. Card-660: A Reliable Evaluation Framework for Rare Word Representation Models.">169. Card-660: A Reliable Evaluation Framework for Rare Word Representation Models.</a></li>
<li><a href="#170. Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention.">170. Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention.</a></li>
<li><a href="#171. Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations.">171. Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations.</a></li>
<li><a href="#172. Streaming word similarity mining on the cheap.">172. Streaming word similarity mining on the cheap.</a></li>
<li><a href="#173. Memory, Show the Way: Memory Based Few Shot Word Representation Learning.">173. Memory, Show the Way: Memory Based Few Shot Word Representation Learning.</a></li>
<li><a href="#174. Disambiguated skip-gram model.">174. Disambiguated skip-gram model.</a></li>
<li><a href="#175. Picking Apart Story Salads.">175. Picking Apart Story Salads.</a></li>
<li><a href="#176. Dynamic Meta-Embeddings for Improved Sentence Representations.">176. Dynamic Meta-Embeddings for Improved Sentence Representations.</a></li>
<li><a href="#177. A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.">177. A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.</a></li>
<li><a href="#178. Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation.">178. Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation.</a></li>
<li><a href="#179. Dissecting Contextual Word Embeddings: Architecture and Representation.">179. Dissecting Contextual Word Embeddings: Architecture and Representation.</a></li>
<li><a href="#180. Preposition Sense Disambiguation and Representation.">180. Preposition Sense Disambiguation and Representation.</a></li>
<li><a href="#181. Auto-Encoding Dictionary Definitions into Consistent Word Embeddings.">181. Auto-Encoding Dictionary Definitions into Consistent Word Embeddings.</a></li>
<li><a href="#182. Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources.">182. Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources.</a></li>
<li><a href="#183. Neural Multitask Learning for Simile Recognition.">183. Neural Multitask Learning for Simile Recognition.</a></li>
<li><a href="#184. Structured Alignment Networks for Matching Sentences.">184. Structured Alignment Networks for Matching Sentences.</a></li>
<li><a href="#185. Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference.">185. Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference.</a></li>
<li><a href="#186. Convolutional Interaction Network for Natural Language Inference.">186. Convolutional Interaction Network for Natural Language Inference.</a></li>
<li><a href="#187. Lessons from Natural Language Inference in the Clinical Domain.">187. Lessons from Natural Language Inference in the Clinical Domain.</a></li>
<li><a href="#188. Question Generation from SQL Queries Improves Neural Semantic Parsing.">188. Question Generation from SQL Queries Improves Neural Semantic Parsing.</a></li>
<li><a href="#189. SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications.">189. SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications.</a></li>
<li><a href="#190. Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing.">190. Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing.</a></li>
<li><a href="#191. A Span Selection Model for Semantic Role Labeling.">191. A Span Selection Model for Semantic Role Labeling.</a></li>
<li><a href="#192. Mapping Language to Code in Programmatic Context.">192. Mapping Language to Code in Programmatic Context.</a></li>
<li><a href="#193. SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task.">193. SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task.</a></li>
<li><a href="#194. Cross-lingual Decompositional Semantic Parsing.">194. Cross-lingual Decompositional Semantic Parsing.</a></li>
<li><a href="#195. Learning to Learn Semantic Parsers from Natural Language Supervision.">195. Learning to Learn Semantic Parsers from Natural Language Supervision.</a></li>
<li><a href="#196. DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers.">196. DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers.</a></li>
<li><a href="#197. What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL.">197. What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL.</a></li>
<li><a href="#198. Better Transition-Based AMR Parsing with Refined Search Space.">198. Better Transition-Based AMR Parsing with Refined Search Space.</a></li>
<li><a href="#199. Heuristically Informed Unsupervised Idiom Usage Recognition.">199. Heuristically Informed Unsupervised Idiom Usage Recognition.</a></li>
<li><a href="#200. Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research.">200. Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research.</a></li>
<li><a href="#201. Predicting Semantic Relations using Global Graph Properties.">201. Predicting Semantic Relations using Global Graph Properties.</a></li>
<li><a href="#202. Learning Scalar Adjective Intensity from Paraphrases.">202. Learning Scalar Adjective Intensity from Paraphrases.</a></li>
<li><a href="#203. Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions.">203. Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions.</a></li>
<li><a href="#204. Neural Related Work Summarization with a Joint Context-driven Attention Mechanism.">204. Neural Related Work Summarization with a Joint Context-driven Attention Mechanism.</a></li>
<li><a href="#205. Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling.">205. Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling.</a></li>
<li><a href="#206. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization.">206. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization.</a></li>
<li><a href="#207. Improving Abstraction in Text Summarization.">207. Improving Abstraction in Text Summarization.</a></li>
<li><a href="#208. Content Selection in Deep Learning Models of Summarization.">208. Content Selection in Deep Learning Models of Summarization.</a></li>
<li><a href="#209. Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment.">209. Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment.</a></li>
<li><a href="#210. Learning Context-Aware Convolutional Filters for Text Processing.">210. Learning Context-Aware Convolutional Filters for Text Processing.</a></li>
<li><a href="#211. Deep Relevance Ranking using Enhanced Document-Query Interactions.">211. Deep Relevance Ranking using Enhanced Document-Query Interactions.</a></li>
<li><a href="#212. Learning Neural Representation for CLIR with Adversarial Framework.">212. Learning Neural Representation for CLIR with Adversarial Framework.</a></li>
<li><a href="#213. AD3: Attentive Deep Document Dater.">213. AD3: Attentive Deep Document Dater.</a></li>
<li><a href="#214. Gromov-Wasserstein Alignment of Word Embedding Spaces.">214. Gromov-Wasserstein Alignment of Word Embedding Spaces.</a></li>
<li><a href="#215. Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.">215. Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.</a></li>
<li><a href="#216. Deriving Machine Attention from Human Rationales.">216. Deriving Machine Attention from Human Rationales.</a></li>
<li><a href="#217. Semi-Supervised Sequence Modeling with Cross-View Training.">217. Semi-Supervised Sequence Modeling with Cross-View Training.</a></li>
<li><a href="#218. A Probabilistic Annotation Model for Crowdsourcing Coreference.">218. A Probabilistic Annotation Model for Crowdsourcing Coreference.</a></li>
<li><a href="#219. A Deterministic Algorithm for Bridging Anaphora Resolution.">219. A Deterministic Algorithm for Bridging Anaphora Resolution.</a></li>
<li><a href="#220. A Knowledge Hunting Framework for Common Sense Reasoning.">220. A Knowledge Hunting Framework for Common Sense Reasoning.</a></li>
<li><a href="#221. Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs.">221. Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs.</a></li>
<li><a href="#222. Differentiating Concepts and Instances for Knowledge Graph Embedding.">222. Differentiating Concepts and Instances for Knowledge Graph Embedding.</a></li>
<li><a href="#223. One-Shot Relational Learning for Knowledge Graphs.">223. One-Shot Relational Learning for Knowledge Graphs.</a></li>
<li><a href="#224. Regular Expression Guided Entity Mention Mining from Noisy Web Data.">224. Regular Expression Guided Entity Mention Mining from Noisy Web Data.</a></li>
<li><a href="#225. HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding.">225. HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding.</a></li>
<li><a href="#226. Neural Adaptation Layers for Cross-domain Named Entity Recognition.">226. Neural Adaptation Layers for Cross-domain Named Entity Recognition.</a></li>
<li><a href="#227. Entity Linking within a Social Media Platform: A Case Study on Yelp.">227. Entity Linking within a Social Media Platform: A Case Study on Yelp.</a></li>
<li><a href="#228. Annotation of a Large Clinical Entity Corpus.">228. Annotation of a Large Clinical Entity Corpus.</a></li>
<li><a href="#229. Visual Supervision in Bootstrapped Information Extraction.">229. Visual Supervision in Bootstrapped Information Extraction.</a></li>
<li><a href="#230. Learning Named Entity Tagger using Domain-Specific Dictionary.">230. Learning Named Entity Tagger using Domain-Specific Dictionary.</a></li>
<li><a href="#231. Zero-Shot Open Entity Typing as Type-Compatible Grounding.">231. Zero-Shot Open Entity Typing as Type-Compatible Grounding.</a></li>
<li><a href="#232. Attention-Guided Answer Distillation for Machine Reading Comprehension.">232. Attention-Guided Answer Distillation for Machine Reading Comprehension.</a></li>
<li><a href="#233. Interpretation of Natural Language Rules in Conversational Machine Reading.">233. Interpretation of Natural Language Rules in Conversational Machine Reading.</a></li>
<li><a href="#234. A State-transition Framework to Answer Complex Questions over Knowledge Base.">234. A State-transition Framework to Answer Complex Questions over Knowledge Base.</a></li>
<li><a href="#235. A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension.">235. A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension.</a></li>
<li><a href="#236. Logician and Orator: Learning from the Duality between Language and Knowledge in Open Domain.">236. Logician and Orator: Learning from the Duality between Language and Knowledge in Open Domain.</a></li>
<li><a href="#237. MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller.">237. MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller.</a></li>
<li><a href="#238. Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension.">238. Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension.</a></li>
<li><a href="#239. Neural Compositional Denotational Semantics for Question Answering.">239. Neural Compositional Denotational Semantics for Question Answering.</a></li>
<li><a href="#240. Cross-Pair Text Representations for Answer Sentence Selection.">240. Cross-Pair Text Representations for Answer Sentence Selection.</a></li>
<li><a href="#241. QuAC: Question Answering in Context.">241. QuAC: Question Answering in Context.</a></li>
<li><a href="#242. Knowledge Base Question Answering via Encoding of Complex Query Graphs.">242. Knowledge Base Question Answering via Encoding of Complex Query Graphs.</a></li>
<li><a href="#243. Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning.">243. Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning.</a></li>
<li><a href="#244. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.">244. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.</a></li>
<li><a href="#245. Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction.">245. Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction.</a></li>
<li><a href="#246. N-ary Relation Extraction using Graph-State LSTM.">246. N-ary Relation Extraction using Graph-State LSTM.</a></li>
<li><a href="#247. Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention.">247. Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention.</a></li>
<li><a href="#248. Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding.">248. Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding.</a></li>
<li><a href="#249. Extracting Entities and Relations with Joint Minimum Risk Training.">249. Extracting Entities and Relations with Joint Minimum Risk Training.</a></li>
<li><a href="#250. Large-scale Exploration of Neural Relation Classification Architectures.">250. Large-scale Exploration of Neural Relation Classification Architectures.</a></li>
<li><a href="#251. Possessors Change Over Time: A Case Study with Artworks.">251. Possessors Change Over Time: A Case Study with Artworks.</a></li>
<li><a href="#252. Using lexical alignment and referring ability to address data sparsity in situated dialog reference resolution.">252. Using lexical alignment and referring ability to address data sparsity in situated dialog reference resolution.</a></li>
<li><a href="#253. Subgoal Discovery for Hierarchical Dialogue Policy Learning.">253. Subgoal Discovery for Hierarchical Dialogue Policy Learning.</a></li>
<li><a href="#254. Supervised Clustering of Questions into Intents for Dialog System Applications.">254. Supervised Clustering of Questions into Intents for Dialog System Applications.</a></li>
<li><a href="#255. Towards Exploiting Background Knowledge for Building Conversation Systems.">255. Towards Exploiting Background Knowledge for Building Conversation Systems.</a></li>
<li><a href="#256. Decoupling Strategy and Generation in Negotiation Dialogues.">256. Decoupling Strategy and Generation in Negotiation Dialogues.</a></li>
<li><a href="#257. Large-scale Cloze Test Dataset Created by Teachers.">257. Large-scale Cloze Test Dataset Created by Teachers.</a></li>
<li><a href="#258. emrQA: A Large Corpus for Question Answering on Electronic Medical Records.">258. emrQA: A Large Corpus for Question Answering on Electronic Medical Records.</a></li>
<li><a href="#259. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.">259. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.</a></li>
<li><a href="#260. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering.">260. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering.</a></li>
<li><a href="#261. Evaluating Theory of Mind in Question Answering.">261. Evaluating Theory of Mind in Question Answering.</a></li>
<li><a href="#262. A Unified Syntax-aware Framework for Semantic Role Labeling.">262. A Unified Syntax-aware Framework for Semantic Role Labeling.</a></li>
<li><a href="#263. Semantics as a Foreign Language.">263. Semantics as a Foreign Language.</a></li>
<li><a href="#264. An AMR Aligner Tuned by Transition-based Parser.">264. An AMR Aligner Tuned by Transition-based Parser.</a></li>
<li><a href="#265. Dependency-based Hybrid Trees for Semantic Parsing.">265. Dependency-based Hybrid Trees for Semantic Parsing.</a></li>
<li><a href="#266. Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations.">266. Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations.</a></li>
<li><a href="#267. Sentence Compression for Arbitrary Languages via Multilingual Pivoting.">267. Sentence Compression for Arbitrary Languages via Multilingual Pivoting.</a></li>
<li><a href="#268. Unsupervised Cross-lingual Transfer of Word Embedding Spaces.">268. Unsupervised Cross-lingual Transfer of Word Embedding Spaces.</a></li>
<li><a href="#269. XNLI: Evaluating Cross-lingual Sentence Representations.">269. XNLI: Evaluating Cross-lingual Sentence Representations.</a></li>
<li><a href="#270. Joint Multilingual Supervision for Cross-lingual Entity Linking.">270. Joint Multilingual Supervision for Cross-lingual Entity Linking.</a></li>
<li><a href="#271. Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.">271. Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.</a></li>
<li><a href="#272. WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition.">272. WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition.</a></li>
<li><a href="#273. A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check.">273. A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check.</a></li>
<li><a href="#274. Neural Quality Estimation of Grammatical Error Correction.">274. Neural Quality Estimation of Grammatical Error Correction.</a></li>
<li><a href="#275. Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging.">275. Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging.</a></li>
<li><a href="#276. Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit.">276. Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit.</a></li>
<li><a href="#277. A Challenge Set and Methods for Noun-Verb Ambiguity.">277. A Challenge Set and Methods for Noun-Verb Ambiguity.</a></li>
<li><a href="#278. What do character-level models learn about morphology? The case of dependency parsing.">278. What do character-level models learn about morphology? The case of dependency parsing.</a></li>
<li><a href="#279. Learning Better Internal Structure of Words for Sequence Labeling.">279. Learning Better Internal Structure of Words for Sequence Labeling.</a></li>
<li><a href="#280. ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection.">280. ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection.</a></li>
<li><a href="#281. Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation.">281. Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation.</a></li>
<li><a href="#282. Grounding Semantic Roles in Images.">282. Grounding Semantic Roles in Images.</a></li>
<li><a href="#283. Commonsense Justification for Action Explanation.">283. Commonsense Justification for Action Explanation.</a></li>
<li><a href="#284. Learning Personas from Dialogue with Attentive Memory Networks.">284. Learning Personas from Dialogue with Attentive Memory Networks.</a></li>
<li><a href="#285. Grounding language acquisition by training semantic parsers using captioned videos.">285. Grounding language acquisition by training semantic parsers using captioned videos.</a></li>
<li><a href="#286. Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.">286. Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.</a></li>
<li><a href="#287. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction.">287. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction.</a></li>
<li><a href="#288. Deconvolutional time series regression: A technique for modeling temporally diffuse effects.">288. Deconvolutional time series regression: A technique for modeling temporally diffuse effects.</a></li>
<li><a href="#289. Is this Sentence Difficult? Do you Agree?">289. Is this Sentence Difficult? Do you Agree?</a></li>
<li><a href="#290. Neural Transition Based Parsing of Web Queries: An Entity Based Approach.">290. Neural Transition Based Parsing of Web Queries: An Entity Based Approach.</a></li>
<li><a href="#291. An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing.">291. An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing.</a></li>
<li><a href="#292. Depth-bounding is effective: Improvements and Evaluation of Unsupervised PCFG Induction.">292. Depth-bounding is effective: Improvements and Evaluation of Unsupervised PCFG Induction.</a></li>
<li><a href="#293. Incremental Computation of Infix Probabilities for Probabilistic Finite Automata.">293. Incremental Computation of Infix Probabilities for Probabilistic Finite Automata.</a></li>
<li><a href="#294. Syntax Encoding with Application in Authorship Attribution.">294. Syntax Encoding with Application in Authorship Attribution.</a></li>
<li><a href="#295. Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks.">295. Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks.</a></li>
<li><a href="#296. Session-level Language Modeling for Conversational Speech.">296. Session-level Language Modeling for Conversational Speech.</a></li>
<li><a href="#297. Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method.">297. Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method.</a></li>
<li><a href="#298. Training Millions of Personalized Dialogue Agents.">298. Training Millions of Personalized Dialogue Agents.</a></li>
<li><a href="#299. Towards Universal Dialogue State Tracking.">299. Towards Universal Dialogue State Tracking.</a></li>
<li><a href="#300. Semantic Parsing for Task Oriented Dialog using Hierarchical Representations.">300. Semantic Parsing for Task Oriented Dialog using Hierarchical Representations.</a></li>
<li><a href="#301. The glass ceiling in NLP.">301. The glass ceiling in NLP.</a></li>
<li><a href="#302. Reducing Gender Bias in Abusive Language Detection.">302. Reducing Gender Bias in Abusive Language Detection.</a></li>
<li><a href="#303. SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories.">303. SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories.</a></li>
<li><a href="#304. Learning multiview embeddings for assessing dementia.">304. Learning multiview embeddings for assessing dementia.</a></li>
<li><a href="#305. WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community.">305. WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community.</a></li>
<li><a href="#306. Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets.">306. Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets.</a></li>
<li><a href="#307. Adversarial training for multi-context joint entity and relation extraction.">307. Adversarial training for multi-context joint entity and relation extraction.</a></li>
<li><a href="#308. Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding.">308. Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding.</a></li>
<li><a href="#309. Deep Exhaustive Model for Nested Named Entity Recognition.">309. Deep Exhaustive Model for Nested Named Entity Recognition.</a></li>
<li><a href="#310. Evaluating the Utility of Hand-crafted Features in Sequence Labeling.">310. Evaluating the Utility of Hand-crafted Features in Sequence Labeling.</a></li>
<li><a href="#311. Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data.">311. Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data.</a></li>
<li><a href="#312. A Framework for Understanding the Role of Morphology in Universal Dependency Parsing.">312. A Framework for Understanding the Role of Morphology in Universal Dependency Parsing.</a></li>
<li><a href="#313. The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.">313. The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.</a></li>
<li><a href="#314. Imitation Learning for Neural Morphological String Transduction.">314. Imitation Learning for Neural Morphological String Transduction.</a></li>
<li><a href="#315. An Encoder-Decoder Approach to the Paradigm Cell Filling Problem.">315. An Encoder-Decoder Approach to the Paradigm Cell Filling Problem.</a></li>
<li><a href="#316. Generating Natural Language Adversarial Examples.">316. Generating Natural Language Adversarial Examples.</a></li>
<li><a href="#317. Multi-Head Attention with Disagreement Regularization.">317. Multi-Head Attention with Disagreement Regularization.</a></li>
<li><a href="#318. Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.">318. Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.</a></li>
<li><a href="#319. Bayesian Compression for Natural Language Processing.">319. Bayesian Compression for Natural Language Processing.</a></li>
<li><a href="#320. Multimodal neural pronunciation modeling for spoken languages with logographic origin.">320. Multimodal neural pronunciation modeling for spoken languages with logographic origin.</a></li>
<li><a href="#321. Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet.">321. Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet.</a></li>
<li><a href="#322. Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models.">322. Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models.</a></li>
<li><a href="#323. How to represent a word and predict it, too: improving tied architectures for language modelling.">323. How to represent a word and predict it, too: improving tied architectures for language modelling.</a></li>
<li><a href="#324. The Importance of Generation Order in Language Modeling.">324. The Importance of Generation Order in Language Modeling.</a></li>
<li><a href="#325. Document-Level Neural Machine Translation with Hierarchical Attention Networks.">325. Document-Level Neural Machine Translation with Hierarchical Attention Networks.</a></li>
<li><a href="#326. Three Strategies to Improve One-to-Many Multilingual Translation.">326. Three Strategies to Improve One-to-Many Multilingual Translation.</a></li>
<li><a href="#327. Multi-Source Syntactic Neural Machine Translation.">327. Multi-Source Syntactic Neural Machine Translation.</a></li>
<li><a href="#328. Fixing Translation Divergences in Parallel Corpora for Neural MT.">328. Fixing Translation Divergences in Parallel Corpora for Neural MT.</a></li>
<li><a href="#329. Adversarial Evaluation of Multimodal Machine Translation.">329. Adversarial Evaluation of Multimodal Machine Translation.</a></li>
<li><a href="#330. Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion.">330. Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion.</a></li>
<li><a href="#331. Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.">331. Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.</a></li>
<li><a href="#332. Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation.">332. Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation.</a></li>
<li><a href="#333. Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism.">333. Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism.</a></li>
<li><a href="#334. Getting Gender Right in Neural MT.">334. Getting Gender Right in Neural MT.</a></li>
<li><a href="#335. Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation.">335. Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation.</a></li>
<li><a href="#336. End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification.">336. End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification.</a></li>
<li><a href="#337. Prediction Improves Simultaneous Neural Machine Translation.">337. Prediction Improves Simultaneous Neural Machine Translation.</a></li>
<li><a href="#338. Training Deeper Neural Machine Translation Models with Transparent Attention.">338. Training Deeper Neural Machine Translation Models with Transparent Attention.</a></li>
<li><a href="#339. Context and Copying in Neural Machine Translation.">339. Context and Copying in Neural Machine Translation.</a></li>
<li><a href="#340. Encoding Gated Translation Memory into Neural Machine Translation.">340. Encoding Gated Translation Memory into Neural Machine Translation.</a></li>
<li><a href="#341. Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach.">341. Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach.</a></li>
<li><a href="#342. Breaking the Beam Search Curse: A Study of (Re-">342. Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation.</a>Scoring Methods and Stopping Criteria for Neural Machine Translation.)</li>
<li><a href="#343. Multi-View Learning: Multilingual and Multi-Representation Entity Typing.">343. Multi-View Learning: Multilingual and Multi-Representation Entity Typing.</a></li>
<li><a href="#344. Word Embeddings for Code-Mixed Language Processing.">344. Word Embeddings for Code-Mixed Language Processing.</a></li>
<li><a href="#345. On the Strength of Character Language Models for Multilingual Named Entity Recognition.">345. On the Strength of Character Language Models for Multilingual Named Entity Recognition.</a></li>
<li><a href="#346. Code-switched Language Models Using Dual RNNs and Same-Source Pretraining.">346. Code-switched Language Models Using Dual RNNs and Same-Source Pretraining.</a></li>
<li><a href="#347. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification.">347. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification.</a></li>
<li><a href="#348. Zero-shot User Intent Detection via Capsule Neural Networks.">348. Zero-shot User Intent Detection via Capsule Neural Networks.</a></li>
<li><a href="#349. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts.">349. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts.</a></li>
<li><a href="#350. Investigating Capsule Networks with Dynamic Routing for Text Classification.">350. Investigating Capsule Networks with Dynamic Routing for Text Classification.</a></li>
<li><a href="#351. Topic Memory Networks for Short Text Classification.">351. Topic Memory Networks for Short Text Classification.</a></li>
<li><a href="#352. Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces.">352. Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces.</a></li>
<li><a href="#353. Automatic Poetry Generation with Mutual Reinforcement Learning.">353. Automatic Poetry Generation with Mutual Reinforcement Learning.</a></li>
<li><a href="#354. Variational Autoregressive Decoder for Neural Response Generation.">354. Variational Autoregressive Decoder for Neural Response Generation.</a></li>
<li><a href="#355. Integrating Transformer and Paraphrase Rules for Sentence Simplification.">355. Integrating Transformer and Paraphrase Rules for Sentence Simplification.</a></li>
<li><a href="#356. Learning Neural Templates for Text Generation.">356. Learning Neural Templates for Text Generation.</a></li>
<li><a href="#357. Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation.">357. Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation.</a></li>
<li><a href="#358. Knowledge Graph Embedding with Hierarchical Relation Structure.">358. Knowledge Graph Embedding with Hierarchical Relation Structure.</a></li>
<li><a href="#359. Embedding Multimodal Relational Data for Knowledge Base Completion.">359. Embedding Multimodal Relational Data for Knowledge Base Completion.</a></li>
<li><a href="#360. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction.">360. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction.</a></li>
<li><a href="#361. Playing 20 Question Game with Policy-Based Reinforcement Learning.">361. Playing 20 Question Game with Policy-Based Reinforcement Learning.</a></li>
<li><a href="#362. Multi-Hop Knowledge Graph Reasoning with Reward Shaping.">362. Multi-Hop Knowledge Graph Reasoning with Reward Shaping.</a></li>
<li><a href="#363. Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting.">363. Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting.</a></li>
<li><a href="#364. Implicational Universals in Stochastic Constraint-Based Phonology.">364. Implicational Universals in Stochastic Constraint-Based Phonology.</a></li>
<li><a href="#365. Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?">365. Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?</a></li>
<li><a href="#366. Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations.">366. Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations.</a></li>
<li><a href="#367. A Computational Exploration of Exaggeration.">367. A Computational Exploration of Exaggeration.</a></li>
<li><a href="#368. Building Context-aware Clause Representations for Situation Entity Type Classification.">368. Building Context-aware Clause Representations for Situation Entity Type Classification.</a></li>
<li><a href="#369. Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain.">369. Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain.</a></li>
<li><a href="#370. Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models.">370. Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models.</a></li>
<li><a href="#371. Neural Ranking Models for Temporal Dependency Structure Parsing.">371. Neural Ranking Models for Temporal Dependency Structure Parsing.</a></li>
<li><a href="#372. Causal Explanation Analysis on Social Media.">372. Causal Explanation Analysis on Social Media.</a></li>
<li><a href="#373. LRMM: Learning to Recommend with Missing Modalities.">373. LRMM: Learning to Recommend with Missing Modalities.</a></li>
<li><a href="#374. Content Explorer: Recommending Novel Entities for a Document Writer.">374. Content Explorer: Recommending Novel Entities for a Document Writer.</a></li>
<li><a href="#375. A Genre-Aware Attention Model to Improve the Likability Prediction of Books.">375. A Genre-Aware Attention Model to Improve the Likability Prediction of Books.</a></li>
<li><a href="#376. Thread Popularity Prediction and Tracking with a Permutation-invariant Model.">376. Thread Popularity Prediction and Tracking with a Permutation-invariant Model.</a></li>
<li><a href="#377. IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis.">377. IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis.</a></li>
<li><a href="#378. Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations.">378. Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations.</a></li>
<li><a href="#379. An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking.">379. An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking.</a></li>
<li><a href="#380. Multi-grained Attention Network for Aspect-Level Sentiment Classification.">380. Multi-grained Attention Network for Aspect-Level Sentiment Classification.</a></li>
<li><a href="#381. Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification.">381. Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification.</a></li>
<li><a href="#382. Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.">382. Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.</a></li>
<li><a href="#383. Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification.">383. Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification.</a></li>
<li><a href="#384. ExtRA: Extracting Prominent Review Aspects from Customer Feedback.">384. ExtRA: Extracting Prominent Review Aspects from Customer Feedback.</a></li>
<li><a href="#385. Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content.">385. Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content.</a></li>
<li><a href="#386. Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts.">386. Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts.</a></li>
<li><a href="#387. Automatic Detection of Vague Words and Sentences in Privacy Policies.">387. Automatic Detection of Vague Words and Sentences in Privacy Policies.</a></li>
<li><a href="#388. Multi-view Models for Political Ideology Detection of News Articles.">388. Multi-view Models for Political Ideology Detection of News Articles.</a></li>
<li><a href="#389. Predicting Factuality of Reporting and Bias of News Media Sources.">389. Predicting Factuality of Reporting and Bias of News Media Sources.</a></li>
<li><a href="#390. Legal Judgment Prediction via Topological Learning.">390. Legal Judgment Prediction via Topological Learning.</a></li>
<li><a href="#391. Hierarchical CVAE for Fine-Grained Hate Speech Classification.">391. Hierarchical CVAE for Fine-Grained Hate Speech Classification.</a></li>
<li><a href="#392. Residualized Factor Adaptation for Community Social Media Prediction Tasks.">392. Residualized Factor Adaptation for Community Social Media Prediction Tasks.</a></li>
<li><a href="#393. Framing and Agenda-Setting in Russian News: a Computational Analysis of Intricate Political Strategies.">393. Framing and Agenda-Setting in Russian News: a Computational Analysis of Intricate Political Strategies.</a></li>
<li><a href="#394. Identifying the narrative styles of YouTube's vloggers.">394. Identifying the narrative styles of YouTube's vloggers.</a></li>
<li><a href="#395. Native Language Identification with User Generated Content.">395. Native Language Identification with User Generated Content.</a></li>
<li><a href="#396. Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.">396. Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.</a></li>
<li><a href="#397. A Study of Reinforcement Learning for Neural Machine Translation.">397. A Study of Reinforcement Learning for Neural Machine Translation.</a></li>
<li><a href="#398. Meta-Learning for Low-Resource Neural Machine Translation.">398. Meta-Learning for Low-Resource Neural Machine Translation.</a></li>
<li><a href="#399. Unsupervised Statistical Machine Translation.">399. Unsupervised Statistical Machine Translation.</a></li>
<li><a href="#400. A Visual Attention Grounding Neural Model for Multimodal Machine Translation.">400. A Visual Attention Grounding Neural Model for Multimodal Machine Translation.</a></li>
<li><a href="#401. Sentiment Classification towards Question-Answering with Hierarchical Matching Network.">401. Sentiment Classification towards Question-Answering with Hierarchical Matching Network.</a></li>
<li><a href="#402. Cross-topic Argument Mining from Heterogeneous Sources.">402. Cross-topic Argument Mining from Heterogeneous Sources.</a></li>
<li><a href="#403. Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised.">403. Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised.</a></li>
<li><a href="#404. CARER: Contextualized Affect Representations for Emotion Recognition.">404. CARER: Contextualized Affect Representations for Emotion Recognition.</a></li>
<li><a href="#405. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency.">405. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency.</a></li>
<li><a href="#406. CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization.">406. CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization.</a></li>
<li><a href="#407. Pathologies of Neural Models Make Interpretation Difficult.">407. Pathologies of Neural Models Make Interpretation Difficult.</a></li>
<li><a href="#408. Phrase-level Self-Attention Networks for Universal Sentence Encoding.">408. Phrase-level Self-Attention Networks for Universal Sentence Encoding.</a></li>
<li><a href="#409. BanditSum: Extractive Summarization as a Contextual Bandit.">409. BanditSum: Extractive Summarization as a Contextual Bandit.</a></li>
<li><a href="#410. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification.">410. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification.</a></li>
<li><a href="#411. Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.">411. Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.</a></li>
<li><a href="#412. Syntactic Scaffolds for Semantic Structures.">412. Syntactic Scaffolds for Semantic Structures.</a></li>
<li><a href="#413. Hierarchical Quantized Representations for Script Generation.">413. Hierarchical Quantized Representations for Script Generation.</a></li>
<li><a href="#414. Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.">414. Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.</a></li>
<li><a href="#415. A Teacher-Student Framework for Maintainable Dialog Manager.">415. A Teacher-Student Framework for Maintainable Dialog Manager.</a></li>
<li><a href="#416. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning.">416. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning.</a></li>
<li><a href="#417. A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding.">417. A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding.</a></li>
<li><a href="#418. Learning End-to-End Goal-Oriented Dialog with Multiple Answers.">418. Learning End-to-End Goal-Oriented Dialog with Multiple Answers.</a></li>
<li><a href="#419. AirDialogue: An Environment for Goal-Oriented Dialogue Research.">419. AirDialogue: An Environment for Goal-Oriented Dialogue Research.</a></li>
<li><a href="#420. QuaSE: Sequence Editing under Quantifiable Guidance.">420. QuaSE: Sequence Editing under Quantifiable Guidance.</a></li>
<li><a href="#421. Paraphrase Generation with Deep Reinforcement Learning.">421. Paraphrase Generation with Deep Reinforcement Learning.</a></li>
<li><a href="#422. Operation-guided Neural Networks for High Fidelity Data-To-Text Generation.">422. Operation-guided Neural Networks for High Fidelity Data-To-Text Generation.</a></li>
<li><a href="#423. Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training.">423. Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training.</a></li>
<li><a href="#424. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks.">424. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks.</a></li>
<li><a href="#425. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.">425. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.</a></li>
<li><a href="#426. Unsupervised Natural Language Generation with Denoising Autoencoders.">426. Unsupervised Natural Language Generation with Denoising Autoencoders.</a></li>
<li><a href="#427. Answer-focused and Position-aware Neural Question Generation.">427. Answer-focused and Position-aware Neural Question Generation.</a></li>
<li><a href="#428. Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation.">428. Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation.</a></li>
<li><a href="#429. Towards a Better Metric for Evaluating Question Generation Systems.">429. Towards a Better Metric for Evaluating Question Generation Systems.</a></li>
<li><a href="#430. Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement.">430. Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement.</a></li>
<li><a href="#431. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints.">431. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints.</a></li>
<li><a href="#432. Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity.">432. Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity.</a></li>
<li><a href="#433. Incorporating Background Knowledge into Video Description Generation.">433. Incorporating Background Knowledge into Video Description Generation.</a></li>
<li><a href="#434. Multimodal Differential Network for Visual Question Generation.">434. Multimodal Differential Network for Visual Question Generation.</a></li>
<li><a href="#435. Entity-aware Image Caption Generation.">435. Entity-aware Image Caption Generation.</a></li>
<li><a href="#436. Learning to Describe Differences Between Pairs of Similar Images.">436. Learning to Describe Differences Between Pairs of Similar Images.</a></li>
<li><a href="#437. Object Hallucination in Image Captioning.">437. Object Hallucination in Image Captioning.</a></li>
<li><a href="#438. Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN.">438. Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN.</a></li>
<li><a href="#439. Keyphrase Generation with Correlation Constraints.">439. Keyphrase Generation with Correlation Constraints.</a></li>
<li><a href="#440. Closed-Book Training to Improve Summarization Encoder Memory.">440. Closed-Book Training to Improve Summarization Encoder Memory.</a></li>
<li><a href="#441. Improving Neural Abstractive Document Summarization with Structural Regularization.">441. Improving Neural Abstractive Document Summarization with Structural Regularization.</a></li>
<li><a href="#442. Iterative Document Representation Learning Towards Summarization with Polishing.">442. Iterative Document Representation Learning Towards Summarization with Polishing.</a></li>
<li><a href="#443. Bottom-Up Abstractive Summarization.">443. Bottom-Up Abstractive Summarization.</a></li>
<li><a href="#444. Controlling Length in Abstractive Summarization Using a Convolutional Neural Network.">444. Controlling Length in Abstractive Summarization Using a Convolutional Neural Network.</a></li>
<li><a href="#445. APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning.">445. APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning.</a></li>
<li><a href="#446. Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.">446. Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.</a></li>
<li><a href="#447. Semi-Supervised Learning for Neural Keyphrase Generation.">447. Semi-Supervised Learning for Neural Keyphrase Generation.</a></li>
<li><a href="#448. MSMO: Multimodal Summarization with Multimodal Output.">448. MSMO: Multimodal Summarization with Multimodal Output.</a></li>
<li><a href="#449. Frustratingly Easy Model Ensemble for Abstractive Summarization.">449. Frustratingly Easy Model Ensemble for Abstractive Summarization.</a></li>
<li><a href="#450. Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.">450. Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.</a></li>
<li><a href="#451. Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks.">451. Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks.</a></li>
<li><a href="#452. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings.">452. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings.</a></li>
<li><a href="#453. What Makes Reading Comprehension Questions Easier?">453. What Makes Reading Comprehension Questions Easier?</a></li>
<li><a href="#454. Commonsense for Generative Multi-Hop Question Answering Tasks.">454. Commonsense for Generative Multi-Hop Question Answering Tasks.</a></li>
<li><a href="#455. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.">455. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.</a></li>
<li><a href="#456. A Nil-Aware Answer Extraction Framework for Question Answering.">456. A Nil-Aware Answer Extraction Framework for Question Answering.</a></li>
<li><a href="#457. Exploiting Deep Representations for Neural Machine Translation.">457. Exploiting Deep Representations for Neural Machine Translation.</a></li>
<li><a href="#458. Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.">458. Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.</a></li>
<li><a href="#459. Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.">459. Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.</a></li>
<li><a href="#460. Speeding Up Neural Machine Translation Decoding by Cube Pruning.">460. Speeding Up Neural Machine Translation Decoding by Cube Pruning.</a></li>
<li><a href="#461. Revisiting Character-Based Neural Machine Translation with Capacity and Compression.">461. Revisiting Character-Based Neural Machine Translation with Capacity and Compression.</a></li>
<li><a href="#462. A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation.">462. A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation.</a></li>
<li><a href="#463. Nexus Network: Connecting the Preceding and the Following in Dialogue Generation.">463. Nexus Network: Connecting the Preceding and the Following in Dialogue Generation.</a></li>
<li><a href="#464. A Neural Local Coherence Model for Text Quality Assessment.">464. A Neural Local Coherence Model for Text Quality Assessment.</a></li>
<li><a href="#465. Deep Attentive Sentence Ordering Network.">465. Deep Attentive Sentence Ordering Network.</a></li>
<li><a href="#466. Getting to "Hearer-old": Charting Referring Expressions Across Time.">466. Getting to "Hearer-old": Charting Referring Expressions Across Time.</a></li>
<li><a href="#467. Making "fetch" happen: The influence of social and linguistic context on nonstandard word growth and decline.">467. Making "fetch" happen: The influence of social and linguistic context on nonstandard word growth and decline.</a></li>
<li><a href="#468. Analyzing Correlated Evolution of Multiple Features Using Latent Representations.">468. Analyzing Correlated Evolution of Multiple Features Using Latent Representations.</a></li>
<li><a href="#469. Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting.">469. Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting.</a></li>
<li><a href="#470. Characterizing Interactions and Relationships between People.">470. Characterizing Interactions and Relationships between People.</a></li>
<li><a href="#471. Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions.">471. Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions.</a></li>
<li><a href="#472. Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks.">472. Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks.</a></li>
<li><a href="#473. Hard Non-Monotonic Attention for Character-Level Transduction.">473. Hard Non-Monotonic Attention for Character-Level Transduction.</a></li>
<li><a href="#474. Speed Reading: Learning to Read ForBackward via Shuttle.">474. Speed Reading: Learning to Read ForBackward via Shuttle.</a></li>
<li><a href="#475. Modeling Localness for Self-Attention Networks.">475. Modeling Localness for Self-Attention Networks.</a></li>
<li><a href="#476. Chargrid: Towards Understanding 2D Documents.">476. Chargrid: Towards Understanding 2D Documents.</a></li>
<li><a href="#477. Simple Recurrent Units for Highly Parallelizable Recurrence.">477. Simple Recurrent Units for Highly Parallelizable Recurrence.</a></li>
<li><a href="#478. NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval.">478. NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval.</a></li>
<li><a href="#479. Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences.">479. Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences.</a></li>
<li><a href="#480. Spherical Latent Spaces for Stable Variational Autoencoders.">480. Spherical Latent Spaces for Stable Variational Autoencoders.</a></li>
<li><a href="#481. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder.">481. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder.</a></li>
<li><a href="#482. Word Mover's Embedding: From Word2Vec to Document Embedding.">482. Word Mover's Embedding: From Word2Vec to Document Embedding.</a></li>
<li><a href="#483. Multilingual Clustering of Streaming News.">483. Multilingual Clustering of Streaming News.</a></li>
<li><a href="#484. Multi-Task Label Embedding for Text Classification.">484. Multi-Task Label Embedding for Text Classification.</a></li>
<li><a href="#485. Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification.">485. Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification.</a></li>
<li><a href="#486. MCapsNet: Capsule Network for Text with Multi-Task Learning.">486. MCapsNet: Capsule Network for Text with Multi-Task Learning.</a></li>
<li><a href="#487. Uncertainty-aware generative models for inferring document class prevalence.">487. Uncertainty-aware generative models for inferring document class prevalence.</a></li>
<li><a href="#488. Challenges of Using Text Classifiers for Causal Inference.">488. Challenges of Using Text Classifiers for Causal Inference.</a></li>
<li><a href="#489. Direct Output Connection for a High-Rank Language Model.">489. Direct Output Connection for a High-Rank Language Model.</a></li>
<li><a href="#490. Disfluency Detection using Auto-Correlational Neural Networks.">490. Disfluency Detection using Auto-Correlational Neural Networks.</a></li>
<li><a href="#491. Pyramidal Recurrent Unit for Language Modeling.">491. Pyramidal Recurrent Unit for Language Modeling.</a></li>
<li><a href="#492. On Tree-Based Neural Sentence Modeling.">492. On Tree-Based Neural Sentence Modeling.</a></li>
<li><a href="#493. Language Modeling with Sparse Product of Sememe Experts.">493. Language Modeling with Sparse Product of Sememe Experts.</a></li>
<li><a href="#494. Siamese Network-Based Supervised Topic Modeling.">494. Siamese Network-Based Supervised Topic Modeling.</a></li>
<li><a href="#495. GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model.">495. GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model.</a></li>
<li><a href="#496. Modeling Online Discourse with Coupled Distributed Topics.">496. Modeling Online Discourse with Coupled Distributed Topics.</a></li>
<li><a href="#497. Learning Disentangled Representations of Texts with Application to Biomedical Abstracts.">497. Learning Disentangled Representations of Texts with Application to Biomedical Abstracts.</a></li>
<li><a href="#498. Multi-Source Domain Adaptation with Mixture of Experts.">498. Multi-Source Domain Adaptation with Mixture of Experts.</a></li>
<li><a href="#499. A Neural Model of Adaptation in Reading.">499. A Neural Model of Adaptation in Reading.</a></li>
<li><a href="#500. Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study.">500. Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study.</a></li>
<li><a href="#501. Lexicosyntactic inference in neural models.">501. Lexicosyntactic inference in neural models.</a></li>
<li><a href="#502. Dual Fixed-Size Ordinally Forgetting Encoding (FOFE">502. Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models.</a> for Competitive Neural Language Models.)</li>
<li><a href="#503. The importance of Being Recurrent for Modeling Hierarchical Structure.">503. The importance of Being Recurrent for Modeling Hierarchical Structure.</a></li>
<li><a href="#504. Joint Learning for Targeted Sentiment Analysis.">504. Joint Learning for Targeted Sentiment Analysis.</a></li>
<li><a href="#505. Revisiting the Importance of Encoding Logic Rules in Sentiment Classification.">505. Revisiting the Importance of Encoding Logic Rules in Sentiment Classification.</a></li>
<li><a href="#506. A Co-attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness.">506. A Co-attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness.</a></li>
<li><a href="#507. Modeling Empathy and Distress in Reaction to News Stories.">507. Modeling Empathy and Distress in Reaction to News Stories.</a></li>
<li><a href="#508. Interpretable Emoji Prediction via Label-Wise Attention LSTMs.">508. Interpretable Emoji Prediction via Label-Wise Attention LSTMs.</a></li>
<li><a href="#509. A Tree-based Decoder for Neural Machine Translation.">509. A Tree-based Decoder for Neural Machine Translation.</a></li>
<li><a href="#510. Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation.">510. Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation.</a></li>
<li><a href="#511. Exploring Recombination for Efficient Decoding of Neural Machine Translation.">511. Exploring Recombination for Efficient Decoding of Neural Machine Translation.</a></li>
<li><a href="#512. Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation.">512. Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation.</a></li>
<li><a href="#513. Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point.">513. Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point.</a></li>
<li><a href="#514. FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation.">514. FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation.</a></li>
<li><a href="#515. A strong baseline for question relevancy ranking.">515. A strong baseline for question relevancy ranking.</a></li>
<li><a href="#516. Learning Sequence Encoders for Temporal Knowledge Graph Completion.">516. Learning Sequence Encoders for Temporal Knowledge Graph Completion.</a></li>
<li><a href="#517. Similar but not the Same - Word Sense Disambiguation Improves Event Detection via Neural Representation Matching.">517. Similar but not the Same - Word Sense Disambiguation Improves Event Detection via Neural Representation Matching.</a></li>
<li><a href="#518. Learning Word Representations with Cross-Sentence Dependencyfor End-to-End Co-reference Resolution.">518. Learning Word Representations with Cross-Sentence Dependencyfor End-to-End Co-reference Resolution.</a></li>
<li><a href="#519. Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings.">519. Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings.</a></li>
<li><a href="#520. Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation.">520. Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation.</a></li>
<li><a href="#521. Learning Gender-Neutral Word Embeddings.">521. Learning Gender-Neutral Word Embeddings.</a></li>
<li><a href="#522. Learning Concept Abstractness Using Weak Supervision.">522. Learning Concept Abstractness Using Weak Supervision.</a></li>
<li><a href="#523. Word Sense Induction with Neural biLM and Symmetric Patterns.">523. Word Sense Induction with Neural biLM and Symmetric Patterns.</a></li>
<li><a href="#524. InferLite: Simple Universal Sentence Representations from Natural Language Inference Data.">524. InferLite: Simple Universal Sentence Representations from Natural Language Inference Data.</a></li>
<li><a href="#525. Similarity-Based Reconstruction Loss for Meaning Representation.">525. Similarity-Based Reconstruction Loss for Meaning Representation.</a></li>
<li><a href="#526. What can we learn from Semantic Tagging?">526. What can we learn from Semantic Tagging?</a></li>
<li><a href="#527. Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop.">527. Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop.</a></li>
<li><a href="#528. Classifying Referential and Non-referential It Using Gaze.">528. Classifying Referential and Non-referential It Using Gaze.</a></li>
<li><a href="#529. State-of-the-art Chinese Word Segmentation with Bi-LSTMs.">529. State-of-the-art Chinese Word Segmentation with Bi-LSTMs.</a></li>
<li><a href="#530. Sanskrit Sandhi Splitting using seq2(seq">530. Sanskrit Sandhi Splitting using seq2(seq)2.</a>2.)</li>
<li><a href="#531. Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling.">531. Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling.</a></li>
<li><a href="#532. LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs.">532. LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs.</a></li>
<li><a href="#533. Recovering Missing Characters in Old Hawaiian Writing.">533. Recovering Missing Characters in Old Hawaiian Writing.</a></li>
<li><a href="#534. When data permutations are pathological: the case of neural natural language inference.">534. When data permutations are pathological: the case of neural natural language inference.</a></li>
<li><a href="#535. Bridging Knowledge Gaps in Neural Entailment via Symbolic Models.">535. Bridging Knowledge Gaps in Neural Entailment via Symbolic Models.</a></li>
<li><a href="#536. The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.">536. The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.</a></li>
<li><a href="#537. Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.">537. Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.</a></li>
<li><a href="#538. Towards Semi-Supervised Learning for Deep Semantic Role Labeling.">538. Towards Semi-Supervised Learning for Deep Semantic Role Labeling.</a></li>
<li><a href="#539. Identifying Domain Adjacent Instances for Semantic Parsers.">539. Identifying Domain Adjacent Instances for Semantic Parsers.</a></li>
<li><a href="#540. Mapping natural language commands to web elements.">540. Mapping natural language commands to web elements.</a></li>
<li><a href="#541. Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection.">541. Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection.</a></li>
<li><a href="#542. Modeling Input Uncertainty in Neural Network Dependency Parsing.">542. Modeling Input Uncertainty in Neural Network Dependency Parsing.</a></li>
<li><a href="#543. Parameter sharing between dependency parsers for related languages.">543. Parameter sharing between dependency parsers for related languages.</a></li>
<li><a href="#544. Grammar Induction with Neural Language Models: An Unusual Replication.">544. Grammar Induction with Neural Language Models: An Unusual Replication.</a></li>
<li><a href="#545. Data Augmentation via Dependency Tree Morphing for Low-Resource Languages.">545. Data Augmentation via Dependency Tree Morphing for Low-Resource Languages.</a></li>
<li><a href="#546. How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks.">546. How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks.</a></li>
<li><a href="#547. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.">547. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.</a></li>
<li><a href="#548. Linguistically-Informed Self-Attention for Semantic Role Labeling.">548. Linguistically-Informed Self-Attention for Semantic Role Labeling.</a></li>
<li><a href="#549. Phrase-Based & Neural Unsupervised Machine Translation.">549. Phrase-Based &amp; Neural Unsupervised Machine Translation.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="EMNLP 2018:Brussels, Belgium">EMNLP 2018:Brussels, Belgium</h1>
<p><a href="https://www.aclweb.org/anthology/volumes/D18-1/">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018.</a> Association for Computational Linguistics
【<a href="https://dblp.uni-trier.de/db/conf/emnlp/emnlp2018.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 549 || Session Num: 0">Paper Num: 549 || Session Num: 0</h2>
<h3 id="1. Privacy-preserving Neural Representations of Text.">1. Privacy-preserving Neural Representations of Text.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1001/">Paper Link</a>】    【Pages】:1-10</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Coavoux:Maximin">Maximin Coavoux</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Narayan:Shashi">Shashi Narayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a></p>
<p>【Abstract】:
This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user’s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.</p>
<p>【Keywords】:</p>
<h3 id="2. Adversarial Removal of Demographic Attributes from Text Data.">2. Adversarial Removal of Demographic Attributes from Text Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1002/">Paper Link</a>】    【Pages】:11-21</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Elazar:Yanai">Yanai Elazar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldberg:Yoav">Yoav Goldberg</a></p>
<p>【Abstract】:
Recent advances in Representation Learning and Adversarial Training seem to succeed in removing unwanted features from the learned representation. We show that demographic information of authors is encoded in—and can be recovered from—the intermediate representations learned by text-based neural classifiers. The implication is that decisions of classifiers trained on textual data are not agnostic to—and likely condition on—demographic attributes. When attempting to remove such demographic information using adversarial training, we find that while the adversarial component achieves chance-level development-set accuracy during training, a post-hoc classifier, trained on the encoded sentences from the first part, still manages to reach substantially higher classification accuracies on the same data. This behavior is consistent across several tasks, demographic properties and datasets. We explore several techniques to improve the effectiveness of the adversarial component. Our main conclusion is a cautionary one: do not rely on the adversarial training to achieve invariant representation to sensitive features.</p>
<p>【Keywords】:</p>
<h3 id="3. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning.">3. DeClarE: Debunking Fake News and False Claims using Evidence-Aware Deep Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1003/">Paper Link</a>】    【Pages】:22-32</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Popat:Kashyap">Kashyap Popat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee:Subhabrata">Subhabrata Mukherjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yates:Andrew">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
Misinformation such as fake news is one of the big challenges of our society. Research on automated fact-checking has proposed methods based on supervised learning, but these approaches do not consider external evidence apart from labeled training instances. Recent approaches counter this deficit by considering external sources related to a claim. However, these methods require substantial feature modeling and rich lexicons. This paper overcomes these limitations of prior work with an end-to-end model for evidence-aware credibility assessment of arbitrary textual claims, without any human intervention. It presents a neural network model that judiciously aggregates signals from external evidence articles, the language of these articles and the trustworthiness of their sources. It also derives informative features for generating user-comprehensible explanations that makes the neural network predictions transparent to the end-user. Experiments with four datasets and ablation studies show the strength of our method.</p>
<p>【Keywords】:</p>
<h3 id="4. It's going to be okay: Measuring Access to Support in Online Communities.">4. It's going to be okay: Measuring Access to Support in Online Communities.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1004/">Paper Link</a>】    【Pages】:33-45</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zijian">Zijian Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurgens:David">David Jurgens</a></p>
<p>【Abstract】:
People use online platforms to seek out support for their informational and emotional needs. Here, we ask what effect does revealing one’s gender have on receiving support. To answer this, we create (i) a new dataset and method for identifying supportive replies and (ii) new methods for inferring gender from text and name. We apply these methods to create a new massive corpus of 102M online interactions with gender-labeled users, each rated by degree of supportiveness. Our analysis shows wide-spread and consistent disparity in support: identifying as a woman is associated with higher rates of support - but also higher rates of disparagement.</p>
<p>【Keywords】:</p>
<h3 id="5. Detecting Gang-Involved Escalation on Social Media Using Context.">5. Detecting Gang-Involved Escalation on Social Media Using Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1005/">Paper Link</a>】    【Pages】:46-56</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Serina">Serina Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Ruiqi">Ruiqi Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Adams:Ethan">Ethan Adams</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Fei=Tzin">Fei-Tzin Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Varia:Siddharth">Siddharth Varia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patton:Desmond">Desmond Patton</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Frey:William_R=">William R. Frey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kedzie:Chris">Chris Kedzie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McKeown:Kathy">Kathy McKeown</a></p>
<p>【Abstract】:
Gang-involved youth in cities such as Chicago have increasingly turned to social media to post about their experiences and intents online. In some situations, when they experience the loss of a loved one, their online expression of emotion may evolve into aggression towards rival gangs and ultimately into real-world violence. In this paper, we present a novel system for detecting Aggression and Loss in social media. Our system features the use of domain-specific resources automatically derived from a large unlabeled corpus, and contextual representations of the emotional and semantic content of the user’s recent tweets as well as their interactions with other users. Incorporating context in our Convolutional Neural Network (CNN) leads to a significant improvement.</p>
<p>【Keywords】:</p>
<h3 id="6. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge.">6. Reasoning about Actions and State Changes by Injecting Commonsense Knowledge.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1006/">Paper Link</a>】    【Pages】:57-66</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tandon:Niket">Niket Tandon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dalvi:Bhavana">Bhavana Dalvi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grus:Joel">Joel Grus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bosselut:Antoine">Antoine Bosselut</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Peter">Peter Clark</a></p>
<p>【Abstract】:
Comprehending procedural text, e.g., a paragraph describing photosynthesis, requires modeling actions and the state changes they produce, so that questions about entities at different timepoints can be answered. Although several recent systems have shown impressive progress in this task, their predictions can be globally inconsistent or highly improbable. In this paper, we show how the predicted effects of actions in the context of a paragraph can be improved in two ways: (1) by incorporating global, commonsense constraints (e.g., a non-existent entity cannot be destroyed), and (2) by biasing reading with preferences from large-scale corpora (e.g., trees rarely move). Unlike earlier methods, we treat the problem as a neural structured prediction task, allowing hard and soft constraints to steer the model away from unlikely predictions. We show that the new model significantly outperforms earlier systems on a benchmark dataset for procedural text comprehension (+8% relative gain), and that it also avoids some of the nonsensical predictions that earlier systems make.</p>
<p>【Keywords】:</p>
<h3 id="7. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.">7. Collecting Diverse Natural Language Inference Problems for Sentence Representation Evaluation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1007/">Paper Link</a>】    【Pages】:67-81</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Poliak:Adam">Adam Poliak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haldar:Aparajita">Aparajita Haldar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rudinger:Rachel">Rachel Rudinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:J=_Edward">J. Edward Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pavlick:Ellie">Ellie Pavlick</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/White:Aaron_Steven">Aaron Steven White</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durme:Benjamin_Van">Benjamin Van Durme</a></p>
<p>【Abstract】:
We present a large-scale collection of diverse natural language inference (NLI) datasets that help provide insight into how well a sentence representation captures distinct types of reasoning. The collection results from recasting 13 existing datasets from 7 semantic phenomena into a common NLI structure, resulting in over half a million labeled context-hypothesis pairs in total. We refer to our collection as the DNC: Diverse Natural Language Inference Collection. The DNC is available online at <a href="https://www.decomp.net">https://www.decomp.net</a>, and will grow over time as additional resources are recast and added from novel sources.</p>
<p>【Keywords】:</p>
<h3 id="8. Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts.">8. Textual Analogy Parsing: What's Shared and What's Compared among Analogous Facts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1008/">Paper Link</a>】    【Pages】:82-92</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lamm:Matthew">Matthew Lamm</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chaganty:Arun_Tejasvi">Arun Tejasvi Chaganty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurafsky:Dan">Dan Jurafsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>【Abstract】:
To understand a sentence like “whereas only 10% of White Americans live at or below the poverty line, 28% of African Americans do” it is important not only to identify individual facts, e.g., poverty rates of distinct demographic groups, but also the higher-order relations between them, e.g., the disparity between them. In this paper, we propose the task of Textual Analogy Parsing (TAP) to model this higher-order meaning. Given a sentence such as the one above, TAP outputs a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10% vs. 28%) between its component facts. Such a meaning representation can enable new applications that rely on discourse understanding such as automated chart generation from quantitative text. We present a new dataset for TAP, baselines, and a model that successfully uses an ILP to enforce the structural constraints of the problem.</p>
<p>【Keywords】:</p>
<h3 id="9. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.">9. SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1009/">Paper Link</a>】    【Pages】:93-104</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zellers:Rowan">Rowan Zellers</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bisk:Yonatan">Yonatan Bisk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:Roy">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>【Abstract】:
Given a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (”then, she examined the engine”). In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning. We present SWAG, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-the-art language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.</p>
<p>【Keywords】:</p>
<h3 id="10. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification.">10. TwoWingOS: A Two-Wing Optimization Strategy for Evidential Claim Verification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1010/">Paper Link</a>】    【Pages】:105-114</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin_0001:Wenpeng">Wenpeng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Determining whether a given claim is supported by evidence is a fundamental NLP problem that is best modeled as Textual Entailment. However, given a large collection of text, finding evidence that could support or refute a given claim is a challenge in itself, amplified by the fact that different evidence might be needed to support or refute a claim. Nevertheless, most prior work decouples evidence finding from determining the truth value of the claim given the evidence. We propose to consider these two aspects jointly. We develop TwoWingOS (two-wing optimization strategy), a system that, while identifying appropriate evidence for a claim, also determines whether or not the claim is supported by the evidence. Given the claim, TwoWingOS attempts to identify a subset of the evidence candidates; given the predicted evidence, it then attempts to determine the truth value of the corresponding claim entailment problem. We treat this problem as coupled optimization problems, training a joint model for it. TwoWingOS offers two advantages: (i) Unlike pipeline systems it facilitates flexible-size evidence set, and (ii) Joint training improves both the claim entailment and the evidence identification. Experiments on a benchmark dataset show state-of-the-art performance.</p>
<p>【Keywords】:</p>
<h3 id="11. Associative Multichannel Autoencoder for Multimodal Word Representation.">11. Associative Multichannel Autoencoder for Multimodal Word Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1011/">Paper Link</a>】    【Pages】:115-124</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shaonan">Shaonan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>【Abstract】:
In this paper we address the problem of learning multimodal word representations by integrating textual, visual and auditory inputs. Inspired by the re-constructive and associative nature of human memory, we propose a novel associative multichannel autoencoder (AMA). Our model first learns the associations between textual and perceptual modalities, so as to predict the missing perceptual information of concepts. Then the textual and predicted perceptual representations are fused through reconstructing their original and associated embeddings. Using a gating mechanism our model assigns different weights to each modality according to the different concepts. Results on six benchmark concept similarity tests show that the proposed method significantly outperforms strong unimodal baselines and state-of-the-art multimodal models.</p>
<p>【Keywords】:</p>
<h3 id="12. Game-Based Video-Context Dialogue.">12. Game-Based Video-Context Dialogue.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1012/">Paper Link</a>】    【Pages】:125-136</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pasunuru:Ramakanth">Ramakanth Pasunuru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on live-broadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrase-matching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.</p>
<p>【Keywords】:</p>
<h3 id="13. simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions.">13. simNet: Stepwise Image-Topic Merging Network for Generating Detailed and Comprehensive Image Captions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1013/">Paper Link</a>】    【Pages】:137-149</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Fenglin">Fenglin Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yuanxin">Yuanxin Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
The encode-decoder framework has shown recent success in image captioning. Visual attention, which is good at detailedness, and semantic attention, which is good at comprehensiveness, have been separately proposed to ground the caption on the image. In this paper, we propose the Stepwise Image-Topic Merging Network (simNet) that makes use of the two kinds of attention at the same time. At each time step when generating the caption, the decoder adaptively merges the attentive information in the extracted topics and the image according to the generated context, so that the visual information and the semantic information can be effectively combined. The proposed approach is evaluated on two benchmark datasets and reaches the state-of-the-art performances.</p>
<p>【Keywords】:</p>
<h3 id="14. Multimodal Language Analysis with Recurrent Multistage Fusion.">14. Multimodal Language Analysis with Recurrent Multistage Fusion.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1014/">Paper Link</a>】    【Pages】:150-161</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Paul_Pu">Paul Pu Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Ziyin">Ziyin Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zadeh_0001:Amir">Amir Zadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morency:Louis=Philippe">Louis-Philippe Morency</a></p>
<p>【Abstract】:
Computational modeling of human multimodal language is an emerging research area in natural language processing spanning the language, visual and acoustic modalities. Comprehending multimodal language requires modeling not only the interactions within each modality (intra-modal interactions) but more importantly the interactions between modalities (cross-modal interactions). In this paper, we propose the Recurrent Multistage Fusion Network (RMFN) which decomposes the fusion problem into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Cross-modal interactions are modeled using this multistage fusion approach which builds upon intermediate representations of previous stages. Temporal and intra-modal interactions are modeled by integrating our proposed fusion approach with a system of recurrent neural networks. The RMFN displays state-of-the-art performance in modeling human multimodal language across three public datasets relating to multimodal sentiment analysis, emotion recognition, and speaker traits recognition. We provide visualizations to show that each stage of fusion focuses on a different subset of multimodal signals, learning increasingly discriminative multimodal representations.</p>
<p>【Keywords】:</p>
<h3 id="15. Temporally Grounding Natural Sentence in Video.">15. Temporally Grounding Natural Sentence in Video.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1015/">Paper Link</a>】    【Pages】:162-171</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jingyuan">Jingyuan Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xinpeng">Xinpeng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Lin">Lin Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jie:Zequn">Zequn Jie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a></p>
<p>【Abstract】:
We introduce an effective and efficient method that grounds (i.e., localizes) natural sentences in long, untrimmed video sequences. Specifically, a novel Temporal GroundNet (TGN) is proposed to temporally capture the evolving fine-grained frame-by-word interactions between video and sentence. TGN sequentially scores a set of temporal candidates ended at each frame based on the exploited frame-by-word interactions, and finally grounds the segment corresponding to the sentence. Unlike traditional methods treating the overlapping segments separately in a sliding window fashion, TGN aggregates the historical information and generates the final grounding result in one single pass. We extensively evaluate our proposed TGN on three public datasets with significant improvements over the state-of-the-arts. We further show the consistent effectiveness and efficiency of TGN through an ablation study and a runtime test.</p>
<p>【Keywords】:</p>
<h3 id="16. PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution.">16. PreCo: A Large-scale Dataset in Preschool Vocabulary for Coreference Resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1016/">Paper Link</a>】    【Pages】:172-181</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hong">Hong Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Zhenhua">Zhenhua Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Hao">Hao Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yuille:Alan_L=">Alan L. Yuille</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rong:Shu">Shu Rong</a></p>
<p>【Abstract】:
We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of English-speaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at <a href="https://preschool-lab.github.io/PreCo/">https://preschool-lab.github.io/PreCo/</a>.</p>
<p>【Keywords】:</p>
<h3 id="17. Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism.">17. Adversarial Transfer Learning for Chinese Named Entity Recognition with Self-Attention Mechanism.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1017/">Paper Link</a>】    【Pages】:182-192</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Pengfei">Pengfei Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0001:Yubo">Yubo Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Kang">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Shengping">Shengping Liu</a></p>
<p>【Abstract】:
Named entity recognition (NER) is an important task in natural language processing area, which needs to determine entities boundaries and classify them into pre-defined categories. For Chinese NER task, there is only a very small amount of annotated data available. Chinese NER task and Chinese word segmentation (CWS) task have many similar word boundaries. There are also specificities in each task. However, existing methods for Chinese NER either do not exploit word boundary information from CWS or cannot filter the specific information of CWS. In this paper, we propose a novel adversarial transfer learning framework to make full use of task-shared boundaries information and prevent the task-specific features of CWS. Besides, since arbitrary character can provide important cues when predicting entity type, we exploit self-attention to explicitly capture long range dependencies between two tokens. Experimental results on two different widely used datasets show that our proposed model significantly and consistently outperforms other state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="18. Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers.">18. Using Linguistic Features to Improve the Generalization Capability of Neural Coreference Resolvers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1018/">Paper Link</a>】    【Pages】:193-203</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moosavi:Nafise_Sadat">Nafise Sadat Moosavi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strube_0001:Michael">Michael Strube</a></p>
<p>【Abstract】:
Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on the CoNLL dataset, they struggle to generalize properly to new domains or datasets. In this paper, we investigate the role of linguistic features in building more generalizable coreference resolvers. We show that generalization improves only slightly by merely using a set of additional linguistic features. However, employing features and subsets of their values that are informative for coreference resolution, considerably improves generalization. Thanks to better generalization, our system achieves state-of-the-art results in out-of-domain evaluations, e.g., on WikiCoref, our system, which is trained on CoNLL, achieves on-par performance with a system designed for this dataset.</p>
<p>【Keywords】:</p>
<h3 id="19. Neural Segmental Hypergraphs for Overlapping Mention Recognition.">19. Neural Segmental Hypergraphs for Overlapping Mention Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1019/">Paper Link</a>】    【Pages】:204-214</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Bailin">Bailin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a></p>
<p>【Abstract】:
In this work, we propose a novel segmental hypergraph representation to model overlapping entity mentions that are prevalent in many practical datasets. We show that our model built on top of such a new representation is able to capture features and interactions that cannot be captured by previous models while maintaining a low time complexity for inference. We also present a theoretical analysis to formally assess how our representation is better than alternative representations reported in the literature in terms of representational power. Coupled with neural networks for feature learning, our model achieves the state-of-the-art performance in three benchmark datasets annotated with overlapping mentions.</p>
<p>【Keywords】:</p>
<h3 id="20. Variational Sequential Labelers for Semi-Supervised Learning.">20. Variational Sequential Labelers for Semi-Supervised Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1020/">Paper Link</a>】    【Pages】:215-226</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Mingda">Mingda Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Qingming">Qingming Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Livescu:Karen">Karen Livescu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gimpel:Kevin">Kevin Gimpel</a></p>
<p>【Abstract】:
We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.</p>
<p>【Keywords】:</p>
<h3 id="21. Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision.">21. Joint Representation Learning of Cross-lingual Words and Entities via Attentive Distant Supervision.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1021/">Paper Link</a>】    【Pages】:227-237</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao_0002:Yixin">Yixin Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Lei">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chengjiang">Chengjiang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xu">Xu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Tiansi">Tiansi Dong</a></p>
<p>【Abstract】:
Jointly representation learning of words and entities benefits many NLP tasks, but has not been well explored in cross-lingual settings. In this paper, we propose a novel method for joint representation learning of cross-lingual words and entities. It captures mutually complementary knowledge, and enables cross-lingual inferences among knowledge bases and texts. Our method does not require parallel corpus, and automatically generates comparable data via distant supervision using multi-lingual knowledge bases. We utilize two types of regularizers to align cross-lingual words and entities, and design knowledge attention and cross-lingual attention to further reduce noises. We conducted a series of experiments on three tasks: word translation, entity relatedness, and cross-lingual entity linking. The results, both qualitative and quantitative, demonstrate the significance of our method.</p>
<p>【Keywords】:</p>
<h3 id="22. Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance.">22. Deep Pivot-Based Modeling for Cross-language Cross-domain Transfer with Minimal Guidance.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1022/">Paper Link</a>】    【Pages】:238-249</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Ziser:Yftah">Yftah Ziser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a></p>
<p>【Abstract】:
While cross-domain and cross-language transfer have long been prominent topics in NLP research, their combination has hardly been explored. In this work we consider this problem, and propose a framework that builds on pivot-based learning, structure-aware Deep Neural Networks (particularly LSTMs and CNNs) and bilingual word embeddings, with the goal of training a model on labeled data from one (language, domain) pair so that it can be effectively applied to another (language, domain) pair. We consider two setups, differing with respect to the unlabeled data available for model training. In the full setup the model has access to unlabeled data from both pairs, while in the lazy setup, which is more realistic for truly resource-poor languages, unlabeled data is available for both domains but only for the source language. We design our model for the lazy setup so that for a given target domain, it can train once on the source language and then be applied to any target language without re-training. In experiments with nine English-German and nine English-French domain pairs our best model substantially outperforms previous models even when it is trained in the lazy setup and previous models are trained in the full setup.</p>
<p>【Keywords】:</p>
<h3 id="23. Multi-lingual Common Semantic Space Construction via Cluster-Consistent Word Embedding.">23. Multi-lingual Common Semantic Space Construction via Cluster-Consistent Word Embedding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1023/">Paper Link</a>】    【Pages】:250-260</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Lifu">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Boliang">Boliang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Knight:Kevin">Kevin Knight</a></p>
<p>【Abstract】:
We construct a multilingual common semantic space based on distributional semantics, where words from multiple languages are projected into a shared space via which all available resources and knowledge can be shared across multiple languages. Beyond word alignment, we introduce multiple cluster-level alignments and enforce the word clusters to be consistently distributed across multiple languages. We exploit three signals for clustering: (1) neighbor words in the monolingual word embedding space; (2) character-level information; and (3) linguistic properties (e.g., apposition, locative suffix) derived from linguistic structure knowledge bases available for thousands of languages. We introduce a new cluster-consistent correlational neural network to construct the common semantic space by aligning words as well as clusters. Intrinsic evaluation on monolingual and multilingual QVEC tasks shows our approach achieves significantly higher correlation with linguistic features which are extracted from manually crafted lexical resources than state-of-the-art multi-lingual embedding learning methods do. Using low-resource language name tagging as a case study for extrinsic evaluation, our approach achieves up to 14.6% absolute F-score gain over the state of the art on cross-lingual direct transfer. Our approach is also shown to be robust even when the size of bilingual dictionary is small.</p>
<p>【Keywords】:</p>
<h3 id="24. Unsupervised Multilingual Word Embeddings.">24. Unsupervised Multilingual Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1024/">Paper Link</a>】    【Pages】:261-270</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xilun">Xilun Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cardie:Claire">Claire Cardie</a></p>
<p>【Abstract】:
Multilingual Word Embeddings (MWEs) represent words from multiple languages in a single distributional vector space. Unsupervised MWE (UMWE) methods acquire multilingual embeddings without cross-lingual supervision, which is a significant advantage over traditional supervised approaches and opens many new possibilities for low-resource languages. Prior art for learning UMWEs, however, merely relies on a number of independently trained Unsupervised Bilingual Word Embeddings (UBWEs) to obtain multilingual embeddings. These methods fail to leverage the interdependencies that exist among many languages. To address this shortcoming, we propose a fully unsupervised framework for learning MWEs that directly exploits the relations between all language pairs. Our model substantially outperforms previous approaches in the experiments on multilingual word translation and cross-lingual word similarity. In addition, our model even beats supervised approaches trained with cross-lingual resources.</p>
<p>【Keywords】:</p>
<h3 id="25. CLUSE: Cross-Lingual Unsupervised Sense Embeddings.">25. CLUSE: Cross-Lingual Unsupervised Sense Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1025/">Paper Link</a>】    【Pages】:271-281</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chi:Ta=Chung">Ta-Chung Chi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yun=Nung">Yun-Nung Chen</a></p>
<p>【Abstract】:
This paper proposes a modularized sense induction and representation learning model that jointly learns bilingual sense embeddings that align well in the vector space, where the cross-lingual signal in the English-Chinese parallel corpus is exploited to capture the collocation and distributed characteristics in the language pair. The model is evaluated on the Stanford Contextual Word Similarity (SCWS) dataset to ensure the quality of monolingual sense embeddings. In addition, we introduce Bilingual Contextual Word Similarity (BCWS), a large and high-quality dataset for evaluating cross-lingual sense embeddings, which is the first attempt of measuring whether the learned embeddings are indeed aligned well in the vector space. The proposed approach shows the superior quality of sense embeddings evaluated in both monolingual and bilingual spaces.</p>
<p>【Keywords】:</p>
<h3 id="26. Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization.">26. Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1026/">Paper Link</a>】    【Pages】:282-293</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Ponti:Edoardo_Maria">Edoardo Maria Ponti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Glavas:Goran">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mrksic:Nikola">Nikola Mrksic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Korhonen:Anna">Anna Korhonen</a></p>
<p>【Abstract】:
Semantic specialization is a process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (e.g., WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (i.e., seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with a adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.</p>
<p>【Keywords】:</p>
<h3 id="27. Improving Cross-Lingual Word Embeddings by Meeting in the Middle.">27. Improving Cross-Lingual Word Embeddings by Meeting in the Middle.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1027/">Paper Link</a>】    【Pages】:294-304</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Doval:Yerai">Yerai Doval</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Camacho=Collados:Jos=eacute=">José Camacho-Collados</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Anke:Luis_Espinosa">Luis Espinosa Anke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schockaert:Steven">Steven Schockaert</a></p>
<p>【Abstract】:
Cross-lingual word embeddings are becoming increasingly important in multilingual NLP. Recently, it has been shown that these embeddings can be effectively learned by aligning two disjoint monolingual vector spaces through linear transformations, using no more than a small bilingual dictionary as supervision. In this work, we propose to apply an additional transformation after the initial alignment step, which moves cross-lingual synonyms towards a middle point between them. By applying this transformation our aim is to obtain a better cross-lingual integration of the vector spaces. In addition, and perhaps surprisingly, the monolingual spaces also improve by this transformation. This is in contrast to the original alignment, which is typically learned such that the structure of the monolingual spaces is preserved. Our experiments confirm that the resulting cross-lingual embeddings outperform state-of-the-art models in both monolingual and cross-lingual evaluation tasks.</p>
<p>【Keywords】:</p>
<h3 id="28. WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse.">28. WikiAtomicEdits: A Multilingual Corpus of Wikipedia Edits for Modeling Language and Discourse.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1028/">Paper Link</a>】    【Pages】:305-315</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Faruqui:Manaal">Manaal Faruqui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pavlick:Ellie">Ellie Pavlick</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tenney:Ian">Ian Tenney</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Das_0001:Dipanjan">Dipanjan Das</a></p>
<p>【Abstract】:
We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.</p>
<p>【Keywords】:</p>
<h3 id="29. On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling.">29. On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1029/">Paper Link</a>】    【Pages】:316-327</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gerz:Daniela">Daniela Gerz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Ponti:Edoardo_Maria">Edoardo Maria Ponti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Korhonen:Anna">Anna Korhonen</a></p>
<p>【Abstract】:
A key challenge in cross-lingual NLP is developing general language-independent architectures that are equally applicable to any language. However, this ambition is largely hampered by the variation in structural and semantic properties, i.e. the typological profiles of the world’s languages. In this work, we analyse the implications of this variation on the language modeling (LM) task. We present a large-scale study of state-of-the art n-gram based and neural language models on 50 typologically diverse languages covering a wide variety of morphological systems. Operating in the full vocabulary LM setup focused on word-level prediction, we demonstrate that a coarse typology of morphological systems is predictive of absolute LM performance. Moreover, fine-grained typological features such as exponence, flexivity, fusion, and inflectional synthesis are borne out to be responsible for the proliferation of low-frequency phenomena which are organically difficult to model by statistical architectures, or for the meaning ambiguity of character n-grams. Our study strongly suggests that these features have to be taken into consideration during the construction of next-level language-agnostic LM architectures, capable of handling morphologically complex languages such as Tamil or Korean.</p>
<p>【Keywords】:</p>
<h3 id="30. A Fast, Compact, Accurate Model for Language Identification of Codemixed Text.">30. A Fast, Compact, Accurate Model for Language Identification of Codemixed Text.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1030/">Paper Link</a>】    【Pages】:328-337</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yuan">Yuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riesa:Jason">Jason Riesa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gillick:Daniel">Daniel Gillick</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bakalov:Anton">Anton Bakalov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldridge:Jason">Jason Baldridge</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weiss:David">David Weiss</a></p>
<p>【Abstract】:
We address fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages. Such text is prevalent online, in documents, social media, and message boards. We show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.5% averaged absolute gain on three codemixed datasets. It furthermore outperforms several benchmark systems on monolingual language identification.</p>
<p>【Keywords】:</p>
<h3 id="31. Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning.">31. Personalized Microblog Sentiment Classification via Adversarial Cross-lingual Multi-task Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1031/">Paper Link</a>】    【Pages】:338-348</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Weichao">Weichao Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Shi">Shi Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao_0001:Wei">Wei Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Daling">Daling Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Yifei">Yifei Zhang</a></p>
<p>【Abstract】:
Sentiment expression in microblog posts can be affected by user’s personal character, opinion bias, political stance and so on. Most of existing personalized microblog sentiment classification methods suffer from the insufficiency of discriminative tweets for personalization learning. We observed that microblog users have consistent individuality and opinion bias in different languages. Based on this observation, in this paper we propose a novel user-attention-based Convolutional Neural Network (CNN) model with adversarial cross-lingual learning framework. The user attention mechanism is leveraged in CNN model to capture user’s language-specific individuality from the posts. Then the attention-based CNN model is incorporated into a novel adversarial cross-lingual learning framework, in which with the help of user properties as bridge between languages, we can extract the language-specific features and language-independent features to enrich the user post representation so as to alleviate the data insufficiency problem. Results on English and Chinese microblog datasets confirm that our method outperforms state-of-the-art baseline algorithms with large margins.</p>
<p>【Keywords】:</p>
<h3 id="32. Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks.">32. Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1032/">Paper Link</a>】    【Pages】:349-357</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhichun">Zhichun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lv:Qingsong">Qingsong Lv</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Xiaohan">Xiaohan Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yu">Yu Zhang</a></p>
<p>【Abstract】:
Multilingual knowledge graphs (KGs) such as DBpedia and YAGO contain structured knowledge of entities in several distinct languages, and they are useful resources for cross-lingual AI and NLP applications. Cross-lingual KG alignment is the task of matching entities with their counterparts in different languages, which is an important way to enrich the cross-lingual links in multilingual KGs. In this paper, we propose a novel approach for cross-lingual KG alignment via graph convolutional networks (GCNs). Given a set of pre-aligned entities, our approach trains GCNs to embed entities of each language into a unified vector space. Entity alignments are discovered based on the distances between entities in the embedding space. Embeddings can be learned from both the structural and attribute information of entities, and the results of structure embedding and attribute embedding are combined to get accurate alignments. In the experiments on aligning real multilingual KGs, our approach gets the best performance compared with other embedding-based KG alignment approaches.</p>
<p>【Keywords】:</p>
<h3 id="33. Cross-lingual Lexical Sememe Prediction.">33. Cross-lingual Lexical Sememe Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1033/">Paper Link</a>】    【Pages】:358-368</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/q/Qi:Fanchao">Fanchao Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Yankai">Yankai Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hao">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Ruobing">Ruobing Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a></p>
<p>【Abstract】:
Sememes are defined as the minimum semantic units of human languages. As important knowledge sources, sememe-based linguistic knowledge bases have been widely used in many NLP tasks. However, most languages still do not have sememe-based linguistic knowledge bases. Thus we present a task of cross-lingual lexical sememe prediction, aiming to automatically predict sememes for words in other languages. We propose a novel framework to model correlations between sememes and multi-lingual words in low-dimensional semantic space for sememe prediction. Experimental results on real-world datasets show that our proposed model achieves consistent and significant improvements as compared to baseline methods in cross-lingual sememe prediction. The codes and data of this paper are available at <a href="https://github.com/thunlp/CL-SP">https://github.com/thunlp/CL-SP</a>.</p>
<p>【Keywords】:</p>
<h3 id="34. Neural Cross-lingual Named Entity Recognition with Minimal Resources.">34. Neural Cross-lingual Named Entity Recognition with Minimal Resources.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1034/">Paper Link</a>】    【Pages】:369-379</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Jiateng">Jiateng Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zhilin">Zhilin Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carbonell:Jaime_G=">Jaime G. Carbonell</a></p>
<p>【Abstract】:
For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across languages, we propose a method that finds translations based on bilingual word embeddings. To improve robustness to word order differences, we propose to use self-attention, which allows for a degree of flexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low-resource language.</p>
<p>【Keywords】:</p>
<h3 id="35. A Stable and Effective Learning Strategy for Trainable Greedy Decoding.">35. A Stable and Effective Learning Strategy for Trainable Greedy Decoding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1035/">Paper Link</a>】    【Pages】:380-390</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0007:Yun">Yun Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Victor_O=_K=">Victor O. K. Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bowman:Samuel_R=">Samuel R. Bowman</a></p>
<p>【Abstract】:
Beam search is a widely used approximate search strategy for neural network decoders, and it generally outperforms simple greedy decoding on tasks like machine translation. However, this improvement comes at substantial computational cost. In this paper, we propose a flexible new method that allows us to reap nearly the full benefits of beam search with nearly no additional computational cost. The method revolves around a small neural network actor that is trained to observe and manipulate the hidden state of a previously-trained decoder. To train this actor network, we introduce the use of a pseudo-parallel corpus built using the output of beam search on a base model, ranked by a target quality metric like BLEU. Our method is inspired by earlier work on this problem, but requires no reinforcement learning, and can be trained reliably on a range of models. Experiments on three parallel corpora and three architectures show that the method yields substantial improvements in translation quality and speed over each base system.</p>
<p>【Keywords】:</p>
<h3 id="36. Addressing Troublesome Words in Neural Machine Translation.">36. Addressing Troublesome Words in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1036/">Paper Link</a>】    【Pages】:391-400</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Yang">Yang Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Zhongjun">Zhongjun He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0003:Hua">Hua Wu</a></p>
<p>【Abstract】:
One of the weaknesses of Neural Machine Translation (NMT) is in handling lowfrequency and ambiguous words, which we refer as troublesome words. To address this problem, we propose a novel memoryenhanced NMT method. First, we investigate different strategies to define and detect the troublesome words. Then, a contextual memory is constructed to memorize which target words should be produced in what situations. Finally, we design a hybrid model to dynamically access the contextual memory so as to correctly translate the troublesome words. The extensive experiments on Chinese-to-English and English-to-German translation tasks demonstrate that our method significantly outperforms the strong baseline models in translation quality, especially in handling troublesome words.</p>
<p>【Keywords】:</p>
<h3 id="37. Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing.">37. Top-down Tree Structured Decoding with Syntactic Connections for Neural Machine Translation and Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1037/">Paper Link</a>】    【Pages】:401-413</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Jetic">Jetic Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shavarani:Hassan_S=">Hassan S. Shavarani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarkar:Anoop">Anoop Sarkar</a></p>
<p>【Abstract】:
The addition of syntax-aware decoding in Neural Machine Translation (NMT) systems requires an effective tree-structured neural network, a syntax-aware attention model and a language generation model that is sensitive to sentence structure. Recent approaches resort to sequential decoding by adding additional neural network units to capture bottom-up structural information, or serialising structured data into sequence. We exploit a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakola (2017) to create an NMT model called Seq2DRNN that combines a sequential encoder with tree-structured decoding augmented with a syntax-aware attention model. Unlike previous approaches to syntax-based NMT which use dependency parsing models our method uses constituency parsing which we argue provides useful information for translation. In addition, we use the syntactic structure of the sentence to add new connections to the tree-structured decoder neural network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models.</p>
<p>【Keywords】:</p>
<h3 id="38. XL-NBT: A Cross-lingual Neural Belief Tracking Framework.">38. XL-NBT: A Cross-lingual Neural Belief Tracking Framework.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1038/">Paper Link</a>】    【Pages】:414-424</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenhu">Wenhu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jianshu">Jianshu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0061:Xin">Xin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Dong">Dong Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement human agents for customer service in call centers. With globalization, the need for providing cross-lingual customer support becomes more urgent than ever. However, cross-lingual support poses great challenges—it requires a large amount of additional annotated data from native speakers. In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework. Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form. Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data. We then distill and transfer its own knowledge to the student state tracker in target languages. We specifically discuss two types of common parallel resources: bilingual corpus and bilingual dictionary, and design different transfer learning strategies accordingly. Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results.</p>
<p>【Keywords】:</p>
<h3 id="39. Contextual Parameter Generation for Universal Neural Machine Translation.">39. Contextual Parameter Generation for Universal Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1039/">Paper Link</a>】    【Pages】:425-435</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Platanios:Emmanouil_Antonios">Emmanouil Antonios Platanios</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sachan:Mrinmaya">Mrinmaya Sachan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>【Abstract】:
We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.</p>
<p>【Keywords】:</p>
<h3 id="40. Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.">40. Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1040/">Paper Link</a>】    【Pages】:436-446</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fadaee:Marzieh">Marzieh Fadaee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Monz:Christof">Christof Monz</a></p>
<p>【Abstract】:
Neural Machine Translation has achieved state-of-the-art performance for several language pairs using a combination of parallel and synthetic data. Synthetic data is often generated by back-translating sentences randomly sampled from monolingual data using a reverse translation model. While back-translation has been shown to be very effective in many cases, it is not entirely clear why. In this work, we explore different aspects of back-translation, and show that words with high prediction loss during training benefit most from the addition of synthetic data. We introduce several variations of sampling strategies targeting difficult-to-predict words using prediction losses and frequencies of words. In addition, we also target the contexts of difficult words and sample sentences that are similar in context. Experimental results for the WMT news translation task show that our method improves translation quality by up to 1.7 and 1.2 Bleu points over back-translation using random sampling for German-English and English-German, respectively.</p>
<p>【Keywords】:</p>
<h3 id="41. Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.">41. Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1041/">Paper Link</a>】    【Pages】:447-457</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Jiali">Jiali Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jinsong">Jinsong Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wen:Huating">Huating Wen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0005:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Jun">Jun Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Yongjing">Yongjing Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jianqiang">Jianqiang Zhao</a></p>
<p>【Abstract】:
With great practical value, the study of Multi-domain Neural Machine Translation (NMT) mainly focuses on using mixed-domain parallel sentences to construct a unified model that allows translation to switch between different domains. Intuitively, words in a sentence are related to its domain to varying degrees, so that they will exert disparate impacts on the multi-domain NMT modeling. Based on this intuition, in this paper, we devote to distinguishing and exploiting word-level domain contexts for multi-domain NMT. To this end, we jointly model NMT with monolingual attention-based domain classification tasks and improve NMT as follows: 1) Based on the sentence representations produced by a domain classifier and an adversarial domain classifier, we generate two gating vectors and use them to construct domain-specific and domain-shared annotations, for later translation predictions via different attention models; 2) We utilize the attention weights derived from target-side domain classifier to adjust the weights of target words in the training objective, enabling domain-related words to have greater impacts during model training. Experimental results on Chinese-English and English-French multi-domain translation tasks demonstrate the effectiveness of the proposed model. Source codes of this paper are available on Github <a href="https://github.com/DeepLearnXMU/WDCNMT">https://github.com/DeepLearnXMU/WDCNMT</a>.</p>
<p>【Keywords】:</p>
<h3 id="42. A Discriminative Latent-Variable Model for Bilingual Lexicon Induction.">42. A Discriminative Latent-Variable Model for Bilingual Lexicon Induction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1042/">Paper Link</a>】    【Pages】:458-468</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ruder:Sebastian">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cotterell:Ryan">Ryan Cotterell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kementchedjhieva:Yova">Yova Kementchedjhieva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Søgaard</a></p>
<p>【Abstract】:
We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.</p>
<p>【Keywords】:</p>
<h3 id="43. Non-Adversarial Unsupervised Word Translation.">43. Non-Adversarial Unsupervised Word Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1043/">Paper Link</a>】    【Pages】:469-478</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hoshen:Yedid">Yedid Hoshen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wolf:Lior">Lior Wolf</a></p>
<p>【Abstract】:
Unsupervised word translation from non-parallel inter-lingual corpora has attracted much research interest. Very recently, neural network methods trained with adversarial loss functions achieved high accuracy on this task. Despite the impressive success of the recent techniques, they suffer from the typical drawbacks of generative adversarial models: sensitivity to hyper-parameters, long training time and lack of interpretability. In this paper, we make the observation that two sufficiently similar distributions can be aligned correctly with iterative matching methods. We present a novel method that first aligns the second moment of the word distributions of the two languages and then iteratively refines the alignment. Extensive experiments on word translation of European and Non-European languages show that our method achieves better performance than recent state-of-the-art deep adversarial approaches and is competitive with the supervised baseline. It is also efficient, easy to parallelize on CPU and interpretable.</p>
<p>【Keywords】:</p>
<h3 id="44. Semi-Autoregressive Neural Machine Translation.">44. Semi-Autoregressive Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1044/">Paper Link</a>】    【Pages】:479-488</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Chunqi">Chunqi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Ji">Ji Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Haiqing">Haiqing Chen</a></p>
<p>【Abstract】:
Existing approaches to neural machine translation are typically autoregressive models. While these models attain state-of-the-art translation quality, they are suffering from low parallelizability and thus slow at decoding long sequences. In this paper, we propose a novel model for fast sequence generation — the semi-autoregressive Transformer (SAT). The SAT keeps the autoregressive property in global but relieves in local and thus are able to produce multiple successive words in parallel at each time step. Experiments conducted on English-German and Chinese-English translation tasks show that the SAT achieves a good balance between translation quality and decoding speed. On WMT’14 English-German translation, the SAT achieves 5.58× speedup while maintaining 88% translation quality, significantly better than the previous non-autoregressive methods. When produces two words at each time step, the SAT is almost lossless (only 1% degeneration in BLEU score).</p>
<p>【Keywords】:</p>
<h3 id="45. Understanding Back-Translation at Scale.">45. Understanding Back-Translation at Scale.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1045/">Paper Link</a>】    【Pages】:489-500</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Edunov:Sergey">Sergey Edunov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ott:Myle">Myle Ott</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Auli:Michael">Michael Auli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grangier:David">David Grangier</a></p>
<p>【Abstract】:
An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set.</p>
<p>【Keywords】:</p>
<h3 id="46. Bootstrapping Transliteration with Guided Discovery for Low-Resource Languages.">46. Bootstrapping Transliteration with Guided Discovery for Low-Resource Languages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1046/">Paper Link</a>】    【Pages】:501-511</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/u/Upadhyay:Shyam">Shyam Upadhyay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kodner:Jordan">Jordan Kodner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Generating the English transliteration of a name written in a foreign script is an important and challenging step in multilingual knowledge acquisition and information extraction. Existing approaches to transliteration generation require a large (&gt;5000) number of training examples. This difficulty contrasts with transliteration discovery, a somewhat easier task that involves picking a plausible transliteration from a given list. In this work, we present a bootstrapping algorithm that uses constrained discovery to improve generation, and can be used with as few as 500 training examples, which we show can be sourced from annotators in a matter of hours. This opens the task to languages for which large number of training examples are unavailable. We evaluate transliteration generation performance itself, as well the improvement it brings to cross-lingual candidate generation for entity linking, a typical downstream task. We present a comprehensive evaluation of our approach on nine languages, each written in a unique script.</p>
<p>【Keywords】:</p>
<h3 id="47. NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.">47. NORMA: Neighborhood Sensitive Maps for Multilingual Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1047/">Paper Link</a>】    【Pages】:512-522</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nakashole:Ndapa">Ndapa Nakashole</a></p>
<p>【Abstract】:
Inducing multilingual word embeddings by learning a linear map between embedding spaces of different languages achieves remarkable accuracy on related languages. However, accuracy drops substantially when translating between distant languages. Given that languages exhibit differences in vocabulary, grammar, written form, or syntax, one would expect that embedding spaces of different languages have different structures especially for distant languages. With the goal of capturing such differences, we propose a method for learning neighborhood sensitive maps, NORMA. Our experiments show that NORMA outperforms current state-of-the-art methods for word translation between distant languages.</p>
<p>【Keywords】:</p>
<h3 id="48. Adaptive Multi-pass Decoder for Neural Machine Translation.">48. Adaptive Multi-pass Decoder for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1048/">Paper Link</a>】    【Pages】:523-532</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Geng:Xinwei">Xinwei Geng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Xiaocheng">Xiaocheng Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin_0001:Bing">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>【Abstract】:
Although end-to-end neural machine translation (NMT) has achieved remarkable progress in the recent years, the idea of adopting multi-pass decoding mechanism into conventional NMT is not well explored. In this paper, we propose a novel architecture called adaptive multi-pass decoder, which introduces a flexible multi-pass polishing mechanism to extend the capacity of NMT via reinforcement learning. More specifically, we adopt an extra policy network to automatically choose a suitable and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on Chinese-English translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU.</p>
<p>【Keywords】:</p>
<h3 id="49. Improving the Transformer Translation Model with Document-Level Context.">49. Improving the Transformer Translation Model with Document-Level Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1049/">Paper Link</a>】    【Pages】:533-542</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiacheng">Jiacheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luan:Huanbo">Huanbo Luan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhai:Feifei">Feifei Zhai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingfang">Jingfang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0005:Yang">Yang Liu</a></p>
<p>【Abstract】:
Although the Transformer translation model (Vaswani et al., 2017) has achieved state-of-the-art performance in a variety of translation tasks, how to use document-level context to deal with discourse phenomena problematic for Transformer still remains a challenge. In this work, we extend the Transformer model with a new context encoder to represent document-level context, which is then incorporated into the original encoder and decoder. As large-scale document-level parallel corpora are usually not available, we introduce a two-step training method to take full advantage of abundant sentence-level parallel corpora and limited document-level parallel corpora. Experiments on the NIST Chinese-English datasets and the IWSLT French-English datasets show that our approach improves over Transformer significantly.</p>
<p>【Keywords】:</p>
<h3 id="50. MTNT: A Testbed for Machine Translation of Noisy Text.">50. MTNT: A Testbed for Machine Translation of Noisy Text.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1050/">Paper Link</a>】    【Pages】:543-553</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Michel:Paul">Paul Michel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.</p>
<p>【Keywords】:</p>
<h3 id="51. SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach.">51. SimpleQuestions Nearly Solved: A New Upperbound and Baseline Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1051/">Paper Link</a>】    【Pages】:554-558</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Petrochuk:Michael">Michael Petrochuk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
The SimpleQuestions dataset is one of the most commonly used benchmarks for studying single-relation factoid questions. In this paper, we present new evidence that this benchmark can be nearly solved by standard methods. First, we show that ambiguity in the data bounds performance at 83.4%; many questions have more than one equally plausible interpretation. Second, we introduce a baseline that sets a new state-of-the-art performance level at 78.1% accuracy, despite using standard methods. Finally, we report an empirical analysis showing that the upperbound is loose; roughly a quarter of the remaining errors are also not resolvable from the linguistic signal. Together, these results suggest that the SimpleQuestions dataset is nearly solved.</p>
<p>【Keywords】:</p>
<h3 id="52. Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension.">52. Phrase-Indexed Question Answering: A New Challenge for Scalable Document Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1052/">Paper Link</a>】    【Pages】:559-564</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Seo:Min_Joon">Min Joon Seo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kwiatkowski:Tom">Tom Kwiatkowski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parikh:Ankur_P=">Ankur P. Parikh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Farhadi:Ali">Ali Farhadi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajishirzi:Hannaneh">Hannaneh Hajishirzi</a></p>
<p>【Abstract】:
We formalize a new modular variant of current question answering tasks by enforcing complete independence of the document encoder from the question encoder. This formulation addresses a key challenge in machine comprehension by building a standalone representation of the document discourse. It additionally leads to a significant scalability advantage since the encoding of the answer candidate phrases in the document can be pre-computed and indexed offline for efficient retrieval. We experiment with baseline models for the new task, which achieve a reasonable accuracy but significantly underperform unconstrained QA models. We invite the QA research community to engage in Phrase-Indexed Question Answering (PIQA, pika) for closing the gap. The leaderboard is at: nlp.cs.washington.edu/piqa</p>
<p>【Keywords】:</p>
<h3 id="53. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering.">53. Ranking Paragraphs for Improving Answer Recall in Open-Domain Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1053/">Paper Link</a>】    【Pages】:565-569</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Jinhyuk">Jinhyuk Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yun:Seongjun">Seongjun Yun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Hyunjae">Hyunjae Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Ko:Miyoung">Miyoung Ko</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kang:Jaewoo">Jaewoo Kang</a></p>
<p>【Abstract】:
Recently, open-domain question answering (QA) has been combined with machine comprehension models to find answers in a large knowledge source. As open-domain QA requires retrieving relevant documents from text corpora to answer questions, its performance largely depends on the performance of document retrievers. However, since traditional information retrieval systems are not effective in obtaining documents with a high probability of containing answers, they lower the performance of QA systems. Simply extracting more documents increases the number of irrelevant documents, which also degrades the performance of QA systems. In this paper, we introduce Paragraph Ranker which ranks paragraphs of retrieved documents for a higher answer recall with less noise. We show that ranking paragraphs and aggregating answers using Paragraph Ranker improves performance of open-domain QA pipeline on the four open-domain QA datasets by 7.8% on average.</p>
<p>【Keywords】:</p>
<h3 id="54. Cut to the Chase: A Context Zoom-in Network for Reading Comprehension.">54. Cut to the Chase: A Context Zoom-in Network for Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1054/">Paper Link</a>】    【Pages】:570-575</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/i/Indurthi:Sathish_Reddy">Sathish Reddy Indurthi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Seunghak">Seunghak Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Back:Seohyun">Seohyun Back</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cuay=aacute=huitl:Heriberto">Heriberto Cuayáhuitl</a></p>
<p>【Abstract】:
In recent years many deep neural networks have been proposed to solve Reading Comprehension (RC) tasks. Most of these models suffer from reasoning over long documents and do not trivially generalize to cases where the answer is not present as a span in a given document. We present a novel neural-based architecture that is capable of extracting relevant regions based on a given question-document pair and generating a well-formed answer. To show the effectiveness of our architecture, we conducted several experiments on the recently proposed and challenging RC dataset ‘NarrativeQA’. The proposed architecture outperforms state-of-the-art results by 12.62% (ROUGE-L) relative improvement.</p>
<p>【Keywords】:</p>
<h3 id="55. Adaptive Document Retrieval for Deep Question Answering.">55. Adaptive Document Retrieval for Deep Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1055/">Paper Link</a>】    【Pages】:576-581</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kratzwald:Bernhard">Bernhard Kratzwald</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feuerriegel:Stefan">Stefan Feuerriegel</a></p>
<p>【Abstract】:
State-of-the-art systems in deep question answering proceed as follows: (1)an initial document retrieval selects relevant documents, which (2) are then processed by a neural network in order to extract the final answer. Yet the exact interplay between both components is poorly understood, especially concerning the number of candidate documents that should be retrieved. We show that choosing a static number of documents - as used in prior research - suffers from a noise-information trade-off and yields suboptimal results. As a remedy, we propose an adaptive document retrieval model. This learns the optimal candidate number for document retrieval, conditional on the size of the corpus and the query. We report extensive experimental results showing that our adaptive approach outperforms state-of-the-art methods on multiple benchmark datasets, as well as in the context of corpora with variable sizes.</p>
<p>【Keywords】:</p>
<h3 id="56. Why is unsupervised alignment of English embeddings from different algorithms so hard?">56. Why is unsupervised alignment of English embeddings from different algorithms so hard?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1056/">Paper Link</a>】    【Pages】:582-586</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hartmann:Mareike">Mareike Hartmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kementchedjhieva:Yova">Yova Kementchedjhieva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Søgaard</a></p>
<p>【Abstract】:
This paper presents a challenge to the community: Generative adversarial networks (GANs) can perfectly align independent English word embeddings induced using the same algorithm, based on distributional information alone; but fails to do so, for two different embeddings algorithms. Why is that? We believe understanding why, is key to understand both modern word embedding algorithms and the limitations and instability dynamics of GANs. This paper shows that (a) in all these cases, where alignment fails, there exists a linear transform between the two embeddings (so algorithm biases do not lead to non-linear differences), and (b) similar effects can not easily be obtained by varying hyper-parameters. One plausible suggestion based on our initial experiments is that the differences in the inductive biases of the embedding algorithms lead to an optimization landscape that is riddled with local optima, leading to a very small basin of convergence, but we present this more as a challenge paper than a technical contribution.</p>
<p>【Keywords】:</p>
<h3 id="57. Quantifying Context Overlap for Training Word Embeddings.">57. Quantifying Context Overlap for Training Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1057/">Paper Link</a>】    【Pages】:587-593</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhuang:Yimeng">Yimeng Zhuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Jinghui">Jinghui Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Yinhe">Yinhe Zheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Xuan">Xuan Zhu</a></p>
<p>【Abstract】:
Most models for learning word embeddings are trained based on the context information of words, more precisely first order co-occurrence relations. In this paper, a metric is designed to estimate second order co-occurrence relations based on context overlap. The estimated values are further used as the augmented data to enhance the learning of word embeddings by joint training with existing neural word embedding models. Experimental results show that better word vectors can be obtained for word similarity tasks and some downstream NLP tasks by the enhanced approach.</p>
<p>【Keywords】:</p>
<h3 id="58. Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space.">58. Neural Latent Relational Analysis to Capture Lexical Semantic Relations in a Vector Space.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1058/">Paper Link</a>】    【Pages】:594-600</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Washio:Koki">Koki Washio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kato:Tsuneaki">Tsuneaki Kato</a></p>
<p>【Abstract】:
Capturing the semantic relations of words in a vector space contributes to many natural language processing tasks. One promising approach exploits lexico-syntactic patterns as features of word pairs. In this paper, we propose a novel model of this pattern-based approach, neural latent relational analysis (NLRA). NLRA can generalize co-occurrences of word pairs and lexico-syntactic patterns, and obtain embeddings of the word pairs that do not co-occur. This overcomes the critical data sparseness problem encountered in previous pattern-based models. Our experimental results on measuring relational similarity demonstrate that NLRA outperforms the previous pattern-based models. In addition, when combined with a vector offset model, NLRA achieves a performance comparable to that of the state-of-the-art model that exploits additional semantic relational data.</p>
<p>【Keywords】:</p>
<h3 id="59. Generalizing Word Embeddings using Bag of Subwords.">59. Generalizing Word Embeddings using Bag of Subwords.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1059/">Paper Link</a>】    【Pages】:601-606</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jinman">Jinman Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mudgal:Sidharth">Sidharth Mudgal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Yingyu">Yingyu Liang</a></p>
<p>【Abstract】:
We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional contextual information. We propose a subword-level word vector generation model that views words as bags of character n-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our model’s ability in capturing the relationship between words’ textual representations and their embeddings.</p>
<p>【Keywords】:</p>
<h3 id="60. Neural Metaphor Detection in Context.">60. Neural Metaphor Detection in Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1060/">Paper Link</a>】    【Pages】:607-613</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Ge">Ge Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Eunsol">Eunsol Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
We present end-to-end neural models for detecting metaphorical word use in context. We show that relatively standard BiLSTM models which operate on complete sentences work well in this setting, in comparison to previous work that used more restricted forms of linguistic context. These models establish a new state-of-the-art on existing verb metaphor detection benchmarks, and show strong performance on jointly predicting the metaphoricity of all words in a running text.</p>
<p>【Keywords】:</p>
<h3 id="61. Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging.">61. Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1061/">Paper Link</a>】    【Pages】:614-620</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Plank:Barbara">Barbara Plank</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agic:Zeljko">Zeljko Agic</a></p>
<p>【Abstract】:
a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages. The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework. The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.</p>
<p>【Keywords】:</p>
<h3 id="62. Unsupervised Bilingual Lexicon Induction via Latent Variable Models.">62. Unsupervised Bilingual Lexicon Induction via Latent Variable Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1062/">Paper Link</a>】    【Pages】:621-626</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dou:Zi=Yi">Zi-Yi Dou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Zhi=Hao">Zhi-Hao Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Shujian">Shujian Huang</a></p>
<p>【Abstract】:
Bilingual lexicon extraction has been studied for decades and most previous methods have relied on parallel corpora or bilingual dictionaries. Recent studies have shown that it is possible to build a bilingual dictionary by aligning monolingual word embedding spaces in an unsupervised way. With the recent advances in generative models, we propose a novel approach which builds cross-lingual dictionaries via latent variable models and adversarial training with no parallel corpora. To demonstrate the effectiveness of our approach, we evaluate our approach on several language pairs and the experimental results show that our model could achieve competitive and even superior performance compared with several state-of-the-art models.</p>
<p>【Keywords】:</p>
<h3 id="63. Learning Unsupervised Word Translations Without Adversaries.">63. Learning Unsupervised Word Translations Without Adversaries.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1063/">Paper Link</a>】    【Pages】:627-632</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mukherjee:Tanmoy">Tanmoy Mukherjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yamada:Makoto">Makoto Yamada</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hospedales:Timothy_M=">Timothy M. Hospedales</a></p>
<p>【Abstract】:
Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks. Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora. However, state of the art methods unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyper-parameter sensitivity. We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised – no seed dictionary or parallel corpora required; and introduces no adversary – therefore being much easier to train. Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.</p>
<p>【Keywords】:</p>
<h3 id="64. Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification.">64. Adversarial Training for Multi-task and Multi-lingual Joint Modeling of Utterance Intent Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1064/">Paper Link</a>】    【Pages】:633-639</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Masumura:Ryo">Ryo Masumura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shinohara:Yusuke">Yusuke Shinohara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Higashinaka:Ryuichiro">Ryuichiro Higashinaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aono:Yushi">Yushi Aono</a></p>
<p>【Abstract】:
This paper proposes an adversarial training method for the multi-task and multi-lingual joint modeling needed for utterance intent classification. In joint modeling, common knowledge can be efficiently utilized among multiple tasks or multiple languages. This is achieved by introducing both language-specific networks shared among different tasks and task-specific networks shared among different languages. However, the shared networks are often specialized in majority tasks or languages, so performance degradation must be expected for some minor data sets. In order to improve the invariance of shared networks, the proposed method introduces both language-specific task adversarial networks and task-specific language adversarial networks; both are leveraged for purging the task or language dependencies of the shared networks. The effectiveness of the adversarial training proposal is demonstrated using Japanese and English data sets for three different utterance intent classification tasks.</p>
<p>【Keywords】:</p>
<h3 id="65. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.">65. Surprisingly Easy Hard-Attention for Sequence to Sequence Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1065/">Paper Link</a>】    【Pages】:640-645</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shankar:Shiv">Shiv Shankar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Garg:Siddhant">Siddhant Garg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarawagi:Sunita">Sunita Sarawagi</a></p>
<p>【Abstract】:
In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.</p>
<p>【Keywords】:</p>
<h3 id="66. Joint Learning for Emotion Classification and Emotion Cause Detection.">66. Joint Learning for Emotion Classification and Emotion Cause Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1066/">Paper Link</a>】    【Pages】:646-651</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0012:Ying">Ying Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Wenjun">Wenjun Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Xiyao">Xiyao Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shoushan">Shoushan Li</a></p>
<p>【Abstract】:
We present a neural network-based joint approach for emotion classification and emotion cause detection, which attempts to capture mutual benefits across the two sub-tasks of emotion analysis. Considering that emotion classification and emotion cause detection need different kinds of features (affective and event-based separately), we propose a joint encoder which uses a unified framework to extract features for both sub-tasks and a joint model trainer which simultaneously learns two models for the two sub-tasks separately. Our experiments on Chinese microblogs show that the joint approach is very promising.</p>
<p>【Keywords】:</p>
<h3 id="67. Exploring Optimism and Pessimism in Twitter Using Deep Learning.">67. Exploring Optimism and Pessimism in Twitter Using Deep Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1067/">Paper Link</a>】    【Pages】:652-658</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Caragea:Cornelia">Cornelia Caragea</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dinu:Liviu_P=">Liviu P. Dinu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dumitru:Bogdan">Bogdan Dumitru</a></p>
<p>【Abstract】:
Identifying optimistic and pessimistic viewpoints and users from Twitter is useful for providing better social support to those who need such support, and for minimizing the negative influence among users and maximizing the spread of positive attitudes and ideas. In this paper, we explore a range of deep learning models to predict optimism and pessimism in Twitter at both tweet and user level and show that these models substantially outperform traditional machine learning classifiers used in prior work. In addition, we show evidence that a sentiment classifier would not be sufficient for accurately predicting optimism and pessimism in Twitter. Last, we study the verb tense usage as well as the presence of polarity words in optimistic and pessimistic tweets.</p>
<p>【Keywords】:</p>
<h3 id="68. Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning.">68. Predicting News Headline Popularity with Syntactic and Semantic Knowledge Using Multi-Task Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1068/">Paper Link</a>】    【Pages】:659-664</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lamprinidis:Sotiris">Sotiris Lamprinidis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hardt:Daniel">Daniel Hardt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Dirk">Dirk Hovy</a></p>
<p>【Abstract】:
Newspapers need to attract readers with headlines, anticipating their readers’ preferences. These preferences rely on topical, structural, and lexical factors. We model each of these factors in a multi-task GRU network to predict headline popularity. We find that pre-trained word embeddings provide significant improvements over untrained embeddings, as do the combination of two auxiliary tasks, news-section prediction and part-of-speech tagging. However, we also find that performance is very similar to that of a simple Logistic Regression model over character n-grams. Feature analysis reveals structural patterns of headline popularity, including the use of forward-looking deictic expressions and second person pronouns.</p>
<p>【Keywords】:</p>
<h3 id="69. Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates.">69. Hybrid Neural Attention for Agreement/Disagreement Inference in Online Debates.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1069/">Paper Link</a>】    【Pages】:665-670</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Di">Di Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Jiachen">Jiachen Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Ruifeng">Ruifeng Xu</a></p>
<p>【Abstract】:
Inferring the agreement/disagreement relation in debates, especially in online debates, is one of the fundamental tasks in argumentation mining. The expressions of agreement/disagreement usually rely on argumentative expressions in text as well as interactions between participants in debates. Previous works usually lack the capability of jointly modeling these two factors. To alleviate this problem, this paper proposes a hybrid neural attention model which combines self and cross attention mechanism to locate salient part from textual context and interaction between users. Experimental results on three (dis)agreement inference datasets show that our model outperforms the state-of-the-art models.</p>
<p>【Keywords】:</p>
<h3 id="70. Improving Author Attribute Prediction by Retrofitting Linguistic Representations with Homophily.">70. Improving Author Attribute Prediction by Retrofitting Linguistic Representations with Homophily.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1070/">Paper Link</a>】    【Pages】:671-677</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Dirk">Dirk Hovy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fornaciari:Tommaso">Tommaso Fornaciari</a></p>
<p>【Abstract】:
Most text-classification approaches represent the input based on textual features, either feature-based or continuous. However, this ignores strong non-linguistic similarities like homophily: people within a demographic group use language more similar to each other than to non-group members. We use homophily cues to retrofit text-based author representations with non-linguistic information, and introduce a trade-off parameter. This approach increases in-class similarity between authors, and improves classification performance by making classes more linearly separable. We evaluate the effect of our method on two author-attribute prediction tasks with various training-set sizes and parameter settings. We find that our method can significantly improve classification performance, especially when the number of labels is large and limited labeled data is available. It is potentially applicable as preprocessing step to any text-classification task.</p>
<p>【Keywords】:</p>
<h3 id="71. A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation.">71. A Syntactically Constrained Bidirectional-Asynchronous Approach for Emotional Conversation Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1071/">Paper Link</a>】    【Pages】:678-683</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jingyuan">Jingyuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Xiao">Xiao Sun</a></p>
<p>【Abstract】:
Traditional neural language models tend to generate generic replies with poor logic and no emotion. In this paper, a syntactically constrained bidirectional-asynchronous approach for emotional conversation generation (E-SCBA) is proposed to address this issue. In our model, pre-generated emotion keywords and topic keywords are asynchronously introduced into the process of decoding. It is much different from most existing methods which generate replies from the first word to the last. Through experiments, the results indicate that our approach not only improves the diversity of replies, but gains a boost on both logic and emotion compared with baselines.</p>
<p>【Keywords】:</p>
<h3 id="72. Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning.">72. Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1072/">Paper Link</a>】    【Pages】:684-689</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Chen">Chen Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Qi">Qi Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sha:Lei">Lei Sha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Lintao">Lintao Zhang</a></p>
<p>【Abstract】:
The lack of labeled data is one of the main challenges when building a task-oriented dialogue system. Existing dialogue datasets usually rely on human labeling, which is expensive, limited in size, and in low coverage. In this paper, we instead propose our framework auto-dialabel to automatically cluster the dialogue intents and slots. In this framework, we collect a set of context features, leverage an autoencoder for feature assembly, and adapt a dynamic hierarchical clustering method for intent and slot labeling. Experimental results show that our framework can promote human labeling cost to a great extent, achieve good intent clustering accuracy (84.1%), and provide reasonable and instructive slot labeling results.</p>
<p>【Keywords】:</p>
<h3 id="73. Extending Neural Generative Conversational Model using External Knowledge Sources.">73. Extending Neural Generative Conversational Model using External Knowledge Sources.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1073/">Paper Link</a>】    【Pages】:690-695</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Parthasarathi:Prasanna">Prasanna Parthasarathi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pineau:Joelle">Joelle Pineau</a></p>
<p>【Abstract】:
The use of connectionist approaches in conversational agents has been progressing rapidly due to the availability of large corpora. However current generative dialogue models often lack coherence and are content poor. This work proposes an architecture to incorporate unstructured knowledge sources to enhance the next utterance prediction in chit-chat type of generative dialogue models. We focus on Sequence-to-Sequence (Seq2Seq) conversational agents trained with the Reddit News dataset, and consider incorporating external knowledge from Wikipedia summaries as well as from the NELL knowledge base. Our experiments show faster training time and improved perplexity when leveraging external knowledge.</p>
<p>【Keywords】:</p>
<h3 id="74. Modeling Temporality of Human Intentions by Domain Adaptation.">74. Modeling Temporality of Human Intentions by Domain Adaptation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1074/">Paper Link</a>】    【Pages】:696-701</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xiaolei">Xiaolei Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Lixing">Lixing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carey:Kate_B=">Kate B. Carey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Woolley:Joshua">Joshua Woolley</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Scherer:Stefan">Stefan Scherer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Borsari:Brian">Brian Borsari</a></p>
<p>【Abstract】:
Categorizing patient’s intentions in conversational assessment can help decision making in clinical treatments. Many conversation corpora span broaden a series of time stages. However, it is not clear that how the themes shift in the conversation impact on the performance of human intention categorization (eg., patients might show different behaviors during the beginning versus the end). This paper proposes a method that models the temporal factor by using domain adaptation on clinical dialogue corpora, Motivational Interviewing (MI). We deploy Bi-LSTM and topic model jointly to learn language usage change across different time sessions. We conduct experiments on the MI corpora to show the promising improvement after considering temporality in the classification task.</p>
<p>【Keywords】:</p>
<h3 id="75. An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation.">75. An Auto-Encoder Matching Model for Learning Utterance-Level Semantic Dependency in Dialogue Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1075/">Paper Link</a>】    【Pages】:702-707</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Liangchen">Liangchen Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingjing">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Junyang">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Qi">Qi Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
Generating semantically coherent responses is still a major challenge in dialogue generation. Different from conventional text generation tasks, the mapping between inputs and responses in conversations is more complicated, which highly demands the understanding of utterance-level semantic dependency, a relation between the whole meanings of inputs and outputs. To address this problem, we propose an Auto-Encoder Matching (AEM) model to learn such dependency. The model contains two auto-encoders and one mapping module. The auto-encoders learn the semantic representations of inputs and responses, and the mapping module learns to connect the utterance-level representations. Experimental results from automatic and human evaluations demonstrate that our model is capable of generating responses of high coherence and fluency compared to baseline models.</p>
<p>【Keywords】:</p>
<h3 id="76. A Dataset for Document Grounded Conversations.">76. A Dataset for Document Grounded Conversations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1076/">Paper Link</a>】    【Pages】:708-713</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Kangyan">Kangyan Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prabhumoye:Shrimai">Shrimai Prabhumoye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Black:Alan_W=">Alan W. Black</a></p>
<p>【Abstract】:
This paper introduces a document grounded dataset for conversations. We define “Document Grounded Conversations” as conversations that are about the contents of a specified document. In this dataset the specified documents were Wikipedia articles about popular movies. The dataset contains 4112 conversations with an average of 21.43 turns per conversation. This positions this dataset to not only provide a relevant chat history while generating responses but also provide a source of information that the models could use. We describe two neural architectures that provide benchmark performance on the task of generating the next response. We also evaluate our models for engagement and fluency, and find that the information from the document helps in generating more engaging and fluent responses.</p>
<p>【Keywords】:</p>
<h3 id="77. Out-of-domain Detection based on Generative Adversarial Network.">77. Out-of-domain Detection based on Generative Adversarial Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1077/">Paper Link</a>】    【Pages】:714-718</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ryu:Seonghan">Seonghan Ryu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koo:Sangjun">Sangjun Koo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Hwanjo">Hwanjo Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Gary_Geunbae">Gary Geunbae Lee</a></p>
<p>【Abstract】:
The main goal of this paper is to develop out-of-domain (OOD) detection for dialog systems. We propose to use only in-domain (IND) sentences to build a generative adversarial network (GAN) of which the discriminator generates low scores for OOD sentences. To improve basic GANs, we apply feature matching loss in the discriminator, use domain-category analysis as an additional task in the discriminator, and remove the biases in the generator. Thereby, we reduce the huge effort of collecting OOD sentences for training OOD detection. For evaluation, we experimented OOD detection on a multi-domain dialog system. The experimental results showed the proposed method was most accurate compared to the existing methods.</p>
<p>【Keywords】:</p>
<h3 id="78. Listening Comprehension over Argumentative Content.">78. Listening Comprehension over Argumentative Content.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1078/">Paper Link</a>】    【Pages】:719-724</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mirkin:Shachar">Shachar Mirkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moshkowich:Guy">Guy Moshkowich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Orbach:Matan">Matan Orbach</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kotlerman:Lili">Lili Kotlerman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kantor:Yoav">Yoav Kantor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lavee:Tamar">Tamar Lavee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jacovi:Michal">Michal Jacovi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bilu:Yonatan">Yonatan Bilu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aharonov:Ranit">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slonim:Noam">Noam Slonim</a></p>
<p>【Abstract】:
This paper presents a task for machine listening comprehension in the argumentation domain and a corresponding dataset in English. We recorded 200 spontaneous speeches arguing for or against 50 controversial topics. For each speech, we formulated a question, aimed at confirming or rejecting the occurrence of potential arguments in the speech. Labels were collected by listening to the speech and marking which arguments were mentioned by the speaker. We applied baseline methods addressing the task, to be used as a benchmark for future work over this dataset. All data used in this work is freely available for research.</p>
<p>【Keywords】:</p>
<h3 id="79. Using Active Learning to Expand Training Data for Implicit Discourse Relation Recognition.">79. Using Active Learning to Expand Training Data for Implicit Discourse Relation Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1079/">Paper Link</a>】    【Pages】:725-731</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruan:Huibin">Huibin Ruan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jianmin">Jianmin Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>【Abstract】:
We tackle discourse-level relation recognition, a problem of determining semantic relations between text spans. Implicit relation recognition is challenging due to the lack of explicit relational clues. The increasingly popular neural network techniques have been proven effective for semantic encoding, whereby widely employed to boost semantic relation discrimination. However, learning to predict semantic relations at a deep level heavily relies on a great deal of training data, but the scale of the publicly available data in this field is limited. In this paper, we follow Rutherford and Xue (2015) to expand the training data set using the corpus of explicitly-related arguments, by arbitrarily dropping the overtly presented discourse connectives. On the basis, we carry out an experiment of sampling, in which a simple active learning approach is used, so as to take the informative instances for data expansion. The goal is to verify whether the selective use of external data not only reduces the time consumption of retraining but also ensures a better system performance. Using the expanded training data, we retrain a convolutional neural network (CNN) based classifer which is a simplified version of Qin et al. (2016)’s stacking gated relation recognizer. Experimental results show that expanding the training set with small-scale carefully-selected external data yields substantial performance gain, with the improvements of about 4% for accuracy and 3.6% for F-score. This allows a weak classifier to achieve a comparable performance against the state-of-the-art systems.</p>
<p>【Keywords】:</p>
<h3 id="80. Learning To Split and Rephrase From Wikipedia Edit History.">80. Learning To Split and Rephrase From Wikipedia Edit History.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1080/">Paper Link</a>】    【Pages】:732-737</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Botha:Jan_A=">Jan A. Botha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Faruqui:Manaal">Manaal Faruqui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Alex:John">John Alex</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldridge:Jason">Jason Baldridge</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Das_0001:Dipanjan">Dipanjan Das</a></p>
<p>【Abstract】:
Split and rephrase is the task of breaking down a sentence into shorter ones that together convey the same meaning. We extract a rich new dataset for this task by mining Wikipedia’s edit history: WikiSplit contains one million naturally occurring sentence rewrites, providing sixty times more distinct split examples and a ninety times larger vocabulary than the WebSplit corpus introduced by Narayan et al. (2017) as a benchmark for this task. Incorporating WikiSplit as training data produces a model with qualitatively better predictions that score 32 BLEU points above the prior best result on the WebSplit benchmark.</p>
<p>【Keywords】:</p>
<h3 id="81. BLEU is Not Suitable for the Evaluation of Text Simplification.">81. BLEU is Not Suitable for the Evaluation of Text Simplification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1081/">Paper Link</a>】    【Pages】:738-744</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sulem:Elior">Elior Sulem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rappoport:Ari">Ari Rappoport</a></p>
<p>【Abstract】:
BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments. We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences.</p>
<p>【Keywords】:</p>
<h3 id="82. S2SPMN: A Simple and Effective Framework for Response Generation with Relevant Information.">82. S2SPMN: A Simple and Effective Framework for Response Generation with Relevant Information.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1082/">Paper Link</a>】    【Pages】:745-750</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pei:Jiaxin">Jiaxin Pei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chenliang">Chenliang Li</a></p>
<p>【Abstract】:
How to generate relevant and informative responses is one of the core topics in response generation area. Following the task formulation of machine translation, previous works mainly consider response generation task as a mapping from a source sentence to a target sentence. To realize this mapping, existing works tend to design intuitive but complex models. However, the relevant information existed in large dialogue corpus is mainly overlooked. In this paper, we propose Sequence to Sequence with Prototype Memory Network (S2SPMN) to exploit the relevant information provided by the large dialogue corpus to enhance response generation. Specifically, we devise two simple approaches in S2SPMN to select the relevant information (named prototypes) from the dialogue corpus. These prototypes are then saved into prototype memory network (PMN). Furthermore, a hierarchical attention mechanism is devised to extract the semantic information from the PMN to assist the response generation process. Empirical studies reveal the advantage of our model over several classical and strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="83. Improving Reinforcement Learning Based Image Captioning with Natural Language Prior.">83. Improving Reinforcement Learning Based Image Captioning with Natural Language Prior.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1083/">Paper Link</a>】    【Pages】:751-756</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Tszhang">Tszhang Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Shiyu">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Mo">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bai:Kun">Kun Bai</a></p>
<p>【Abstract】:
Recently, Reinforcement Learning (RL) approaches have demonstrated advanced performance in image captioning by directly optimizing the metric used for testing. However, this shaped reward introduces learning biases, which reduces the readability of generated text. In addition, the large sample space makes training unstable and slow.To alleviate these issues, we propose a simple coherent solution that constrains the action space using an n-gram language prior. Quantitative and qualitative evaluations on benchmarks show that RL with the simple add-on module performs favorably against its counterpart in terms of both readability and speed of convergence. Human evaluation results show that our model is more human readable and graceful. The implementation will become publicly available upon the acceptance of the paper.</p>
<p>【Keywords】:</p>
<h3 id="84. Training for Diversity in Image Paragraph Captioning.">84. Training for Diversity in Image Paragraph Captioning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1084/">Paper Link</a>】    【Pages】:757-761</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Melas=Kyriazi:Luke">Luke Melas-Kyriazi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:George">George Han</a></p>
<p>【Abstract】:
Image paragraph captioning models aim to produce detailed descriptions of a source image. These models use similar techniques as standard image captioning models, but they have encountered issues in text generation, notably a lack of diversity between sentences, that have limited their effectiveness. In this work, we consider applying sequence-level training for this task. We find that standard self-critical training produces poor results, but when combined with an integrated penalty on trigram repetition produces much more diverse paragraphs. This simple training approach improves on the best result on the Visual Genome paragraph captioning dataset from 16.9 to 30.6 CIDEr, with gains on METEOR and BLEU as well, without requiring any architectural changes.</p>
<p>【Keywords】:</p>
<h3 id="85. A Graph-Theoretic Summary Evaluation for Rouge.">85. A Graph-Theoretic Summary Evaluation for Rouge.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1085/">Paper Link</a>】    【Pages】:762-767</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/ShafieiBavani:Elaheh">Elaheh ShafieiBavani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ebrahimi:Mohammad">Mohammad Ebrahimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Raymond_K=">Raymond K. Wong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0001:Fang">Fang Chen</a></p>
<p>【Abstract】:
ROUGE is one of the first and most widely used evaluation metrics for text summarization. However, its assessment merely relies on surface similarities between peer and model summaries. Consequently, ROUGE is unable to fairly evaluate summaries including lexical variations and paraphrasing. We propose a graph-based approach adopted into ROUGE to evaluate summaries based on both lexical and semantic similarities. Experiment results over TAC AESOP datasets show that exploiting the lexico-semantic similarity of the words used in summaries would significantly help ROUGE correlate better with human judgments.</p>
<p>【Keywords】:</p>
<h3 id="86. Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation.">86. Guided Neural Language Generation for Abstractive Summarization using Abstract Meaning Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1086/">Paper Link</a>】    【Pages】:768-773</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hardy:">Hardy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vlachos:Andreas">Andreas Vlachos</a></p>
<p>【Abstract】:
Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standard AMR parses and parses obtained from an off-the-shelf parser respectively. We also find that the summarization performance on later parses is 2 ROUGE-2 points higher than that of a well-established neural encoder-decoder approach trained on a larger dataset.</p>
<p>【Keywords】:</p>
<h3 id="87. Evaluating Multiple System Summary Lengths: A Case Study.">87. Evaluating Multiple System Summary Lengths: A Case Study.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1087/">Paper Link</a>】    【Pages】:774-778</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shapira:Ori">Ori Shapira</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gabay:David">David Gabay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ronen:Hadar">Hadar Ronen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bar=Ilan:Judit">Judit Bar-Ilan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Amsterdamer:Yael">Yael Amsterdamer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nenkova:Ani">Ani Nenkova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dagan:Ido">Ido Dagan</a></p>
<p>【Abstract】:
Practical summarization systems are expected to produce summaries of varying lengths, per user needs. While a couple of early summarization benchmarks tested systems across multiple summary lengths, this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths. In this paper, we raise the research question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths. For that, we have analyzed a couple of datasets as a case study, using several variants of the ROUGE metric that are standard in summarization evaluation. Our findings indicate that the evaluation protocol in question is indeed competitive. This result paves the way to practically evaluating varying-length summaries with simple, possibly existing, summarization benchmarks.</p>
<p>【Keywords】:</p>
<h3 id="88. Neural Latent Extractive Document Summarization.">88. Neural Latent Extractive Document Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1088/">Paper Link</a>】    【Pages】:779-784</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xingxing">Xingxing Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a></p>
<p>【Abstract】:
Extractive summarization models need sentence level labels, which are usually created with rule-based methods since most summarization datasets only have document summary pairs. These labels might be suboptimal. We propose a latent variable extractive model, where sentences are viewed as latent variables and sentences with activated variables are used to infer gold summaries. During training, the loss can come directly from gold summaries. Experiments on CNN/Dailymail dataset show our latent extractive model outperforms a strong extractive baseline trained on rule-based labels and also performs competitively with several recent models.</p>
<p>【Keywords】:</p>
<h3 id="89. On the Abstractiveness of Neural Document Summarization.">89. On the Abstractiveness of Neural Document Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1089/">Paper Link</a>】    【Pages】:785-790</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Fangfang">Fangfang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jin=ge">Jin-ge Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Rui">Rui Yan</a></p>
<p>【Abstract】:
Many modern neural document summarization systems based on encoder-decoder networks are designed to produce abstractive summaries. We attempted to verify the degree of abstractiveness of modern neural abstractive summarization systems by calculating overlaps in terms of various types of units. Upon the observation that many abstractive systems tend to be near-extractive in practice, we also implemented a pure copy system, which achieved comparable results as abstractive summarizers while being far more computationally efficient. These findings suggest the possibility for future efforts towards more efficient systems that could better utilize the vocabulary in the original document.</p>
<p>【Keywords】:</p>
<h3 id="90. Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.">90. Automatic Essay Scoring Incorporating Rating Schema via Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1090/">Paper Link</a>】    【Pages】:791-797</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yucheng">Yucheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Zhongyu">Zhongyu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Yaqian">Yaqian Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
Automatic essay scoring (AES) is the task of assigning grades to essays without human interference. Existing systems for AES are typically trained to predict the score of each single essay at a time without considering the rating schema. In order to address this issue, we propose a reinforcement learning framework for essay scoring that incorporates quadratic weighted kappa as guidance to optimize the scoring system. Experiment results on benchmark datasets show the effectiveness of our framework.</p>
<p>【Keywords】:</p>
<h3 id="91. Identifying Well-formed Natural Language Questions.">91. Identifying Well-formed Natural Language Questions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1091/">Paper Link</a>】    【Pages】:798-803</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Faruqui:Manaal">Manaal Faruqui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Das_0001:Dipanjan">Dipanjan Das</a></p>
<p>【Abstract】:
Understanding search queries is a hard problem as it involves dealing with “word salad” text ubiquitously issued by users. However, if a query resembles a well-formed question, a natural language processing pipeline is able to perform more accurate interpretation, thus reducing downstream compounding errors. Hence, identifying whether or not a query is well formed can enhance query understanding. Here, we introduce a new task of identifying a well-formed natural language question. We construct and release a dataset of 25,100 publicly available questions classified into well-formed and non-wellformed categories and report an accuracy of 70.7% on the test set. We also show that our classifier can be used to improve the performance of neural sequence-to-sequence models for generating questions for reading comprehension.</p>
<p>【Keywords】:</p>
<h3 id="92. Self-Governing Neural Networks for On-Device Short Text Classification.">92. Self-Governing Neural Networks for On-Device Short Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1092/">Paper Link</a>】    【Pages】:804-810</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ravi:Sujith">Sujith Ravi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kozareva:Zornitsa">Zornitsa Kozareva</a></p>
<p>【Abstract】:
Deep neural networks reach state-of-the-art performance for wide range of natural language processing, computer vision and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as mobile phones or smart watches with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and complex networks with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy.</p>
<p>【Keywords】:</p>
<h3 id="93. HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization.">93. HFT-CNN: Learning Hierarchical Category Structure for Multi-label Short Text Categorization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1093/">Paper Link</a>】    【Pages】:811-816</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shimura:Kazuya">Kazuya Shimura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jiyi">Jiyi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fukumoto:Fumiyo">Fumiyo Fukumoto</a></p>
<p>【Abstract】:
We focus on the multi-label categorization task for short texts and explore the use of a hierarchical structure (HS) of categories. In contrast to the existing work using non-hierarchical flat model, the method leverages the hierarchical relations between the pre-defined categories to tackle the data sparsity problem. The lower the HS level, the less the categorization performance. Because the number of training data per category in a lower level is much smaller than that in an upper level. We propose an approach which can effectively utilize the data in the upper levels to contribute the categorization in the lower levels by applying the Convolutional Neural Network (CNN) with a fine-tuning technique. The results using two benchmark datasets show that proposed method, Hierarchical Fine-Tuning based CNN (HFT-CNN) is competitive with the state-of-the-art CNN based methods.</p>
<p>【Keywords】:</p>
<h3 id="94. A Hierarchical Neural Attention-based Text Classifier.">94. A Hierarchical Neural Attention-based Text Classifier.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1094/">Paper Link</a>】    【Pages】:817-823</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sinha:Koustuv">Koustuv Sinha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Yue">Yue Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruths:Derek">Derek Ruths</a></p>
<p>【Abstract】:
Deep neural networks have been displaying superior performance over traditional supervised classifiers in text classification. They learn to extract useful features automatically when sufficient amount of data is presented. However, along with the growth in the number of documents comes the increase in the number of categories, which often results in poor performance of the multiclass classifiers. In this work, we use external knowledge in the form of topic category taxonomies to aide the classification by introducing a deep hierarchical neural attention-based classifier. Our model performs better than or comparable to state-of-the-art hierarchical models at significantly lower computational cost while maintaining high interpretability.</p>
<p>【Keywords】:</p>
<h3 id="95. Labeled Anchors and a Scalable, Transparent, and Interactive Classifier.">95. Labeled Anchors and a Scalable, Transparent, and Interactive Classifier.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1095/">Paper Link</a>】    【Pages】:824-829</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lund:Jeffrey">Jeffrey Lund</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cowley:Stephen">Stephen Cowley</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fearn:Wilson">Wilson Fearn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hales:Emily">Emily Hales</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Seppi:Kevin_D=">Kevin D. Seppi</a></p>
<p>【Abstract】:
We propose Labeled Anchors, an interactive and supervised topic model based on the anchor words algorithm (Arora et al., 2013). Labeled Anchors is similar to Supervised Anchors (Nguyen et al., 2014) in that it extends the vector-space representation of words to include document labels. However, our formulation also admits a classifier which requires no training beyond inferring topics, which means our approach is also fast enough to be interactive. We run a small user study that demonstrates that untrained users can interactively update topics in order to improve classification accuracy.</p>
<p>【Keywords】:</p>
<h3 id="96. Coherence-Aware Neural Topic Modeling.">96. Coherence-Aware Neural Topic Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1096/">Paper Link</a>】    【Pages】:830-836</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Ran">Ran Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nallapati:Ramesh">Ramesh Nallapati</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiang:Bing">Bing Xiang</a></p>
<p>【Abstract】:
Topic models are evaluated based on their ability to describe documents well (i.e. low perplexity) and to produce topics that carry coherent semantic meaning. In topic modeling so far, perplexity is a direct optimization target. However, topic coherence, owing to its challenging computation, is not optimized for and is only evaluated after training. In this work, under a neural variational inference framework, we propose methods to incorporate a topic coherence objective into the training process. We demonstrate that such a coherence-aware topic model exhibits a similar level of perplexity as baseline models but achieves substantially higher topic coherence.</p>
<p>【Keywords】:</p>
<h3 id="97. Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.">97. Utilizing Character and Word Embeddings for Text Normalization with Sequence-to-Sequence Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1097/">Paper Link</a>】    【Pages】:837-843</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Watson:Daniel">Daniel Watson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zalmout:Nasser">Nasser Zalmout</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Habash:Nizar">Nizar Habash</a></p>
<p>【Abstract】:
Text normalization is an important enabling technology for several NLP tasks. Recently, neural-network-based approaches have outperformed well-established models in this task. However, in languages other than English, there has been little exploration in this direction. Both the scarcity of annotated data and the complexity of the language increase the difficulty of the problem. To address these challenges, we use a sequence-to-sequence model with character-based attention, which in addition to its self-learned character embeddings, uses word embeddings pre-trained with an approach that also models subword information. This provides the neural model with access to more linguistic information especially suitable for text normalization, without large parallel corpora. We show that providing the model with word-level features bridges the gap for the neural network approach to achieve a state-of-the-art F1 score on a standard Arabic language correction shared task dataset.</p>
<p>【Keywords】:</p>
<h3 id="98. Topic Intrusion for Automatic Topic Model Evaluation.">98. Topic Intrusion for Automatic Topic Model Evaluation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1098/">Paper Link</a>】    【Pages】:844-849</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bhatia:Shraey">Shraey Bhatia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lau:Jey_Han">Jey Han Lau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a></p>
<p>【Abstract】:
Topic coherence is increasingly being used to evaluate topic models and filter topics for end-user applications. Topic coherence measures how well topic words relate to each other, but offers little insight on the utility of the topics in describing the documents. In this paper, we explore the topic intrusion task — the task of guessing an outlier topic given a document and a few topics — and propose a method to automate it. We improve upon the state-of-the-art substantially, demonstrating its viability as an alternative method for topic model evaluation.</p>
<p>【Keywords】:</p>
<h3 id="99. Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents.">99. Supervised and Unsupervised Methods for Robust Separation of Section Titles and Prose Text in Web Documents.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1099/">Paper Link</a>】    【Pages】:850-855</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gopinath:Abhijith_Athreya_Mysore">Abhijith Athreya Mysore Gopinath</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wilson:Shomir">Shomir Wilson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sadeh:Norman_M=">Norman M. Sadeh</a></p>
<p>【Abstract】:
The text in many web documents is organized into a hierarchy of section titles and corresponding prose content, a structure which provides potentially exploitable information on discourse structure and topicality. However, this organization is generally discarded during text collection, and collecting it is not straightforward: the same visual organization can be implemented in a myriad of different ways in the underlying HTML. To remedy this, we present a flexible system for automatically extracting the hierarchical section titles and prose organization of web documents irrespective of differences in HTML representation. This system uses features from syntax, semantics, discourse and markup to build two models which classify HTML text into section titles and prose text. When tested on three different domains of web text, our domain-independent system achieves an overall precision of 0.82 and a recall of 0.98. The domain-dependent variation produces very high precision (0.99) at the expense of recall (0.75). These results exhibit a robust level of accuracy suitable for enhancing question answering, information extraction, and summarization.</p>
<p>【Keywords】:</p>
<h3 id="100. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation.">100. SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1100/">Paper Link</a>】    【Pages】:856-861</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Xinyi">Xinyi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pham:Hieu">Hieu Pham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Zihang">Zihang Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.</p>
<p>【Keywords】:</p>
<h3 id="101. Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder.">101. Improving Unsupervised Word-by-Word Translation with Language Model and Denoising Autoencoder.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1101/">Paper Link</a>】    【Pages】:862-868</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Yunsu">Yunsu Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Geng:Jiahui">Jiahui Geng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ney:Hermann">Hermann Ney</a></p>
<p>【Abstract】:
Unsupervised learning of cross-lingual word embedding offers elegant matching of words across languages, but has fundamental limitations in translating sentences. In this paper, we propose simple yet effective methods to improve word-by-word translation of cross-lingual embeddings, using only monolingual corpora but without any back-translation. We integrate a language model for context-aware search, and use a novel denoising autoencoder to handle reordering. Our system surpasses state-of-the-art unsupervised translation systems without costly iterative training. We also analyze the effect of vocabulary size and denoising type on the translation performance, which provides better understanding of learning the cross-lingual word embedding and its usage in translation.</p>
<p>【Keywords】:</p>
<h3 id="102. Decipherment of Substitution Ciphers with Neural Language Models.">102. Decipherment of Substitution Ciphers with Neural Language Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1102/">Paper Link</a>】    【Pages】:869-874</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kambhatla:Nishant">Nishant Kambhatla</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bigvand:Anahita_Mansouri">Anahita Mansouri Bigvand</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarkar:Anoop">Anoop Sarkar</a></p>
<p>【Abstract】:
Decipherment of homophonic substitution ciphers using language models is a well-studied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram language models. The most widely used technique is the use of beam search with n-gram language models proposed by Nuhn et al.(2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural language model. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural language model. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes.</p>
<p>【Keywords】:</p>
<h3 id="103. Rapid Adaptation of Neural Machine Translation to New Languages.">103. Rapid Adaptation of Neural Machine Translation to New Languages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1103/">Paper Link</a>】    【Pages】:875-880</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Junjie">Junjie Hu</a></p>
<p>【Abstract】:
This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.</p>
<p>【Keywords】:</p>
<h3 id="104. Compact Personalized Models for Neural Machine Translation.">104. Compact Personalized Models for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1104/">Paper Link</a>】    【Pages】:881-886</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wuebker:Joern">Joern Wuebker</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Simianer:Patrick">Patrick Simianer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/DeNero:John">John DeNero</a></p>
<p>【Abstract】:
We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture–combining a state-of-the-art self-attentive model with compact domain adaptation–provides high quality personalized machine translation that is both space and time efficient.</p>
<p>【Keywords】:</p>
<h3 id="105. Self-Governing Neural Networks for On-Device Short Text Classification.">105. Self-Governing Neural Networks for On-Device Short Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1105/">Paper Link</a>】    【Pages】:887-893</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ravi:Sujith">Sujith Ravi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kozareva:Zornitsa">Zornitsa Kozareva</a></p>
<p>【Abstract】:
Deep neural networks reach state-of-the-art performance for wide range of natural language processing, computer vision and speech applications. Yet, one of the biggest challenges is running these complex networks on devices such as mobile phones or smart watches with tiny memory footprint and low computational capacity. We propose on-device Self-Governing Neural Networks (SGNNs), which learn compact projection vectors with local sensitive hashing. The key advantage of SGNNs over existing work is that they surmount the need for pre-trained word embeddings and complex networks with huge parameters. We conduct extensive evaluation on dialog act classification and show significant improvement over state-of-the-art results. Our findings show that SGNNs are effective at capturing low-dimensional semantic text representations, while maintaining high accuracy.</p>
<p>【Keywords】:</p>
<h3 id="106. Supervised Domain Enablement Attention for Personalized Domain Classification.">106. Supervised Domain Enablement Attention for Personalized Domain Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1106/">Paper Link</a>】    【Pages】:894-899</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Joo=Kyung">Joo-Kyung Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Young=Bum">Young-Bum Kim</a></p>
<p>【Abstract】:
In large-scale domain classification for natural language understanding, leveraging each user’s domain enablement information, which refers to the preferred or authenticated domains by the user, with attention mechanism has been shown to improve the overall domain classification performance. In this paper, we propose a supervised enablement attention mechanism, which utilizes sigmoid activation for the attention weighting so that the attention can be computed with more expressive power without the weight sum constraint of softmax attention. The attention weights are explicitly encouraged to be similar to the corresponding elements of the output one-hot vector, and self-distillation is used to leverage the attention information of the other enabled domains. By evaluating on the actual utterances from a large-scale IPDA, we show that our approach significantly improves domain classification performance</p>
<p>【Keywords】:</p>
<h3 id="107. A Deep Neural Network Sentence Level Classification Method with Context Information.">107. A Deep Neural Network Sentence Level Classification Method with Context Information.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1107/">Paper Link</a>】    【Pages】:900-904</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Song:Xingyi">Xingyi Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Petrak:Johann">Johann Petrak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roberts:Angus">Angus Roberts</a></p>
<p>【Abstract】:
In the sentence classification task, context formed from sentences adjacent to the sentence being classified can provide important information for classification. This context is, however, often ignored. Where methods do make use of context, only small amounts are considered, making it difficult to scale. We present a new method for sentence classification, Context-LSTM-CNN, that makes use of potentially large contexts. The method also utilizes long-range dependencies within the sentence being classified, using an LSTM, and short-span features, using a stacked CNN. Our experiments demonstrate that this approach consistently improves over previous methods on two different datasets.</p>
<p>【Keywords】:</p>
<h3 id="108. Towards Dynamic Computation Graphs via Sparse Latent Structure.">108. Towards Dynamic Computation Graphs via Sparse Latent Structure.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1108/">Paper Link</a>】    【Pages】:905-911</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Niculae:Vlad">Vlad Niculae</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Martins:Andr=eacute=_F=_T=">André F. T. Martins</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cardie:Claire">Claire Cardie</a></p>
<p>【Abstract】:
Deep NLP models benefit from underlying structures in the data—e.g., parse trees—typically extracted using off-the-shelf parsers. Recent attempts to jointly learn the latent structure encounter a tradeoff: either make factorization assumptions that limit expressiveness, or sacrifice end-to-end differentiability. Using the recently proposed SparseMAP inference, which retrieves a sparse distribution over latent structures, we propose a novel approach for end-to-end learning of latent structure predictors jointly with a downstream predictor. To the best of our knowledge, our method is the first to enable unrestricted dynamic computation graph construction from the global latent structure, while maintaining differentiability.</p>
<p>【Keywords】:</p>
<h3 id="109. Convolutional Neural Networks with Recurrent Neural Filters.">109. Convolutional Neural Networks with Recurrent Neural Filters.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1109/">Paper Link</a>】    【Pages】:912-917</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yi">Yi Yang</a></p>
<p>【Abstract】:
We introduce a class of convolutional neural networks (CNNs) that utilize recurrent neural networks (RNNs) as convolution filters. A convolution filter is typically implemented as a linear affine transformation followed by a non-linear function, which fails to account for language compositionality. As a result, it limits the use of high-order filters that are often warranted for natural language processing tasks. In this work, we model convolution filters with RNNs that naturally capture compositionality and long-term dependencies in language. We show that simple CNN architectures equipped with recurrent neural filters (RNFs) achieve results that are on par with the best published ones on the Stanford Sentiment Treebank and two answer sentence selection datasets.</p>
<p>【Keywords】:</p>
<h3 id="110. Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model.">110. Exploiting Rich Syntactic Information for Semantic Parsing with Graph-to-Sequence Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1110/">Paper Link</a>】    【Pages】:918-924</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kun">Kun Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lingfei">Lingfei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhiguo">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Mo">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Liwei">Liwei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sheinin:Vadim">Vadim Sheinin</a></p>
<p>【Abstract】:
Existing neural semantic parsers mainly utilize a sequence encoder, i.e., a sequential LSTM, to extract word order features while neglecting other valuable syntactic information such as dependency or constituent trees. In this paper, we first propose to use the syntactic graph to represent three types of syntactic information, i.e., word order, dependency and constituency features; then employ a graph-to-sequence model to encode the syntactic graph and decode a logical form. Experimental results on benchmark datasets show that our model is comparable to the state-of-the-art on Jobs640, ATIS, and Geo880. Experimental results on adversarial examples demonstrate the robustness of the model is also improved by encoding more syntactic information.</p>
<p>【Keywords】:</p>
<h3 id="111. Retrieval-Based Neural Code Generation.">111. Retrieval-Based Neural Code Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1111/">Paper Link</a>】    【Pages】:925-930</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hayati:Shirley_Anugrah">Shirley Anugrah Hayati</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Olivier:Raphael">Raphael Olivier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Avvaru:Pravalika">Pravalika Avvaru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Pengcheng">Pengcheng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tomasic:Anthony">Anthony Tomasic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
In models to generate program source code from natural language, representing this code in a tree structure has been a common approach. However, existing methods often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures. We introduce RECODE, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model. First, we retrieve sentences that are similar to input sentences using a dynamic-programming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.</p>
<p>【Keywords】:</p>
<h3 id="112. SQL-to-Text Generation with Graph-to-Sequence Model.">112. SQL-to-Text Generation with Graph-to-Sequence Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1112/">Paper Link</a>】    【Pages】:931-936</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kun">Kun Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lingfei">Lingfei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhiguo">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yansong">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sheinin:Vadim">Vadim Sheinin</a></p>
<p>【Abstract】:
Previous work approaches the SQL-to-text generation task using vanilla Seq2Seq models, which may not fully capture the inherent graph-structured information in SQL query. In this paper, we propose a graph-to-sequence model to encode the global structure information into node embeddings. This model can effectively learn the correlation between the SQL query pattern and its interpretation. Experimental results on the WikiSQL dataset and Stackoverflow dataset show that our model outperforms the Seq2Seq and Tree2Seq baselines, achieving the state-of-the-art performance.</p>
<p>【Keywords】:</p>
<h3 id="113. Generating Syntactic Paraphrases.">113. Generating Syntactic Paraphrases.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1113/">Paper Link</a>】    【Pages】:937-943</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Colin:Emilie">Emilie Colin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gardent:Claire">Claire Gardent</a></p>
<p>【Abstract】:
We study the automatic generation of syntactic paraphrases using four different models for generation: data-to-text generation, text-to-text generation, text reduction and text expansion, We derive training data for each of these tasks from the WebNLG dataset and we show (i) that conditioning generation on syntactic constraints effectively permits the generation of syntactically distinct paraphrases for the same input and (ii) that exploiting different types of input (data, text or data+text) further increases the number of distinct paraphrases that can be generated for a given input.</p>
<p>【Keywords】:</p>
<h3 id="114. Neural-Davidsonian Semantic Proto-role Labeling.">114. Neural-Davidsonian Semantic Proto-role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1114/">Paper Link</a>】    【Pages】:944-955</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rudinger:Rachel">Rachel Rudinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Teichert:Adam_R=">Adam R. Teichert</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Culkin:Ryan">Ryan Culkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0012:Sheng">Sheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durme:Benjamin_Van">Benjamin Van Durme</a></p>
<p>【Abstract】:
We present a model for semantic proto-role labeling (SPRL) using an adapted bidirectional LSTM encoding strategy that we call NeuralDavidsonian: predicate-argument structure is represented as pairs of hidden states corresponding to predicate and argument head tokens of the input sequence. We demonstrate: (1) state-of-the-art results in SPRL, and (2) that our network naturally shares parameters between attributes, allowing for learning new attribute types with limited added supervision.</p>
<p>【Keywords】:</p>
<h3 id="115. Conversational Decision Making Model for Predicting King's Decision in the Annals of the Joseon Dynasty.">115. Conversational Decision Making Model for Predicting King's Decision in the Annals of the Joseon Dynasty.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1115/">Paper Link</a>】    【Pages】:956-961</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bak:JinYeong">JinYeong Bak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oh:Alice">Alice Oh</a></p>
<p>【Abstract】:
Styles of leaders when they make decisions in groups vary, and the different styles affect the performance of the group. To understand the key words and speakers associated with decisions, we initially formalize the problem as one of predicting leaders’ decisions from discussion with group members. As a dataset, we introduce conversational meeting records from a historical corpus, and develop a hierarchical RNN structure with attention and pre-trained speaker embedding in the form of a, Conversational Decision Making Model (CDMM). The CDMM outperforms other baselines to predict leaders’ final decisions from the data. We explain why CDMM works better than other methods by showing the key words and speakers discovered from the attentions as evidence.</p>
<p>【Keywords】:</p>
<h3 id="116. Toward Fast and Accurate Neural Discourse Segmentation.">116. Toward Fast and Accurate Neural Discourse Segmentation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1116/">Paper Link</a>】    【Pages】:962-967</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yizhong">Yizhong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Jingfeng">Jingfeng Yang</a></p>
<p>【Abstract】:
Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance.</p>
<p>【Keywords】:</p>
<h3 id="117. A Dataset for Telling the Stories of Social Media Videos.">117. A Dataset for Telling the Stories of Social Media Videos.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1117/">Paper Link</a>】    【Pages】:968-974</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gella:Spandana">Spandana Gella</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rohrbach:Marcus">Marcus Rohrbach</a></p>
<p>【Abstract】:
Video content on social media platforms constitutes a major part of the communication between people, as it allows everyone to share their stories. However, if someone is unable to consume video, either due to a disability or network bandwidth, this severely limits their participation and communication. Automatically telling the stories using multi-sentence descriptions of videos would allow bridging this gap. To learn and evaluate such models, we introduce VideoStory a new large-scale dataset for video description as a new challenge for multi-sentence video description. Our VideoStory captions dataset is complementary to prior work and contains 20k videos posted publicly on a social media platform amounting to 396 hours of video with 123k sentences, temporally aligned to the video.</p>
<p>【Keywords】:</p>
<h3 id="118. Cascaded Mutual Modulation for Visual Reasoning.">118. Cascaded Mutual Modulation for Visual Reasoning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1118/">Paper Link</a>】    【Pages】:975-980</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Yiqun">Yiqun Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jiaming">Jiaming Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0023:Feng">Feng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0002:Bo">Bo Xu</a></p>
<p>【Abstract】:
Visual reasoning is a special visual question answering problem that is multi-step and compositional by nature, and also requires intensive text-vision interactions. We propose CMM: Cascaded Mutual Modulation as a novel end-to-end visual reasoning model. CMM includes a multi-step comprehension process for both question and image. In each step, we use a Feature-wise Linear Modulation (FiLM) technique to enable textual/visual pipeline to mutually control each other. Experiments show that CMM significantly outperforms most related models, and reach state-of-the-arts on two visual reasoning benchmarks: CLEVR and NLVR, collected from both synthetic and natural languages. Ablation studies confirm the effectiveness of CMM to comprehend natural language logics under the guidence of images. Our code is available at <a href="https://github.com/FlamingHorizon/CMM-VR">https://github.com/FlamingHorizon/CMM-VR</a>.</p>
<p>【Keywords】:</p>
<h3 id="119. How agents see things: On visual representations in an emergent language game.">119. How agents see things: On visual representations in an emergent language game.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1119/">Paper Link</a>】    【Pages】:981-985</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bouchacourt:Diane">Diane Bouchacourt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baroni:Marco">Marco Baroni</a></p>
<p>【Abstract】:
There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents’ symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.</p>
<p>【Keywords】:</p>
<h3 id="120. Attention-Based Capsule Network with Dynamic Routing for Relation Extraction.">120. Attention-Based Capsule Network with Dynamic Routing for Relation Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1120/">Paper Link</a>】    【Pages】:986-992</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Ningyu">Ningyu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Shumin">Shumin Deng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Zhanling">Zhanling Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0003:Xi">Xi Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Wei">Wei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Huajun">Huajun Chen</a></p>
<p>【Abstract】:
A capsule is a group of neurons, whose activity vector represents the instantiation parameters of a specific type of entity. In this paper, we explore the capsule networks used for relation extraction in a multi-instance multi-label learning framework and propose a novel neural approach based on capsule networks with attention mechanisms. We evaluate our method with different benchmarks, and it is demonstrated that our method improves the precision of the predicted relations. Particularly, we show that capsule networks improve multiple entity pairs relation extraction.</p>
<p>【Keywords】:</p>
<h3 id="121. Put It Back: Entity Typing with Language Model Enhancement.">121. Put It Back: Entity Typing with Language Model Enhancement.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1121/">Paper Link</a>】    【Pages】:993-998</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xin:Ji">Ji Xin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hao">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xu">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>【Abstract】:
Entity typing aims to classify semantic types of an entity mention in a specific context. Most existing models obtain training data using distant supervision, and inevitably suffer from the problem of noisy labels. To address this issue, we propose entity typing with language model enhancement. It utilizes a language model to measure the compatibility between context sentences and labels, and thereby automatically focuses more on context-dependent labels. Experiments on benchmark datasets demonstrate that our method is capable of enhancing the entity typing model with information from the language model, and significantly outperforms the state-of-the-art baseline. Code and data for this paper can be found from <a href="https://github.com/thunlp/LME">https://github.com/thunlp/LME</a>.</p>
<p>【Keywords】:</p>
<h3 id="122. Event Detection with Neural Networks: A Rigorous Empirical Evaluation.">122. Event Detection with Neural Networks: A Rigorous Empirical Evaluation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1122/">Paper Link</a>】    【Pages】:999-1004</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/o/Orr:John_Walker">John Walker Orr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tadepalli:Prasad">Prasad Tadepalli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli_Z=">Xiaoli Z. Fern</a></p>
<p>【Abstract】:
Detecting events and classifying them into predefined types is an important step in knowledge extraction from natural language texts. While the neural network models have generally led the state-of-the-art, the differences in performance between different architectures have not been rigorously studied. In this paper we present a novel GRU-based model that combines syntactic information along with temporal structure through an attention mechanism. We show that it is competitive with other neural network architectures through empirical evaluations under different random initializations and training-validation-test splits of ACE2005 dataset.</p>
<p>【Keywords】:</p>
<h3 id="123. PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages.">123. PubSE: A Hierarchical Model for Publication Extraction from Academic Homepages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1123/">Paper Link</a>】    【Pages】:1005-1010</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yiqing">Yiqing Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi_0001:Jianzhong">Jianzhong Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Rui">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Chuandong">Chuandong Yin</a></p>
<p>【Abstract】:
Publication information in a researcher’s academic homepage provides insights about the researcher’s expertise, research interests, and collaboration networks. We aim to extract all the publication strings from a given academic homepage. This is a challenging task because the publication strings in different academic homepages may be located at different positions with different structures. To capture the positional and structural diversity, we propose an end-to-end hierarchical model named PubSE based on Bi-LSTM-CRF. We further propose an alternating training method for training the model. Experiments on real data show that PubSE outperforms the state-of-the-art models by up to 11.8% in F1-score.</p>
<p>【Keywords】:</p>
<h3 id="124. A Neural Transition-based Model for Nested Mention Recognition.">124. A Neural Transition-based Model for Nested Mention Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1124/">Paper Link</a>】    【Pages】:1011-1017</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Bailin">Bailin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yu">Yu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Hongxia">Hongxia Jin</a></p>
<p>【Abstract】:
It is common that entity mentions can contain other mentions recursively. This paper introduces a scalable transition-based method to model the nested structure of mentions. We first map a sentence with nested mentions to a designated forest where each mention corresponds to a constituent of the forest. Our shift-reduce based system then learns to construct the forest structure in a bottom-up manner through an action sequence whose maximal length is guaranteed to be three times of the sentence length. Based on Stack-LSTM which is employed to efficiently and effectively represent the states of the system in a continuous space, our system is further incorporated with a character-based component to capture letter-level patterns. Our model gets the state-of-the-art performances in ACE datasets, showing its effectiveness in detecting nested mentions.</p>
<p>【Keywords】:</p>
<h3 id="125. Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction.">125. Genre Separation Network with Adversarial Training for Cross-genre Relation Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1125/">Paper Link</a>】    【Pages】:1018-1023</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Ge">Ge Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Chong">Chong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Lifu">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Boliang">Boliang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liao:Lejian">Lejian Liao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Heyan">Heyan Huang</a></p>
<p>【Abstract】:
Relation Extraction suffers from dramatical performance decrease when training a model on one genre and directly applying it to a new genre, due to the distinct feature distributions. Previous studies address this problem by discovering a shared space across genres using manually crafted features, which requires great human effort. To effectively automate this process, we design a genre-separation network, which applies two encoders, one genre-independent and one genre-shared, to explicitly extract genre-specific and genre-agnostic features. Then we train a relation classifier using the genre-agnostic features on the source genre and directly apply to the target genre. Experiment results on three distinct genres of the ACE dataset show that our approach achieves up to 6.1% absolute F1-score gain compared to previous methods. By incorporating a set of external linguistic features, our approach outperforms the state-of-the-art by 1.7% absolute F1 gain. We make all programs of our model publicly available for research purpose</p>
<p>【Keywords】:</p>
<h3 id="126. Effective Use of Context in Noisy Entity Linking.">126. Effective Use of Context in Noisy Entity Linking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1126/">Paper Link</a>】    【Pages】:1024-1029</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mueller:David">David Mueller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durrett:Greg">Greg Durrett</a></p>
<p>【Abstract】:
To disambiguate between closely related concepts, entity linking systems need to effectively distill cues from their context, which may be quite noisy. We investigate several techniques for using these cues in the context of noisy entity linking on short texts. Our starting point is a state-of-the-art attention-based model from prior work; while this model’s attention typically identifies context that is topically relevant, it fails to identify some of the most indicative surface strings, especially those exhibiting lexical overlap with the true title. Augmenting the model with convolutional networks over characters still leaves it largely unable to pick up on these cues compared to sparse features that target them directly, indicating that automatically learning how to identify relevant character-level context features is a hard problem. Our final system outperforms past work on the WikilinksNED test set by 2.8% absolute.</p>
<p>【Keywords】:</p>
<h3 id="127. Exploiting Contextual Information via Dynamic Memory Network for Event Detection.">127. Exploiting Contextual Information via Dynamic Memory Network for Event Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1127/">Paper Link</a>】    【Pages】:1030-1035</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Shaobo">Shaobo Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Rui">Rui Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Xiaoming">Xiaoming Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
The task of event detection involves identifying and categorizing event triggers. Contextual information has been shown effective on the task. However, existing methods which utilize contextual information only process the context once. We argue that the context can be better exploited by processing the context multiple times, allowing the model to perform complex reasoning and to generate better context representation, thus improving the overall performance. Meanwhile, dynamic memory network (DMN) has demonstrated promising capability in capturing contextual information and has been applied successfully to various tasks. In light of the multi-hop mechanism of the DMN to model the context, we propose the trigger detection dynamic memory network (TD-DMN) to tackle the event detection problem. We performed a five-fold cross-validation on the ACE-2005 dataset and experimental results show that the multi-hop mechanism does improve the performance and the proposed model achieves best F1 score compared to the state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="128. Do explanations make VQA models more predictable to a human?">128. Do explanations make VQA models more predictable to a human?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1128/">Paper Link</a>】    【Pages】:1036-1042</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chandrasekaran:Arjun">Arjun Chandrasekaran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prabhu:Viraj">Viraj Prabhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yadav:Deshraj">Deshraj Yadav</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chattopadhyay:Prithvijit">Prithvijit Chattopadhyay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parikh:Devi">Devi Parikh</a></p>
<p>【Abstract】:
A rich line of research attempts to make deep neural networks more transparent by generating human-interpretable ‘explanations’ of their decision process, especially for interactive tasks like Visual Question Answering (VQA). In this work, we analyze if existing explanations indeed make a VQA model — its responses as well as failures — more predictable to a human. Surprisingly, we find that they do not. On the other hand, we find that human-in-the-loop approaches that treat the model as a black-box do.</p>
<p>【Keywords】:</p>
<h3 id="129. Facts That Matter.">129. Facts That Matter.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1129/">Paper Link</a>】    【Pages】:1043-1048</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Ponza:Marco">Marco Ponza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Corro:Luciano_Del">Luciano Del Corro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
This work introduces fact salience: The task of generating a machine-readable representation of the most prominent information in a text document as a set of facts. We also present SalIE, the first fact salience system. SalIE is unsupervised and knowledge agnostic, based on open information extraction to detect facts in natural language text, PageRank to determine their relevance, and clustering to promote diversity. We compare SalIE with several baselines (including positional, standard for saliency tasks), and in an extrinsic evaluation, with state-of-the-art automatic text summarizers. SalIE outperforms baselines and text summarizers showing that facts are an effective way to compress information.</p>
<p>【Keywords】:</p>
<h3 id="130. Entity Tracking Improves Cloze-style Reading Comprehension.">130. Entity Tracking Improves Cloze-style Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1130/">Paper Link</a>】    【Pages】:1049-1055</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hoang:Luong">Luong Hoang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wiseman:Sam">Sam Wiseman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a></p>
<p>【Abstract】:
Recent work has improved on modeling for reading comprehension tasks with simple approaches such as the Attention Sum-Reader; however, automatic systems still significantly trail human performance. Analysis suggests that many of the remaining hard instances are related to the inability to track entity-references throughout documents. This work focuses on these hard entity tracking cases with two extensions: (1) additional entity features, and (2) training with a multi-task tracking objective. We show that these simple modifications improve performance both independently and in combination, and we outperform the previous state of the art on the LAMBADA dataset by 8 pts, particularly on difficult entity examples. We also effectively match the performance of more complicated models on the named entity portion of the CBT dataset.</p>
<p>【Keywords】:</p>
<h3 id="131. Adversarial Domain Adaptation for Duplicate Question Detection.">131. Adversarial Domain Adaptation for Duplicate Question Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1131/">Paper Link</a>】    【Pages】:1056-1063</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shah:Darsh_J=">Darsh J. Shah</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lei_0001:Tao">Tao Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Romeo:Salvatore">Salvatore Romeo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nakov:Preslav">Preslav Nakov</a></p>
<p>【Abstract】:
We address the problem of detecting duplicate questions in forums, which is an important step towards automating the process of answering new questions. As finding and annotating such potential duplicates manually is very tedious and costly, automatic methods based on machine learning are a viable alternative. However, many forums do not have annotated data, i.e., questions labeled by experts as duplicates, and thus a promising solution is to use domain adaptation from another forum that has such annotations. Here we focus on adversarial domain adaptation, deriving important findings about when it performs well and what properties of the domains are important in this regard. Our experiments with StackExchange data show an average improvement of 5.6% over the best baseline across multiple pairs of domains.</p>
<p>【Keywords】:</p>
<h3 id="132. Translating Math Word Problem to Expression Tree.">132. Translating Math Word Problem to Expression Tree.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1132/">Paper Link</a>】    【Pages】:1064-1069</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Lei">Lei Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yan">Yan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Deng">Deng Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Dongxiang">Dongxiang Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaojiang">Xiaojiang Liu</a></p>
<p>【Abstract】:
Sequence-to-sequence (SEQ2SEQ) models have been successfully applied to automatic math word problem solving. Despite its simplicity, a drawback still remains: a math word problem can be correctly solved by more than one equations. This non-deterministic transduction harms the performance of maximum likelihood estimation. In this paper, by considering the uniqueness of expression tree, we propose an equation normalization method to normalize the duplicated equations. Moreover, we analyze the performance of three popular SEQ2SEQ models on the math word problem solving. We find that each model has its own specialty in solving problems, consequently an ensemble model is then proposed to combine their advantages. Experiments on dataset Math23K show that the ensemble model with equation normalization significantly outperforms the previous state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="133. Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection.">133. Semantic Linking in Convolutional Neural Networks for Answer Sentence Selection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1133/">Paper Link</a>】    【Pages】:1070-1076</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nicosia:Massimo">Massimo Nicosia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a></p>
<p>【Abstract】:
State-of-the-art networks that model relations between two pieces of text often use complex architectures and attention. In this paper, instead of focusing on architecture engineering, we take advantage of small amounts of labelled data that model semantic phenomena in text to encode matching features directly in the word representations. This greatly boosts the accuracy of our reference network, while keeping the model simple and fast to train. Our approach also beats a tree kernel model that uses similar input encodings, and neural models which use advanced attention and compare-aggregate mechanisms.</p>
<p>【Keywords】:</p>
<h3 id="134. A dataset and baselines for sequential open-domain question answering.">134. A dataset and baselines for sequential open-domain question answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1134/">Paper Link</a>】    【Pages】:1077-1083</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Elgohary:Ahmed">Ahmed Elgohary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Chen">Chen Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a></p>
<p>【Abstract】:
Previous work on question-answering systems mainly focuses on answering individual questions, assuming they are independent and devoid of context. Instead, we investigate sequential question answering, asking multiple related questions. We present QBLink, a new dataset of fully human-authored questions. We extend existing strong question answering frameworks to include previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publicly available at <a href="http://sequential.qanta.org">http://sequential.qanta.org</a>.</p>
<p>【Keywords】:</p>
<h3 id="135. Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set.">135. Improving the results of string kernels in sentiment analysis and Arabic dialect identification by adapting them to your test set.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1135/">Paper Link</a>】    【Pages】:1084-1090</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/i/Ionescu:Radu_Tudor">Radu Tudor Ionescu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Butnaru:Andrei_M=">Andrei M. Butnaru</a></p>
<p>【Abstract】:
Recently, string kernels have obtained state-of-the-art results in various text classification tasks such as Arabic dialect identification or native language identification. In this paper, we apply two simple yet effective transductive learning approaches to further improve the results of string kernels. The first approach is based on interpreting the pairwise string kernel similarities between samples in the training set and samples in the test set as features. Our second approach is a simple self-training method based on two learning iterations. In the first iteration, a classifier is trained on the training set and tested on the test set, as usual. In the second iteration, a number of test samples (to which the classifier associated higher confidence scores) are added to the training set for another round of training. However, the ground-truth labels of the added test samples are not necessary. Instead, we use the labels predicted by the classifier in the first training iteration. By adapting string kernels to the test set, we report significantly better accuracy rates in English polarity classification and Arabic dialect identification.</p>
<p>【Keywords】:</p>
<h3 id="136. Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification.">136. Parameterized Convolutional Neural Networks for Aspect Level Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1136/">Paper Link</a>】    【Pages】:1091-1096</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Binxuan">Binxuan Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carley:Kathleen_M=">Kathleen M. Carley</a></p>
<p>【Abstract】:
We introduce a novel parameterized convolutional neural network for aspect level sentiment classification. Using parameterized filters and parameterized gates, we incorporate aspect information into convolutional neural networks (CNN). Experiments demonstrate that our parameterized filters and parameterized gates effectively capture the aspect-specific features, and our CNN-based models achieve excellent results on SemEval 2014 datasets.</p>
<p>【Keywords】:</p>
<h3 id="137. Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network.">137. Improving Multi-label Emotion Classification via Sentiment Classification with Dual Attention Transfer Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1137/">Paper Link</a>】    【Pages】:1097-1102</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Jianfei">Jianfei Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marujo:Lu=iacute=s">Luís Marujo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang_0001:Jing">Jing Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Karuturi:Pradeep">Pradeep Karuturi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brendel:William">William Brendel</a></p>
<p>【Abstract】:
In this paper, we target at improving the performance of multi-label emotion classification with the help of sentiment classification. Specifically, we propose a new transfer learning architecture to divide the sentence representation into two different feature spaces, which are expected to respectively capture the general sentiment words and the other important emotion-specific words via a dual attention mechanism. Experimental results on two benchmark datasets demonstrate the effectiveness of our proposed method.</p>
<p>【Keywords】:</p>
<h3 id="138. Learning Sentiment Memories for Sentiment Modification without Parallel Data.">138. Learning Sentiment Memories for Sentiment Modification without Parallel Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1138/">Paper Link</a>】    【Pages】:1103-1108</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0050:Yi">Yi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingjing">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Pengcheng">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
The task of sentiment modification requires reversing the sentiment of the input and preserving the sentiment-independent content. However, aligned sentences with the same content but different sentiments are usually unavailable. Due to the lack of such parallel data, it is hard to extract sentiment independent content and reverse the sentiment in an unsupervised way. Previous work usually can not reconcile sentiment transformation and content preservation. In this paper, motivated by the fact the non-emotional context (e.g., “staff”) provides strong cues for the occurrence of emotional words (e.g., “friendly”), we propose a novel method that automatically extracts appropriate sentiment information from learned sentiment memories according to the specific context. Experiments show that our method substantially improves the content preservation degree and achieves the state-of-the-art performance.</p>
<p>【Keywords】:</p>
<h3 id="139. Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.">139. Joint Aspect and Polarity Classification for Aspect-based Sentiment Analysis with End-to-End Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1139/">Paper Link</a>】    【Pages】:1109-1114</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schmitt:Martin">Martin Schmitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Steinheber:Simon">Simon Steinheber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schreiber:Konrad">Konrad Schreiber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth_0001:Benjamin">Benjamin Roth</a></p>
<p>【Abstract】:
In this work, we propose a new model for aspect-based sentiment analysis. In contrast to previous approaches, we jointly model the detection of aspects and the classification of their polarity in an end-to-end trainable neural network. We conduct experiments with different neural architectures and word representations on the recent GermEval 2017 dataset. We were able to show considerable performance gains by using the joint modeling approach in all settings compared to pipeline approaches. The combination of a convolutional neural network and fasttext embeddings outperformed the best submission of the shared task in 2017, establishing a new state of the art.</p>
<p>【Keywords】:</p>
<h3 id="140. Representing Social Media Users for Sarcasm Detection.">140. Representing Social Media Users for Sarcasm Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1140/">Paper Link</a>】    【Pages】:1115-1121</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kolchinski:Y=_Alex">Y. Alex Kolchinski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Potts:Christopher">Christopher Potts</a></p>
<p>【Abstract】:
We explore two methods for representing authors in the context of textual sarcasm detection: a Bayesian approach that directly represents authors’ propensities to be sarcastic, and a dense embedding approach that can learn interactions between the author and the text. Using the SARC dataset of Reddit comments, we show that augmenting a bidirectional RNN with these representations improves performance; the Bayesian approach suffices in homogeneous contexts, whereas the added power of the dense embeddings proves valuable in more diverse ones.</p>
<p>【Keywords】:</p>
<h3 id="141. Syntactical Analysis of the Weaknesses of Sentiment Analyzers.">141. Syntactical Analysis of the Weaknesses of Sentiment Analyzers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1141/">Paper Link</a>】    【Pages】:1122-1127</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Verma:Rohil">Rohil Verma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Samuel">Samuel Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Walter:David">David Walter</a></p>
<p>【Abstract】:
We carry out a syntactic analysis of two state-of-the-art sentiment analyzers, Google Cloud Natural Language and Stanford CoreNLP, to assess their classification accuracy on sentences with negative polarity items. We were motivated by the absence of studies investigating sentiment analyzer performance on sentences with polarity items, a common construct in human language. Our analysis focuses on two sentential structures: downward entailment and non-monotone quantifiers; and demonstrates weaknesses of Google Natural Language and CoreNLP in capturing polarity item information. We describe the particular syntactic phenomenon that these analyzers fail to understand that any ideal sentiment analyzer must. We also provide a set of 150 test sentences that any ideal sentiment analyzer must be able to understand.</p>
<p>【Keywords】:</p>
<h3 id="142. Is Nike female? Exploring the role of sound symbolism in predicting brand name gender.">142. Is Nike female? Exploring the role of sound symbolism in predicting brand name gender.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1142/">Paper Link</a>】    【Pages】:1128-1132</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moorthy:Sridhar">Sridhar Moorthy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pogacar:Ruth">Ruth Pogacar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khan:Samin">Samin Khan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a></p>
<p>【Abstract】:
Are brand names such as Nike female or male? Previous research suggests that the sound of a person’s first name is associated with the person’s gender, but no research has tried to use this knowledge to assess the gender of brand names. We present a simple computational approach that uses sound symbolism to address this open issue. Consistent with previous research, a model trained on various linguistic features of name endings predicts human gender with high accuracy. Applying this model to a data set of over a thousand commercially-traded brands in 17 product categories, our results reveal an overall bias toward male names, cutting across both male-oriented product categories as well as female-oriented categories. In addition, we find variation within categories, suggesting that firms might be seeking to imbue their brands with differentiating characteristics as part of their competitive strategy.</p>
<p>【Keywords】:</p>
<h3 id="143. Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging.">143. Improving Large-Scale Fact-Checking using Decomposable Attention Models and Lexical Tagging.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1143/">Paper Link</a>】    【Pages】:1133-1138</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Nayeon">Nayeon Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Chien=Sheng">Chien-Sheng Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fung:Pascale">Pascale Fung</a></p>
<p>【Abstract】:
Fact-checking of textual sources needs to effectively extract relevant information from large knowledge bases. In this paper, we extend an existing pipeline approach to better tackle this problem. We propose a neural ranker using a decomposable attention model that dynamically selects sentences to achieve promising improvement in evidence retrieval F1 by 38.80%, with (x65) speedup compared to a TF-IDF method. Moreover, we incorporate lexical tagging methods into our pipeline framework to simplify the tasks and render the model more generalizable. As a result, our framework achieves promising performance on a large-scale fact extraction and verification dataset with speedup.</p>
<p>【Keywords】:</p>
<h3 id="144. Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations.">144. Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1144/">Paper Link</a>】    【Pages】:1139-1145</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kano:Ryuji">Ryuji Kano</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miura:Yasuhide">Yasuhide Miura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taniguchi:Motoki">Motoki Taniguchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yan=Ying">Yan-Ying Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Francine">Francine Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ohkuma:Tomoko">Tomoko Ohkuma</a></p>
<p>【Abstract】:
We leverage a popularity measure in social media as a distant label for extractive summarization of online conversations. In social media, users can vote, share, or bookmark a post they prefer. The number of these actions is regarded as a measure of popularity. However, popularity is not determined solely by content of a post, e.g., a text or an image it contains, but is highly based on its contexts, e.g., timing, and authority. We propose Disjunctive model that computes the contribution of content and context separately. For evaluation, we build a dataset where the informativeness of comments is annotated. We evaluate the results with ranking metrics, and show that our model outperforms the baseline models which directly use popularity as a measure of informativeness.</p>
<p>【Keywords】:</p>
<h3 id="145. Identifying Locus of Control in Social Media Language.">145. Identifying Locus of Control in Social Media Language.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1145/">Paper Link</a>】    【Pages】:1146-1152</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rouhizadeh:Masoud">Masoud Rouhizadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jaidka:Kokil">Kokil Jaidka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Laura">Laura Smith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buffone:Anneke">Anneke Buffone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a></p>
<p>【Abstract】:
Individuals express their locus of control, or “control”, in their language when they identify whether or not they are in control of their circumstances. Although control is a core concept underlying rhetorical style, it is not clear whether control is expressed by how or by what authors write. We explore the roles of syntax and semantics in expressing users’ sense of control –i.e. being “controlled by” or “in control of” their circumstances– in a corpus of annotated Facebook posts. We present rich insights into these linguistic aspects and find that while the language signaling control is easy to identify, it is more challenging to label it is internally or externally controlled, with lexical features outperforming syntactic features at the task. Our findings could have important implications for studying self-expression in social media.</p>
<p>【Keywords】:</p>
<h3 id="146. Somm: Into the Model.">146. Somm: Into the Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1146/">Paper Link</a>】    【Pages】:1153-1159</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Shengli">Shengli Hu</a></p>
<p>【Abstract】:
To what extent could the sommelier profession, or wine stewardship, be displaced by machine leaning algorithms? There are at least three essential skills that make a qualified sommelier: wine theory, blind tasting, and beverage service, as exemplified in the rigorous certification processes of certified sommeliers and above (advanced and master) with the most authoritative body in the industry, the Court of Master Sommelier (hereafter CMS). We propose and train corresponding machine learning models that match these skills, and compare algorithmic results with real data collected from a large group of wine professionals. We find that our machine learning models outperform human sommeliers on most tasks — most notably in the section of blind tasting, where hierarchically supervised Latent Dirichlet Allocation outperforms sommeliers’ judgment calls by over 6% in terms of F1-score; and in the section of beverage service, especially wine and food pairing, a modified Siamese neural network based on BiLSTM achieves better results than sommeliers by 2%. This demonstrates, contrary to popular opinion in the industry, that the sommelier profession is at least to some extent automatable, barring economic (Kleinberg et al., 2017) and psychological (Dietvorst et al., 2015) complications.</p>
<p>【Keywords】:</p>
<h3 id="147. Fine-Grained Emotion Detection in Health-Related Online Posts.">147. Fine-Grained Emotion Detection in Health-Related Online Posts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1147/">Paper Link</a>】    【Pages】:1160-1166</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khanpour:Hamed">Hamed Khanpour</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Caragea:Cornelia">Cornelia Caragea</a></p>
<p>【Abstract】:
Detecting fine-grained emotions in online health communities provides insightful information about patients’ emotional states. However, current computational approaches to emotion detection from health-related posts focus only on identifying messages that contain emotions, with no emphasis on the emotion type, using a set of handcrafted features. In this paper, we take a step further and propose to detect fine-grained emotion types from health-related posts and show how high-level and abstract features derived from deep neural networks combined with lexicon-based features can be employed to detect emotions.</p>
<p>【Keywords】:</p>
<h3 id="148. The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions.">148. The Remarkable Benefit of User-Level Aggregation for Lexical-based Population-Level Predictions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1148/">Paper Link</a>】    【Pages】:1167-1172</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Giorgi:Salvatore">Salvatore Giorgi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Preotiuc=Pietro:Daniel">Daniel Preotiuc-Pietro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buffone:Anneke">Anneke Buffone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rieman:Daniel">Daniel Rieman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a></p>
<p>【Abstract】:
Nowcasting based on social media text promises to provide unobtrusive and near real-time predictions of community-level outcomes. These outcomes are typically regarding people, but the data is often aggregated without regard to users in the Twitter populations of each community. This paper describes a simple yet effective method for building community-level models using Twitter language aggregated by user. Results on four different U.S. county-level tasks, spanning demographic, health, and psychological outcomes show large and consistent improvements in prediction accuracies (e.g. from Pearson r=.73 to .82 for median income prediction or r=.37 to .47 for life satisfaction prediction) over the standard approach of aggregating all tweets. We make our aggregated and anonymized community-level data, derived from 37 billion tweets – over 1 billion of which were mapped to counties, available for research.</p>
<p>【Keywords】:</p>
<h3 id="149. Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.">149. Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1149/">Paper Link</a>】    【Pages】:1173-1182</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Jason">Jason Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mansimov:Elman">Elman Mansimov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a></p>
<p>【Abstract】:
We propose a conditional non-autoregressive neural sequence model based on iterative refinement. The proposed model is designed based on the principles of latent variable models and denoising autoencoders, and is generally applicable to any sequence generation task. We extensively evaluate the proposed model on machine translation (En-De and En-Ro) and image caption generation, and observe that it significantly speeds up decoding while maintaining the generation quality comparable to the autoregressive counterpart.</p>
<p>【Keywords】:</p>
<h3 id="150. Large Margin Neural Language Model.">150. Large Margin Neural Language Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1150/">Paper Link</a>】    【Pages】:1183-1191</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Jiaji">Jiaji Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yi">Yi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Ping:Wei">Wei Ping</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a></p>
<p>【Abstract】:
We propose a large margin criterion for training neural language models. Conventionally, neural language models are trained by minimizing perplexity (PPL) on grammatical sentences. However, we demonstrate that PPL may not be the best metric to optimize in some tasks, and further propose a large margin formulation. The proposed method aims to enlarge the margin between the “good” and “bad” sentences in a task-specific sense. It is trained end-to-end and can be widely applied to tasks that involve re-scoring of generated text. Compared with minimum-PPL training, our method gains up to 1.1 WER reduction for speech recognition and 1.0 BLEU increase for machine translation.</p>
<p>【Keywords】:</p>
<h3 id="151. Targeted Syntactic Evaluation of Language Models.">151. Targeted Syntactic Evaluation of Language Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1151/">Paper Link</a>】    【Pages】:1192-1202</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Marvin:Rebecca">Rebecca Marvin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Linzen:Tal">Tal Linzen</a></p>
<p>【Abstract】:
We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM’s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.</p>
<p>【Keywords】:</p>
<h3 id="152. Rational Recurrences.">152. Rational Recurrences.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1152/">Paper Link</a>】    【Pages】:1203-1214</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Hao">Hao Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:Roy">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thomson:Sam">Sam Thomson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
Despite the tremendous empirical success of neural models in natural language processing, many of them lack the strong intuitions that accompany classical machine learning approaches. Recently, connections have been shown between convolutional neural networks (CNNs) and weighted finite state automata (WFSAs), leading to new interpretations and insights. In this work, we show that some recurrent neural networks also share this connection to WFSAs. We characterize this connection formally, defining rational recurrences to be recurrent hidden state update functions that can be written as the Forward calculation of a finite set of WFSAs. We show that several recent neural models use rational recurrences. Our analysis provides a fresh view of these models and facilitates devising new neural architectures that draw inspiration from WFSAs. We present one such model, which performs better than two recent baselines on language modeling and text classification. Our results demonstrate that transferring intuitions from classical models like WFSAs can be an effective approach to designing and understanding neural models.</p>
<p>【Keywords】:</p>
<h3 id="153. Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling.">153. Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1153/">Paper Link</a>】    【Pages】:1215-1225</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Liyuan">Liyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xiang">Xiang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shang:Jingbo">Jingbo Shang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Xiaotao">Xiaotao Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng_0001:Jian">Jian Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han_0001:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific task. As different layers of the model keep different information, we develop a layer selection method for model pruning using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In model training, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our method.</p>
<p>【Keywords】:</p>
<h3 id="154. Automatic Event Salience Identification.">154. Automatic Event Salience Identification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1154/">Paper Link</a>】    【Pages】:1226-1236</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhengzhong">Zhengzhong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Chenyan">Chenyan Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitamura:Teruko">Teruko Mitamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>【Abstract】:
Identifying the salience (i.e. importance) of discourse units is an important task in language understanding. While events play important roles in text documents, little research exists on analyzing their saliency status. This paper empirically studies Event Salience and proposes two salience detection models based on discourse relations. The first is a feature based salience model that incorporates cohesion among discourse units. The second is a neural model that captures more complex interactions between discourse units. In our new large-scale event salience corpus, both methods significantly outperform the strong frequency baseline, while our neural model further improves the feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures).</p>
<p>【Keywords】:</p>
<h3 id="155. Temporal Information Extraction by Predicting Relative Time-lines.">155. Temporal Information Extraction by Predicting Relative Time-lines.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1155/">Paper Link</a>】    【Pages】:1237-1246</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Leeuwenberg:Artuur">Artuur Leeuwenberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moens:Marie=Francine">Marie-Francine Moens</a></p>
<p>【Abstract】:
The current leading paradigm for temporal information extraction from text consists of three phases: (1) recognition of events and temporal expressions, (2) recognition of temporal relations among them, and (3) time-line construction from the temporal relations. In contrast to the first two phases, the last phase, time-line construction, received little attention and is the focus of this work. In this paper, we propose a new method to construct a linear time-line from a set of (extracted) temporal relations. But more importantly, we propose a novel paradigm in which we directly predict start and end-points for events from the text, constituting a time-line without going through the intermediate step of prediction of temporal relations as in earlier work. Within this paradigm, we propose two models that predict in linear complexity, and a new training loss using TimeML-style annotations, yielding promising results.</p>
<p>【Keywords】:</p>
<h3 id="156. Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation.">156. Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1156/">Paper Link</a>】    【Pages】:1247-1256</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiao">Xiao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Zhunchen">Zhunchen Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Heyan">Heyan Huang</a></p>
<p>【Abstract】:
Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="157. RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information.">157. RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1157/">Paper Link</a>】    【Pages】:1257-1266</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vashishth:Shikhar">Shikhar Vashishth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joshi:Rishabh">Rishabh Joshi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prayaga:Sai_Suman">Sai Suman Prayaga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Chiranjib">Chiranjib Bhattacharyya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness. We have made RESIDE’s source code available to encourage reproducible research.</p>
<p>【Keywords】:</p>
<h3 id="158. Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms.">158. Collective Event Detection via a Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1158/">Paper Link</a>】    【Pages】:1267-1276</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0001:Yubo">Yubo Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Hang">Hang Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Kang">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jia:Yantao">Yantao Jia</a></p>
<p>【Abstract】:
Traditional approaches to the task of ACE event detection primarily regard multiple events in one sentence as independent ones and recognize them separately by using sentence-level information. However, events in one sentence are usually interdependent and sentence-level information is often insufficient to resolve ambiguities for some types of events. This paper proposes a novel framework dubbed as Hierarchical and Bias Tagging Networks with Gated Multi-level Attention Mechanisms (HBTNGMA) to solve the two problems simultaneously. Firstly, we propose a hierachical and bias tagging networks to detect multiple events in one sentence collectively. Then, we devise a gated multi-level attention to automatically extract and dynamically fuse the sentence-level and document-level information. The experimental results on the widely used ACE 2005 dataset show that our approach significantly outperforms other state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="159. Valency-Augmented Dependency Parsing.">159. Valency-Augmented Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1159/">Paper Link</a>】    【Pages】:1277-1291</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Tianze">Tianze Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Lillian">Lillian Lee</a></p>
<p>【Abstract】:
We present a complete, automated, and efficient approach for utilizing valency analysis in making dependency parsing decisions. It includes extraction of valency patterns, a probabilistic model for tagging these patterns, and a joint decoding process that explicitly considers the number and types of each token’s syntactic dependents. On 53 treebanks representing 41 languages in the Universal Dependencies data, we find that incorporating valency information yields higher precision and F1 scores on the core arguments (subjects and complements) and functional relations (e.g., auxiliaries) that we employ for valency analysis. Precision on core arguments improves from 80.87 to 85.43. We further show that our approach can be applied to an ostensibly different formalism and dataset, Tree Adjoining Grammar as extracted from the Penn Treebank; there, we outperform the previous state-of-the-art labeled attachment score by 0.7. Finally, we explore the potential of extending valency patterns beyond their traditional domain by confirming their helpfulness in improving PP attachment decisions.</p>
<p>【Keywords】:</p>
<h3 id="160. Unsupervised Learning of Syntactic Structure with Invertible Neural Projections.">160. Unsupervised Learning of Syntactic Structure with Invertible Neural Projections.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1160/">Paper Link</a>】    【Pages】:1292-1302</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Junxian">Junxian He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg=Kirkpatrick:Taylor">Taylor Berg-Kirkpatrick</a></p>
<p>【Abstract】:
Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.</p>
<p>【Keywords】:</p>
<h3 id="161. Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing.">161. Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1161/">Paper Link</a>】    【Pages】:1303-1313</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fern=aacute=ndez=Gonz=aacute=lez:Daniel">Daniel Fernández-González</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/G=oacute=mez=Rodr=iacute=guez:Carlos">Carlos Gómez-Rodríguez</a></p>
<p>【Abstract】:
We introduce novel dynamic oracles for training two of the most accurate known shift-reduce algorithms for constituent parsing: the top-down and in-order transition-based parsers. In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training. In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fully-supervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.</p>
<p>【Keywords】:</p>
<h3 id="162. Constituent Parsing as Sequence Labeling.">162. Constituent Parsing as Sequence Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1162/">Paper Link</a>】    【Pages】:1314-1324</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/G=oacute=mez=Rodr=iacute=guez:Carlos">Carlos Gómez-Rodríguez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vilares:David">David Vilares</a></p>
<p>【Abstract】:
We introduce a method to reduce constituent parsing to sequence labeling. For each word wt, it generates a label that encodes: (1) the number of ancestors in the tree that the words wt and wt+1 have in common, and (2) the nonterminal symbol at the lowest common ancestor. We first prove that the proposed encoding function is injective for any tree without unary branches. In practice, the approach is made extensible to all constituency trees by collapsing unary branches. We then use the PTB and CTB treebanks as testbeds and propose a set of fast baselines. We achieve 90% F-score on the PTB test set, outperforming the Vinyals et al. (2015) sequence-to-sequence parser. In addition, sacrificing some accuracy, our approach achieves the fastest constituent parsing speeds reported to date on PTB by a wide margin.</p>
<p>【Keywords】:</p>
<h3 id="163. Synthetic Data Made to Order: The Case of Parsing.">163. Synthetic Data Made to Order: The Case of Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1163/">Paper Link</a>】    【Pages】:1325-1337</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Dingquan">Dingquan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eisner:Jason">Jason Eisner</a></p>
<p>【Abstract】:
To approximately parse an unfamiliar language, it helps to have a treebank of a similar language. But what if the closest available treebank still has the wrong word order? We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language. The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum). This optimization procedure yields trees for a new artificial language that resembles the target language. We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.</p>
<p>【Keywords】:</p>
<h3 id="164. Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions.">164. Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1164/">Paper Link</a>】    【Pages】:1338-1346</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li_0003:Qing">Qing Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Jianlong">Jianlong Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Dongfei">Dongfei Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mei_0001:Tao">Tao Mei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Jiebo">Jiebo Luo</a></p>
<p>【Abstract】:
In Visual Question Answering, most existing approaches adopt the pipeline of representing an image via pre-trained CNNs, and then using the uninterpretable CNN features in conjunction with the question to predict the answer. Although such end-to-end models might report promising performance, they rarely provide any insight, apart from the answer, into the VQA process. In this work, we propose to break up the end-to-end VQA into two steps: explaining and reasoning, in an attempt towards a more explainable VQA by shedding light on the intermediate results between these two steps. To that end, we first extract attributes and generate descriptions as explanations for an image. Next, a reasoning module utilizes these explanations in place of the image to infer an answer. The advantages of such a breakdown include: (1) the attributes and captions can reflect what the system extracts from the image, thus can provide some insights for the predicted answer; (2) these intermediate results can help identify the inabilities of the image understanding or the answer inference part when the predicted answer is wrong. We conduct extensive experiments on a popular VQA dataset and our system achieves comparable performance with the baselines, yet with added benefits of explanability and the inherent ability to further improve with higher quality explanations.</p>
<p>【Keywords】:</p>
<h3 id="165. Learning a Policy for Opportunistic Active Learning.">165. Learning a Policy for Opportunistic Active Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1165/">Paper Link</a>】    【Pages】:1347-1357</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Padmakumar:Aishwarya">Aishwarya Padmakumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stone:Peter">Peter Stone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mooney:Raymond_J=">Raymond J. Mooney</a></p>
<p>【Abstract】:
Active learning identifies data points to label that are expected to be the most useful in improving a supervised model. Opportunistic active learning incorporates active learning into interactive tasks that constrain possible queries during interactions. Prior work has shown that opportunistic active learning can be used to improve grounding of natural language descriptions in an interactive object retrieval task. In this work, we use reinforcement learning for such an object retrieval task, to learn a policy that effectively trades off task completion with model improvement that would benefit future tasks.</p>
<p>【Keywords】:</p>
<h3 id="166. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.">166. RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1166/">Paper Link</a>】    【Pages】:1358-1368</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yagcioglu:Semih">Semih Yagcioglu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Erdem:Aykut">Aykut Erdem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Erdem:Erkut">Erkut Erdem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Ikizler=Cinbis:Nazli">Nazli Ikizler-Cinbis</a></p>
<p>【Abstract】:
Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at <a href="http://hucvl.github.io/recipeqa">http://hucvl.github.io/recipeqa</a>.</p>
<p>【Keywords】:</p>
<h3 id="167. TVQA: Localized, Compositional Video Question Answering.">167. TVQA: Localized, Compositional Video Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1167/">Paper Link</a>】    【Pages】:1369-1379</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Jie">Jie Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Licheng">Licheng Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg:Tamara_L=">Tamara L. Berg</a></p>
<p>【Abstract】:
Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task. The dataset is publicly available at <a href="http://tvqa.cs.unc.edu">http://tvqa.cs.unc.edu</a>.</p>
<p>【Keywords】:</p>
<h3 id="168. Localizing Moments in Video with Temporal Language.">168. Localizing Moments in Video with Temporal Language.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1168/">Paper Link</a>】    【Pages】:1380-1390</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hendricks:Lisa_Anne">Lisa Anne Hendricks</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Oliver">Oliver Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shechtman:Eli">Eli Shechtman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sivic:Josef">Josef Sivic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Darrell:Trevor">Trevor Darrell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Russell:Bryan_C=">Bryan C. Russell</a></p>
<p>【Abstract】:
Localizing moments in a longer video via natural language queries is a new, challenging task at the intersection of language and video understanding. Though moment localization with natural language is similar to other language and vision tasks like natural language object retrieval in images, moment localization offers an interesting opportunity to model temporal dependencies and reasoning in text. We propose a new model that explicitly reasons about different temporal segments in a video, and shows that temporal context is important for localizing phrases which include temporal language. To benchmark whether our model, and other recent video localization models, can effectively reason about temporal language, we collect the novel TEMPOral reasoning in video and language (TEMPO) dataset. Our dataset consists of two parts: a dataset with real videos and template sentences (TEMPO - Template Language) which allows for controlled studies on temporal language, and a human language dataset which consists of temporal sentences annotated by humans (TEMPO - Human Language).</p>
<p>【Keywords】:</p>
<h3 id="169. Card-660: A Reliable Evaluation Framework for Rare Word Representation Models.">169. Card-660: A Reliable Evaluation Framework for Rare Word Representation Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1169/">Paper Link</a>】    【Pages】:1391-1401</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pilehvar:Mohammad_Taher">Mohammad Taher Pilehvar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kartsaklis:Dimitri">Dimitri Kartsaklis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prokhorov:Victor">Victor Prokhorov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collier:Nigel">Nigel Collier</a></p>
<p>【Abstract】:
Rare word representation has recently enjoyed a surge of interest, owing to the crucial role that effective handling of infrequent words can play in accurate semantic understanding. However, there is a paucity of reliable benchmarks for evaluation and comparison of these techniques. We show in this paper that the only existing benchmark (the Stanford Rare Word dataset) suffers from low-confidence annotations and limited vocabulary; hence, it does not constitute a solid comparison framework. In order to fill this evaluation gap, we propose Cambridge Rare word Dataset (Card-660), an expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques. Through a set of experiments we show that even the best mainstream word embeddings, with millions of words in their vocabularies, are unable to achieve performances higher than 0.43 (Pearson correlation) on the dataset, compared to a human-level upperbound of 0.90. We release the dataset and the annotation materials at <a href="https://pilehvar.github.io/card-660/">https://pilehvar.github.io/card-660/</a>.</p>
<p>【Keywords】:</p>
<h3 id="170. Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention.">170. Leveraging Gloss Knowledge in Neural Word Sense Disambiguation by Hierarchical Co-Attention.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1170/">Paper Link</a>】    【Pages】:1402-1411</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Fuli">Fuli Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyu">Tianyu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Zexue">Zexue He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xia:Qiaolin">Qiaolin Xia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sui:Zhifang">Zhifang Sui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a></p>
<p>【Abstract】:
The goal of Word Sense Disambiguation (WSD) is to identify the correct meaning of a word in the particular context. Traditional supervised methods only use labeled data (context), while missing rich lexical knowledge such as the gloss which defines the meaning of a word sense. Recent studies have shown that incorporating glosses into neural networks for WSD has made significant improvement. However, the previous models usually build the context representation and gloss representation separately. In this paper, we find that the learning for the context and gloss representation can benefit from each other. Gloss can help to highlight the important words in the context, thus building a better context representation. Context can also help to locate the key words in the gloss of the correct word sense. Therefore, we introduce a co-attention mechanism to generate co-dependent representations for the context and gloss. Furthermore, in order to capture both word-level and sentence-level information, we extend the attention mechanism in a hierarchical fashion. Experimental results show that our model achieves the state-of-the-art results on several standard English all-words WSD test datasets.</p>
<p>【Keywords】:</p>
<h3 id="171. Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations.">171. Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1171/">Paper Link</a>】    【Pages】:1412-1424</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dinh:Erik=L=acirc=n_Do">Erik-Lân Do Dinh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wieland:Hannah">Hannah Wieland</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>【Abstract】:
We encounter metaphors every day, but only a few jump out on us and make us stumble. However, little effort has been devoted to investigating more novel metaphors in comparison to general metaphor detection efforts. We attribute this gap primarily to the lack of larger datasets that distinguish between conventionalized, i.e., very common, and novel metaphors. The goal of this paper is to alleviate this situation by introducing a crowdsourced novel metaphor annotation layer for an existing metaphor corpus. Further, we analyze our corpus and investigate correlations between novelty and features that are typically used in metaphor detection, such as concreteness ratings and more semantic features like the Potential for Metaphoricity. Finally, we present a baseline approach to assess novelty in metaphors based on our annotations.</p>
<p>【Keywords】:</p>
<h3 id="172. Streaming word similarity mining on the cheap.">172. Streaming word similarity mining on the cheap.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1172/">Paper Link</a>】    【Pages】:1425-1434</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/G=ouml=rnerup:Olof">Olof Görnerup</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gillblad:Daniel">Daniel Gillblad</a></p>
<p>【Abstract】:
Accurately and efficiently estimating word similarities from text is fundamental in natural language processing. In this paper, we propose a fast and lightweight method for estimating similarities from streams by explicitly counting second-order co-occurrences. The method rests on the observation that words that are highly correlated with respect to such counts are also highly similar with respect to first-order co-occurrences. Using buffers of co-occurred words per word to count second-order co-occurrences, we can then estimate similarities in a single pass over data without having to do prohibitively expensive similarity calculations. We demonstrate that this approach is scalable, converges rapidly, behaves robustly under parameter changes, and that it captures word similarities on par with those given by state-of-the-art word embeddings.</p>
<p>【Keywords】:</p>
<h3 id="173. Memory, Show the Way: Memory Based Few Shot Word Representation Learning.">173. Memory, Show the Way: Memory Based Few Shot Word Representation Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1173/">Paper Link</a>】    【Pages】:1435-1444</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Jingyuan">Jingyuan Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shaonan">Shaonan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>【Abstract】:
Distributional semantic models (DSMs) generally require sufficient examples for a word to learn a high quality representation. This is in stark contrast with human who can guess the meaning of a word from one or a few referents only. In this paper, we propose Mem2Vec, a memory based embedding learning method capable of acquiring high quality word representations from fairly limited context. Our method directly adapts the representations produced by a DSM with a longterm memory to guide its guess of a novel word. Based on a pre-trained embedding space, the proposed method delivers impressive performance on two challenging few-shot word similarity tasks. Embeddings learned with our method also lead to considerable improvements over strong baselines on NER and sentiment classification.</p>
<p>【Keywords】:</p>
<h3 id="174. Disambiguated skip-gram model.">174. Disambiguated skip-gram model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1174/">Paper Link</a>】    【Pages】:1445-1454</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Grzegorczyk:Karol">Karol Grzegorczyk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurdziel:Marcin">Marcin Kurdziel</a></p>
<p>【Abstract】:
We present disambiguated skip-gram: a neural-probabilistic model for learning multi-sense distributed representations of words. Disambiguated skip-gram jointly estimates a skip-gram-like context word prediction model and a word sense disambiguation model. Unlike previous probabilistic models for learning multi-sense word embeddings, disambiguated skip-gram is end-to-end differentiable and can be interpreted as a simple feed-forward neural network. We also introduce an effective pruning strategy for the embeddings learned by disambiguated skip-gram. This allows us to control the granularity of representations learned by our model. In experimental evaluation disambiguated skip-gram improves state-of-the are results in several word sense induction benchmarks.</p>
<p>【Keywords】:</p>
<h3 id="175. Picking Apart Story Salads.">175. Picking Apart Story Salads.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1175/">Paper Link</a>】    【Pages】:1455-1465</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Su">Su Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Holgate:Eric">Eric Holgate</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durrett:Greg">Greg Durrett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Erk:Katrin">Katrin Erk</a></p>
<p>【Abstract】:
During natural disasters and conflicts, information about what happened is often confusing and messy, and distributed across many sources. We would like to be able to automatically identify relevant information and assemble it into coherent narratives of what happened. To make this task accessible to neural models, we introduce Story Salads, mixtures of multiple documents that can be generated at scale. By exploiting the Wikipedia hierarchy, we can generate salads that exhibit challenging inference problems. Story salads give rise to a novel, challenging clustering task, where the objective is to group sentences from the same narratives. We demonstrate that simple bag-of-words similarity clustering falls short on this task, and that it is necessary to take into account global context and coherence.</p>
<p>【Keywords】:</p>
<h3 id="176. Dynamic Meta-Embeddings for Improved Sentence Representations.">176. Dynamic Meta-Embeddings for Improved Sentence Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1176/">Paper Link</a>】    【Pages】:1466-1477</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kiela:Douwe">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Changhan">Changhan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a></p>
<p>【Abstract】:
While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we argue that such a step is better left for neural networks to figure out by themselves. To that end, we introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP systems.</p>
<p>【Keywords】:</p>
<h3 id="177. A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.">177. A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1177/">Paper Link</a>】    【Pages】:1478-1487</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Ailem:Melissa">Melissa Ailem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Bowen">Bowen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bellet:Aur=eacute=lien">Aurélien Bellet</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Denis:Pascal">Pascal Denis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sha:Fei">Fei Sha</a></p>
<p>【Abstract】:
Several recent studies have shown the benefits of combining language and perception to infer word embeddings. These multimodal approaches either simply combine pre-trained textual and visual representations (e.g. features extracted from convolutional neural networks), or use the latter to bias the learning of textual word embeddings. In this work, we propose a novel probabilistic model to formalize how linguistic and perceptual inputs can work in concert to explain the observed word-context pairs in a text corpus. Our approach learns textual and visual representations jointly: latent visual factors couple together a skip-gram model for co-occurrence in linguistic data and a generative latent variable model for visual data. Extensive experimental studies validate the proposed model. Concretely, on the tasks of assessing pairwise word similarity and image/caption retrieval, our approach attains equally competitive or stronger results when compared to other state-of-the-art multimodal models.</p>
<p>【Keywords】:</p>
<h3 id="178. Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation.">178. Transfer and Multi-Task Learning for Noun-Noun Compound Interpretation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1178/">Paper Link</a>】    【Pages】:1488-1498</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fares:Murhaf">Murhaf Fares</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oepen:Stephan">Stephan Oepen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Velldal:Erik">Erik Velldal</a></p>
<p>【Abstract】:
In this paper, we empirically evaluate the utility of transfer and multi-task learning on a challenging semantic classification task: semantic interpretation of noun–noun compounds. Through a comprehensive series of experiments and in-depth error analysis, we show that transfer learning via parameter initialization and multi-task learning via parameter sharing can help a neural classification model generalize over a highly skewed distribution of relations. Further, we demonstrate how dual annotation with two distinct sets of relations over the same set of compounds can be exploited to improve the overall accuracy of a neural classifier and its F1 scores on the less frequent, but more difficult relations.</p>
<p>【Keywords】:</p>
<h3 id="179. Dissecting Contextual Word Embeddings: Architecture and Representation.">179. Dissecting Contextual Word Embeddings: Architecture and Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1179/">Paper Link</a>】    【Pages】:1499-1509</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peters:Matthew_E=">Matthew E. Peters</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neumann:Mark">Mark Neumann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a></p>
<p>【Abstract】:
Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.</p>
<p>【Keywords】:</p>
<h3 id="180. Preposition Sense Disambiguation and Representation.">180. Preposition Sense Disambiguation and Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1180/">Paper Link</a>】    【Pages】:1510-1521</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Hongyu">Hongyu Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mu:Jiaqi">Jiaqi Mu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhat:Suma">Suma Bhat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Viswanath:Pramod">Pramod Viswanath</a></p>
<p>【Abstract】:
Prepositions are highly polysemous, and their variegated senses encode significant semantic information. In this paper we match each preposition’s left- and right context, and their interplay to the geometry of the word vectors to the left and right of the preposition. Extracting these features from a large corpus and using them with machine learning models makes for an efficient preposition sense disambiguation (PSD) algorithm, which is comparable to and better than state-of-the-art on two benchmark datasets. Our reliance on no linguistic tool allows us to scale the PSD algorithm to a large corpus and learn sense-specific preposition representations. The crucial abstraction of preposition senses as word representations permits their use in downstream applications–phrasal verb paraphrasing and preposition selection–with new state-of-the-art results.</p>
<p>【Keywords】:</p>
<h3 id="181. Auto-Encoding Dictionary Definitions into Consistent Word Embeddings.">181. Auto-Encoding Dictionary Definitions into Consistent Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1181/">Paper Link</a>】    【Pages】:1522-1532</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bosc:Tom">Tom Bosc</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vincent:Pascal">Pascal Vincent</a></p>
<p>【Abstract】:
Monolingual dictionaries are widespread and semantically rich resources. This paper presents a simple model that learns to compute word embeddings by processing dictionary definitions and trying to reconstruct them. It exploits the inherent recursivity of dictionaries by encouraging consistency between the representations it uses as inputs and the representations it produces as outputs. The resulting embeddings are shown to capture semantic similarity better than regular distributional methods and other dictionary-based methods. In addition, our method shows strong performance when trained exclusively on dictionary data and generalizes in one shot.</p>
<p>【Keywords】:</p>
<h3 id="182. Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources.">182. Spot the Odd Man Out: Exploring the Associative Power of Lexical Resources.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1182/">Paper Link</a>】    【Pages】:1533-1542</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stanovsky:Gabriel">Gabriel Stanovsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hopkins:Mark">Mark Hopkins</a></p>
<p>【Abstract】:
We propose Odd-Man-Out, a novel task which aims to test different properties of word representations. An Odd-Man-Out puzzle is composed of 5 (or more) words, and requires the system to choose the one which does not belong with the others. We show that this simple setup is capable of teasing out various properties of different popular lexical resources (like WordNet and pre-trained word embeddings), while being intuitive enough to annotate on a large scale. In addition, we propose a novel technique for training multi-prototype word representations, based on unsupervised clustering of ELMo embeddings, and show that it surpasses all other representations on all Odd-Man-Out collections.</p>
<p>【Keywords】:</p>
<h3 id="183. Neural Multitask Learning for Simile Recognition.">183. Neural Multitask Learning for Simile Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1183/">Paper Link</a>】    【Pages】:1543-1553</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Lizhen">Lizhen Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Xiao">Xiao Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Wei">Wei Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Ruiji">Ruiji Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Guoping">Guoping Hu</a></p>
<p>【Abstract】:
Simile is a special type of metaphor, where comparators such as like and as are used to compare two objects. Simile recognition is to recognize simile sentences and extract simile components, i.e., the tenor and the vehicle. This paper presents a study of simile recognition in Chinese. We construct an annotated corpus for this research, which consists of 11.3k sentences that contain a comparator. We propose a neural network framework for jointly optimizing three tasks: simile sentence classification, simile component extraction and language modeling. The experimental results show that the neural network based approaches can outperform all rule-based and feature-based baselines. Both simile sentence classification and simile component extraction can benefit from multitask learning. The former can be solved very well, while the latter is more difficult.</p>
<p>【Keywords】:</p>
<h3 id="184. Structured Alignment Networks for Matching Sentences.">184. Structured Alignment Networks for Matching Sentences.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1184/">Paper Link</a>】    【Pages】:1554-1564</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0124:Yang">Yang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gardner_0001:Matt">Matt Gardner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
Many tasks in natural language processing involve comparing two sentences to compute some notion of relevance, entailment, or similarity. Typically this comparison is done either at the word level or at the sentence level, with no attempt to leverage the inherent structure of the sentence. When sentence structure is used for comparison, it is obtained during a non-differentiable pre-processing step, leading to propagation of errors. We introduce a model of structured alignments between sentences, showing how to compare two sentences by matching their latent structures. Using a structured attention mechanism, our model matches candidate spans in the first sentence to candidate spans in the second sentence, simultaneously discovering the tree structure of each sentence. Our model is fully differentiable and trained only on the matching objective. We evaluate this model on two tasks, natural entailment detection and answer sentence selection, and find that modeling latent tree structures results in superior performance. Analysis of the learned sentence structures shows they can reflect some syntactic phenomena.</p>
<p>【Keywords】:</p>
<h3 id="185. Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference.">185. Compare, Compress and Propagate: Enhancing Neural Architectures with Alignment Factorization for Natural Language Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1185/">Paper Link</a>】    【Pages】:1565-1575</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tay:Yi">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luu:Anh_Tuan">Anh Tuan Luu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a></p>
<p>【Abstract】:
This paper presents a new deep learning architecture for Natural Language Inference (NLI). Firstly, we introduce a new architecture where alignment pairs are compared, compressed and then propagated to upper layers for enhanced representation learning. Secondly, we adopt factorization layers for efficient and expressive compression of alignment vectors into scalar features, which are then used to augment the base word representations. The design of our approach is aimed to be conceptually simple, compact and yet powerful. We conduct experiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving competitive performance on all. A lightweight parameterization of our model also enjoys a 3 times reduction in parameter size compared to the existing state-of-the-art models, e.g., ESIM and DIIN, while maintaining competitive performance. Additionally, visual analysis shows that our propagated features are highly interpretable.</p>
<p>【Keywords】:</p>
<h3 id="186. Convolutional Interaction Network for Natural Language Inference.">186. Convolutional Interaction Network for Natural Language Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1186/">Paper Link</a>】    【Pages】:1576-1585</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Jingjing">Jingjing Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qiu:Xipeng">Xipeng Qiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xinchi">Xinchi Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Dong">Dong Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
Attention-based neural models have achieved great success in natural language inference (NLI). In this paper, we propose the Convolutional Interaction Network (CIN), a general model to capture the interaction between two sentences, which can be an alternative to the attention mechanism for NLI. Specifically, CIN encodes one sentence with the filters dynamically generated based on another sentence. Since the filters may be designed to have various numbers and sizes, CIN can capture more complicated interaction patterns. Experiments on three large datasets demonstrate CIN’s efficacy.</p>
<p>【Keywords】:</p>
<h3 id="187. Lessons from Natural Language Inference in the Clinical Domain.">187. Lessons from Natural Language Inference in the Clinical Domain.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1187/">Paper Link</a>】    【Pages】:1586-1596</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Romanov:Alexey">Alexey Romanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shivade:Chaitanya">Chaitanya Shivade</a></p>
<p>【Abstract】:
State of the art models using deep neural networks have become very good in learning an accurate mapping from inputs to outputs. However, they still lack generalization capabilities in conditions that differ from the ones encountered during training. This is even more challenging in specialized, and knowledge intensive domains, where training data is limited. To address this gap, we introduce MedNLI - a dataset annotated by doctors, performing a natural language inference task (NLI), grounded in the medical history of patients. We present strategies to: 1) leverage transfer learning using datasets from the open domain, (e.g. SNLI) and 2) incorporate domain knowledge from external data and lexical sources (e.g. medical terminologies). Our results demonstrate performance gains using both strategies.</p>
<p>【Keywords】:</p>
<h3 id="188. Question Generation from SQL Queries Improves Neural Semantic Parsing.">188. Question Generation from SQL Queries Improves Neural Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1188/">Paper Link</a>】    【Pages】:1597-1607</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Daya">Daya Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Yibo">Yibo Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Duyu">Duyu Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Nan">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Jian">Jian Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chi:Hong">Hong Chi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cao:James">James Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Peng">Peng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>【Abstract】:
In this paper, we study how to learn a semantic parser of state-of-the-art accuracy with less supervised training data. We conduct our study on WikiSQL, the largest hand-annotated semantic parsing dataset to date. First, we demonstrate that question generation is an effective method that empowers us to learn a state-of-the-art neural network based semantic parser with thirty percent of the supervised training data. Second, we show that applying question generation to the full supervised training data further improves the state-of-the-art model. In addition, we observe that there is a logarithmic relationship between the accuracy of a semantic parser and the amount of training data.</p>
<p>【Keywords】:</p>
<h3 id="189. SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications.">189. SemRegex: A Semantics-Based Approach for Generating Regular Expressions from Natural Language Specifications.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1189/">Paper Link</a>】    【Pages】:1608-1618</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Zexuan">Zexuan Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Jiaqi">Jiaqi Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0013:Wei">Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng_0001:Jian">Jian Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie_0001:Tao">Tao Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lou:Jian=Guang">Jian-Guang Lou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Ting">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Dongmei">Dongmei Zhang</a></p>
<p>【Abstract】:
Recent research proposes syntax-based approaches to address the problem of generating programs from natural language specifications. These approaches typically train a sequence-to-sequence learning model using a syntax-based objective: maximum likelihood estimation (MLE). Such syntax-based approaches do not effectively address the goal of generating semantically correct programs, because these approaches fail to handle Program Aliasing, i.e., semantically equivalent programs may have many syntactically different forms. To address this issue, in this paper, we propose a semantics-based approach named SemRegex. SemRegex provides solutions for a subtask of the program-synthesis problem: generating regular expressions from natural language. Different from the existing syntax-based approaches, SemRegex trains the model by maximizing the expected semantic correctness of the generated regular expressions. The semantic correctness is measured using the DFA-equivalence oracle, random test cases, and distinguishing test cases. The experiments on three public datasets demonstrate the superiority of SemRegex over the existing state-of-the-art approaches.</p>
<p>【Keywords】:</p>
<h3 id="190. Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing.">190. Decoupling Structure and Lexicon for Zero-Shot Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1190/">Paper Link</a>】    【Pages】:1619-1629</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Herzig:Jonathan">Jonathan Herzig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berant:Jonathan">Jonathan Berant</a></p>
<p>【Abstract】:
Building a semantic parser quickly in a new domain is a fundamental challenge for conversational interfaces, as current semantic parsers require expensive supervision and lack the ability to generalize to new domains. In this paper, we introduce a zero-shot approach to semantic parsing that can parse utterances in unseen domains while only being trained on examples in other source domains. First, we map an utterance to an abstract, domain independent, logical form that represents the structure of the logical form, but contains slots instead of KB constants. Then, we replace slots with KB constants via lexical alignment scores and global inference. Our model reaches an average accuracy of 53.4% on 7 domains in the OVERNIGHT dataset, substantially better than other zero-shot baselines, and performs as good as a parser trained on over 30% of the target domain examples.</p>
<p>【Keywords】:</p>
<h3 id="191. A Span Selection Model for Semantic Role Labeling.">191. A Span Selection Model for Semantic Role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1191/">Paper Link</a>】    【Pages】:1630-1642</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/o/Ouchi:Hiroki">Hiroki Ouchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shindo:Hiroyuki">Hiroyuki Shindo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Matsumoto_0001:Yuji">Yuji Matsumoto</a></p>
<p>【Abstract】:
We present a simple and accurate span-based model for semantic role labeling (SRL). Our model directly takes into account all possible argument spans and scores them for each label. At decoding time, we greedily select higher scoring labeled spans. One advantage of our model is to allow us to design and use span-level features, that are difficult to use in token-based BIO tagging approaches. Experimental results demonstrate that our ensemble model achieves the state-of-the-art results, 87.4 F1 and 87.0 F1 on the CoNLL-2005 and 2012 datasets, respectively.</p>
<p>【Keywords】:</p>
<h3 id="192. Mapping Language to Code in Programmatic Context.">192. Mapping Language to Code in Programmatic Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1192/">Paper Link</a>】    【Pages】:1643-1652</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/i/Iyer:Srinivasan">Srinivasan Iyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Konstas:Ioannis">Ioannis Konstas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Alvin">Alvin Cheung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
Source code is rarely written in isolation. It depends significantly on the programmatic context, such as the class that the code would reside in. To study this phenomenon, we introduce the task of generating class member functions given English documentation and the programmatic context provided by the rest of the class. This task is challenging because the desired code can vary greatly depending on the functionality the class provides (e.g., a sort function may or may not be available when we are asked to “return the smallest element” in a particular member variable list). We introduce CONCODE, a new large dataset with over 100,000 examples consisting of Java classes from online code repositories, and develop a new encoder-decoder architecture that models the interaction between the method documentation and the class environment. We also present a detailed error analysis suggesting that there is significant room for future work on this task.</p>
<p>【Keywords】:</p>
<h3 id="193. SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task.">193. SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-Domain Text-to-SQL Task.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1193/">Paper Link</a>】    【Pages】:1653-1663</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Tao">Tao Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yasunaga:Michihiro">Michihiro Yasunaga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Kai">Kai Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0037:Rui">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Dongxu">Dongxu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zifan">Zifan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radev:Dragomir_R=">Dragomir R. Radev</a></p>
<p>【Abstract】:
Most existing studies in text-to-SQL tasks do not require generating complex SQL queries with multiple clauses or sub-queries, and generalizing to new, unseen databases. In this paper we propose SyntaxSQLNet, a syntax tree network to address the complex and cross-domain text-to-SQL generation task. SyntaxSQLNet employs a SQL specific syntax tree-based decoder with SQL generation path history and table-aware column attention encoders. We evaluate SyntaxSQLNet on a new large-scale text-to-SQL corpus containing databases with multiple tables and complex SQL queries containing multiple SQL clauses and nested queries. We use a database split setting where databases in the test set are unseen during training. Experimental results show that SyntaxSQLNet can handle a significantly greater number of complex SQL examples than prior work, outperforming the previous state-of-the-art model by 9.5% in exact matching accuracy. To our knowledge, we are the first to study this complex text-to-SQL task. Our task and models with the latest updates are available at <a href="https://yale-lily.github.io/seq2sql/spider">https://yale-lily.github.io/seq2sql/spider</a>.</p>
<p>【Keywords】:</p>
<h3 id="194. Cross-lingual Decompositional Semantic Parsing.">194. Cross-lingual Decompositional Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1194/">Paper Link</a>】    【Pages】:1664-1675</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0012:Sheng">Sheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Xutai">Xutai Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rudinger:Rachel">Rachel Rudinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duh:Kevin">Kevin Duh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durme:Benjamin_Van">Benjamin Van Durme</a></p>
<p>【Abstract】:
We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score.</p>
<p>【Keywords】:</p>
<h3 id="195. Learning to Learn Semantic Parsers from Natural Language Supervision.">195. Learning to Learn Semantic Parsers from Natural Language Supervision.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1195/">Paper Link</a>】    【Pages】:1676-1690</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Labutov:Igor">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Bishan">Bishan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>【Abstract】:
As humans, we often rely on language to learn language. For example, when corrected in a conversation, we may learn from that correction, over time improving our language fluency. Inspired by this observation, we propose a learning algorithm for training semantic parsers from supervision (feedback) expressed in natural language. Our algorithm learns a semantic parser from users’ corrections such as “no, what I really meant was before his job, not after”, by also simultaneously learning to parse this natural language feedback in order to leverage it as a form of supervision. Unlike supervision with gold-standard logical forms, our method does not require the user to be familiar with the underlying logical formalism, and unlike supervision from denotation, it does not require the user to know the correct answer to their query. This makes our learning algorithm naturally scalable in settings where existing conversational logs are available and can be leveraged as training data. We construct a novel dataset of natural language feedback in a conversational setting, and show that our method is effective at learning a semantic parser from such natural language supervision.</p>
<p>【Keywords】:</p>
<h3 id="196. DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers.">196. DeepCx: A transition-based approach for shallow semantic parsing with complex constructional triggers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1196/">Paper Link</a>】    【Pages】:1691-1701</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dunietz:Jesse">Jesse Dunietz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carbonell:Jaime_G=">Jaime G. Carbonell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levin:Lori_S=">Lori S. Levin</a></p>
<p>【Abstract】:
This paper introduces the surface construction labeling (SCL) task, which expands the coverage of Shallow Semantic Parsing (SSP) to include frames triggered by complex constructions. We present DeepCx, a neural, transition-based system for SCL. As a test case for the approach, we apply DeepCx to the task of tagging causal language in English, which relies on a wider variety of constructions than are typically addressed in SSP. We report substantial improvements over previous tagging efforts on a causal language dataset. We also propose ways DeepCx could be extended to still more difficult constructions and to other semantic domains once appropriate datasets become available.</p>
<p>【Keywords】:</p>
<h3 id="197. What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL.">197. What It Takes to Achieve 100 Percent Condition Accuracy on WikiSQL.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1197/">Paper Link</a>】    【Pages】:1702-1711</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yavuz:Semih">Semih Yavuz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gur:Izzeddin">Izzeddin Gur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a></p>
<p>【Abstract】:
WikiSQL is a newly released dataset for studying the natural language sequence to SQL translation problem. The SQL queries in WikiSQL are simple: Each involves one relation and does not have any join operation. Despite of its simplicity, none of the publicly reported structured query generation models can achieve an accuracy beyond 62%, which is still far from enough for practical use. In this paper, we ask two questions, “Why is the accuracy still low for such simple queries?” and “What does it take to achieve 100% accuracy on WikiSQL?” To limit the scope of our study, we focus on the WHERE clause in SQL. The answers will help us gain insights about the directions we should explore in order to further improve the translation accuracy. We will then investigate alternative solutions to realize the potential ceiling performance on WikiSQL. Our proposed solution can reach up to 88.6% condition accuracy on the WikiSQL dataset.</p>
<p>【Keywords】:</p>
<h3 id="198. Better Transition-Based AMR Parsing with Refined Search Space.">198. Better Transition-Based AMR Parsing with Refined Search Space.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1198/">Paper Link</a>】    【Pages】:1712-1722</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Zhijiang">Zhijiang Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a></p>
<p>【Abstract】:
This paper introduces a simple yet effective transition-based system for Abstract Meaning Representation (AMR) parsing. We argue that a well-defined search space involved in a transition system is crucial for building an effective parser. We propose to conduct the search in a refined search space based on a new compact AMR graph and an improved oracle. Our end-to-end parser achieves the state-of-the-art performance on various datasets with minimal additional information.</p>
<p>【Keywords】:</p>
<h3 id="199. Heuristically Informed Unsupervised Idiom Usage Recognition.">199. Heuristically Informed Unsupervised Idiom Usage Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1199/">Paper Link</a>】    【Pages】:1723-1731</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Changsheng">Changsheng Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwa:Rebecca">Rebecca Hwa</a></p>
<p>【Abstract】:
Many idiomatic expressions can be interpreted figuratively or literally depending on their contexts. This paper proposes an unsupervised learning method for recognizing the intended usages of idioms. We treat the usages as a latent variable in probabilistic models and train them in a linguistically motivated feature space. Crucially, we show that distributional semantics is a helpful heuristic for distinguishing the literal usage of idioms, giving us a way to formulate a literal usage metric to estimate the likelihood that the idiom is intended literally. This information then serves as a form of distant supervision to guide the unsupervised training process for the probabilistic models. Experiments show that our overall model performs competitively against supervised methods.</p>
<p>【Keywords】:</p>
<h3 id="200. Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research.">200. Coming to Your Senses: on Controls and Evaluation Sets in Polysemy Research.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1200/">Paper Link</a>】    【Pages】:1732-1740</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dubossarsky:Haim">Haim Dubossarsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grossman:Eitan">Eitan Grossman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weinshall:Daphna">Daphna Weinshall</a></p>
<p>【Abstract】:
The point of departure of this article is the claim that sense-specific vectors provide an advantage over normal vectors due to the polysemy that they presumably represent. This claim is based on performance gains observed in gold standard evaluation tests such as word similarity tasks. We demonstrate that this claim, at least as it is instantiated in prior art, is unfounded in two ways. Furthermore, we provide empirical data and an analytic discussion that may account for the previously reported improved performance. First, we show that ground-truth polysemy degrades performance in word similarity tasks. Therefore word similarity tasks are not suitable as an evaluation test for polysemy representation. Second, random assignment of words to senses is shown to improve performance in the same task. This and additional results point to the conclusion that performance gains as reported in previous work may be an artifact of random sense assignment, which is equivalent to sub-sampling and multiple estimation of word vector representations. Theoretical analysis shows that this may on its own be beneficial for the estimation of word similarity, by reducing the bias in the estimation of the cosine distance.</p>
<p>【Keywords】:</p>
<h3 id="201. Predicting Semantic Relations using Global Graph Properties.">201. Predicting Semantic Relations using Global Graph Properties.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1201/">Paper Link</a>】    【Pages】:1741-1751</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pinter:Yuval">Yuval Pinter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eisenstein:Jacob">Jacob Eisenstein</a></p>
<p>【Abstract】:
Semantic graphs, such as WordNet, are resources which curate natural language on two distinguishable layers. On the local level, individual relations between synsets (semantic building blocks) such as hypernymy and meronymy enhance our understanding of the words used to express their meanings. Globally, analysis of graph-theoretic properties of the entire net sheds light on the structure of human language as a whole. In this paper, we combine global and local properties of semantic graphs through the framework of Max-Margin Markov Graph Models (M3GM), a novel extension of Exponential Random Graph Model (ERGM) that scales to large multi-relational graphs. We demonstrate how such global modeling improves performance on the local task of predicting semantic relations between synsets, yielding new state-of-the-art results on the WN18RR dataset, a challenging version of WordNet link prediction in which “easy” reciprocal cases are removed. In addition, the M3GM model identifies multirelational motifs that are characteristic of well-formed lexical semantic ontologies.</p>
<p>【Keywords】:</p>
<h3 id="202. Learning Scalar Adjective Intensity from Paraphrases.">202. Learning Scalar Adjective Intensity from Paraphrases.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1202/">Paper Link</a>】    【Pages】:1752-1762</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cocos:Anne">Anne Cocos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wharton:Veronica">Veronica Wharton</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pavlick:Ellie">Ellie Pavlick</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Apidianaki:Marianna">Marianna Apidianaki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Callison=Burch:Chris">Chris Callison-Burch</a></p>
<p>【Abstract】:
Adjectives like “warm”, “hot”, and “scalding” all describe temperature but differ in intensity. Understanding these differences between adjectives is a necessary part of reasoning about natural language. We propose a new paraphrase-based method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives. Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair “really hot” &lt;–&gt; “scalding” suggests that “hot” &lt; “scalding”. We show that combining this paraphrase evidence with existing, complementary pattern- and lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to “yes/no” questions.</p>
<p>【Keywords】:</p>
<h3 id="203. Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions.">203. Pointwise HSIC: A Linear-Time Kernelized Co-occurrence Norm for Sparse Linguistic Expressions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1203/">Paper Link</a>】    【Pages】:1763-1775</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yokoi:Sho">Sho Yokoi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Sosuke">Sosuke Kobayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fukumizu:Kenji">Kenji Fukumizu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Suzuki:Jun">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inui:Kentaro">Kentaro Inui</a></p>
<p>【Abstract】:
In this paper, we propose a new kernel-based co-occurrence measure that can be applied to sparse linguistic expressions (e.g., sentences) with a very short learning time, as an alternative to pointwise mutual information (PMI). As well as deriving PMI from mutual information, we derive this new measure from the Hilbert–Schmidt independence criterion (HSIC); thus, we call the new measure the pointwise HSIC (PHSIC). PHSIC can be interpreted as a smoothed variant of PMI that allows various similarity metrics (e.g., sentence embeddings) to be plugged in as kernels. Moreover, PHSIC can be estimated by simple and fast (linear in the size of the data) matrix calculations regardless of whether we use linear or nonlinear kernels. Empirically, in a dialogue response selection task, PHSIC is learned thousands of times faster than an RNN-based PMI while outperforming PMI in accuracy. In addition, we also demonstrate that PHSIC is beneficial as a criterion of a data selection task for machine translation owing to its ability to give high (low) scores to a consistent (inconsistent) pair with other pairs.</p>
<p>【Keywords】:</p>
<h3 id="204. Neural Related Work Summarization with a Joint Context-driven Attention Mechanism.">204. Neural Related Work Summarization with a Joint Context-driven Attention Mechanism.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1204/">Paper Link</a>】    【Pages】:1776-1786</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yongzhen">Yongzhen Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaozhong">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Zheng">Zheng Gao</a></p>
<p>【Abstract】:
Conventional solutions to automatic related work summarization rely heavily on human-engineered features. In this paper, we develop a neural data-driven summarizer by leveraging the seq2seq paradigm, in which a joint context-driven attention mechanism is proposed to measure the contextual relevance within full texts and a heterogeneous bibliography graph simultaneously. Our motivation is to maintain the topic coherency between a related work section and its target document, where both the textual and graphic contexts play a big role in characterizing the relationship among scientific publications accurately. Experimental results on a large dataset show that our approach achieves a considerable improvement over a typical seq2seq summarizer and five classical summarization baselines.</p>
<p>【Keywords】:</p>
<h3 id="205. Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling.">205. Improving Neural Abstractive Document Summarization with Explicit Information Selection Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1205/">Paper Link</a>】    【Pages】:1787-1796</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li_0107:Wei">Wei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Xinyan">Xinyan Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Yajuan">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yuanzhuo">Yuanzhuo Wang</a></p>
<p>【Abstract】:
Information selection is the most important component in document summarization task. In this paper, we propose to extend the basic neural encoding-decoding framework with an information selection layer to explicitly model and optimize the information selection process in abstractive document summarization. Specifically, our information selection layer consists of two parts: gated global information filtering and local sentence selection. Unnecessary information in the original document is first globally filtered, then salient sentences are selected locally while generating each summary sentence sequentially. To optimize the information selection process directly, distantly-supervised training guided by the golden summary is also imported. Experimental results demonstrate that the explicit modeling and optimizing of the information selection process improves document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperform state-of-the-art neural abstractive methods.</p>
<p>【Keywords】:</p>
<h3 id="206. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization.">206. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1206/">Paper Link</a>】    【Pages】:1797-1807</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Narayan:Shashi">Shashi Narayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
We introduce “extreme summarization”, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question “What is the article about?”. We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article’s topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.</p>
<p>【Keywords】:</p>
<h3 id="207. Improving Abstraction in Text Summarization.">207. Improving Abstraction in Text Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1207/">Paper Link</a>】    【Pages】:1808-1817</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kryscinski:Wojciech">Wojciech Kryscinski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Paulus:Romain">Romain Paulus</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Caiming">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Socher:Richard">Richard Socher</a></p>
<p>【Abstract】:
Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.</p>
<p>【Keywords】:</p>
<h3 id="208. Content Selection in Deep Learning Models of Summarization.">208. Content Selection in Deep Learning Models of Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1208/">Paper Link</a>】    【Pages】:1818-1828</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kedzie:Chris">Chris Kedzie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McKeown:Kathleen_R=">Kathleen R. McKeown</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daum=eacute=_III:Hal">Hal Daumé III</a></p>
<p>【Abstract】:
We carry out experiments with deep learning models of summarization across the domains of news, personal stories, meetings, and medical articles in order to understand how content selection is performed. We find that many sophisticated features of state of the art extractive summarizers do not improve performance over simpler models. These results suggest that it is easier to create a summarizer for a new domain than previous work suggests and bring into question the benefit of deep learning models for summarization for those domains that do have massive datasets (i.e., news). At the same time, they suggest important questions for new research in summarization; namely, new forms of sentence representations or external knowledge sources are needed that are better suited to the sumarization task.</p>
<p>【Keywords】:</p>
<h3 id="209. Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment.">209. Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1209/">Paper Link</a>】    【Pages】:1829-1838</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Dinghan">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Xinyuan">Xinyuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henao:Ricardo">Ricardo Henao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
Network embeddings, which learns low-dimensional representations for each vertex in a large-scale network, have received considerable attention in recent years. For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc. In this paper, we propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices. We introduce an word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function. In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification. The experimental results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.</p>
<p>【Keywords】:</p>
<h3 id="210. Learning Context-Aware Convolutional Filters for Text Processing.">210. Learning Context-Aware Convolutional Filters for Text Processing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1210/">Paper Link</a>】    【Pages】:1839-1848</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Dinghan">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Min:Martin_Renqiang">Martin Renqiang Min</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yitong">Yitong Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP). Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences. In this paper, we consider an approach of using a small meta network to learn context-sensitive convolutional filters for text processing. The role of meta network is to abstract the contextual information of a sentence or document into a set of input-sensitive filters. We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations. In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-sensitive filters, consistently outperforms the standard CNN and attention-based CNN baselines. By visualizing the learned context-sensitive filters, we further validate and rationalize the effectiveness of proposed framework.</p>
<p>【Keywords】:</p>
<h3 id="211. Deep Relevance Ranking using Enhanced Document-Query Interactions.">211. Deep Relevance Ranking using Enhanced Document-Query Interactions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1211/">Paper Link</a>】    【Pages】:1849-1860</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/McDonald:Ryan">Ryan McDonald</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brokos:George">George Brokos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Androutsopoulos:Ion">Ion Androutsopoulos</a></p>
<p>【Abstract】:
We explore several new models for document relevance ranking, building upon the Deep Relevance Matching Model (DRMM) of Guo et al. (2016). Unlike DRMM, which uses context-insensitive encodings of terms and query-document term interactions, we inject rich context-sensitive encodings throughout our models, inspired by PACRR’s (Hui et al., 2017) convolutional n-gram matching features, but extended in several ways including multiple views of query and document inputs. We test our models on datasets from the BIOASQ question answering challenge (Tsatsaronis et al., 2015) and TREC ROBUST 2004 (Voorhees, 2005), showing they outperform BM25-based baselines, DRMM, and PACRR.</p>
<p>【Keywords】:</p>
<h3 id="212. Learning Neural Representation for CLIR with Adversarial Framework.">212. Learning Neural Representation for CLIR with Adversarial Framework.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1212/">Paper Link</a>】    【Pages】:1861-1870</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Bo">Bo Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Ping">Ping Cheng</a></p>
<p>【Abstract】:
The existing studies in cross-language information retrieval (CLIR) mostly rely on general text representation models (e.g., vector space model or latent semantic analysis). These models are not optimized for the target retrieval task. In this paper, we follow the success of neural representation in natural language processing (NLP) and develop a novel text representation model based on adversarial learning, which seeks a task-specific embedding space for CLIR. Adversarial learning is implemented as an interplay between the generator process and the discriminator process. In order to adapt adversarial learning to CLIR, we design three constraints to direct representation learning, which are (1) a matching constraint capturing essential characteristics of cross-language ranking, (2) a translation constraint bridging language gaps, and (3) an adversarial constraint forcing both language and media invariant to be reached more efficiently and effectively. Through the joint exploitation of these constraints in an adversarial manner, the underlying cross-language semantics relevant to retrieval tasks are better preserved in the embedding space. Standard CLIR experiments show that our model significantly outperforms state-of-the-art continuous space models and is better than the strong machine translation baseline.</p>
<p>【Keywords】:</p>
<h3 id="213. AD3: Attentive Deep Document Dater.">213. AD3: Attentive Deep Document Dater.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1213/">Paper Link</a>】    【Pages】:1871-1880</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ray:Swayambhu_Nath">Swayambhu Nath Ray</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dasgupta:Shib_Sankar">Shib Sankar Dasgupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Knowledge of the creation date of documents facilitates several tasks such as summarization, event extraction, temporally focused information extraction etc. Unfortunately, for most of the documents on the Web, the time-stamp metadata is either missing or can’t be trusted. Thus, predicting creation time from document content itself is an important task. In this paper, we propose Attentive Deep Document Dater (AD3), an attention-based neural document dating system which utilizes both context and temporal information in documents in a flexible and principled manner. We perform extensive experimentation on multiple real-world datasets to demonstrate the effectiveness of AD3 over neural and non-neural baselines.</p>
<p>【Keywords】:</p>
<h3 id="214. Gromov-Wasserstein Alignment of Word Embedding Spaces.">214. Gromov-Wasserstein Alignment of Word Embedding Spaces.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1214/">Paper Link</a>】    【Pages】:1881-1890</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alvarez=Melis:David">David Alvarez-Melis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jaakkola:Tommi_S=">Tommi S. Jaakkola</a></p>
<p>【Abstract】:
Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning. Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools. Current state-of-the-art methods, however, involve multiple steps, including heuristic post-hoc refinement strategies. In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms. Indeed, we exploit the Gromov-Wasserstein distance that measures how similarities between pairs of words relate across languages. We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.</p>
<p>【Keywords】:</p>
<h3 id="215. Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.">215. Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1215/">Paper Link</a>】    【Pages】:1891-1902</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Hai">Hai Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poon:Hoifung">Hoifung Poon</a></p>
<p>【Abstract】:
Deep learning has emerged as a versatile tool for a wide range of NLP tasks, due to its superior capacity in representation learning. But its applicability is limited by the reliance on annotated examples, which are difficult to produce at scale. Indirect supervision has emerged as a promising direction to address this bottleneck, either by introducing labeling functions to automatically generate noisy examples from unlabeled text, or by imposing constraints over interdependent label decisions. A plethora of methods have been proposed, each with respective strengths and limitations. Probabilistic logic offers a unifying language to represent indirect supervision, but end-to-end modeling with probabilistic logic is often infeasible due to intractable inference and learning. In this paper, we propose deep probabilistic logic (DPL) as a general framework for indirect supervision, by composing probabilistic logic with deep learning. DPL models label decisions as latent variables, represents prior knowledge on their relations using weighted first-order logical formulas, and alternates between learning a deep neural network for the end task and refining uncertain formula weights for indirect supervision, using variational EM. This framework subsumes prior indirect supervision methods as special cases, and enables novel combination via infusion of rich domain and linguistic knowledge. Experiments on biomedical machine reading demonstrate the promise of this approach.</p>
<p>【Keywords】:</p>
<h3 id="216. Deriving Machine Attention from Human Rationales.">216. Deriving Machine Attention from Human Rationales.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1216/">Paper Link</a>】    【Pages】:1903-1913</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bao:Yujia">Yujia Bao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Shiyu">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Mo">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barzilay:Regina">Regina Barzilay</a></p>
<p>【Abstract】:
Attention-based models are successful when trained on large amounts of data. In this paper, we demonstrate that even in the low-resource scenario, attention can be learned effectively. To this end, we start with discrete human-annotated rationales and map them into continuous attention. Our central hypothesis is that this mapping is general across domains, and thus can be transferred from resource-rich domains to low-resource ones. Our model jointly learns a domain-invariant representation and induces the desired mapping between rationales and attention. Our empirical results validate this hypothesis and show that our approach delivers significant gains over state-of-the-art baselines, yielding over 15% average error reduction on benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="217. Semi-Supervised Sequence Modeling with Cross-View Training.">217. Semi-Supervised Sequence Modeling with Cross-View Training.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1217/">Paper Link</a>】    【Pages】:1914-1925</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Kevin">Kevin Clark</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luong:Minh=Thang">Minh-Thang Luong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Le:Quoc_V=">Quoc V. Le</a></p>
<p>【Abstract】:
Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.</p>
<p>【Keywords】:</p>
<h3 id="218. A Probabilistic Annotation Model for Crowdsourcing Coreference.">218. A Probabilistic Annotation Model for Crowdsourcing Coreference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1218/">Paper Link</a>】    【Pages】:1926-1937</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Paun:Silviu">Silviu Paun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chamberlain:Jon">Jon Chamberlain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kruschwitz:Udo">Udo Kruschwitz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Juntao">Juntao Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poesio:Massimo">Massimo Poesio</a></p>
<p>【Abstract】:
The availability of large scale annotated corpora for coreference is essential to the development of the field. However, creating resources at the required scale via expert annotation would be too expensive. Crowdsourcing has been proposed as an alternative; but this approach has not been widely used for coreference. This paper addresses one crucial hurdle on the way to make this possible, by introducing a new model of annotation for aggregating crowdsourced anaphoric annotations. The model is evaluated along three dimensions: the accuracy of the inferred mention pairs, the quality of the post-hoc constructed silver chains, and the viability of using the silver chains as an alternative to the expert-annotated chains in training a state of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation.</p>
<p>【Keywords】:</p>
<h3 id="219. A Deterministic Algorithm for Bridging Anaphora Resolution.">219. A Deterministic Algorithm for Bridging Anaphora Resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1219/">Paper Link</a>】    【Pages】:1938-1948</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Yufang">Yufang Hou</a></p>
<p>【Abstract】:
Previous work on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013) use syntactic preposition patterns to calculate word relatedness. However, such patterns only consider NPs’ head nouns and hence do not fully capture the semantics of NPs. Recently, Hou (2018) created word embeddings (embeddings_PP) to capture associative similarity (i.e., relatedness) between nouns by exploring the syntactic structure of noun phrases. But embeddings_PP only contains word representations for nouns. In this paper, we create new word vectors by combining embeddings_PP with GloVe. This new word embeddings (embeddings_bridging) are a more general lexical knowledge resource for bridging and allow us to represent the meaning of an NP beyond its head easily. We therefore develop a deterministic approach for bridging anaphora resolution, which represents the semantics of an NP based on its head noun and modifications. We show that this simple approach achieves the competitive results compared to the best system in Hou et al. (2013) which explores Markov Logic Networks to model the problem. Additionally, we further improve the results for bridging anaphora resolution reported in Hou (2018) by combining our simple deterministic approach with Hou et al. (2013)’s best system MLN II.</p>
<p>【Keywords】:</p>
<h3 id="220. A Knowledge Hunting Framework for Common Sense Reasoning.">220. A Knowledge Hunting Framework for Common Sense Reasoning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1220/">Paper Link</a>】    【Pages】:1949-1958</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Emami:Ali">Ali Emami</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cruz:Noelia_De_La">Noelia De La Cruz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Trischler:Adam">Adam Trischler</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Suleman:Kaheer">Kaheer Suleman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a></p>
<p>【Abstract】:
We introduce an automatic system that achieves state-of-the-art results on the Winograd Schema Challenge (WSC), a common sense reasoning task that requires diverse, complex forms of inference and knowledge. Our method uses a knowledge hunting module to gather text from the web, which serves as evidence for candidate problem resolutions. Given an input problem, our system generates relevant queries to send to a search engine, then extracts and classifies knowledge from the returned results and weighs them to make a resolution. Our approach improves F1 performance on the full WSC by 0.21 over the previous best and represents the first system to exceed 0.5 F1. We further demonstrate that the approach is competitive on the Choice of Plausible Alternatives (COPA) task, which suggests that it is generally applicable.</p>
<p>【Keywords】:</p>
<h3 id="221. Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs.">221. Mapping Text to Knowledge Graph Entities using Multi-Sense LSTMs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1221/">Paper Link</a>】    【Pages】:1959-1970</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kartsaklis:Dimitri">Dimitri Kartsaklis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pilehvar:Mohammad_Taher">Mohammad Taher Pilehvar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collier:Nigel">Nigel Collier</a></p>
<p>【Abstract】:
This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results.</p>
<p>【Keywords】:</p>
<h3 id="222. Differentiating Concepts and Instances for Knowledge Graph Embedding.">222. Differentiating Concepts and Instances for Knowledge Graph Embedding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1222/">Paper Link</a>】    【Pages】:1971-1979</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lv:Xin">Xin Lv</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hou:Lei">Lei Hou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a></p>
<p>【Abstract】:
Concepts, which represent a group of different instances sharing common properties, are essential information in knowledge representation. Most conventional knowledge embedding methods encode both entities (concepts and instances) and relations as vectors in a low dimensional semantic space equally, ignoring the difference between concepts and instances. In this paper, we propose a novel knowledge graph embedding model named TransC by differentiating concepts and instances. Specifically, TransC encodes each concept in knowledge graph as a sphere and each instance as a vector in the same semantic space. We use the relative positions to model the relations between concepts and instances (i.e.,instanceOf), and the relations between concepts and sub-concepts (i.e., subClassOf). We evaluate our model on both link prediction and triple classification tasks on the dataset based on YAGO. Experimental results show that TransC outperforms state-of-the-art methods, and captures the semantic transitivity for instanceOf and subClassOf relation. Our codes and datasets can be obtained from <a href="https://github.com/davidlvxin/TransC">https://github.com/davidlvxin/TransC</a>.</p>
<p>【Keywords】:</p>
<h3 id="223. One-Shot Relational Learning for Knowledge Graphs.">223. One-Shot Relational Learning for Knowledge Graphs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1223/">Paper Link</a>】    【Pages】:1980-1990</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Wenhan">Wenhan Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Mo">Mo Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Shiyu">Shiyu Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Xiaoxiao">Xiaoxiao Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Knowledge graphs (KG) are the key components of various natural language processing applications. To further expand KGs’ coverage, previous studies on knowledge graph completion usually require a large number of positive examples for each relation. However, we observe long-tail relations are actually more common in KGs and those newly added relations often do not have many known triples for training. In this work, we aim at predicting new facts under a challenging setting where only one training instance is available. We propose a one-shot relational learning framework, which utilizes the knowledge distilled by embedding models and learns a matching metric by considering both the learned embeddings and one-hop graph structures. Empirically, our model yields considerable performance improvements over existing embedding models, and also eliminates the need of re-training the embedding models when dealing with newly added relations.</p>
<p>【Keywords】:</p>
<h3 id="224. Regular Expression Guided Entity Mention Mining from Noisy Web Data.">224. Regular Expression Guided Entity Mention Mining from Noisy Web Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1224/">Paper Link</a>】    【Pages】:1991-2000</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Shanshan">Shanshan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Lihong">Lihong He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vucetic:Slobodan">Slobodan Vucetic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dragut:Eduard_C=">Eduard C. Dragut</a></p>
<p>【Abstract】:
Many important entity types in web documents, such as dates, times, email addresses, and course numbers, follow or closely resemble patterns that can be described by Regular Expressions (REs). Due to a vast diversity of web documents and ways in which they are being generated, even seemingly straightforward tasks such as identifying mentions of date in a document become very challenging. It is reasonable to claim that it is impossible to create a RE that is capable of identifying such entities from web documents with perfect precision and recall. Rather than abandoning REs as a go-to approach for entity detection, this paper explores ways to combine the expressive power of REs, ability of deep learning to learn from large data, and human-in-the loop approach into a new integrated framework for entity identification from web data. The framework starts by creating or collecting the existing REs for a particular type of an entity. Those REs are then used over a large document corpus to collect weak labels for the entity mentions and a neural network is trained to predict those RE-generated weak labels. Finally, a human expert is asked to label a small set of documents and the neural network is fine tuned on those documents. The experimental evaluation on several entity identification problems shows that the proposed framework achieves impressive accuracy, while requiring very modest human effort.</p>
<p>【Keywords】:</p>
<h3 id="225. HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding.">225. HyTE: Hyperplane-based Temporally aware Knowledge Graph Embedding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1225/">Paper Link</a>】    【Pages】:2001-2011</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dasgupta:Shib_Sankar">Shib Sankar Dasgupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ray:Swayambhu_Nath">Swayambhu Nath Ray</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Knowledge Graph (KG) embedding has emerged as an active area of research resulting in the development of several KG embedding methods. Relational facts in KG often show temporal dynamics, e.g., the fact (Cristiano_Ronaldo, playsFor, Manchester_United) is valid only from 2003 to 2009. Most of the existing KG embedding methods ignore this temporal dimension while learning embeddings of the KG elements. In this paper, we propose HyTE, a temporally aware KG embedding method which explicitly incorporates time in the entity-relation space by associating each timestamp with a corresponding hyperplane. HyTE not only performs KG inference using temporal guidance, but also predicts temporal scopes for relational facts with missing time annotations. Through extensive experimentation on temporal datasets extracted from real-world KGs, we demonstrate the effectiveness of our model over both traditional as well as temporal KG embedding methods.</p>
<p>【Keywords】:</p>
<h3 id="226. Neural Adaptation Layers for Cross-domain Named Entity Recognition.">226. Neural Adaptation Layers for Cross-domain Named Entity Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1226/">Paper Link</a>】    【Pages】:2012-2022</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Bill_Yuchen">Bill Yuchen Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a></p>
<p>【Abstract】:
Recent research efforts have shown that neural architectures can be effective in conventional information extraction tasks such as named entity recognition, yielding state-of-the-art results on standard newswire datasets. However, despite significant resources required for training such models, the performance of a model trained on one domain typically degrades dramatically when applied to a different domain, yet extracting entities from new emerging domains such as social media can be of significant interest. In this paper, we empirically investigate effective methods for conveniently adapting an existing, well-trained neural NER model for a new domain. Unlike existing approaches, we propose lightweight yet effective methods for performing domain adaptation for neural models. Specifically, we introduce adaptation layers on top of existing neural architectures, where no re-training using the source domain data is required. We conduct extensive empirical studies and show that our approach significantly outperforms state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="227. Entity Linking within a Social Media Platform: A Case Study on Yelp.">227. Entity Linking within a Social Media Platform: A Case Study on Yelp.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1227/">Paper Link</a>】    【Pages】:2023-2032</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Hongliang">Hongliang Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yangqiu">Yangqiu Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qiu:Liwei">Liwei Qiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Rijia">Rijia Liu</a></p>
<p>【Abstract】:
In this paper, we study a new entity linking problem where both the entity mentions and the target entities are within a same social media platform. Compared with traditional entity linking problems that link mentions to a knowledge base, this new problem have less information about the target entities. However, if we can successfully link mentions to entities within a social media platform, we can improve a lot of applications such as comparative study in business intelligence and opinion leader finding. To study this problem, we constructed a dataset called Yelp-EL, where the business mentions in Yelp reviews are linked to their corresponding businesses on the platform. We conducted comprehensive experiments and analysis on this dataset with a learning to rank model that takes different types of features as input, as well as a few state-of-the-art entity linking approaches. Our experimental results show that two types of features that are not available in traditional entity linking: social features and location features, can be very helpful for this task.</p>
<p>【Keywords】:</p>
<h3 id="228. Annotation of a Large Clinical Entity Corpus.">228. Annotation of a Large Clinical Entity Corpus.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1228/">Paper Link</a>】    【Pages】:2033-2042</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Patel:Pinal">Pinal Patel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Davey:Disha">Disha Davey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Panchal:Vishal">Vishal Panchal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pathak:Parth">Parth Pathak</a></p>
<p>【Abstract】:
Having an entity annotated corpus of the clinical domain is one of the basic requirements for detection of clinical entities using machine learning (ML) approaches. Past researches have shown the superiority of statistical/ML approaches over the rule based approaches. But in order to take full advantage of the ML approaches, an accurately annotated corpus becomes an essential requirement. Though there are a few annotated corpora available either on a small data set, or covering a narrower domain (like cancer patients records, lab reports), annotation of a large data set representing the entire clinical domain has not been created yet. In this paper, we have described in detail the annotation guidelines, annotation process and our approaches in creating a CER (clinical entity recognition) corpus of 5,160 clinical documents from forty different clinical specialities. The clinical entities range across various types such as diseases, procedures, medications, medical devices and so on. We have classified them into eleven categories for annotation. Our annotation also reflects the relations among the group of entities that constitute larger concepts altogether.</p>
<p>【Keywords】:</p>
<h3 id="229. Visual Supervision in Bootstrapped Information Extraction.">229. Visual Supervision in Bootstrapped Information Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1229/">Paper Link</a>】    【Pages】:2043-2053</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Berger:Matthew">Matthew Berger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nagesh:Ajay">Ajay Nagesh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levine:Joshua_A=">Joshua A. Levine</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Surdeanu:Mihai">Mihai Surdeanu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Helen">Helen Zhang</a></p>
<p>【Abstract】:
We challenge a common assumption in active learning, that a list-based interface populated by informative samples provides for efficient and effective data annotation. We show how a 2D scatterplot populated with diverse and representative samples can yield improved models given the same time budget. We consider this for bootstrapping-based information extraction, in particular named entity classification, where human and machine jointly label data. To enable effective data annotation in a scatterplot, we have developed an embedding-based bootstrapping model that learns the distributional similarity of entities through the patterns that match them in a large data corpus, while being discriminative with respect to human-labeled and machine-promoted entities. We conducted a user study to assess the effectiveness of these different interfaces, and analyze bootstrapping performance in terms of human labeling accuracy, label quantity, and labeling consensus across multiple users. Our results suggest that supervision acquired from the scatterplot interface, despite being noisier, yields improvements in classification performance compared with the list interface, due to a larger quantity of supervision acquired.</p>
<p>【Keywords】:</p>
<h3 id="230. Learning Named Entity Tagger using Domain-Specific Dictionary.">230. Learning Named Entity Tagger using Domain-Specific Dictionary.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1230/">Paper Link</a>】    【Pages】:2054-2064</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shang:Jingbo">Jingbo Shang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Liyuan">Liyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Xiaotao">Xiaotao Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xiang">Xiang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Teng">Teng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han_0001:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
Recent advances in deep neural models allow us to build reliable named entity recognition (NER) systems without handcrafting features. However, such methods require large amounts of manually-labeled training data. There have been efforts on replacing human annotations with distant supervision (in conjunction with external dictionaries), but the generated noisy labels pose significant challenges on learning effective neural models. Here we propose two neural models to suit noisy distant supervision from the dictionary. First, under the traditional sequence labeling framework, we propose a revised fuzzy CRF layer to handle tokens with multiple possible labels. After identifying the nature of noisy labels in distant supervision, we go beyond the traditional framework and propose a novel, more effective neural model AutoNER with a new Tie or Break scheme. In addition, we discuss how to refine distant supervision for better NER performance. Extensive experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort, and delivers competitive results with state-of-the-art supervised benchmarks.</p>
<p>【Keywords】:</p>
<h3 id="231. Zero-Shot Open Entity Typing as Type-Compatible Grounding.">231. Zero-Shot Open Entity Typing as Type-Compatible Grounding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1231/">Paper Link</a>】    【Pages】:2065-2076</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Ben">Ben Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khashabi:Daniel">Daniel Khashabi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsai:Chen=Tse">Chen-Tse Tsai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
The problem of entity-typing has been studied predominantly as a supervised learning problems, mostly with task-specific annotations (for coarse types) and sometimes with distant supervision (for fine types). While such approaches have strong performance within datasets they often lack the flexibility to transfer across text genres and to generalize to new type taxonomies. In this work we propose a zero-shot entity typing approach that requires no annotated data and can flexibly identify newly defined types. Given a type taxonomy, the entries of which we define as Boolean functions of freebase “types,” we ground a given mention to a set of type-compatible Wikipedia entries, and then infer the target mention’s type using an inference algorithm that makes use of the types of these entries. We evaluate our system on a broad range of datasets, including standard fine-grained and coarse-grained entity typing datasets, and on a dataset in the biological domain. Our system is shown to be competitive with state-of-the-art supervised NER systems, and to outperform them on out-of-training datasets. We also show that our system significantly outperforms other zero-shot fine typing systems.</p>
<p>【Keywords】:</p>
<h3 id="232. Attention-Guided Answer Distillation for Machine Reading Comprehension.">232. Attention-Guided Answer Distillation for Machine Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1232/">Paper Link</a>】    【Pages】:2077-2086</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Minghao">Minghao Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Yuxing">Yuxing Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0006:Zhen">Zhen Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Dongsheng">Dongsheng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0002:Nan">Nan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a></p>
<p>【Abstract】:
Despite that current reading comprehension systems have achieved significant advancements, their promising performances are often obtained at the cost of making an ensemble of numerous models. Besides, existing approaches are also vulnerable to adversarial attacks. This paper tackles these problems by leveraging knowledge distillation, which aims to transfer knowledge from an ensemble model to a single model. We first demonstrate that vanilla knowledge distillation applied to answer span prediction is effective for reading comprehension systems. We then propose two novel approaches that not only penalize the prediction on confusing answers but also guide the training with alignment information distilled from the ensemble. Experiments show that our best student model has only a slight drop of 0.4% F1 on the SQuAD test set compared to the ensemble teacher, while running 12x faster during inference. It even outperforms the teacher on adversarial SQuAD datasets and NarrativeQA benchmark.</p>
<p>【Keywords】:</p>
<h3 id="233. Interpretation of Natural Language Rules in Conversational Machine Reading.">233. Interpretation of Natural Language Rules in Conversational Machine Reading.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1233/">Paper Link</a>】    【Pages】:2087-2097</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Saeidi:Marzieh">Marzieh Saeidi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bartolo:Max">Max Bartolo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Patrick_S=_H=">Patrick S. H. Lewis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh_0001:Sameer">Sameer Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rockt=auml=schel:Tim">Tim Rocktäschel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sheldon:Mike">Mike Sheldon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bouchard:Guillaume">Guillaume Bouchard</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riedel_0001:Sebastian">Sebastian Riedel</a></p>
<p>【Abstract】:
Most work in machine reading focuses on question answering problems where the answer is directly expressed in the text to read. However, many real-world question answering problems require the reading of text not because it contains the literal answer, but because it contains a recipe to derive an answer together with the reader’s background knowledge. One example is the task of interpreting regulations to answer “Can I...?” or “Do I have to...?” questions such as “I am working in Canada. Do I have to carry on paying UK National Insurance?” after reading a UK government website about this topic. This task requires both the interpretation of rules and the application of background knowledge. It is further complicated due to the fact that, in practice, most questions are underspecified, and a human assistant will regularly have to ask clarification questions such as “How long have you been working abroad?” when the answer cannot be directly derived from the question and text. In this paper, we formalise this task and develop a crowd-sourcing strategy to collect 37k task instances based on real-world rules and crowd-generated questions and scenarios. We analyse the challenges of this task and assess its difficulty by evaluating the performance of rule-based and machine-learning baselines. We observe promising results when no background knowledge is necessary, and substantial room for improvement whenever background knowledge is needed.</p>
<p>【Keywords】:</p>
<h3 id="234. A State-transition Framework to Answer Complex Questions over Knowledge Base.">234. A State-transition Framework to Answer Complex Questions over Knowledge Base.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1234/">Paper Link</a>】    【Pages】:2098-2108</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Sen">Sen Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zou_0001:Lei">Lei Zou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xinbo">Xinbo Zhang</a></p>
<p>【Abstract】:
Although natural language question answering over knowledge graphs have been studied in the literature, existing methods have some limitations in answering complex questions. To address that, in this paper, we propose a State Transition-based approach to translate a complex natural language question N to a semantic query graph (SQG), which is used to match the underlying knowledge graph to find the answers to question N. In order to generate SQG, we propose four primitive operations (expand, fold, connect and merge) and a learning-based state transition approach. Extensive experiments on several benchmarks (such as QALD, WebQuestions and ComplexQuestions) with two knowledge bases (DBpedia and Freebase) confirm the superiority of our approach compared with state-of-the-arts.</p>
<p>【Keywords】:</p>
<h3 id="235. A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension.">235. A Multi-answer Multi-task Framework for Real-world Machine Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1235/">Paper Link</a>】    【Pages】:2109-2118</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jiahua">Jiahua Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Wan">Wan Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hao">Hao Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Du:Yantao">Yantao Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Dekang">Dekang Lin</a></p>
<p>【Abstract】:
The task of machine reading comprehension (MRC) has evolved from answering simple questions from well-edited text to answering real questions from users out of web data. In the real-world setting, full-body text from multiple relevant documents in the top search results are provided as context for questions from user queries, including not only questions with a single, short, and factual answer, but also questions about reasons, procedures, and opinions. In this case, multiple answers could be equally valid for a single question and each answer may occur multiple times in the context, which should be taken into consideration when we build MRC system. We propose a multi-answer multi-task framework, in which different loss functions are used for multiple reference answers. Minimum Risk Training is applied to solve the multi-occurrence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09.</p>
<p>【Keywords】:</p>
<h3 id="236. Logician and Orator: Learning from the Duality between Language and Knowledge in Open Domain.">236. Logician and Orator: Learning from the Duality between Language and Knowledge in Open Domain.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1236/">Paper Link</a>】    【Pages】:2119-2130</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Mingming">Mingming Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xu">Xu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Ping">Ping Li</a></p>
<p>【Abstract】:
We propose the task of Open-Domain Information Narration (OIN) as the reverse task of Open Information Extraction (OIE), to implement the dual structure between language and knowledge in the open domain. Then, we develop an agent, called Orator, to accomplish the OIN task, and assemble the Orator and the recently proposed OIE agent — Logician into a dual system to utilize the duality structure with a reinforcement learning paradigm. Experimental results reveal the dual structure between OIE and OIN tasks helps to build better both OIE agents and OIN agents.</p>
<p>【Keywords】:</p>
<h3 id="237. MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller.">237. MemoReader: Large-Scale Reading Comprehension through Neural Memory Controller.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1237/">Paper Link</a>】    【Pages】:2131-2140</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Back:Seohyun">Seohyun Back</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Seunghak">Seunghak Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Indurthi:Sathish_Reddy">Sathish Reddy Indurthi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Jihie">Jihie Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choo:Jaegul">Jaegul Choo</a></p>
<p>【Abstract】:
Machine reading comprehension helps machines learn to utilize most of the human knowledge written in the form of text. Existing approaches made a significant progress comparable to human-level performance, but they are still limited in understanding, up to a few paragraphs, failing to properly comprehend lengthy document. In this paper, we propose a novel deep neural network architecture to handle a long-range dependency in RC tasks. In detail, our method has two novel aspects: (1) an advanced memory-augmented architecture and (2) an expanded gated recurrent unit with dense connections that mitigate potential information distortion occurring in the memory. Our proposed architecture is widely applicable to other models. We have performed extensive experiments with well-known benchmark datasets such as TriviaQA, QUASAR-T, and SQuAD. The experimental results demonstrate that the proposed method outperforms existing methods, especially for lengthy documents.</p>
<p>【Keywords】:</p>
<h3 id="238. Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension.">238. Multi-Granular Sequence Encoding via Dilated Compositional Units for Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1238/">Paper Link</a>】    【Pages】:2141-2151</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tay:Yi">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luu:Anh_Tuan">Anh Tuan Luu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a></p>
<p>【Abstract】:
Sequence encoders are crucial components in many neural architectures for learning to read and comprehend. This paper presents a new compositional encoder for reading comprehension (RC). Our proposed encoder is not only aimed at being fast but also expressive. Specifically, the key novelty behind our encoder is that it explicitly models across multiple granularities using a new dilated composition mechanism. In our approach, gating functions are learned by modeling relationships and reasoning over multi-granular sequence information, enabling compositional learning that is aware of both long and short term information. We conduct experiments on three RC datasets, showing that our proposed encoder demonstrates very promising results both as a standalone encoder as well as a complementary building block. Empirical results show that simple Bi-Attentive architectures augmented with our proposed encoder not only achieves state-of-the-art / highly competitive results but is also considerably faster than other published works.</p>
<p>【Keywords】:</p>
<h3 id="239. Neural Compositional Denotational Semantics for Question Answering.">239. Neural Compositional Denotational Semantics for Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1239/">Paper Link</a>】    【Pages】:2152-2161</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Nitish">Nitish Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a></p>
<p>【Abstract】:
Answering compositional questions requiring multi-step reasoning is challenging. We introduce an end-to-end differentiable model for interpreting questions about a knowledge graph (KG), which is inspired by formal approaches to semantics. Each span of text is represented by a denotation in a KG and a vector that captures ungrounded aspects of meaning. Learned composition modules recursively combine constituent spans, culminating in a grounding for the complete sentence which answers the question. For example, to interpret “not green”, the model represents “green” as a set of KG entities and “not” as a trainable ungrounded vector—and then uses this vector to parameterize a composition function that performs a complement operation. For each sentence, we build a parse chart subsuming all possible parses, allowing the model to jointly learn both the composition operators and output structure by gradient descent from end-task supervision. The model learns a variety of challenging semantic operators, such as quantifiers, disjunctions and composed relations, and infers latent syntactic structure. It also generalizes well to longer questions than seen in its training data, in contrast to RNN, its tree-based variants, and semantic parsing baselines.</p>
<p>【Keywords】:</p>
<h3 id="240. Cross-Pair Text Representations for Answer Sentence Selection.">240. Cross-Pair Text Representations for Answer Sentence Selection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1240/">Paper Link</a>】    【Pages】:2162-2173</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tymoshenko:Kateryna">Kateryna Tymoshenko</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a></p>
<p>【Abstract】:
High-level semantics tasks, e.g., paraphrasing, textual entailment or question answering, involve modeling of text pairs. Before the emergence of neural networks, this has been mostly performed using intra-pair features, which incorporate similarity scores or rewrite rules computed between the members within the same pair. In this paper, we compute scalar products between vectors representing similarity between members of different pairs, in place of simply using a single vector for each pair. This allows us to obtain a representation specific to any pair of pairs, which delivers the state of the art in answer sentence selection. Most importantly, our approach can outperform much more complex algorithms based on neural networks.</p>
<p>【Keywords】:</p>
<h3 id="241. QuAC: Question Answering in Context.">241. QuAC: Question Answering in Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1241/">Paper Link</a>】    【Pages】:2174-2184</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Eunsol">Eunsol Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:He">He He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iyyer:Mohit">Mohit Iyyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yatskar:Mark">Mark Yatskar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at <a href="http://quac.ai">http://quac.ai</a>.</p>
<p>【Keywords】:</p>
<h3 id="242. Knowledge Base Question Answering via Encoding of Complex Query Graphs.">242. Knowledge Base Question Answering via Encoding of Complex Query Graphs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1242/">Paper Link</a>】    【Pages】:2185-2194</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Kangqi">Kangqi Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Fengli">Fengli Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Xusheng">Xusheng Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Kenny_Q=">Kenny Q. Zhu</a></p>
<p>【Abstract】:
Answering complex questions that involve multiple entities and multiple relations using a standard knowledge base is an open and challenging task. Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure. In this work, we encode such complex query structure into a uniform vector representation, and thus successfully capture the interactions between individual semantic components within a complex question. This approach consistently outperforms existing methods on complex questions while staying competitive on simple questions.</p>
<p>【Keywords】:</p>
<h3 id="243. Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning.">243. Neural Relation Extraction via Inner-Sentence Noise Reduction and Transfer Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1243/">Paper Link</a>】    【Pages】:2195-2204</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyi">Tianyi Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xinsong">Xinsong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Wanhao">Wanhao Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jia:Weijia">Weijia Jia</a></p>
<p>【Abstract】:
Extracting relations is critical for knowledge base completion and construction in which distant supervised methods are widely used to extract relational facts automatically with the existing knowledge bases. However, the automatically constructed datasets comprise amounts of low-quality sentences containing noisy words, which is neglected by current distant supervised methods resulting in unacceptable precisions. To mitigate this problem, we propose a novel word-level distant supervised approach for relation extraction. We first build Sub-Tree Parse(STP) to remove noisy words that are irrelevant to relations. Then we construct a neural network inputting the sub-tree while applying the entity-wise attention to identify the important semantic features of relational words in each instance. To make our model more robust against noisy words, we initialize our network with a priori knowledge learned from the relevant task of entity classification by transfer learning. We conduct extensive experiments using the corpora of New York Times(NYT) and Freebase. Experiments show that our approach is effective and improves the area of Precision/Recall(PR) from 0.35 to 0.39 over the state-of-the-art work.</p>
<p>【Keywords】:</p>
<h3 id="244. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.">244. Graph Convolution over Pruned Dependency Trees Improves Relation Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1244/">Paper Link</a>】    【Pages】:2205-2215</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yuhao">Yuhao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi_0003:Peng">Peng Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a></p>
<p>【Abstract】:
Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.</p>
<p>【Keywords】:</p>
<h3 id="245. Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction.">245. Multi-Level Structured Self-Attentions for Distantly Supervised Relation Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1245/">Paper Link</a>】    【Pages】:2216-2225</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Du:Jinhua">Jinhua Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Jingguang">Jingguang Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan:Dadong">Dadong Wan</a></p>
<p>【Abstract】:
Attention mechanism is often used in deep neural networks for distantly supervised relation extraction (DS-RE) to distinguish valid from noisy instances. However, traditional 1-D vector attention model is insufficient for learning of different contexts in the selection of valid instances to predict the relationship for an entity pair. To alleviate this issue, we propose a novel multi-level structured (2-D matrix) self-attention mechanism for DS-RE in a multi-instance learning (MIL) framework using bidirectional recurrent neural networks (BiRNN). In the proposed method, a structured word-level self-attention learns a 2-D matrix where each row vector represents a weight distribution for different aspects of an instance regarding two entities. Targeting the MIL issue, the structured sentence-level attention learns a 2-D matrix where each row vector represents a weight distribution on selection of different valid instances. Experiments conducted on two publicly available DS-RE datasets show that the proposed framework with multi-level structured self-attention mechanism significantly outperform baselines in terms of PR curves, P@N and F1 measures.</p>
<p>【Keywords】:</p>
<h3 id="246. N-ary Relation Extraction using Graph-State LSTM.">246. N-ary Relation Extraction using Graph-State LSTM.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1246/">Paper Link</a>】    【Pages】:2226-2235</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Song:Linfeng">Linfeng Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhiguo">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gildea:Daniel">Daniel Gildea</a></p>
<p>【Abstract】:
Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature.</p>
<p>【Keywords】:</p>
<h3 id="247. Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention.">247. Hierarchical Relation Extraction with Coarse-to-Fine Grained Attention.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1247/">Paper Link</a>】    【Pages】:2236-2245</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xu">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Pengfei">Pengfei Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0030:Peng">Peng Li</a></p>
<p>【Abstract】:
Distantly supervised relation extraction employs existing knowledge graphs to automatically collect training data. While distant supervision is effective to scale relation extraction up to large-scale corpora, it inevitably suffers from the wrong labeling problem. Many efforts have been devoted to identifying valid instances from noisy data. However, most existing methods handle each relation in isolation, regardless of rich semantic correlations located in relation hierarchies. In this paper, we aim to incorporate the hierarchical information of relations for distantly supervised relation extraction and propose a novel hierarchical attention scheme. The multiple layers of our hierarchical attention scheme provide coarse-to-fine granularity to better identify valid instances, which is especially effective for extracting those long-tail relations. The experimental results on a large-scale benchmark dataset demonstrate that our models are capable of modeling the hierarchical information of relations and significantly outperform other baselines. The source code of this paper can be obtained from <a href="https://github.com/thunlp/HNRE">https://github.com/thunlp/HNRE</a>.</p>
<p>【Keywords】:</p>
<h3 id="248. Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding.">248. Label-Free Distant Supervision for Relation Extraction via Knowledge Graph Embedding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1248/">Paper Link</a>】    【Pages】:2246-2255</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guanying">Guanying Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Wen">Wen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Ruoxu">Ruoxu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Yalin">Yalin Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0003:Xi">Xi Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Wei">Wei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hai">Hai Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Huajun">Huajun Chen</a></p>
<p>【Abstract】:
Distant supervision is an effective method to generate large scale labeled data for relation extraction, which assumes that if a pair of entities appears in some relation of a Knowledge Graph (KG), all sentences containing those entities in a large unlabeled corpus are then labeled with that relation to train a relation classifier. However, when the pair of entities has multiple relationships in the KG, this assumption may produce noisy relation labels. This paper proposes a label-free distant supervision method, which makes no use of the relation labels under this inadequate assumption, but only uses the prior knowledge derived from the KG to supervise the learning of the classifier directly and softly. Specifically, we make use of the type information and the translation law derived from typical KG embedding model to learn embeddings for certain sentence patterns. As the supervision signal is only determined by the two aligned entities, neither hard relation labels nor extra noise-reduction model for the bag of sentences is needed in this way. The experiments show that the approach performs well in current distant supervision dataset.</p>
<p>【Keywords】:</p>
<h3 id="249. Extracting Entities and Relations with Joint Minimum Risk Training.">249. Extracting Entities and Relations with Joint Minimum Risk Training.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1249/">Paper Link</a>】    【Pages】:2256-2265</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Changzhi">Changzhi Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yuanbin">Yuanbin Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Man">Man Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Shiliang">Shiliang Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenting">Wenting Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Kuang=Chih">Kuang-Chih Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0003:Kewen">Kewen Wu</a></p>
<p>【Abstract】:
We investigate the task of joint entity relation extraction. Unlike prior efforts, we propose a new lightweight joint learning paradigm based on minimum risk training (MRT). Specifically, our algorithm optimizes a global loss function which is flexible and effective to explore interactions between the entity model and the relation model. We implement a strong and simple neural network where the MRT is executed. Experiment results on the benchmark ACE05 and NYT datasets show that our model is able to achieve state-of-the-art joint extraction performances.</p>
<p>【Keywords】:</p>
<h3 id="250. Large-scale Exploration of Neural Relation Classification Architectures.">250. Large-scale Exploration of Neural Relation Classification Architectures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1250/">Paper Link</a>】    【Pages】:2266-2277</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Le:Hoang=Quynh">Hoang-Quynh Le</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Can:Duy=Cat">Duy-Cat Can</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vu:Sinh_T=">Sinh T. Vu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dang:Thanh_Hai">Thanh Hai Dang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pilehvar:Mohammad_Taher">Mohammad Taher Pilehvar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collier:Nigel">Nigel Collier</a></p>
<p>【Abstract】:
Experimental performance on the task of relation classification has generally improved using deep neural network architectures. One major drawback of reported studies is that individual models have been evaluated on a very narrow range of datasets, raising questions about the adaptability of the architectures, while making comparisons between approaches difficult. In this work, we present a systematic large-scale analysis of neural relation classification architectures on six benchmark datasets with widely varying characteristics. We propose a novel multi-channel LSTM model combined with a CNN that takes advantage of all currently popular linguistic and architectural features. Our ‘Man for All Seasons’ approach achieves state-of-the-art performance on two datasets. More importantly, in our view, the model allowed us to obtain direct insights into the continued challenges faced by neural language models on this task.</p>
<p>【Keywords】:</p>
<h3 id="251. Possessors Change Over Time: A Case Study with Artworks.">251. Possessors Change Over Time: A Case Study with Artworks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1251/">Paper Link</a>】    【Pages】:2278-2287</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chinnappa:Dhivya">Dhivya Chinnappa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blanco_0002:Eduardo">Eduardo Blanco</a></p>
<p>【Abstract】:
This paper presents a corpus and experimental results to extract possession relations over time. We work with Wikipedia articles about artworks, and extract possession relations along with temporal information indicating when these relations are true. The annotation scheme yields many possessors over time for a given artwork, and experimental results show that an LSTM ensemble can automate the task.</p>
<p>【Keywords】:</p>
<h3 id="252. Using lexical alignment and referring ability to address data sparsity in situated dialog reference resolution.">252. Using lexical alignment and referring ability to address data sparsity in situated dialog reference resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1252/">Paper Link</a>】    【Pages】:2288-2297</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shore:Todd">Todd Shore</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Skantze:Gabriel">Gabriel Skantze</a></p>
<p>【Abstract】:
Referring to entities in situated dialog is a collaborative process, whereby interlocutors often expand, repair and/or replace referring expressions in an iterative process, converging on conceptual pacts of referring language use in doing so. Nevertheless, much work on exophoric reference resolution (i.e. resolution of references to entities outside of a given text) follows a literary model, whereby individual referring expressions are interpreted as unique identifiers of their referents given the state of the dialog the referring expression is initiated. In this paper, we address this collaborative nature to improve dialogic reference resolution in two ways: First, we trained a words-as-classifiers logistic regression model of word semantics and incrementally adapt the model to idiosyncratic language between dyad partners during evaluation of the dialog. We then used these semantic models to learn the general referring ability of each word, which is independent of referent features. These methods facilitate accurate automatic reference resolution in situated dialog without annotation of referring expressions, even with little background data.</p>
<p>【Keywords】:</p>
<h3 id="253. Subgoal Discovery for Hierarchical Dialogue Policy Learning.">253. Subgoal Discovery for Hierarchical Dialogue Policy Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1253/">Paper Link</a>】    【Pages】:2298-2309</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Da">Da Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiujun">Xiujun Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Chong">Chong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Lihong">Lihong Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jebara:Tony">Tony Jebara</a></p>
<p>【Abstract】:
Developing agents to engage in complex goal-oriented dialogues is challenging partly because the main learning signals are very sparse in long conversations. In this paper, we propose a divide-and-conquer approach that discovers and exploits the hidden structure of the task to enable efficient policy learning. First, given successful example dialogues, we propose the Subgoal Discovery Network (SDN) to divide a complex goal-oriented task into a set of simpler subgoals in an unsupervised fashion. We then use these subgoals to learn a multi-level policy by hierarchical reinforcement learning. We demonstrate our method by building a dialogue agent for the composite task of travel planning. Experiments with simulated and real users show that our approach performs competitively against a state-of-the-art method that requires human-defined subgoals. Moreover, we show that the learned subgoals are often human comprehensible.</p>
<p>【Keywords】:</p>
<h3 id="254. Supervised Clustering of Questions into Intents for Dialog System Applications.">254. Supervised Clustering of Questions into Intents for Dialog System Applications.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1254/">Paper Link</a>】    【Pages】:2310-2321</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Haponchyk:Iryna">Iryna Haponchyk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Uva_0001:Antonio">Antonio Uva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Seunghak">Seunghak Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Uryupina:Olga">Olga Uryupina</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a></p>
<p>【Abstract】:
Modern automated dialog systems require complex dialog managers able to deal with user intent triggered by high-level semantic questions. In this paper, we propose a model for automatically clustering questions into user intents to help the design tasks. Since questions are short texts, uncovering their semantics to group them together can be very challenging. We approach the problem by using powerful semantic classifiers from question duplicate/matching research along with a novel idea of supervised clustering methods based on structured output. We test our approach on two intent clustering corpora, showing an impressive improvement over previous methods for two languages/domains.</p>
<p>【Keywords】:</p>
<h3 id="255. Towards Exploiting Background Knowledge for Building Conversation Systems.">255. Towards Exploiting Background Knowledge for Building Conversation Systems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1255/">Paper Link</a>】    【Pages】:2322-2332</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moghe:Nikita">Nikita Moghe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Arora:Siddhartha">Siddhartha Arora</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Banerjee:Suman">Suman Banerjee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khapra:Mitesh_M=">Mitesh M. Khapra</a></p>
<p>【Abstract】:
Existing dialog datasets contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has resulted in the development of models which treat conversation as a sequence-to-sequence generation task (i.e., given a sequence of utterances generate the response sequence). This is not only an overly simplistic view of conversation but it is also emphatically different from the way humans converse by heavily relying on their background knowledge about the topic (as opposed to simply relying on the previous sequence of utterances). For example, it is common for humans to (involuntarily) produce utterances which are copied or suitably modified from background articles they have read about the topic. To facilitate the development of such natural conversation models which mimic the human process of conversing, we create a new dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie. We establish baseline results on this dataset (90K utterances from 9K conversations) using three different models: (i) pure generation based models which ignore the background knowledge (ii) generation based models which learn to copy information from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge.</p>
<p>【Keywords】:</p>
<h3 id="256. Decoupling Strategy and Generation in Negotiation Dialogues.">256. Decoupling Strategy and Generation in Negotiation Dialogues.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1256/">Paper Link</a>】    【Pages】:2333-2343</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:He">He He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Derek">Derek Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Balakrishnan:Anusha">Anusha Balakrishnan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>【Abstract】:
We consider negotiation settings in which two agents use natural language to bargain on goods. Agents need to decide on both high-level strategy (e.g., proposing $50) and the execution of that strategy (e.g., generating “The bike is brand new. Selling for just $50!”). Recent work on negotiation trains neural models, but their end-to-end nature makes it hard to control their strategy, and reinforcement learning tends to lead to degenerate solutions. In this paper, we propose a modular approach based on coarse dialogue acts (e.g., propose(price=50)) that decouples strategy and generation. We show that we can flexibly set the strategy using supervised learning, reinforcement learning, or domain-specific knowledge without degeneracy, while our retrieval-based generation can maintain context-awareness and produce diverse utterances. We test our approach on the recently proposed DEALORNODEAL game, and we also collect a richer dataset based on real items on Craigslist. Human evaluation shows that our systems achieve higher task success rate and more human-like negotiation behavior than previous approaches.</p>
<p>【Keywords】:</p>
<h3 id="257. Large-scale Cloze Test Dataset Created by Teachers.">257. Large-scale Cloze Test Dataset Created by Teachers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1257/">Paper Link</a>】    【Pages】:2344-2356</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Qizhe">Qizhe Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lai:Guokun">Guokun Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Zihang">Zihang Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>【Abstract】:
Cloze tests are widely adopted in language exams to evaluate students’ language proficiency. In this paper, we propose the first large-scale human-created cloze test dataset CLOTH, containing questions used in middle-school and high-school language exams. With missing blanks carefully created by teachers and candidate choices purposely designed to be nuanced, CLOTH requires a deeper language understanding and a wider attention span than previously automatically-generated cloze datasets. We test the performance of dedicatedly designed baseline models including a language model trained on the One Billion Word Corpus and show humans outperform them by a significant margin. We investigate the source of the performance gap, trace model deficiencies to some distinct properties of CLOTH, and identify the limited ability of comprehending the long-term context to be the key bottleneck.</p>
<p>【Keywords】:</p>
<h3 id="258. emrQA: A Large Corpus for Question Answering on Electronic Medical Records.">258. emrQA: A Large Corpus for Question Answering on Electronic Medical Records.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1258/">Paper Link</a>】    【Pages】:2357-2368</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pampari:Anusri">Anusri Pampari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Raghavan:Preethi">Preethi Raghavan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Jennifer_J=">Jennifer J. Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng_0001:Jian">Jian Peng</a></p>
<p>【Abstract】:
We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks. We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets. The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs. We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.</p>
<p>【Keywords】:</p>
<h3 id="259. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.">259. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1259/">Paper Link</a>】    【Pages】:2369-2380</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zhilin">Zhilin Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi_0003:Peng">Peng Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Saizheng">Saizheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bengio:Yoshua">Yoshua Bengio</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a></p>
<p>【Abstract】:
Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems’ ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.</p>
<p>【Keywords】:</p>
<h3 id="260. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering.">260. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1260/">Paper Link</a>】    【Pages】:2381-2391</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mihaylov:Todor">Todor Mihaylov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Peter">Peter Clark</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khot:Tushar">Tushar Khot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish">Ashish Sabharwal</a></p>
<p>【Abstract】:
We present a new kind of question answering dataset, OpenBookQA, modeled after open book exams for assessing human understanding of a subject. The open book that comes with our questions is a set of 1326 elementary level science facts. Roughly 6000 questions probe an understanding of these facts and their application to novel situations. This requires combining an open book fact (e.g., metals conduct electricity) with broad common knowledge (e.g., a suit of armor is made of metal) obtained from other sources. While existing QA datasets over documents or knowledge bases, being generally self-contained, focus on linguistic understanding, OpenBookQA probes a deeper understanding of both the topic—in the context of common knowledge—and the language it is expressed in. Human performance on OpenBookQA is close to 92%, but many state-of-the-art pre-trained QA methods perform surprisingly poorly, worse than several simple neural baselines we develop. Our oracle experiments designed to circumvent the knowledge retrieval bottleneck demonstrate the value of both the open book and additional facts. We leave it as a challenge to solve the retrieval problem in this multi-hop setting and to close the large gap to human performance.</p>
<p>【Keywords】:</p>
<h3 id="261. Evaluating Theory of Mind in Question Answering.">261. Evaluating Theory of Mind in Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1261/">Paper Link</a>】    【Pages】:2392-2400</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nematzadeh:Aida">Aida Nematzadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Burns:Kaylee">Kaylee Burns</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grant:Erin">Erin Grant</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gopnik:Alison">Alison Gopnik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Griffiths:Thomas_L=">Thomas L. Griffiths</a></p>
<p>【Abstract】:
We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs. Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality. We evaluate a number of recent neural models with memory augmentation. We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models’ accuracy decreases notably when random sentences are introduced to the tasks at test.</p>
<p>【Keywords】:</p>
<h3 id="262. A Unified Syntax-aware Framework for Semantic Role Labeling.">262. A Unified Syntax-aware Framework for Semantic Role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1262/">Paper Link</a>】    【Pages】:2401-2411</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zuchao">Zuchao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Shexia">Shexia He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Jiaxun">Jiaxun Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Zhuosheng">Zhuosheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Gongshen">Gongshen Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Linlin">Linlin Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Semantic role labeling (SRL) aims to recognize the predicate-argument structure of a sentence. Syntactic information has been paid a great attention over the role of enhancing SRL. However, the latest advance shows that syntax would not be so important for SRL with the emerging much smaller gap between syntax-aware and syntax-agnostic SRL. To comprehensively explore the role of syntax for SRL task, we extend existing models and propose a unified framework to investigate more effective and more diverse ways of incorporating syntax into sequential neural networks. Exploring the effect of syntactic input quality on SRL performance, we confirm that high-quality syntactic parse could still effectively enhance syntactically-driven SRL. Using empirically optimized integration strategy, we even enlarge the gap between syntax-aware and syntax-agnostic SRL. Our framework achieves state-of-the-art results on CoNLL-2009 benchmarks both for English and Chinese, substantially outperforming all previous models.</p>
<p>【Keywords】:</p>
<h3 id="263. Semantics as a Foreign Language.">263. Semantics as a Foreign Language.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1263/">Paper Link</a>】    【Pages】:2412-2421</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stanovsky:Gabriel">Gabriel Stanovsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dagan:Ido">Ido Dagan</a></p>
<p>【Abstract】:
We propose a novel approach to semantic dependency parsing (SDP) by casting the task as an instance of multi-lingual machine translation, where each semantic representation is a different foreign dialect. To that end, we first generalize syntactic linearization techniques to account for the richer semantic dependency graph structure. Following, we design a neural sequence-to-sequence framework which can effectively recover our graph linearizations, performing almost on-par with previous SDP state-of-the-art while requiring less parallel training annotations. Beyond SDP, our linearization technique opens the door to integration of graph-based semantic representations as features in neural models for downstream applications.</p>
<p>【Keywords】:</p>
<h3 id="264. An AMR Aligner Tuned by Transition-based Parser.">264. An AMR Aligner Tuned by Transition-based Parser.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1264/">Paper Link</a>】    【Pages】:2422-2430</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yijia">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Bo">Bo Zheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin_0001:Bing">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>【Abstract】:
In this paper, we propose a new rich resource enhanced AMR aligner which produces multiple alignments and a new transition system for AMR parsing along with its oracle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highest-scored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the current state-of-the-art parser.</p>
<p>【Keywords】:</p>
<h3 id="265. Dependency-based Hybrid Trees for Semantic Parsing.">265. Dependency-based Hybrid Trees for Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1265/">Paper Link</a>】    【Pages】:2431-2441</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jie:Zhanming">Zhanming Jie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0011:Wei">Wei Lu</a></p>
<p>【Abstract】:
We propose a novel dependency-based hybrid tree model for semantic parsing, which converts natural language utterance into machine interpretable meaning representations. Unlike previous state-of-the-art models, the semantic information is interpreted as the latent dependency between the natural language words in our joint representation. Such dependency information can capture the interactions between the semantics and natural language words. We integrate a neural component into our model and propose an efficient dynamic-programming algorithm to perform tractable inference. Through extensive experiments on the standard multilingual GeoQuery dataset with eight languages, we demonstrate that our proposed approach is able to achieve state-of-the-art performance across several languages. Analysis also justifies the effectiveness of using our new dependency-based representation.</p>
<p>【Keywords】:</p>
<h3 id="266. Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations.">266. Policy Shaping and Generalized Update Equations for Semantic Parsing from Denotations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1266/">Paper Link</a>】    【Pages】:2442-2452</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Misra:Dipendra">Dipendra Misra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Ming=Wei">Ming-Wei Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0001:Xiaodong">Xiaodong He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a></p>
<p>【Abstract】:
Semantic parsing from denotations faces two key challenges in model training: (1) given only the denotations (e.g., answers), search for good candidate semantic parses, and (2) choose the best model update algorithm. We propose effective and general solutions to each of them. Using policy shaping, we bias the search procedure towards semantic parses that are more compatible to the text, which provide better supervision signals for training. In addition, we propose an update equation that generalizes three different families of learning algorithms, which enables fast model exploration. When experimented on a recently proposed sequential question answering dataset, our framework leads to a new state-of-the-art model that outperforms previous work by 5.0% absolute on exact match accuracy.</p>
<p>【Keywords】:</p>
<h3 id="267. Sentence Compression for Arbitrary Languages via Multilingual Pivoting.">267. Sentence Compression for Arbitrary Languages via Multilingual Pivoting.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1267/">Paper Link</a>】    【Pages】:2453-2464</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mallinson:Jonathan">Jonathan Mallinson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
In this paper we advocate the use of bilingual corpora which are abundantly available for training sentence compression models. Our approach borrows much of its machinery from neural machine translation and leverages bilingual pivoting: compressions are obtained by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length. Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release. Moss, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres.</p>
<p>【Keywords】:</p>
<h3 id="268. Unsupervised Cross-lingual Transfer of Word Embedding Spaces.">268. Unsupervised Cross-lingual Transfer of Word Embedding Spaces.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1268/">Paper Link</a>】    【Pages】:2465-2474</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Ruochen">Ruochen Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yiming">Yiming Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Otani:Naoki">Naoki Otani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yuexin">Yuexin Wu</a></p>
<p>【Abstract】:
Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.</p>
<p>【Keywords】:</p>
<h3 id="269. XNLI: Evaluating Cross-lingual Sentence Representations.">269. XNLI: Evaluating Cross-lingual Sentence Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1269/">Paper Link</a>】    【Pages】:2475-2485</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Conneau:Alexis">Alexis Conneau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rinott:Ruty">Ruty Rinott</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lample:Guillaume">Guillaume Lample</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Williams:Adina">Adina Williams</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bowman:Samuel_R=">Samuel R. Bowman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwenk:Holger">Holger Schwenk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stoyanov:Veselin">Veselin Stoyanov</a></p>
<p>【Abstract】:
State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.</p>
<p>【Keywords】:</p>
<h3 id="270. Joint Multilingual Supervision for Cross-lingual Entity Linking.">270. Joint Multilingual Supervision for Cross-lingual Entity Linking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1270/">Paper Link</a>】    【Pages】:2486-2495</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/u/Upadhyay:Shyam">Shyam Upadhyay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Nitish">Nitish Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Cross-lingual Entity Linking (XEL) aims to ground entity mentions written in any language to an English Knowledge Base (KB), such as Wikipedia. XEL for most languages is challenging, owing to limited availability of resources as supervision. We address this challenge by developing the first XEL approach that combines supervision from multiple languages jointly. This enables our approach to: (a) augment the limited supervision in the target language with additional supervision from a high-resource language (like English), and (b) train a single entity linking model for multiple languages, improving upon individually trained models for each language. Extensive evaluation on three benchmark datasets across 8 languages shows that our approach significantly improves over the current state-of-the-art. We also provide analyses in two limited resource settings: (a) zero-shot setting, when no supervision in the target language is available, and in (b) low-resource setting, when some supervision in the target language is available. Our analysis provides insights into the limitations of zero-shot XEL approaches in realistic scenarios, and shows the value of joint supervision in low-resource settings.</p>
<p>【Keywords】:</p>
<h3 id="271. Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.">271. Fine-grained Coordinated Cross-lingual Text Stream Alignment for Endless Language Knowledge Acquisition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1271/">Paper Link</a>】    【Pages】:2496-2506</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ge:Tao">Tao Ge</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dou:Qing">Qing Dou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cui:Lei">Lei Cui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sui:Zhifang">Zhifang Sui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>【Abstract】:
This paper proposes to study fine-grained coordinated cross-lingual text stream alignment through a novel information network decipherment paradigm. We use Burst Information Networks as media to represent text streams and present a simple yet effective network decipherment algorithm with diverse clues to decipher the networks for accurate text stream alignment. Experiments on Chinese-English news streams show our approach not only outperforms previous approaches on bilingual lexicon extraction from coordinated text streams but also can harvest high-quality alignments from large amounts of streaming data for endless language knowledge mining, which makes it promising to be a new paradigm for automatic language knowledge acquisition.</p>
<p>【Keywords】:</p>
<h3 id="272. WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition.">272. WECA：A WordNet-Encoded Collocation-Attention Network for Homographic Pun Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1272/">Paper Link</a>】    【Pages】:2507-2516</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Diao:Yufeng">Yufeng Diao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Hongfei">Hongfei Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Di">Di Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0003:Liang">Liang Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kan">Kan Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Zhihao">Zhihao Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0021:Jian">Jian Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Shaowu">Shaowu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0009:Bo">Bo Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Dongyu">Dongyu Zhang</a></p>
<p>【Abstract】:
Homographic puns have a long history in human writing, widely used in written and spoken literature, which usually occur in a certain syntactic or stylistic structure. How to recognize homographic puns is an important research. However, homographic pun recognition does not solve very well in existing work. In this work, we first use WordNet to understand and expand word embedding for settling the polysemy of homographic puns, and then propose a WordNet-Encoded Collocation-Attention network model (WECA) which combined with the context weights for recognizing the puns. Our experiments on the SemEval2017 Task7 and Pun of the Day demonstrate that the proposed model is able to distinguish between homographic pun and non-homographic pun texts. We show the effectiveness of the model to present the capability of choosing qualitatively informative words. The results show that our model achieves the state-of-the-art performance on homographic puns recognition.</p>
<p>【Keywords】:</p>
<h3 id="273. A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check.">273. A Hybrid Approach to Automatic Corpus Generation for Chinese Spelling Check.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1273/">Paper Link</a>】    【Pages】:2517-2527</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Dingmin">Dingmin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yan">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jing">Jing Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Jialong">Jialong Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Haisong">Haisong Zhang</a></p>
<p>【Abstract】:
Chinese spelling check (CSC) is a challenging yet meaningful task, which not only serves as a preprocessing in many natural language processing(NLP) applications, but also facilitates reading and understanding of running texts in peoples’ daily lives. However, to utilize data-driven approaches for CSC, there is one major limitation that annotated corpora are not enough in applying algorithms and building models. In this paper, we propose a novel approach of constructing CSC corpus with automatically generated spelling errors, which are either visually or phonologically resembled characters, corresponding to the OCR- and ASR-based methods, respectively. Upon the constructed corpus, different models are trained and evaluated for CSC with respect to three standard test sets. Experimental results demonstrate the effectiveness of the corpus, therefore confirm the validity of our approach.</p>
<p>【Keywords】:</p>
<h3 id="274. Neural Quality Estimation of Grammatical Error Correction.">274. Neural Quality Estimation of Grammatical Error Correction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1274/">Paper Link</a>】    【Pages】:2528-2539</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chollampatt:Shamil">Shamil Chollampatt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a></p>
<p>【Abstract】:
Grammatical error correction (GEC) systems deployed in language learning environments are expected to accurately correct errors in learners’ writing. However, in practice, they often produce spurious corrections and fail to correct many errors, thereby misleading learners. This necessitates the estimation of the quality of output sentences produced by GEC systems so that instructors can selectively intervene and re-correct the sentences which are poorly corrected by the system and ensure that learners get accurate feedback. We propose the first neural approach to automatic quality estimation of GEC output sentences that does not employ any hand-crafted features. Our system is trained in a supervised manner on learner sentences and corresponding GEC system outputs with quality score labels computed using human-annotated references. Our neural quality estimation models for GEC show significant improvements over a strong feature-based baseline. We also show that a state-of-the-art GEC system can be improved when quality scores are used as features for re-ranking the N-best candidates.</p>
<p>【Keywords】:</p>
<h3 id="275. Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging.">275. Transferring from Formal Newswire Domain with Hypernet for Twitter POS Tagging.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1275/">Paper Link</a>】    【Pages】:2540-2549</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gui:Tao">Tao Gui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Qi">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Jingjing">Jingjing Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Minlong">Minlong Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Di">Di Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Keyu">Keyu Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
Part-of-Speech (POS) tagging for Twitter has received considerable attention in recent years. Because most POS tagging methods are based on supervised models, they usually require a large amount of labeled data for training. However, the existing labeled datasets for Twitter are much smaller than those for newswire text. Hence, to help POS tagging for Twitter, most domain adaptation methods try to leverage newswire datasets by learning the shared features between the two domains. However, from a linguistic perspective, Twitter users not only tend to mimic the formal expressions of traditional media, like news, but they also appear to be developing linguistically informal styles. Therefore, POS tagging for the formal Twitter context can be learned together with the newswire dataset, while POS tagging for the informal Twitter context should be learned separately. To achieve this task, in this work, we propose a hypernetwork-based method to generate different parameters to separately model contexts with different expression styles. Experimental results on three different datasets show that our approach achieves better performance than state-of-the-art methods in most cases.</p>
<p>【Keywords】:</p>
<h3 id="276. Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit.">276. Free as in Free Word Order: An Energy Based Model for Word Segmentation and Morphological Tagging in Sanskrit.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1276/">Paper Link</a>】    【Pages】:2550-2561</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Krishna:Amrith">Amrith Krishna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Santra:Bishal">Bishal Santra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bandaru:Sasi_Prasanth">Sasi Prasanth Bandaru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sahu:Gaurav">Gaurav Sahu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Vishnu_Dutt">Vishnu Dutt Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Satuluri:Pavankumar">Pavankumar Satuluri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goyal:Pawan">Pawan Goyal</a></p>
<p>【Abstract】:
The configurational information in sentences of a free word order language such as Sanskrit is of limited use. Thus, the context of the entire sentence will be desirable even for basic processing tasks such as word segmentation. We propose a structured prediction framework that jointly solves the word segmentation and morphological tagging tasks in Sanskrit. We build an energy based model where we adopt approaches generally employed in graph based parsing techniques (McDonald et al., 2005a; Carreras, 2007). Our model outperforms the state of the art with an F-Score of 96.92 (percentage improvement of 7.06%) while using less than one tenth of the task-specific training data. We find that the use of a graph based approach instead of a traditional lattice-based sequential labelling approach leads to a percentage gain of 12.6% in F-Score for the segmentation task.</p>
<p>【Keywords】:</p>
<h3 id="277. A Challenge Set and Methods for Noun-Verb Ambiguity.">277. A Challenge Set and Methods for Noun-Verb Ambiguity.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1277/">Paper Link</a>】    【Pages】:2562-2572</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/El=Kahky:Ali">Ali El-Kahky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Webster:Kellie">Kellie Webster</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Andor:Daniel">Daniel Andor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pitler:Emily">Emily Pitler</a></p>
<p>【Abstract】:
English part-of-speech taggers regularly make egregious errors related to noun-verb ambiguity, despite having achieved 97%+ accuracy on the WSJ Penn Treebank since 2002. These mistakes have been difficult to quantify and make taggers less useful to downstream tasks such as translation and text-to-speech synthesis. This paper creates a new dataset of over 30,000 naturally-occurring non-trivial examples of noun-verb ambiguity. Taggers within 1% of each other when measured on the WSJ have accuracies ranging from 57% to 75% accuracy on this challenge set. Enhancing the strongest existing tagger with contextual word embeddings and targeted training data improves its accuracy to 89%, a 14% absolute (52% relative) improvement. Downstream, using just this enhanced tagger yields a 28% reduction in error over the prior best learned model for homograph disambiguation for textto-speech synthesis.</p>
<p>【Keywords】:</p>
<h3 id="278. What do character-level models learn about morphology? The case of dependency parsing.">278. What do character-level models learn about morphology? The case of dependency parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1278/">Paper Link</a>】    【Pages】:2573-2583</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vania:Clara">Clara Vania</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grivas:Andreas">Andreas Grivas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lopez:Adam">Adam Lopez</a></p>
<p>【Abstract】:
When parsing morphologically-rich languages with neural models, it is beneficial to model input at the character level, and it has been claimed that this is because character-level models learn morphology. We test these claims by comparing character-level models to an oracle with access to explicit morphological analysis on twelve languages with varying morphological typologies. Our results highlight many strengths of character-level models, but also show that they are poor at disambiguating some words, particularly in the face of case syncretism. We then demonstrate that explicitly modeling morphological case improves our best model, showing that character-level models can benefit from targeted forms of explicit morphological modeling.</p>
<p>【Keywords】:</p>
<h3 id="279. Learning Better Internal Structure of Words for Sequence Labeling.">279. Learning Better Internal Structure of Words for Sequence Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1279/">Paper Link</a>】    【Pages】:2584-2593</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xin:Yingwei">Yingwei Xin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hart:Ethan">Ethan Hart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mahajan:Vibhuti">Vibhuti Mahajan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruvini:Jean=David">Jean-David Ruvini</a></p>
<p>【Abstract】:
Character-based neural models have recently proven very useful for many NLP tasks. However, there is a gap of sophistication between methods for learning representations of sentences and words. While, most character models for learning representations of sentences are deep and complex, models for learning representations of words are shallow and simple. Also, in spite of considerable research on learning character embeddings, it is still not clear which kind of architecture is the best for capturing character-to-word representations. To address these questions, we first investigate the gaps between methods for learning word and sentence representations. We conduct detailed experiments and comparisons on different state-of-the-art convolutional models, and also investigate the advantages and disadvantages of their constituents. Furthermore, we propose IntNet, a funnel-shaped wide convolutional neural architecture with no down-sampling for learning representations of the internal structure of words by composing their characters from limited, supervised training corpora. We evaluate our proposed model on six sequence labeling datasets, including named entity recognition, part-of-speech tagging, and syntactic chunking. Our in-depth analysis shows that IntNet significantly outperforms other character embedding models and obtains new state-of-the-art performance without relying on any external knowledge or resources.</p>
<p>【Keywords】:</p>
<h3 id="280. ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection.">280. ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1280/">Paper Link</a>】    【Pages】:2594-2604</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hazarika:Devamanyu">Devamanyu Hazarika</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mihalcea:Rada">Rada Mihalcea</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cambria:Erik">Erik Cambria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zimmermann:Roger">Roger Zimmermann</a></p>
<p>【Abstract】:
Emotion recognition in conversations is crucial for building empathetic machines. Present works in this domain do not explicitly consider the inter-personal influences that thrive in the emotional dynamics of dialogues. To this end, we propose Interactive COnversational memory Network (ICON), a multimodal emotion detection framework that extracts multimodal features from conversational videos and hierarchically models the self- and inter-speaker emotional influences into global memories. Such memories generate contextual summaries which aid in predicting the emotional orientation of utterance-videos. Our model outperforms state-of-the-art networks on multiple classification and regression tasks in two benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="281. Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation.">281. Discriminative Learning of Open-Vocabulary Object Retrieval and Localization by Negative Phrase Augmentation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1281/">Paper Link</a>】    【Pages】:2605-2615</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hinami:Ryota">Ryota Hinami</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Satoh:Shin=ichi">Shin&apos;ichi Satoh</a></p>
<p>【Abstract】:
Thanks to the success of object detection technology, we can retrieve objects of the specified classes even from huge image collections. However, the current state-of-the-art object detectors (such as Faster R-CNN) can only handle pre-specified classes. In addition, large amounts of positive and negative visual samples are required for training. In this paper, we address the problem of open-vocabulary object retrieval and localization, where the target object is specified by a textual query (e.g., a word or phrase). We first propose Query-Adaptive R-CNN, a simple extension of Faster R-CNN adapted to open-vocabulary queries, by transforming the text embedding vector into an object classifier and localization regressor. Then, for discriminative training, we then propose negative phrase augmentation (NPA) to mine hard negative samples which are visually similar to the query and at the same time semantically mutually exclusive of the query. The proposed method can retrieve and localize objects specified by a textual query from one million images in only 0.5 seconds with high precision.</p>
<p>【Keywords】:</p>
<h3 id="282. Grounding Semantic Roles in Images.">282. Grounding Semantic Roles in Images.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1282/">Paper Link</a>】    【Pages】:2616-2626</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Silberer:Carina">Carina Silberer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pinkal:Manfred">Manfred Pinkal</a></p>
<p>【Abstract】:
We address the task of visual semantic role labeling (vSRL), the identification of the participants of a situation or event in a visual scene, and their labeling with their semantic relations to the event or situation. We render candidate participants as image regions of objects, and train a model which learns to ground roles in the regions which depict the corresponding participant. Experimental results demonstrate that we can train a vSRL model without reliance on prohibitive image-based role annotations, by utilizing noisy data which we extract automatically from image captions using a linguistic SRL system. Furthermore, our model induces frame—semantic visual representations, and their comparison to previous work on supervised visual verb sense disambiguation yields overall better results.</p>
<p>【Keywords】:</p>
<h3 id="283. Commonsense Justification for Action Explanation.">283. Commonsense Justification for Action Explanation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1283/">Paper Link</a>】    【Pages】:2627-2637</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Shaohua">Shaohua Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Qiaozi">Qiaozi Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saba=Sadiya:Sari">Sari Saba-Sadiya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chai:Joyce_Yue">Joyce Yue Chai</a></p>
<p>【Abstract】:
To enable collaboration and communication between humans and agents, this paper investigates learning to acquire commonsense evidence for action justification. In particular, we have developed an approach based on the generative Conditional Variational Autoencoder(CVAE) that models object relations/attributes of the world as latent variables and jointly learns a performer that predicts actions and an explainer that gathers commonsense evidence to justify the action. Our empirical results have shown that, compared to a typical attention-based model, CVAE achieves significantly higher performance in both action prediction and justification. A human subject study further shows that the commonsense evidence gathered by CVAE can be communicated to humans to achieve a significantly higher common ground between humans and agents.</p>
<p>【Keywords】:</p>
<h3 id="284. Learning Personas from Dialogue with Attentive Memory Networks.">284. Learning Personas from Dialogue with Attentive Memory Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1284/">Paper Link</a>】    【Pages】:2638-2646</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chu:Eric">Eric Chu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vijayaraghavan:Prashanth">Prashanth Vijayaraghavan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roy:Deb">Deb Roy</a></p>
<p>【Abstract】:
The ability to infer persona from dialogue can have applications in areas ranging from computational narrative analysis to personalized dialogue generation. We introduce neural models to learn persona embeddings in a supervised character trope classification task. The models encode dialogue snippets from IMDB into representations that can capture the various categories of film characters. The best-performing models use a multi-level attention mechanism over a set of utterances. We also utilize prior knowledge in the form of textual descriptions of the different tropes. We apply the learned embeddings to find similar characters across different movies, and cluster movies according to the distribution of the embeddings. The use of short conversational text as input, and the ability to learn from prior knowledge using memory, suggests these methods could be applied to other domains.</p>
<p>【Keywords】:</p>
<h3 id="285. Grounding language acquisition by training semantic parsers using captioned videos.">285. Grounding language acquisition by training semantic parsers using captioned videos.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1285/">Paper Link</a>】    【Pages】:2647-2656</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ross:Candace">Candace Ross</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barbu:Andrei">Andrei Barbu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berzak:Yevgeni">Yevgeni Berzak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Myanganbayar:Battushig">Battushig Myanganbayar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Katz:Boris">Boris Katz</a></p>
<p>【Abstract】:
We develop a semantic parser that is trained in a grounded setting using pairs of videos captioned with sentences. This setting is both data-efficient, requiring little annotation, and similar to the experience of children where they observe their environment and listen to speakers. The semantic parser recovers the meaning of English sentences despite not having access to any annotated sentences. It does so despite the ambiguity inherent in vision where a sentence may refer to any combination of objects, object properties, relations or actions taken by any agent in a video. For this task, we collected a new dataset for grounded language acquisition. Learning a grounded semantic parser — turning sentences into logical forms using captioned videos — can significantly expand the range of data that parsers can be trained on, lower the effort of training a semantic parser, and ultimately lead to a better understanding of child language acquisition.</p>
<p>【Keywords】:</p>
<h3 id="286. Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.">286. Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1286/">Paper Link</a>】    【Pages】:2657-2666</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zang:Xiaoxue">Xiaoxue Zang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pokle:Ashwini">Ashwini Pokle</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/V=aacute=zquez:Marynel">Marynel Vázquez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Kevin">Kevin Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niebles:Juan_Carlos">Juan Carlos Niebles</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Soto:Alvaro">Alvaro Soto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Savarese:Silvio">Silvio Savarese</a></p>
<p>【Abstract】:
We propose an end-to-end deep learning model for translating free-form natural language instructions to a high-level plan for behavioral robot navigation. We use attention models to connect information from both the user instructions and a topological representation of the environment. We evaluate our model’s performance on a new dataset containing 10,050 pairs of navigation instructions. Our model significantly outperforms baseline approaches. Furthermore, our results suggest that it is possible to leverage the environment map as a relevant knowledge base to facilitate the translation of free-form navigational instruction.</p>
<p>【Keywords】:</p>
<h3 id="287. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction.">287. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1287/">Paper Link</a>】    【Pages】:2667-2678</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Misra:Dipendra_Kumar">Dipendra Kumar Misra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bennett:Andrew">Andrew Bennett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blukis:Valts">Valts Blukis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niklasson:Eyvind">Eyvind Niklasson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shatkhin:Max">Max Shatkhin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Artzi:Yoav">Yoav Artzi</a></p>
<p>【Abstract】:
We propose to decompose instruction execution to goal prediction and action generation. We design a model that maps raw visual observations to goals using LINGUNET, a language-conditioned image generation network, and then generates the actions required to complete them. Our model is trained from demonstration only without external resources. To evaluate our approach, we introduce two benchmarks for instruction following: LANI, a navigation task; and CHAI, where an agent executes household instructions. Our evaluation demonstrates the advantages of our model decomposition, and illustrates the challenges posed by our new benchmarks.</p>
<p>【Keywords】:</p>
<h3 id="288. Deconvolutional time series regression: A technique for modeling temporally diffuse effects.">288. Deconvolutional time series regression: A technique for modeling temporally diffuse effects.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1288/">Paper Link</a>】    【Pages】:2679-2689</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shain:Cory">Cory Shain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schuler:William">William Schuler</a></p>
<p>【Abstract】:
Researchers in computational psycholinguistics frequently use linear models to study time series data generated by human subjects. However, time series may violate the assumptions of these models through temporal diffusion, where stimulus presentation has a lingering influence on the response as the rest of the experiment unfolds. This paper proposes a new statistical model that borrows from digital signal processing by recasting the predictors and response as convolutionally-related signals, using recent advances in machine learning to fit latent impulse response functions (IRFs) of arbitrary shape. A synthetic experiment shows successful recovery of true latent IRFs, and psycholinguistic experiments reveal plausible, replicable, and fine-grained estimates of latent temporal dynamics, with comparable or improved prediction quality to widely-used alternatives.</p>
<p>【Keywords】:</p>
<h3 id="289. Is this Sentence Difficult? Do you Agree?">289. Is this Sentence Difficult? Do you Agree?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1289/">Paper Link</a>】    【Pages】:2690-2699</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Brunato:Dominique">Dominique Brunato</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mattei:Lorenzo_De">Lorenzo De Mattei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dell=Orletta:Felice">Felice Dell&apos;Orletta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iavarone:Benedetta">Benedetta Iavarone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Venturi:Giulia">Giulia Venturi</a></p>
<p>【Abstract】:
In this paper, we present a crowdsourcing-based approach to model the human perception of sentence complexity. We collect a large corpus of sentences rated with judgments of complexity for two typologically-different languages, Italian and English. We test our approach in two experimental scenarios aimed to investigate the contribution of a wide set of lexical, morpho-syntactic and syntactic phenomena in predicting i) the degree of agreement among annotators independently from the assigned judgment and ii) the perception of sentence complexity.</p>
<p>【Keywords】:</p>
<h3 id="290. Neural Transition Based Parsing of Web Queries: An Entity Based Approach.">290. Neural Transition Based Parsing of Web Queries: An Entity Based Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1290/">Paper Link</a>】    【Pages】:2700-2710</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Malca:Rivka">Rivka Malca</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a></p>
<p>【Abstract】:
Web queries with question intent manifest a complex syntactic structure and the processing of this structure is important for their interpretation. Pinter et al. (2016) has formalized the grammar of these queries and proposed semi-supervised algorithms for the adaptation of parsers originally designed to parse according to the standard dependency grammar, so that they can account for the unique forest grammar of queries. However, their algorithms rely on resources typically not available outside of big web corporates. We propose a new BiLSTM query parser that: (1) Explicitly accounts for the unique grammar of web queries; and (2) Utilizes named entity (NE) information from a BiLSTM NE tagger, that can be jointly trained with the parser. In order to train our model we annotate the query treebank of Pinter et al. (2016) with NEs. When trained on 2500 annotated queries our parser achieves UAS of 83.5% and segmentation F1-score of 84.5, substantially outperforming existing state-of-the-art parsers.</p>
<p>【Keywords】:</p>
<h3 id="291. An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing.">291. An Investigation of the Interactions Between Pre-Trained Word Embeddings, Character Models and POS Tags in Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1291/">Paper Link</a>】    【Pages】:2711-2720</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Aaron">Aaron Smith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lhoneux:Miryam_de">Miryam de Lhoneux</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stymne:Sara">Sara Stymne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nivre:Joakim">Joakim Nivre</a></p>
<p>【Abstract】:
We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining them quickly leads to diminishing returns. We categorise words by frequency, POS tag and language in order to systematically investigate how each of the techniques affects parsing quality. For many word categories, applying any two of the three techniques is almost as good as the full combined system. Character models tend to be more important for low-frequency open-class words, especially in morphologically rich languages, while POS tags can help disambiguate high-frequency function words. We also show that large character embedding sizes help even for languages with small character sets, especially in morphologically rich languages.</p>
<p>【Keywords】:</p>
<h3 id="292. Depth-bounding is effective: Improvements and Evaluation of Unsupervised PCFG Induction.">292. Depth-bounding is effective: Improvements and Evaluation of Unsupervised PCFG Induction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1292/">Paper Link</a>】    【Pages】:2721-2731</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Lifeng">Lifeng Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Doshi=Velez:Finale">Finale Doshi-Velez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miller:Timothy_A=">Timothy A. Miller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schuler:William">William Schuler</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:Lane">Lane Schwartz</a></p>
<p>【Abstract】:
There have been several recent attempts to improve the accuracy of grammar induction systems by bounding the recursive complexity of the induction model. Modern depth-bounded grammar inducers have been shown to be more accurate than early unbounded PCFG inducers, but this technique has never been compared against unbounded induction within the same system, in part because most previous depth-bounding models are built around sequence models, the complexity of which grows exponentially with the maximum allowed depth. The present work instead applies depth bounds within a chart-based Bayesian PCFG inducer, where bounding can be switched on and off, and then samples trees with or without bounding. Results show that depth-bounding is indeed significantly effective in limiting the search space of the inducer and thereby increasing accuracy of resulting parsing model, independent of the contribution of modern Bayesian induction techniques. Moreover, parsing results on English, Chinese and German show that this bounded model is able to produce parse trees more accurately than or competitively with state-of-the-art constituency grammar induction models.</p>
<p>【Keywords】:</p>
<h3 id="293. Incremental Computation of Infix Probabilities for Probabilistic Finite Automata.">293. Incremental Computation of Infix Probabilities for Probabilistic Finite Automata.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1293/">Paper Link</a>】    【Pages】:2732-2741</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cognetta:Marco">Marco Cognetta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Yo=Sub">Yo-Sub Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kwon:Soon_Chan">Soon Chan Kwon</a></p>
<p>【Abstract】:
In natural language processing, a common task is to compute the probability of a phrase appearing in a document or to calculate the probability of all phrases matching a given pattern. For instance, one computes affix (prefix, suffix, infix, etc.) probabilities of a string or a set of strings with respect to a probability distribution of patterns. The problem of computing infix probabilities of strings when the pattern distribution is given by a probabilistic context-free grammar or by a probabilistic finite automaton is already solved, yet it was open to compute the infix probabilities in an incremental manner. The incremental computation is crucial when a new query is built from a previous query. We tackle this problem and suggest a method that computes infix probabilities incrementally for probabilistic finite automata by representing all the probabilities of matching strings as a series of transition matrix calculations. We show that the proposed approach is theoretically faster than the previous method and, using real world data, demonstrate that our approach has vastly better performance in practice.</p>
<p>【Keywords】:</p>
<h3 id="294. Syntax Encoding with Application in Authorship Attribution.">294. Syntax Encoding with Application in Authorship Attribution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1294/">Paper Link</a>】    【Pages】:2742-2753</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Richong">Richong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Zhiyuan">Zhiyuan Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Hongyu">Hongyu Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mao:Yongyi">Yongyi Mao</a></p>
<p>【Abstract】:
We propose a novel strategy to encode the syntax parse tree of sentence into a learnable distributed representation. The proposed syntax encoding scheme is provably information-lossless. In specific, an embedding vector is constructed for each word in the sentence, encoding the path in the syntax tree corresponding to the word. The one-to-one correspondence between these “syntax-embedding” vectors and the words (hence their embedding vectors) in the sentence makes it easy to integrate such a representation with all word-level NLP models. We empirically show the benefits of the syntax embeddings on the Authorship Attribution domain, where our approach improves upon the prior art and achieves new performance records on five benchmarking data sets.</p>
<p>【Keywords】:</p>
<h3 id="295. Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks.">295. Sanskrit Word Segmentation Using Character-level Recurrent and Convolutional Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1295/">Paper Link</a>】    【Pages】:2754-2763</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hellwig:Oliver">Oliver Hellwig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nehrdich:Sebastian">Sebastian Nehrdich</a></p>
<p>【Abstract】:
The paper introduces end-to-end neural network models that tokenize Sanskrit by jointly splitting compounds and resolving phonetic merges (Sandhi). Tokenization of Sanskrit depends on local phonetic and distant semantic features that are incorporated using convolutional and recurrent elements. Contrary to most previous systems, our models do not require feature engineering or extern linguistic resources, but operate solely on parallel versions of raw and segmented text. The models discussed in this paper clearly improve over previous approaches to Sanskrit word segmentation. As they are language agnostic, we will demonstrate that they also outperform the state of the art for the related task of German compound splitting.</p>
<p>【Keywords】:</p>
<h3 id="296. Session-level Language Modeling for Conversational Speech.">296. Session-level Language Modeling for Conversational Speech.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1296/">Paper Link</a>】    【Pages】:2764-2768</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Wayne">Wayne Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lingfeng">Lingfeng Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jun">Jun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stolcke:Andreas">Andreas Stolcke</a></p>
<p>【Abstract】:
We propose to generalize language models for conversational speech recognition to allow them to operate across utterance boundaries and speaker changes, thereby capturing conversation-level phenomena such as adjacency pairs, lexical entrainment, and topical coherence. The model consists of a long-short-term memory (LSTM) recurrent network that reads the entire word-level history of a conversation, as well as information about turn taking and speaker overlap, in order to predict each next word. The model is applied in a rescoring framework, where the word history prior to the current utterance is approximated with preliminary recognition results. In experiments in the conversational telephone speech domain (Switchboard) we find that such a model gives substantial perplexity reductions over a standard LSTM-LM with utterance scope, as well as improvements in word error rate.</p>
<p>【Keywords】:</p>
<h3 id="297. Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method.">297. Towards Less Generic Responses in Neural Conversation Models: A Statistical Re-weighting Method.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1297/">Paper Link</a>】    【Pages】:2769-2774</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yahui">Yahui Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bi:Wei">Wei Bi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jun">Jun Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaojiang">Xiaojiang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jian">Jian Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a></p>
<p>【Abstract】:
Sequence-to-sequence neural generation models have achieved promising performance on short text conversation tasks. However, they tend to generate generic/dull responses, leading to unsatisfying dialogue experience. We observe that in the conversation tasks, each query could have multiple responses, which forms a 1-to-n or m-to-n relationship in the view of the total corpus. The objective function used in standard sequence-to-sequence models will be dominated by loss terms with generic patterns. Inspired by this observation, we introduce a statistical re-weighting method that assigns different weights for the multiple responses of the same query, and trains the common neural generation model with the weights. Experimental results on a large Chinese dialogue corpus show that our method improves the acceptance rate of generated responses compared with several baseline models and significantly reduces the number of generated generic responses.</p>
<p>【Keywords】:</p>
<h3 id="298. Training Millions of Personalized Dialogue Agents.">298. Training Millions of Personalized Dialogue Agents.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1298/">Paper Link</a>】    【Pages】:2775-2779</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mazar=eacute=:Pierre=Emmanuel">Pierre-Emmanuel Mazaré</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Humeau:Samuel">Samuel Humeau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Raison:Martin">Martin Raison</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bordes:Antoine">Antoine Bordes</a></p>
<p>【Abstract】:
Current dialogue systems fail at being engaging for users, especially when trained end-to-end without relying on proactive reengaging scripted strategies. Zhang et al. (2018) showed that the engagement level of end-to-end dialogue models increases when conditioning them on text personas providing some personalized back-story to the model. However, the dataset used in Zhang et al. (2018) is synthetic and only contains around 1k different personas. In this paper we introduce a new dataset providing 5 million personas and 700 million persona-based dialogues. Our experiments show that, at this scale, training using personas still improves the performance of end-to-end systems. In addition, we show that other tasks benefit from the wide coverage of our dataset by fine-tuning our model on the data from Zhang et al. (2018) and achieving state-of-the-art results.</p>
<p>【Keywords】:</p>
<h3 id="299. Towards Universal Dialogue State Tracking.">299. Towards Universal Dialogue State Tracking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1299/">Paper Link</a>】    【Pages】:2780-2786</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Liliang">Liliang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Kaige">Kaige Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Lu">Lu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Kai">Kai Yu</a></p>
<p>【Abstract】:
Dialogue state tracker is the core part of a spoken dialogue system. It estimates the beliefs of possible user’s goals at every dialogue turn. However, for most current approaches, it’s difficult to scale to large dialogue domains. They have one or more of following limitations: (a) Some models don’t work in the situation where slot values in ontology changes dynamically; (b) The number of model parameters is proportional to the number of slots; (c) Some models extract features based on hand-crafted lexicons. To tackle these challenges, we propose StateNet, a universal dialogue state tracker. It is independent of the number of values, shares parameters across all slots, and uses pre-trained word vectors instead of explicit semantic dictionaries. Our experiments on two datasets show that our approach not only overcomes the limitations, but also significantly outperforms the performance of state-of-the-art approaches.</p>
<p>【Keywords】:</p>
<h3 id="300. Semantic Parsing for Task Oriented Dialog using Hierarchical Representations.">300. Semantic Parsing for Task Oriented Dialog using Hierarchical Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1300/">Paper Link</a>】    【Pages】:2787-2792</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Sonal">Sonal Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shah:Rushin">Rushin Shah</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mohit:Mrinal">Mrinal Mohit</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Anuj">Anuj Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a></p>
<p>【Abstract】:
Task oriented dialog systems typically first parse user utterances to semantic frames comprised of intents and slots. Previous work on task oriented intent and slot-filling work has been restricted to one intent per query and one slot label per token, and thus cannot model complex compositional requests. Alternative semantic parsing systems have represented queries as logical forms, but these are challenging to annotate and parse. We propose a hierarchical annotation scheme for semantic parsing that allows the representation of compositional queries, and can be efficiently and accurately parsed by standard constituency parsing models. We release a dataset of 44k annotated queries (<a href="http://fb.me/semanticparsingdialog">http://fb.me/semanticparsingdialog</a>), and show that parsing models outperform sequence-to-sequence approaches on this dataset.</p>
<p>【Keywords】:</p>
<h3 id="301. The glass ceiling in NLP.">301. The glass ceiling in NLP.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1301/">Paper Link</a>】    【Pages】:2793-2798</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schluter:Natalie">Natalie Schluter</a></p>
<p>【Abstract】:
In this paper, we provide empirical evidence based on a rigourously studied mathematical model for bi-populated networks, that a glass ceiling within the field of NLP has developed since the mid 2000s.</p>
<p>【Keywords】:</p>
<h3 id="302. Reducing Gender Bias in Abusive Language Detection.">302. Reducing Gender Bias in Abusive Language Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1302/">Paper Link</a>】    【Pages】:2799-2804</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Park:Ji_Ho">Ji Ho Park</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shin:Jamin">Jamin Shin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fung:Pascale">Pascale Fung</a></p>
<p>【Abstract】:
Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, “You are a good woman” was considered “sexist” when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.</p>
<p>【Keywords】:</p>
<h3 id="303. SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories.">303. SafeCity: Understanding Diverse Forms of Sexual Harassment Personal Stories.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1303/">Paper Link</a>】    【Pages】:2805-2811</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Karlekar:Sweta">Sweta Karlekar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
With the recent rise of #MeToo, an increasing number of personal stories about sexual harassment and sexual abuse have been shared online. In order to push forward the fight against such harassment and abuse, we present the task of automatically categorizing and analyzing various forms of sexual harassment, based on stories shared on the online forum SafeCity. For the labels of groping, ogling, and commenting, our single-label CNN-RNN model achieves an accuracy of 86.5%, and our multi-label model achieves a Hamming score of 82.5%. Furthermore, we present analysis using LIME, first-derivative saliency heatmaps, activation clustering, and embedding visualization to interpret neural model predictions and demonstrate how this helps extract features that can help automatically fill out incident reports, identify unsafe areas, avoid unsafe practices, and ‘pin the creeps’.</p>
<p>【Keywords】:</p>
<h3 id="304. Learning multiview embeddings for assessing dementia.">304. Learning multiview embeddings for assessing dementia.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1304/">Paper Link</a>】    【Pages】:2812-2817</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pou=Prom:Chlo=eacute=">Chloé Pou-Prom</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rudzicz:Frank">Frank Rudzicz</a></p>
<p>【Abstract】:
As the incidence of Alzheimer’s Disease (AD) increases, early detection becomes crucial. Unfortunately, datasets for AD assessment are often sparse and incomplete. In this work, we leverage the multiview nature of a small AD dataset, DementiaBank, to learn an embedding that captures different modes of cognitive impairment. We apply generalized canonical correlation analysis (GCCA) to our dataset and demonstrate the added benefit of using multiview embeddings in two downstream tasks: identifying AD and predicting clinical scores. By including multiview embeddings, we obtain an F1 score of 0.82 in the classification task and a mean absolute error of 3.42 in the regression task. Furthermore, we show that multiview embeddings can be obtained from other datasets as well.</p>
<p>【Keywords】:</p>
<h3 id="305. WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community.">305. WikiConv: A Corpus of the Complete Conversational History of a Large Online Collaborative Community.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1305/">Paper Link</a>】    【Pages】:2818-2823</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hua:Yiqing">Yiqing Hua</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Danescu=Niculescu=Mizil:Cristian">Cristian Danescu-Niculescu-Mizil</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taraborelli:Dario">Dario Taraborelli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thain:Nithum">Nithum Thain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sorensen:Jeffery">Jeffery Sorensen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dixon:Lucas">Lucas Dixon</a></p>
<p>【Abstract】:
We present a corpus that encompasses the complete history of conversations between contributors to Wikipedia, one of the largest online collaborative communities. By recording the intermediate states of conversations - including not only comments and replies, but also their modifications, deletions and restorations - this data offers an unprecedented view of online conversation. Our framework is designed to be language agnostic, and we show that it extracts high quality data in both Chinese and English. This level of detail supports new research questions pertaining to the process (and challenges) of large-scale online collaboration. We illustrate the corpus’ potential with two case studies on English Wikipedia that highlight new perspectives on earlier work. First, we explore how a person’s conversational behavior depends on how they relate to the discussion’s venue. Second, we show that community moderation of toxic behavior happens at a higher rate than previously estimated.</p>
<p>【Keywords】:</p>
<h3 id="306. Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets.">306. Marginal Likelihood Training of BiLSTM-CRF for Biomedical Named Entity Recognition from Disjoint Label Sets.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1306/">Paper Link</a>】    【Pages】:2824-2829</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Greenberg:Nathan">Nathan Greenberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Trapit">Trapit Bansal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verga:Patrick">Patrick Verga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>【Abstract】:
Extracting typed entity mentions from text is a fundamental component to language understanding and reasoning. While there exist substantial labeled text datasets for multiple subsets of biomedical entity types—such as genes and proteins, or chemicals and diseases—it is rare to find large labeled datasets containing labels for all desired entity types together. This paper presents a method for training a single CRF extractor from multiple datasets with disjoint or partially overlapping sets of entity types. Our approach employs marginal likelihood training to insist on labels that are present in the data, while filling in “missing labels”. This allows us to leverage all the available data within a single model. In experimental results on the Biocreative V CDR (chemicals/diseases), Biocreative VI ChemProt (chemicals/proteins) and MedMentions (19 entity types) datasets, we show that joint training on multiple datasets improves NER F1 over training in isolation, and our methods achieve state-of-the-art results.</p>
<p>【Keywords】:</p>
<h3 id="307. Adversarial training for multi-context joint entity and relation extraction.">307. Adversarial training for multi-context joint entity and relation extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1307/">Paper Link</a>】    【Pages】:2830-2836</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bekoulis:Giannis">Giannis Bekoulis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deleu:Johannes">Johannes Deleu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Demeester:Thomas">Thomas Demeester</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Develder:Chris">Chris Develder</a></p>
<p>【Abstract】:
Adversarial training (AT) is a regularization method that can be used to improve the robustness of neural network methods by adding small perturbations in the training data. We show how to use AT for the tasks of entity recognition and relation extraction. In particular, we demonstrate that applying AT to a general purpose baseline model for jointly extracting entities and relations, allows improving the state-of-the-art effectiveness on several datasets in different contexts (i.e., news, biomedical, and real estate data) and for different languages (English and Dutch).</p>
<p>【Keywords】:</p>
<h3 id="308. Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding.">308. Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree Decoding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1308/">Paper Link</a>】    【Pages】:2837-2842</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Gaurav">Gaurav Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thomas:James">James Thomas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marshall:Iain_James">Iain James Marshall</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shawe=Taylor:John">John Shawe-Taylor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wallace:Byron_C=">Byron C. Wallace</a></p>
<p>【Abstract】:
We propose a model for tagging unstructured texts with an arbitrary number of terms drawn from a tree-structured vocabulary (i.e., an ontology). We treat this as a special case of sequence-to-sequence learning in which the decoder begins at the root node of an ontological tree and recursively elects to expand child nodes as a function of the input text, the current node, and the latent decoder state. We demonstrate that this method yields state-of-the-art results on the important task of assigning MeSH terms to biomedical abstracts.</p>
<p>【Keywords】:</p>
<h3 id="309. Deep Exhaustive Model for Nested Named Entity Recognition.">309. Deep Exhaustive Model for Nested Named Entity Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1309/">Paper Link</a>】    【Pages】:2843-2849</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sohrab:Mohammad_Golam">Mohammad Golam Sohrab</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miwa:Makoto">Makoto Miwa</a></p>
<p>【Abstract】:
We propose a simple deep neural model for nested named entity recognition (NER). Most NER models focused on flat entities and ignored nested entities, which failed to fully capture underlying semantic information in texts. The key idea of our model is to enumerate all possible regions or spans as potential entity mentions and classify them with deep neural networks. To reduce the computational costs and capture the information of the contexts around the regions, the model represents the regions using the outputs of shared underlying bidirectional long short-term memory. We evaluate our exhaustive model on the GENIA and JNLPBA corpora in biomedical domain, and the results show that our model outperforms state-of-the-art models on nested and flat NER, achieving 77.1% and 78.4% respectively in terms of F-score, without any external knowledge resources.</p>
<p>【Keywords】:</p>
<h3 id="310. Evaluating the Utility of Hand-crafted Features in Sequence Labeling.">310. Evaluating the Utility of Hand-crafted Features in Sequence Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1310/">Paper Link</a>】    【Pages】:2850-2856</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Minghao">Minghao Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0023:Fei">Fei Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>【Abstract】:
Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora. In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component. We evaluate on the task of named entity recognition (NER), where we show that including manual features for part-of-speech, word shapes and gazetteers can improve the performance of a neural CRF model. We obtain a F 1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models. We also present an ablation study showing the importance of auto-encoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.</p>
<p>【Keywords】:</p>
<h3 id="311. Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data.">311. Improved Dependency Parsing using Implicit Word Connections Learned from Unlabeled Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1311/">Paper Link</a>】    【Pages】:2857-2863</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenhui">Wenhui Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mansur:Mairgup">Mairgup Mansur</a></p>
<p>【Abstract】:
Pre-trained word embeddings and language model have been shown useful in a lot of tasks. However, both of them cannot directly capture word connections in a sentence, which is important for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly capture word connections from unlabeled data by a word ordering model with self-attention mechanism. Experiments show that these implicit word connections do improve our parsing model. Furthermore, by combining with a pre-trained language model, our model gets state-of-the-art performance on the English PTB dataset, achieving 96.35% UAS and 95.25% LAS.</p>
<p>【Keywords】:</p>
<h3 id="312. A Framework for Understanding the Role of Morphology in Universal Dependency Parsing.">312. A Framework for Understanding the Role of Morphology in Universal Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1312/">Paper Link</a>】    【Pages】:2864-2870</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dehouck:Mathieu">Mathieu Dehouck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Denis:Pascal">Pascal Denis</a></p>
<p>【Abstract】:
This paper presents a simple framework for characterizing morphological complexity and how it encodes syntactic information. In particular, we propose a new measure of morpho-syntactic complexity in terms of governor-dependent preferential attachment that explains parsing performance. Through experiments on dependency parsing with data from Universal Dependencies (UD), we show that representations derived from morphological attributes deliver important parsing performance improvements over standard word form embeddings when trained on the same datasets. We also show that the new morpho-syntactic complexity measure is predictive of the gains provided by using morphological attributes over plain forms on parsing scores, making it a tool to distinguish languages using morphology as a syntactic marker from others.</p>
<p>【Keywords】:</p>
<h3 id="313. The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.">313. The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1313/">Paper Link</a>】    【Pages】:2871-2876</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bisazza:Arianna">Arianna Bisazza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tump:Clara">Clara Tump</a></p>
<p>【Abstract】:
Neural sequence-to-sequence models have proven very effective for machine translation, but at the expense of model interpretability. To shed more light into the role played by linguistic structure in the process of neural machine translation, we perform a fine-grained analysis of how various source-side morphological features are captured at different levels of the NMT encoder while varying the target language. Differently from previous work, we find no correlation between the accuracy of source morphology encoding and translation quality. We do find that morphological features are only captured in context and only to the extent that they are directly transferable to the target words.</p>
<p>【Keywords】:</p>
<h3 id="314. Imitation Learning for Neural Morphological String Transduction.">314. Imitation Learning for Neural Morphological String Transduction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1314/">Paper Link</a>】    【Pages】:2877-2882</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Makarov:Peter">Peter Makarov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clematide:Simon">Simon Clematide</a></p>
<p>【Abstract】:
We employ imitation learning to train a neural transition-based string transducer for morphological tasks such as inflection generation and lemmatization. Previous approaches to training this type of model either rely on an external character aligner for the production of gold action sequences, which results in a suboptimal model due to the unwarranted dependence on a single gold action sequence despite spurious ambiguity, or require warm starting with an MLE model. Our approach only requires a simple expert policy, eliminating the need for a character aligner or warm start. It also addresses familiar MLE training biases and leads to strong and state-of-the-art performance on several benchmarks.</p>
<p>【Keywords】:</p>
<h3 id="315. An Encoder-Decoder Approach to the Paradigm Cell Filling Problem.">315. An Encoder-Decoder Approach to the Paradigm Cell Filling Problem.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1315/">Paper Link</a>】    【Pages】:2883-2889</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Silfverberg:Miikka">Miikka Silfverberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hulden:Mans">Mans Hulden</a></p>
<p>【Abstract】:
The Paradigm Cell Filling Problem in morphology asks to complete word inflection tables from partial ones. We implement novel neural models for this task, evaluating them on 18 data sets in 8 languages, showing performance that is comparable with previous work with far less training data. We also publish a new dataset for this task and code implementing the system described in this paper.</p>
<p>【Keywords】:</p>
<h3 id="316. Generating Natural Language Adversarial Examples.">316. Generating Natural Language Adversarial Examples.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1316/">Paper Link</a>】    【Pages】:2890-2896</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alzantot:Moustafa">Moustafa Alzantot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Yash">Yash Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Elgohary:Ahmed">Ahmed Elgohary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Ho:Bo=Jhang">Bo-Jhang Ho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Srivastava:Mani_B=">Mani B. Srivastava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Kai=Wei">Kai-Wei Chang</a></p>
<p>【Abstract】:
Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations can often be made virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.</p>
<p>【Keywords】:</p>
<h3 id="317. Multi-Head Attention with Disagreement Regularization.">317. Multi-Head Attention with Disagreement Regularization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1317/">Paper Link</a>】    【Pages】:2897-2903</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jian">Jian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Baosong">Baosong Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Michael_R=">Michael R. Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Tong">Tong Zhang</a></p>
<p>【Abstract】:
Multi-head attention is appealing for the ability to jointly attend to information from different representation subspaces at different positions. In this work, we introduce a disagreement regularization to explicitly encourage the diversity among multiple attention heads. Specifically, we propose three types of disagreement regularization, which respectively encourage the subspace, the attended positions, and the output representation associated with each attention head to be different from other heads. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation tasks demonstrate the effectiveness and universality of the proposed approach.</p>
<p>【Keywords】:</p>
<h3 id="318. Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.">318. Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1318/">Paper Link</a>】    【Pages】:2904-2909</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Siddhant:Aditya">Aditya Siddhant</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lipton:Zachary_C=">Zachary C. Lipton</a></p>
<p>【Abstract】:
Several recent papers investigate Active Learning (AL) for mitigating the data dependence of deep learning for natural language processing. However, the applicability of AL to real-world problems remains an open question. While in supervised learning, practitioners can try many different methods, evaluating each against a validation set before selecting a model, AL affords no such luxury. Over the course of one AL run, an agent annotates its dataset exhausting its labeling budget. Thus, given a new task, we have no opportunity to compare models and acquisition functions. This paper provides a large-scale empirical study of deep active learning, addressing multiple tasks and, for each, multiple datasets, multiple models, and a full suite of acquisition functions. We find that across all settings, Bayesian active learning by disagreement, using uncertainty estimates provided either by Dropout or Bayes-by-Backprop significantly improves over i.i.d. baselines and usually outperforms classic uncertainty sampling.</p>
<p>【Keywords】:</p>
<h3 id="319. Bayesian Compression for Natural Language Processing.">319. Bayesian Compression for Natural Language Processing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1319/">Paper Link</a>】    【Pages】:2910-2915</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chirkova:Nadezhda">Nadezhda Chirkova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lobacheva:Ekaterina">Ekaterina Lobacheva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vetrov:Dmitry_P=">Dmitry P. Vetrov</a></p>
<p>【Abstract】:
In natural language processing, a lot of the tasks are successfully solved with recurrent neural networks, but such models have a huge number of parameters. The majority of these parameters are often concentrated in the embedding layer, which size grows proportionally to the vocabulary length. We propose a Bayesian sparsification technique for RNNs which allows compressing the RNN dozens or hundreds of times without time-consuming hyperparameters tuning. We also generalize the model for vocabulary sparsification to filter out unnecessary words and compress the RNN even further. We show that the choice of the kept words is interpretable.</p>
<p>【Keywords】:</p>
<h3 id="320. Multimodal neural pronunciation modeling for spoken languages with logographic origin.">320. Multimodal neural pronunciation modeling for spoken languages with logographic origin.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1320/">Paper Link</a>】    【Pages】:2916-2922</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen_0002:Minh">Minh Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ngo:Hoang_Gia">Hoang Gia Ngo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Nancy_F=">Nancy F. Chen</a></p>
<p>【Abstract】:
Graphemes of most languages encode pronunciation, though some are more explicit than others. Languages like Spanish have a straightforward mapping between its graphemes and phonemes, while this mapping is more convoluted for languages like English. Spoken languages such as Cantonese present even more challenges in pronunciation modeling: (1) they do not have a standard written form, (2) the closest graphemic origins are logographic Han characters, of which only a subset of these logographic characters implicitly encodes pronunciation. In this work, we propose a multimodal approach to predict the pronunciation of Cantonese logographic characters, using neural networks with a geometric representation of logographs and pronunciation of cognates in historically related languages. The proposed framework improves performance by 18.1% and 25.0% respective to unimodal and multimodal baselines.</p>
<p>【Keywords】:</p>
<h3 id="321. Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet.">321. Chinese Pinyin Aided IME, Input What You Have Not Keystroked Yet.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1321/">Paper Link</a>】    【Pages】:2923-2929</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Yafang">Yafang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>【Abstract】:
Chinese pinyin input method engine (IME) converts pinyin into character so that Chinese characters can be conveniently inputted into computer through common keyboard. IMEs work relying on its core component, pinyin-to-character conversion (P2C). Usually Chinese IMEs simply predict a list of character sequences for user choice only according to user pinyin input at each turn. However, Chinese inputting is a multi-turn online procedure, which can be supposed to be exploited for further user experience promoting. This paper thus for the first time introduces a sequence-to-sequence model with gated-attention mechanism for the core task in IMEs. The proposed neural P2C model is learned by encoding previous input utterance as extra context to enable our IME capable of predicting character sequence with incomplete pinyin input. Our model is evaluated in different benchmark datasets showing great user experience improvement compared to traditional models, which demonstrates the first engineering practice of building Chinese aided IME.</p>
<p>【Keywords】:</p>
<h3 id="322. Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models.">322. Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1322/">Paper Link</a>】    【Pages】:2930-2935</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Noraset:Thanapon">Thanapon Noraset</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Downey:Doug">Doug Downey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a></p>
<p>【Abstract】:
Recurrent neural network language models (RNNLMs) are the current standard-bearer for statistical language modeling. However, RNNLMs only estimate probabilities for complete sequences of text, whereas some applications require context-independent phrase probabilities instead. In this paper, we study how to compute an RNNLM’s em marginal probability: the probability that the model assigns to a short sequence of text when the preceding context is not known. We introduce a simple method of altering the RNNLM training to make the model more accurate at marginal estimation. Our experiments demonstrate that the technique is effective compared to baselines including the traditional RNNLM probability and an importance sampling approach. Finally, we show how we can use the marginal estimation to improve an RNNLM by training the marginals to match n-gram probabilities from a larger corpus.</p>
<p>【Keywords】:</p>
<h3 id="323. How to represent a word and predict it, too: improving tied architectures for language modelling.">323. How to represent a word and predict it, too: improving tied architectures for language modelling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1323/">Paper Link</a>】    【Pages】:2936-2941</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gulordava:Kristina">Kristina Gulordava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aina:Laura">Laura Aina</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boleda:Gemma">Gemma Boleda</a></p>
<p>【Abstract】:
Recent state-of-the-art neural language models share the representations of words given by the input and output mappings. We propose a simple modification to these architectures that decouples the hidden state from the word embedding prediction. Our architecture leads to comparable or better results compared to previous tied models and models without tying, with a much smaller number of parameters. We also extend our proposal to word2vec models, showing that tying is appropriate for general word prediction tasks.</p>
<p>【Keywords】:</p>
<h3 id="324. The Importance of Generation Order in Language Modeling.">324. The Importance of Generation Order in Language Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1324/">Paper Link</a>】    【Pages】:2942-2946</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Ford:Nicolas">Nicolas Ford</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duckworth:Daniel">Daniel Duckworth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Norouzi_0002:Mohammad">Mohammad Norouzi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dahl:George_E=">George E. Dahl</a></p>
<p>【Abstract】:
Neural language models are a critical component of state-of-the-art systems for machine translation, summarization, audio transcription, and other tasks. These language models are almost universally autoregressive in nature, generating sentences one token at a time from left to right. This paper studies the influence of token generation order on model quality via a novel two-pass language model that produces partially-filled sentence “templates” and then fills in missing tokens. We compare various strategies for structuring these two passes and observe a surprisingly large variation in model quality. We find the most effective strategy generates function words in the first pass followed by content words in the second. We believe these experimental results justify a more extensive investigation of the generation order for neural language models.</p>
<p>【Keywords】:</p>
<h3 id="325. Document-Level Neural Machine Translation with Hierarchical Attention Networks.">325. Document-Level Neural Machine Translation with Hierarchical Attention Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1325/">Paper Link</a>】    【Pages】:2947-2954</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Werlen:Lesly_Miculicich">Lesly Miculicich Werlen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ram:Dhananjay">Dhananjay Ram</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pappas_0002:Nikolaos">Nikolaos Pappas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henderson:James">James Henderson</a></p>
<p>【Abstract】:
Neural Machine Translation (NMT) can be improved by including document-level contextual information. For this purpose, we propose a hierarchical attention model to capture the context in a structured and dynamic manner. The model is integrated in the original NMT architecture as another level of abstraction, conditioning on the NMT model’s own previous hidden states. Experiments show that hierarchical attention significantly improves the BLEU score over a strong NMT baseline with the state-of-the-art in context-aware methods, and that both the encoder and decoder benefit from context in complementary ways.</p>
<p>【Keywords】:</p>
<h3 id="326. Three Strategies to Improve One-to-Many Multilingual Translation.">326. Three Strategies to Improve One-to-Many Multilingual Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1326/">Paper Link</a>】    【Pages】:2955-2960</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yining">Yining Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhai:Feifei">Feifei Zhai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingfang">Jingfang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>【Abstract】:
Due to the benefits of model compactness, multilingual translation (including many-to-one, many-to-many and one-to-many) based on a universal encoder-decoder architecture attracts more and more attention. However, previous studies show that one-to-many translation based on this framework cannot perform on par with the individually trained models. In this work, we introduce three strategies to improve one-to-many multilingual translation by balancing the shared and unique features. Within the architecture of one decoder for all target languages, we first exploit the use of unique initial states for different target languages. Then, we employ language-dependent positional embeddings. Finally and especially, we propose to divide the hidden cells of the decoder into shared and language-dependent ones. The extensive experiments demonstrate that our proposed methods can obtain remarkable improvements over the strong baselines. Moreover, our strategies can achieve comparable or even better performance than the individually trained translation models.</p>
<p>【Keywords】:</p>
<h3 id="327. Multi-Source Syntactic Neural Machine Translation.">327. Multi-Source Syntactic Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1327/">Paper Link</a>】    【Pages】:2961-2966</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Currey:Anna">Anna Currey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Heafield:Kenneth">Kenneth Heafield</a></p>
<p>【Abstract】:
We introduce a novel multi-source technique for incorporating source syntax into neural machine translation using linearized parses. This is achieved by employing separate encoders for the sequential and parsed versions of the same source sentence; the resulting representations are then combined using a hierarchical attention mechanism. The proposed model improves over both seq2seq and parsed baselines by over 1 BLEU on the WMT17 English-German task. Further analysis shows that our multi-source syntactic model is able to translate successfully without any parsed input, unlike standard parsed methods. In addition, performance does not deteriorate as much on long sentences as for the baselines.</p>
<p>【Keywords】:</p>
<h3 id="328. Fixing Translation Divergences in Parallel Corpora for Neural MT.">328. Fixing Translation Divergences in Parallel Corpora for Neural MT.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1328/">Paper Link</a>】    【Pages】:2967-2973</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pham:Minh_Quang">Minh Quang Pham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Crego:Josep_Maria">Josep Maria Crego</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Senellart:Jean">Jean Senellart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yvon:Fran=ccedil=ois">François Yvon</a></p>
<p>【Abstract】:
Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance.</p>
<p>【Keywords】:</p>
<h3 id="329. Adversarial Evaluation of Multimodal Machine Translation.">329. Adversarial Evaluation of Multimodal Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1329/">Paper Link</a>】    【Pages】:2974-2978</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Elliott:Desmond">Desmond Elliott</a></p>
<p>【Abstract】:
The promise of combining language and vision in multimodal machine translation is that systems will produce better translations by leveraging the image data. However, the evidence surrounding whether the images are useful is unconvincing due to inconsistencies between text-similarity metrics and human judgements. We present an adversarial evaluation to directly examine the utility of the image data in this task. Our evaluation tests whether systems perform better when paired with congruent images or incongruent images. This evaluation shows that only one out of three publicly available systems is sensitive to this perturbation of the data. We recommend that multimodal translation systems should be able to pass this sanity check in the future.</p>
<p>【Keywords】:</p>
<h3 id="330. Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion.">330. Loss in Translation: Learning Bilingual Word Mapping with a Retrieval Criterion.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1330/">Paper Link</a>】    【Pages】:2979-2984</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Joulin:Armand">Armand Joulin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bojanowski:Piotr">Piotr Bojanowski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mikolov:Tomas">Tomas Mikolov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/J=eacute=gou:Herv=eacute=">Hervé Jégou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grave:Edouard">Edouard Grave</a></p>
<p>【Abstract】:
Continuous word representations learned separately on distinct languages can be aligned so that their words become comparable in a common space. Existing works typically solve a quadratic problem to learn a orthogonal matrix aligning a bilingual lexicon, and use a retrieval criterion for inference. In this paper, we propose an unified formulation that directly optimizes a retrieval criterion in an end-to-end fashion. Our experiments on standard benchmarks show that our approach outperforms the state of the art on word translation, with the biggest improvements observed for distant language pairs such as English-Chinese.</p>
<p>【Keywords】:</p>
<h3 id="331. Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.">331. Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1331/">Paper Link</a>】    【Pages】:2985-2990</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Junyang">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Muyu">Muyu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Qi">Qi Su</a></p>
<p>【Abstract】:
Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mechanism treats the decoding at each time step equally with the same matrix, which is problematic since the softness of the attention for different types of words (e.g. content words and function words) should differ. Therefore, we propose a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature. Experimental results on the Chinese-English translation and English-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality.</p>
<p>【Keywords】:</p>
<h3 id="332. Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation.">332. Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1332/">Paper Link</a>】    【Pages】:2991-2996</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bogoychev:Nikolay">Nikolay Bogoychev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Heafield:Kenneth">Kenneth Heafield</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aji:Alham_Fikri">Alham Fikri Aji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Junczys=Dowmunt:Marcin">Marcin Junczys-Dowmunt</a></p>
<p>【Abstract】:
In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27% faster than an optimized baseline with negligible penalty in BLEU.</p>
<p>【Keywords】:</p>
<h3 id="333. Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism.">333. Learning to Jointly Translate and Predict Dropped Pronouns with a Shared Reconstruction Mechanism.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1333/">Paper Link</a>】    【Pages】:2997-3002</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Longyue">Longyue Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>【Abstract】:
Pronouns are frequently omitted in pro-drop languages, such as Chinese, generally leading to significant challenges with respect to the production of complete translations. Recently, Wang et al. (2018) proposed a novel reconstruction-based approach to alleviating dropped pronoun (DP) translation problems for neural machine translation models. In this work, we improve the original model from two perspectives. First, we employ a shared reconstructor to better exploit encoder and decoder representations. Second, we jointly learn to translate and predict DPs in an end-to-end manner, to avoid the errors propagated from an external DP prediction model. Experimental results show that our approach significantly improves both translation performance and DP prediction accuracy.</p>
<p>【Keywords】:</p>
<h3 id="334. Getting Gender Right in Neural MT.">334. Getting Gender Right in Neural MT.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1334/">Paper Link</a>】    【Pages】:3003-3008</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vanmassenhove:Eva">Eva Vanmassenhove</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hardmeier:Christian">Christian Hardmeier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Way:Andy">Andy Way</a></p>
<p>【Abstract】:
Speakers of different languages must attend to and encode strikingly different aspects of the world in order to use their language correctly (Sapir, 1921; Slobin, 1996). One such difference is related to the way gender is expressed in a language. Saying “I am happy” in English, does not encode any additional knowledge of the speaker that uttered the sentence. However, many other languages do have grammatical gender systems and so such knowledge would be encoded. In order to correctly translate such a sentence into, say, French, the inherent gender information needs to be retained/recovered. The same sentence would become either “Je suis heureux”, for a male speaker or “Je suis heureuse” for a female one. Apart from morphological agreement, demographic factors (gender, age, etc.) also influence our use of language in terms of word choices or syntactic constructions (Tannen, 1991; Pennebaker et al., 2003). We integrate gender information into NMT systems. Our contribution is two-fold: (1) the compilation of large datasets with speaker information for 20 language pairs, and (2) a simple set of experiments that incorporate gender information into NMT for multiple language pairs. Our experiments show that adding a gender feature to an NMT system significantly improves the translation quality for some language pairs.</p>
<p>【Keywords】:</p>
<h3 id="335. Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation.">335. Towards Two-Dimensional Sequence to Sequence Model in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1335/">Paper Link</a>】    【Pages】:3009-3015</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bahar:Parnia">Parnia Bahar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brix:Christopher">Christopher Brix</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ney:Hermann">Hermann Ney</a></p>
<p>【Abstract】:
This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German&lt;-&gt;English.</p>
<p>【Keywords】:</p>
<h3 id="336. End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification.">336. End-to-End Non-Autoregressive Neural Machine Translation with Connectionist Temporal Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1336/">Paper Link</a>】    【Pages】:3016-3021</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Libovick=yacute=:Jindrich">Jindrich Libovický</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Helcl:Jindrich">Jindrich Helcl</a></p>
<p>【Abstract】:
Autoregressive decoding is the only part of sequence-to-sequence models that prevents them from massive parallelization at inference time. Non-autoregressive models enable the decoder to generate all output symbols independently in parallel. We present a novel non-autoregressive architecture based on connectionist temporal classification and evaluate it on the task of neural machine translation. Unlike other non-autoregressive methods which operate in several steps, our model can be trained end-to-end. We conduct experiments on the WMT English-Romanian and English-German datasets. Our models achieve a significant speedup over the autoregressive models, keeping the translation quality comparable to other non-autoregressive models.</p>
<p>【Keywords】:</p>
<h3 id="337. Prediction Improves Simultaneous Neural Machine Translation.">337. Prediction Improves Simultaneous Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1337/">Paper Link</a>】    【Pages】:3022-3027</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alinejad:Ashkan">Ashkan Alinejad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Siahbani:Maryam">Maryam Siahbani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarkar:Anoop">Anoop Sarkar</a></p>
<p>【Abstract】:
Simultaneous speech translation aims to maintain translation quality while minimizing the delay between reading input and incrementally producing the output. We propose a new general-purpose prediction action which predicts future words in the input to improve quality and minimize delay in simultaneous translation. We train this agent using reinforcement learning with a novel reward function. Our agent with prediction has better translation quality and less delay compared to an agent-based simultaneous translation system without prediction.</p>
<p>【Keywords】:</p>
<h3 id="338. Training Deeper Neural Machine Translation Models with Transparent Attention.">338. Training Deeper Neural Machine Translation Models with Transparent Attention.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1338/">Paper Link</a>】    【Pages】:3028-3033</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bapna:Ankur">Ankur Bapna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Mia_Xu">Mia Xu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Firat:Orhan">Orhan Firat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Yuan">Yuan Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yonghui">Yonghui Wu</a></p>
<p>【Abstract】:
While current state-of-the-art NMT models, such as RNN seq2seq and Transformers, possess a large number of parameters, they are still shallow in comparison to convolutional models used for both text and vision applications. In this work we attempt to train significantly (2-3x) deeper Transformer and Bi-RNN encoders for machine translation. We propose a simple modification to the attention mechanism that eases the optimization of deeper models, and results in consistent gains of 0.7-1.1 BLEU on the benchmark WMT’14 English-German and WMT’15 Czech-English tasks for both architectures.</p>
<p>【Keywords】:</p>
<h3 id="339. Context and Copying in Neural Machine Translation.">339. Context and Copying in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1339/">Paper Link</a>】    【Pages】:3034-3041</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Knowles:Rebecca">Rebecca Knowles</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koehn:Philipp">Philipp Koehn</a></p>
<p>【Abstract】:
Neural machine translation systems with subword vocabularies are capable of translating or copying unknown words. In this work, we show that they learn to copy words based on both the context in which the words appear as well as features of the words themselves. In contexts that are particularly copy-prone, they even copy words that they have already learned they should translate. We examine the influence of context and subword features on this and other types of copying behavior.</p>
<p>【Keywords】:</p>
<h3 id="340. Encoding Gated Translation Memory into Neural Machine Translation.">340. Encoding Gated Translation Memory into Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1340/">Paper Link</a>】    【Pages】:3042-3047</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Qian">Qian Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a></p>
<p>【Abstract】:
Translation memories (TM) facilitate human translators to reuse existing repetitive translation fragments. In this paper, we propose a novel method to combine the strengths of both TM and neural machine translation (NMT) for high-quality translation. We treat the target translation of a TM match as an additional reference input and encode it into NMT with an extra encoder. A gating mechanism is further used to balance the impact of the TM match on the NMT decoder. Experiment results on the UN corpus demonstrate that when fuzzy matches are higher than 50%, the quality of NMT translation can be significantly improved by over 10 BLEU points.</p>
<p>【Keywords】:</p>
<h3 id="341. Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach.">341. Automatic Post-Editing of Machine Translation: A Neural Programmer-Interpreter Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1341/">Paper Link</a>】    【Pages】:3048-3053</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vu:Thuy=Trang">Thuy-Trang Vu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a></p>
<p>【Abstract】:
Automated Post-Editing (PE) is the task of automatically correct common and repetitive errors found in machine translation (MT) output. In this paper, we present a neural programmer-interpreter approach to this task, resembling the way that human perform post-editing using discrete edit operations, wich we refer to as programs. Our model outperforms previous neural models for inducing PE programs on the WMT17 APE task for German-English up to +1 BLEU score and -0.7 TER scores.</p>
<p>【Keywords】:</p>
<h3 id="342. Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation.">342. Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1342/">Paper Link</a>】    【Pages】:3054-3059</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yilin">Yilin Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Mingbo">Mingbo Ma</a></p>
<p>【Abstract】:
Beam search is widely used in neural machine translation, and usually improves translation quality compared to greedy search. It has been widely observed that, however, beam sizes larger than 5 hurt translation quality. We explain why this happens, and propose several methods to address this problem. Furthermore, we discuss the optimal stopping criteria for these methods. Results show that our hyperparameter-free methods outperform the widely-used hyperparameter-free heuristic of length normalization by +2.0 BLEU, and achieve the best results among all methods on Chinese-to-English translation.</p>
<p>【Keywords】:</p>
<h3 id="343. Multi-View Learning: Multilingual and Multi-Representation Entity Typing.">343. Multi-View Learning: Multilingual and Multi-Representation Entity Typing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1343/">Paper Link</a>】    【Pages】:3060-3066</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yaghoobzadeh:Yadollah">Yadollah Yaghoobzadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
Accurate and complete knowledge bases (KBs) are paramount in NLP. We employ mul-itiview learning for increasing the accuracy and coverage of entity type information in KBs. We rely on two metaviews: language and representation. For language, we consider high-resource and low-resource languages from Wikipedia. For representation, we consider representations based on the context distribution of the entity (i.e., on its embedding), on the entity’s name (i.e., on its surface form) and on its description in Wikipedia. The two metaviews language and representation can be freely combined: each pair of language and representation (e.g., German embedding, English description, Spanish name) is a distinct view. Our experiments on entity typing with fine-grained classes demonstrate the effectiveness of multiview learning. We release MVET, a large multiview — and, in particular, multilingual — entity typing dataset we created. Mono- and multilingual fine-grained entity typing systems can be evaluated on this dataset.</p>
<p>【Keywords】:</p>
<h3 id="344. Word Embeddings for Code-Mixed Language Processing.">344. Word Embeddings for Code-Mixed Language Processing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1344/">Paper Link</a>】    【Pages】:3067-3072</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pratapa:Adithya">Adithya Pratapa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choudhury:Monojit">Monojit Choudhury</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sitaram:Sunayana">Sunayana Sitaram</a></p>
<p>【Abstract】:
We compare three existing bilingual word embedding approaches, and a novel approach of training skip-grams on synthetic code-mixed text generated through linguistic models of code-mixing, on two tasks - sentiment analysis and POS tagging for code-mixed text. Our results show that while CVM and CCA based embeddings perform as well as the proposed embedding technique on semantic and syntactic tasks respectively, the proposed approach provides the best performance for both tasks overall. Thus, this study demonstrates that existing bilingual embedding techniques are not ideal for code-mixed text processing and there is a need for learning multilingual word embedding from the code-mixed text.</p>
<p>【Keywords】:</p>
<h3 id="345. On the Strength of Character Language Models for Multilingual Named Entity Recognition.">345. On the Strength of Character Language Models for Multilingual Named Entity Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1345/">Paper Link</a>】    【Pages】:3073-3077</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Xiaodong">Xiaodong Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mayhew_0001:Stephen">Stephen Mayhew</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sammons:Mark">Mark Sammons</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Character-level patterns have been widely used as features in English Named Entity Recognition (NER) systems. However, to date there has been no direct investigation of the inherent differences between name and nonname tokens in text, nor whether this property holds across multiple languages. This paper analyzes the capabilities of corpus-agnostic Character-level Language Models (CLMs) in the binary task of distinguishing name tokens from non-name tokens. We demonstrate that CLMs provide a simple and powerful model for capturing these differences, identifying named entity tokens in a diverse set of languages at close to the performance of full NER systems. Moreover, by adding very simple CLM-based features we can significantly improve the performance of an off-the-shelf NER system for multiple languages.</p>
<p>【Keywords】:</p>
<h3 id="346. Code-switched Language Models Using Dual RNNs and Same-Source Pretraining.">346. Code-switched Language Models Using Dual RNNs and Same-Source Pretraining.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1346/">Paper Link</a>】    【Pages】:3078-3083</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Garg:Saurabh">Saurabh Garg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parekh:Tanmay">Tanmay Parekh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jyothi:Preethi">Preethi Jyothi</a></p>
<p>【Abstract】:
This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in perplexity.</p>
<p>【Keywords】:</p>
<h3 id="347. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification.">347. Part-of-Speech Tagging for Code-Switched, Transliterated Texts without Explicit Language Identification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1347/">Paper Link</a>】    【Pages】:3084-3089</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Ball:Kelsey">Kelsey Ball</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Garrette:Dan">Dan Garrette</a></p>
<p>【Abstract】:
Code-switching, the use of more than one language within a single utterance, is ubiquitous in much of the world, but remains a challenge for NLP largely due to the lack of representative data for training models. In this paper, we present a novel model architecture that is trained exclusively on monolingual resources, but can be applied to unseen code-switched text at inference time. The model accomplishes this by jointly maintaining separate word representations for each of the possible languages, or scripts in the case of transliteration, allowing each to contribute to inferences without forcing the model to commit to a language. Experiments on Hindi-English part-of-speech tagging demonstrate that our approach outperforms standard models when training on monolingual text without transliteration, and testing on code-switched text with alternate scripts.</p>
<p>【Keywords】:</p>
<h3 id="348. Zero-shot User Intent Detection via Capsule Neural Networks.">348. Zero-shot User Intent Detection via Capsule Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1348/">Paper Link</a>】    【Pages】:3090-3099</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xia:Congying">Congying Xia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Chenwei">Chenwei Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xiaohui">Xiaohui Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Philip_S=">Philip S. Yu</a></p>
<p>【Abstract】:
User intent detection plays a critical role in question-answering and dialog systems. Most previous works treat intent detection as a classification problem where utterances are labeled with predefined intents. However, it is labor-intensive and time-consuming to label users’ utterances as intents are diversely expressed and novel intents will continually be involved. Instead, we study the zero-shot intent detection problem, which aims to detect emerging user intents where no labeled utterances are currently available. We propose two capsule-based architectures: IntentCapsNet that extracts semantic features from utterances and aggregates them to discriminate existing intents, and IntentCapsNet-ZSL which gives IntentCapsNet the zero-shot learning ability to discriminate emerging intents via knowledge transfer from existing intents. Experiments on two real-world datasets show that our model not only can better discriminate diversely expressed existing intents, but is also able to discriminate emerging intents when no labeled utterances are available.</p>
<p>【Keywords】:</p>
<h3 id="349. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts.">349. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1349/">Paper Link</a>】    【Pages】:3100-3109</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Di">Di Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Szolovits:Peter">Peter Szolovits</a></p>
<p>【Abstract】:
Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.</p>
<p>【Keywords】:</p>
<h3 id="350. Investigating Capsule Networks with Dynamic Routing for Text Classification.">350. Investigating Capsule Networks with Dynamic Routing for Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1350/">Paper Link</a>】    【Pages】:3110-3119</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Min">Min Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wei">Wei Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Jianbo">Jianbo Ye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Zeyang">Zeyang Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Soufei">Soufei Zhang</a></p>
<p>【Abstract】:
In this study, we explore capsule networks with dynamic routing for text classification. We propose three strategies to stabilize the dynamic routing process to alleviate the disturbance of some noise capsules which may contain “background” information or have not been successfully trained. A series of experiments are conducted with capsule networks on six text classification benchmarks. Capsule networks achieve state of the art on 4 out of 6 datasets, which shows the effectiveness of capsule networks for text classification. We additionally show that capsule networks exhibit significant improvement when transfer single-label to multi-label text classification over strong baseline methods. To the best of our knowledge, this is the first work that capsule networks have been empirically investigated for text modeling.</p>
<p>【Keywords】:</p>
<h3 id="351. Topic Memory Networks for Short Text Classification.">351. Topic Memory Networks for Short Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1351/">Paper Link</a>】    【Pages】:3120-3131</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Jichuan">Jichuan Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jing">Jing Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yan">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Cuiyun">Cuiyun Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Michael_R=">Michael R. Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/King:Irwin">Irwin King</a></p>
<p>【Abstract】:
Many classification models work poorly on short texts due to data sparsity. To address this issue, we propose topic memory networks for short text classification with a novel topic memory mechanism to encode latent topic representations indicative of class labels. Different from most prior work that focuses on extending features with external knowledge or pre-trained topics, our model jointly explores topic inference and text classification with memory networks in an end-to-end manner. Experimental results on four benchmark datasets show that our model outperforms state-of-the-art models on short text classification, meanwhile generates coherent topics.</p>
<p>【Keywords】:</p>
<h3 id="352. Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces.">352. Few-Shot and Zero-Shot Multi-Label Learning for Structured Label Spaces.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1352/">Paper Link</a>】    【Pages】:3132-3142</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rios:Anthony">Anthony Rios</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kavuluru:Ramakanth">Ramakanth Kavuluru</a></p>
<p>【Abstract】:
Large multi-label datasets contain labels that occur thousands of times (frequent group), those that occur only a few times (few-shot group), and labels that never appear in the training dataset (zero-shot group). Multi-label few- and zero-shot label prediction is mostly unexplored on datasets with large label spaces, especially for text classification. In this paper, we perform a fine-grained evaluation to understand how state-of-the-art methods perform on infrequent labels. Furthermore, we develop few- and zero-shot methods for multi-label text classification when there is a known structure over the label space, and evaluate them on two publicly available medical text datasets: MIMIC II and MIMIC III. For few-shot labels we achieve improvements of 6.2% and 4.8% in R@10 for MIMIC II and MIMIC III, respectively, over prior efforts; the corresponding R@10 improvements for zero-shot labels are 17.3% and 19%.</p>
<p>【Keywords】:</p>
<h3 id="353. Automatic Poetry Generation with Mutual Reinforcement Learning.">353. Automatic Poetry Generation with Mutual Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1353/">Paper Link</a>】    【Pages】:3143-3153</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yi:Xiaoyuan">Xiaoyuan Yi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Ruoyu">Ruoyu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Wenhao">Wenhao Li</a></p>
<p>【Abstract】:
Poetry is one of the most beautiful forms of human language art. As a crucial step towards computer creativity, automatic poetry generation has drawn researchers’ attention for decades. In recent years, some neural models have made remarkable progress in this task. However, they are all based on maximum likelihood estimation, which only learns common patterns of the corpus and results in loss-evaluation mismatch. Human experts evaluate poetry in terms of some specific criteria, instead of word-level likelihood. To handle this problem, we directly model the criteria and use them as explicit rewards to guide gradient update by reinforcement learning, so as to motivate the model to pursue higher scores. Besides, inspired by writing theories, we propose a novel mutual reinforcement learning schema. We simultaneously train two learners (generators) which learn not only from the teacher (rewarder) but also from each other to further improve performance. We experiment on Chinese poetry. Based on a strong basic model, our method achieves better results and outperforms the current state-of-the-art method.</p>
<p>【Keywords】:</p>
<h3 id="354. Variational Autoregressive Decoder for Neural Response Generation.">354. Variational Autoregressive Decoder for Neural Response Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1354/">Paper Link</a>】    【Pages】:3154-3163</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Du:Jiachen">Jiachen Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Yulan">Yulan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Ruifeng">Ruifeng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Xuan">Xuan Wang</a></p>
<p>【Abstract】:
Combining the virtues of probability graphic models and neural networks, Conditional Variational Auto-encoder (CVAE) has shown promising performance in applications such as response generation. However, existing CVAE-based models often generate responses from a single latent variable which may not be sufficient to model high variability in responses. To solve this problem, we propose a novel model that sequentially introduces a series of latent variables to condition the generation of each word in the response sequence. In addition, the approximate posteriors of these latent variables are augmented with a backward Recurrent Neural Network (RNN), which allows the latent variables to capture long-term dependencies of future tokens in generation. To facilitate training, we supplement our model with an auxiliary objective that predicts the subsequent bag of words. Empirical experiments conducted on Opensubtitle and Reddit datasets show that the proposed model leads to significant improvement on both relevance and diversity over state-of-the-art baselines.</p>
<p>【Keywords】:</p>
<h3 id="355. Integrating Transformer and Paraphrase Rules for Sentence Simplification.">355. Integrating Transformer and Paraphrase Rules for Sentence Simplification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1355/">Paper Link</a>】    【Pages】:3164-3173</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Sanqiang">Sanqiang Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Rui">Rui Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Daqing">Daqing He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saptono:Andi">Andi Saptono</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parmanto:Bambang">Bambang Parmanto</a></p>
<p>【Abstract】:
Sentence simplification aims to reduce the complexity of a sentence while retaining its original meaning. Current models for sentence simplification adopted ideas from machine translation studies and implicitly learned simplification mapping rules from normal-simple sentence pairs. In this paper, we explore a novel model based on a multi-layer and multi-head attention architecture and we propose two innovative approaches to integrate the Simple PPDB (A Paraphrase Database for Simplification), an external paraphrase knowledge base for simplification that covers a wide range of real-world simplification rules. The experiments show that the integration provides two major benefits: (1) the integrated model outperforms multiple state-of-the-art baseline models for sentence simplification in the literature (2) through analysis of the rule utilization, the model seeks to select more accurate simplification rules. The code and models used in the paper are available at <a href="https://github.com/Sanqiang/text_simplification">https://github.com/Sanqiang/text_simplification</a>.</p>
<p>【Keywords】:</p>
<h3 id="356. Learning Neural Templates for Text Generation.">356. Learning Neural Templates for Text Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1356/">Paper Link</a>】    【Pages】:3174-3187</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wiseman:Sam">Sam Wiseman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shieber:Stuart_M=">Stuart M. Shieber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a></p>
<p>【Abstract】:
While neural, encoder-decoder models have had significant empirical success in text generation, there remain several unaddressed problems with this style of generation. Encoder-decoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content. This work proposes a neural generation system using a hidden semi-markov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate. We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable. Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoder-decoder text generation models.</p>
<p>【Keywords】:</p>
<h3 id="357. Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation.">357. Multi-Reference Training with Pseudo-References for Neural Translation and Text Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1357/">Paper Link</a>】    【Pages】:3188-3197</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Renjie">Renjie Zheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Mingbo">Mingbo Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a></p>
<p>【Abstract】:
Neural text generation, including neural machine translation, image captioning, and summarization, has been quite successful recently. However, during training time, typically only one reference is considered for each example, even though there are often multiple references available, e.g., 4 references in NIST MT evaluations, and 5 references in image captioning data. We first investigate several different ways of utilizing multiple human references during training. But more importantly, we then propose an algorithm to generate exponentially many pseudo-references by first compressing existing human references into lattices and then traversing them to generate new pseudo-references. These approaches lead to substantial improvements over strong baselines in both machine translation (+1.5 BLEU) and image captioning (+3.1 BLEU / +11.7 CIDEr).</p>
<p>【Keywords】:</p>
<h3 id="358. Knowledge Graph Embedding with Hierarchical Relation Structure.">358. Knowledge Graph Embedding with Hierarchical Relation Structure.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1358/">Paper Link</a>】    【Pages】:3198-3207</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhao">Zhao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhuang:Fuzhen">Fuzhen Zhuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qu:Meng">Meng Qu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Fen">Fen Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Qing">Qing He</a></p>
<p>【Abstract】:
The rapid development of knowledge graphs (KGs), such as Freebase and WordNet, has changed the paradigm for AI-related applications. However, even though these KGs are impressively large, most of them are suffering from incompleteness, which leads to performance degradation of AI applications. Most existing researches are focusing on knowledge graph embedding (KGE) models. Nevertheless, those models simply embed entities and relations into latent vectors without leveraging the rich information from the relation structure. Indeed, relations in KGs conform to a three-layer hierarchical relation structure (HRS), i.e., semantically similar relations can make up relation clusters and some relations can be further split into several fine-grained sub-relations. Relation clusters, relations and sub-relations can fit in the top, the middle and the bottom layer of three-layer HRS respectively. To this end, in this paper, we extend existing KGE models TransE, TransH and DistMult, to learn knowledge representations by leveraging the information from the HRS. Particularly, our approach is capable to extend other KGE models. Finally, the experiment results clearly validate the effectiveness of the proposed approach against baselines.</p>
<p>【Keywords】:</p>
<h3 id="359. Embedding Multimodal Relational Data for Knowledge Base Completion.">359. Embedding Multimodal Relational Data for Knowledge Base Completion.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1359/">Paper Link</a>】    【Pages】:3208-3218</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pezeshkpour:Pouya">Pouya Pezeshkpour</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Liyan">Liyan Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh_0001:Sameer">Sameer Singh</a></p>
<p>【Abstract】:
Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in knowledge bases, such as text, images, and numerical values. In this paper, we propose multimodal knowledge base embeddings (MKBE) that use different neural encoders for this variety of observed data, and combine them with existing relational models to learn embeddings of the entities and multimodal data. Further, using these learned embedings and different neural decoders, we introduce a novel multimodal imputation model to generate missing multimodal values, like text and images, from information in the knowledge base. We enrich existing relational datasets to create two novel benchmarks that contain additional information such as textual descriptions and images of the original entities. We demonstrate that our models utilize this additional information effectively to provide more accurate link prediction, achieving state-of-the-art results with a considerable gap of 5-7% over existing methods. Further, we evaluate the quality of our generated multimodal values via a user study.</p>
<p>【Keywords】:</p>
<h3 id="360. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction.">360. Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1360/">Paper Link</a>】    【Pages】:3219-3232</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luan:Yi">Yi Luan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Luheng">Luheng He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ostendorf:Mari">Mari Ostendorf</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajishirzi:Hannaneh">Hannaneh Hajishirzi</a></p>
<p>【Abstract】:
We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.</p>
<p>【Keywords】:</p>
<h3 id="361. Playing 20 Question Game with Policy-Based Reinforcement Learning.">361. Playing 20 Question Game with Policy-Based Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1361/">Paper Link</a>】    【Pages】:3233-3242</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Huang">Huang Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Xianchao">Xianchao Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Bingfeng">Bingfeng Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tao:Chongyang">Chongyang Tao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Can">Can Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0014:Wei">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Zhan">Zhan Chen</a></p>
<p>【Abstract】:
The 20 Questions (Q20) game is a well known game which encourages deductive reasoning and creativity. In the game, the answerer first thinks of an object such as a famous person or a kind of animal. Then the questioner tries to guess the object by asking 20 questions. In a Q20 game system, the user is considered as the answerer while the system itself acts as the questioner which requires a good strategy of question selection to figure out the correct object and win the game. However, the optimal policy of question selection is hard to be derived due to the complexity and volatility of the game environment. In this paper, we propose a novel policy-based Reinforcement Learning (RL) method, which enables the questioner agent to learn the optimal policy of question selection through continuous interactions with users. To facilitate training, we also propose to use a reward network to estimate the more informative reward. Compared to previous methods, our RL method is robust to noisy answers and does not rely on the Knowledge Base of objects. Experimental results show that our RL method clearly outperforms an entropy-based engineering system and has competitive performance in a noisy-free simulation environment.</p>
<p>【Keywords】:</p>
<h3 id="362. Multi-Hop Knowledge Graph Reasoning with Reward Shaping.">362. Multi-Hop Knowledge Graph Reasoning with Reward Shaping.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1362/">Paper Link</a>】    【Pages】:3243-3253</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Xi_Victoria">Xi Victoria Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Socher:Richard">Richard Socher</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Caiming">Caiming Xiong</a></p>
<p>【Abstract】:
Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.</p>
<p>【Keywords】:</p>
<h3 id="363. Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting.">363. Neural Transductive Learning and Beyond: Morphological Generation in the Minimal-Resource Setting.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1363/">Paper Link</a>】    【Pages】:3254-3264</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kann:Katharina">Katharina Kann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
Neural state-of-the-art sequence-to-sequence (seq2seq) models often do not perform well for small training sets. We address paradigm completion, the morphological task of, given a partial paradigm, generating all missing forms. We propose two new methods for the minimal-resource setting: (i) Paradigm transduction: Since we assume only few paradigms available for training, neural seq2seq models are able to capture relationships between paradigm cells, but are tied to the idiosyncracies of the training set. Paradigm transduction mitigates this problem by exploiting the input subset of inflected forms at test time. (ii) Source selection with high precision (SHIP): Multi-source models which learn to automatically select one or multiple sources to predict a target inflection do not perform well in the minimal-resource setting. SHIP is an alternative to identify a reliable source if training data is limited. On a 52-language benchmark dataset, we outperform the previous state of the art by up to 9.71% absolute accuracy.</p>
<p>【Keywords】:</p>
<h3 id="364. Implicational Universals in Stochastic Constraint-Based Phonology.">364. Implicational Universals in Stochastic Constraint-Based Phonology.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1364/">Paper Link</a>】    【Pages】:3265-3274</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Magri:Giorgio">Giorgio Magri</a></p>
<p>【Abstract】:
This paper focuses on the most basic implicational universals in phonological theory, called T-orders after Anttila and Andrus (2006). It shows that the T-orders predicted by stochastic (and partial order) Optimality Theory coincide with those predicted by categorical OT. Analogously, the T-orders predicted by stochastic Harmonic Grammar coincide with those predicted by categorical HG. In other words, these stochastic constraint-based frameworks do not tamper with the typological structure induced by the original categorical frameworks.</p>
<p>【Keywords】:</p>
<h3 id="365. Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?">365. Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1365/">Paper Link</a>】    【Pages】:3275-3284</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Godin:Fr=eacute=deric">Fréderic Godin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Demuynck:Kris">Kris Demuynck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dambre:Joni">Joni Dambre</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neve:Wesley_De">Wesley De Neve</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Demeester:Thomas">Thomas Demeester</a></p>
<p>【Abstract】:
Character-level features are currently used in different neural network-based natural language processing algorithms. However, little is known about the character-level patterns those models learn. Moreover, models are often compared only quantitatively while a qualitative analysis is missing. In this paper, we investigate which character-level patterns neural networks learn and if those patterns coincide with manually-defined word segmentations and annotations. To that end, we extend the contextual decomposition technique (Murdoch et al. 2018) to convolutional neural networks which allows us to compare convolutional neural networks and bidirectional long short-term memory networks. We evaluate and compare these models for the task of morphological tagging on three morphologically different languages and show that these models implicitly discover understandable linguistic rules.</p>
<p>【Keywords】:</p>
<h3 id="366. Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations.">366. Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1366/">Paper Link</a>】    【Pages】:3285-3295</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chaudhary:Aditi">Aditi Chaudhary</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Chunting">Chunting Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levin:Lori_S=">Lori S. Levin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mortensen:David_R=">David R. Mortensen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carbonell:Jaime_G=">Jaime G. Carbonell</a></p>
<p>【Abstract】:
Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.</p>
<p>【Keywords】:</p>
<h3 id="367. A Computational Exploration of Exaggeration.">367. A Computational Exploration of Exaggeration.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1367/">Paper Link</a>】    【Pages】:3296-3304</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Troiano:Enrica">Enrica Troiano</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strapparava:Carlo">Carlo Strapparava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/=/=Ouml=zbal:G=ouml=zde">Gözde Özbal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tekiroglu:Serra_Sinem">Serra Sinem Tekiroglu</a></p>
<p>【Abstract】:
Several NLP studies address the problem of figurative language, but among non-literal phenomena, they have neglected exaggeration. This paper presents a first computational approach to this figure of speech. We explore the possibility to automatically detect exaggerated sentences. First, we introduce HYPO, a corpus containing overstatements (or hyperboles) collected on the web and validated via crowdsourcing. Then, we evaluate a number of models trained on HYPO, and bring evidence that the task of hyperbole identification can be successfully performed based on a small set of semantic features.</p>
<p>【Keywords】:</p>
<h3 id="368. Building Context-aware Clause Representations for Situation Entity Type Classification.">368. Building Context-aware Clause Representations for Situation Entity Type Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1368/">Paper Link</a>】    【Pages】:3305-3315</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Zeyu">Zeyu Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Ruihong">Ruihong Huang</a></p>
<p>【Abstract】:
Capabilities to categorize a clause based on the type of situation entity (e.g., events, states and generic statements) the clause introduces to the discourse can benefit many NLP applications. Observing that the situation entity type of a clause depends on discourse functions the clause plays in a paragraph and the interpretation of discourse functions depends heavily on paragraph-wide contexts, we propose to build context-aware clause representations for predicting situation entity types of clauses. Specifically, we propose a hierarchical recurrent neural network model to read a whole paragraph at a time and jointly learn representations for all the clauses in the paragraph by extensively modeling context influences and inter-dependencies of clauses. Experimental results show that our model achieves the state-of-the-art performance for clause-level situation entity classification on the genre-rich MASC+Wiki corpus, which approaches human-level performance.</p>
<p>【Keywords】:</p>
<h3 id="369. Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain.">369. Hierarchical Dirichlet Gaussian Marked Hawkes Process for Narrative Reconstruction in Continuous Time Domain.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1369/">Paper Link</a>】    【Pages】:3316-3325</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Seonwoo:Yeon">Yeon Seonwoo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oh:Alice">Alice Oh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Park:Sungjoon">Sungjoon Park</a></p>
<p>【Abstract】:
In news and discussions, many articles and posts are provided without their related previous articles or posts. Hence, it is difficult to understand the context from which the articles and posts have occurred. In this paper, we propose the Hierarchical Dirichlet Gaussian Marked Hawkes process (HD-GMHP) for reconstructing the narratives and thread structures of news articles and discussion posts. HD-GMHP unifies three modeling strategies in previous research: temporal characteristics, triggering event relations, and meta information of text in news articles and discussion threads. To show the effectiveness of the model, we perform experiments in narrative reconstruction and thread reconstruction with real world datasets: articles from the New York Times and a corpus of Wikipedia conversations. The experimental results show that HD-GMHP outperforms the baselines of LDA, HDP, and HDHP for both tasks.</p>
<p>【Keywords】:</p>
<h3 id="370. Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models.">370. Investigating the Role of Argumentation in the Rhetorical Analysis of Scientific Publications with Neural Multi-Task Learning Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1370/">Paper Link</a>】    【Pages】:3326-3338</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lauscher:Anne">Anne Lauscher</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Glavas:Goran">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Ponzetto:Simone_Paolo">Simone Paolo Ponzetto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eckert_0001:Kai">Kai Eckert</a></p>
<p>【Abstract】:
Exponential growth in the number of scientific publications yields the need for effective automatic analysis of rhetorical aspects of scientific writing. Acknowledging the argumentative nature of scientific text, in this work we investigate the link between the argumentative structure of scientific publications and rhetorical aspects such as discourse categories or citation contexts. To this end, we (1) augment a corpus of scientific publications annotated with four layers of rhetoric annotations with argumentation annotations and (2) investigate neural multi-task learning architectures combining argument extraction with a set of rhetorical classification tasks. By coupling rhetorical classifiers with the extraction of argumentative components in a joint multi-task learning setting, we obtain significant performance gains for different rhetorical analysis tasks.</p>
<p>【Keywords】:</p>
<h3 id="371. Neural Ranking Models for Temporal Dependency Structure Parsing.">371. Neural Ranking Models for Temporal Dependency Structure Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1371/">Paper Link</a>】    【Pages】:3339-3349</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yuchen">Yuchen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xue:Nianwen">Nianwen Xue</a></p>
<p>【Abstract】:
We design and build the first neural temporal dependency parser. It utilizes a neural ranking model with minimal feature engineering, and parses time expressions and events in a text into a temporal dependency tree structure. We evaluate our parser on two domains: news reports and narrative stories. In a parsing-only evaluation setup where gold time expressions and events are provided, our parser reaches 0.81 and 0.70 f-score on unlabeled and labeled parsing respectively, a result that is very competitive against alternative approaches. In an end-to-end evaluation setup where time expressions and events are automatically recognized, our parser beats two strong baselines on both data domains. Our experimental results and discussions shed light on the nature of temporal dependency structures in different domains and provide insights that we believe will be valuable to future research in this area.</p>
<p>【Keywords】:</p>
<h3 id="372. Causal Explanation Analysis on Social Media.">372. Causal Explanation Analysis on Social Media.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1372/">Paper Link</a>】    【Pages】:3350-3359</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Son:Youngseo">Youngseo Son</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bayas:Nipun">Nipun Bayas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a></p>
<p>【Abstract】:
Understanding causal explanations - reasons given for happenings in one’s life - has been found to be an important psychological factor linked to physical and mental health. Causal explanations are often studied through manual identification of phrases over limited samples of personal writing. Automatic identification of causal explanations in social media, while challenging in relying on contextual and sequential cues, offers a larger-scale alternative to expensive manual ratings and opens the door for new applications (e.g. studying prevailing beliefs about causes, such as climate change). Here, we explore automating causal explanation analysis, building on discourse parsing, and presenting two novel subtasks: causality detection (determining whether a causal explanation exists at all) and causal explanation identification (identifying the specific phrase that is the explanation). We achieve strong accuracies for both tasks but find different approaches best: an SVM for causality prediction (F1 = 0.791) and a hierarchy of Bidirectional LSTMs for causal explanation identification (F1 = 0.853). Finally, we explore applications of our complete pipeline (F1 = 0.868), showing demographic differences in mentions of causal explanation and that the association between a word and sentiment can change when it is used within a causal explanation.</p>
<p>【Keywords】:</p>
<h3 id="373. LRMM: Learning to Recommend with Missing Modalities.">373. LRMM: Learning to Recommend with Missing Modalities.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1373/">Paper Link</a>】    【Pages】:3360-3370</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Cheng">Cheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niepert:Mathias">Mathias Niepert</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Hui">Hui Li</a></p>
<p>【Abstract】:
Multimodal learning has shown promising performance in content-based recommendation due to the auxiliary user and item information of multiple modalities such as text and images. However, the problem of incomplete and missing modality is rarely explored and most existing methods fail in learning a recommendation model with missing or corrupted modalities. In this paper, we propose LRMM, a novel framework that mitigates not only the problem of missing modalities but also more generally the cold-start problem of recommender systems. We propose modality dropout (m-drop) and a multimodal sequential autoencoder (m-auto) to learn multimodal representations for complementing and imputing missing modalities. Extensive experiments on real-world Amazon data show that LRMM achieves state-of-the-art performance on rating prediction tasks. More importantly, LRMM is more robust to previous methods in alleviating data-sparsity and the cold-start problem.</p>
<p>【Keywords】:</p>
<h3 id="374. Content Explorer: Recommending Novel Entities for a Document Writer.">374. Content Explorer: Recommending Novel Entities for a Document Writer.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1374/">Paper Link</a>】    【Pages】:3371-3380</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lukasik:Michal">Michal Lukasik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zens:Richard">Richard Zens</a></p>
<p>【Abstract】:
Background research is an essential part of document writing. Search engines are great for retrieving information once we know what to look for. However, the bigger challenge is often identifying topics for further research. Automated tools could help significantly in this discovery process and increase the productivity of the writer. In this paper, we formulate the problem of recommending topics to a writer. We consider this as a supervised learning problem and run a user study to validate this approach. We propose an evaluation metric and perform an empirical comparison of state-of-the-art models for extreme multi-label classification on a large data set. We demonstrate how a simple modification of the cross-entropy loss function leads to improved results of the deep learning models.</p>
<p>【Keywords】:</p>
<h3 id="375. A Genre-Aware Attention Model to Improve the Likability Prediction of Books.">375. A Genre-Aware Attention Model to Improve the Likability Prediction of Books.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1375/">Paper Link</a>】    【Pages】:3381-3391</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Maharjan:Suraj">Suraj Maharjan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Montes=y=G=oacute=mez:Manuel">Manuel Montes-y-Gómez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gonz=aacute=lez:Fabio_A=">Fabio A. González</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Solorio:Thamar">Thamar Solorio</a></p>
<p>【Abstract】:
Likability prediction of books has many uses. Readers, writers, as well as the publishing industry, can all benefit from automatic book likability prediction systems. In order to make reliable decisions, these systems need to assimilate information from different aspects of a book in a sensible way. We propose a novel multimodal neural architecture that incorporates genre supervision to assign weights to individual feature types. Our proposed method is capable of dynamically tailoring weights given to feature types based on the characteristics of each book. Our architecture achieves competitive results and even outperforms state-of-the-art for this task.</p>
<p>【Keywords】:</p>
<h3 id="376. Thread Popularity Prediction and Tracking with a Permutation-invariant Model.">376. Thread Popularity Prediction and Tracking with a Permutation-invariant Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1376/">Paper Link</a>】    【Pages】:3392-3401</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chan:Hou_Pong">Hou Pong Chan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/King:Irwin">Irwin King</a></p>
<p>【Abstract】:
The task of thread popularity prediction and tracking aims to recommend a few popular comments to subscribed users when a batch of new comments arrive in a discussion thread. This task has been formulated as a reinforcement learning problem, in which the reward of the agent is the sum of positive responses received by the recommended comments. In this work, we propose a novel approach to tackle this problem. First, we propose a deep neural network architecture to model the expected cumulative reward (Q-value) of a recommendation (action). Unlike the state-of-the-art approach, which treats an action as a sequence, our model uses an attention mechanism to integrate information from a set of comments. Thus, the prediction of Q-value is invariant to the permutation of the comments, which leads to a more consistent agent behavior. Second, we employ a greedy procedure to approximate the action that maximizes the predicted Q-value from a combinatorial action space. Different from the state-of-the-art approach, this procedure does not require an additional pre-trained model to generate candidate actions. Experiments on five real-world datasets show that our approach outperforms the state-of-the-art.</p>
<p>【Keywords】:</p>
<h3 id="377. IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis.">377. IARM: Inter-Aspect Relation Modeling with Memory Networks in Aspect-Based Sentiment Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1377/">Paper Link</a>】    【Pages】:3402-3411</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Majumder:Navonil">Navonil Majumder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gelbukh:Alexander_F=">Alexander F. Gelbukh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Akhtar:Md=_Shad">Md. Shad Akhtar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cambria:Erik">Erik Cambria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ekbal:Asif">Asif Ekbal</a></p>
<p>【Abstract】:
Sentiment analysis has immense implications in e-commerce through user feedback mining. Aspect-based sentiment analysis takes this one step further by enabling businesses to extract aspect specific sentimental information. In this paper, we present a novel approach of incorporating the neighboring aspects related information into the sentiment classification of the target aspect using memory networks. We show that our method outperforms the state of the art by 1.6% on average in two distinct domains: restaurant and laptop.</p>
<p>【Keywords】:</p>
<h3 id="378. Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations.">378. Limbic: Author-Based Sentiment Aspect Modeling Regularized with Word Embeddings and Discourse Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1378/">Paper Link</a>】    【Pages】:3412-3422</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhe">Zhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Munindar_P=">Munindar P. Singh</a></p>
<p>【Abstract】:
We propose Limbic, an unsupervised probabilistic model that addresses the problem of discovering aspects and sentiments and associating them with authors of opinionated texts. Limbic combines three ideas, incorporating authors, discourse relations, and word embeddings. For discourse relations, Limbic adopts a generative process regularized by a Markov Random Field. To promote words with high semantic similarity into the same topic, Limbic captures semantic regularities from word embeddings via a generalized Pólya Urn process. We demonstrate that Limbic (1) discovers aspects associated with sentiments with high lexical diversity; (2) outperforms state-of-the-art models by a substantial margin in topic cohesion and sentiment classification.</p>
<p>【Keywords】:</p>
<h3 id="379. An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking.">379. An Interpretable Neural Network with Topical Information for Relevant Emotion Ranking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1379/">Paper Link</a>】    【Pages】:3423-3432</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yang">Yang Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Deyu">Deyu Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Yulan">Yulan He</a></p>
<p>【Abstract】:
Text might express or evoke multiple emotions with varying intensities. As such, it is crucial to predict and rank multiple relevant emotions by their intensities. Moreover, as emotions might be evoked by hidden topics, it is important to unveil and incorporate such topical information to understand how the emotions are evoked. We proposed a novel interpretable neural network approach for relevant emotion ranking. Specifically, motivated by transfer learning, the neural network is initialized to make the hidden layer approximate the behavior of topic models. Moreover, a novel error function is defined to optimize the whole neural network for relevant emotion ranking. Experimental results on three real-world corpora show that the proposed approach performs remarkably better than the state-of-the-art emotion detection approaches and multi-label learning methods. Moreover, the extracted emotion-associated topic words indeed represent emotion-evoking events and are in line with our common-sense knowledge.</p>
<p>【Keywords】:</p>
<h3 id="380. Multi-grained Attention Network for Aspect-Level Sentiment Classification.">380. Multi-grained Attention Network for Aspect-Level Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1380/">Paper Link</a>】    【Pages】:3433-3442</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Feifan">Feifan Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yansong">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a></p>
<p>【Abstract】:
We propose a novel multi-grained attention network (MGAN) model for aspect level sentiment classification. Existing approaches mostly adopt coarse-grained attention mechanism, which may bring information loss if the aspect has multiple words or larger context. We propose a fine-grained attention mechanism, which can capture the word-level interaction between aspect and context. And then we leverage the fine-grained and coarse-grained attention mechanisms to compose the MGAN framework. Moreover, unlike previous works which train each aspect with its context separately, we design an aspect alignment loss to depict the aspect-level interactions among the aspects that have the same context. We evaluate the proposed approach on three datasets: laptop and restaurant are from SemEval 2014, and the last one is a twitter dataset. Experimental results show that the multi-grained attention network consistently outperforms the state-of-the-art methods on all three datasets. We also conduct experiments to evaluate the effectiveness of aspect alignment loss, which indicates the aspect-level interactions can bring extra useful information and further improve the performance.</p>
<p>【Keywords】:</p>
<h3 id="381. Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification.">381. Attentive Gated Lexicon Reader with Contrastive Contextual Co-Attention for Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1381/">Paper Link</a>】    【Pages】:3443-3453</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tay:Yi">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luu:Anh_Tuan">Anh Tuan Luu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jian">Jian Su</a></p>
<p>【Abstract】:
This paper proposes a new neural architecture that exploits readily available sentiment lexicon resources. The key idea is that that incorporating a word-level prior can aid in the representation learning process, eventually improving model performance. To this end, our model employs two distinctly unique components, i.e., (1) we introduce a lexicon-driven contextual attention mechanism to imbue lexicon words with long-range contextual information and (2), we introduce a contrastive co-attention mechanism that models contrasting polarities between all positive and negative words in a sentence. Via extensive experiments, we show that our approach outperforms many other neural baselines on sentiment classification tasks on multiple benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="382. Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.">382. Contextual Inter-modal Attention for Multi-modal Sentiment Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1382/">Paper Link</a>】    【Pages】:3454-3466</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghosal:Deepanway">Deepanway Ghosal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Akhtar:Md=_Shad">Md. Shad Akhtar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chauhan:Dushyant_Singh">Dushyant Singh Chauhan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Ekbal:Asif">Asif Ekbal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a></p>
<p>【Abstract】:
Multi-modal sentiment analysis offers various challenges, one being the effective combination of different input modalities, namely text, visual and acoustic. In this paper, we propose a recurrent neural network based multi-modal attention framework that leverages the contextual information for utterance-level sentiment prediction. The proposed approach applies attention on multi-modal multi-utterance representations and tries to learn the contributing features amongst them. We evaluate our proposed approach on two multi-modal sentiment analysis benchmark datasets, viz. CMU Multi-modal Opinion-level Sentiment Intensity (CMU-MOSI) corpus and the recently released CMU Multi-modal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) corpus. Evaluation results show the effectiveness of our proposed approach with the accuracies of 82.31% and 79.80% for the MOSI and MOSEI datasets, respectively. These are approximately 2 and 1 points performance improvement over the state-of-the-art models for the datasets.</p>
<p>【Keywords】:</p>
<h3 id="383. Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification.">383. Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1383/">Paper Link</a>】    【Pages】:3467-3476</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Ruidan">Ruidan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Wee_Sun">Wee Sun Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dahlmeier:Daniel">Daniel Dahlmeier</a></p>
<p>【Abstract】:
We consider the cross-domain sentiment classification problem, where a sentiment classifier is to be learned from a source domain and to be generalized to a target domain. Our approach explicitly minimizes the distance between the source and the target instances in an embedded feature space. With the difference between source and target minimized, we then exploit additional information from the target domain by consolidating the idea of semi-supervised learning, for which, we jointly employ two regularizations — entropy minimization and self-ensemble bootstrapping — to incorporate the unlabeled target data for classifier refinement. Our experimental results demonstrate that the proposed approach can better leverage unlabeled data from the target domain and achieve substantial improvements over baseline methods in various experimental settings.</p>
<p>【Keywords】:</p>
<h3 id="384. ExtRA: Extracting Prominent Review Aspects from Customer Feedback.">384. ExtRA: Extracting Prominent Review Aspects from Customer Feedback.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1384/">Paper Link</a>】    【Pages】:3477-3486</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Zhiyi">Zhiyi Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Shanshan">Shanshan Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Frank_F=">Frank F. Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Bill_Yuchen">Bill Yuchen Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Hanyuan">Hanyuan Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Kenny_Q=">Kenny Q. Zhu</a></p>
<p>【Abstract】:
Many existing systems for analyzing and summarizing customer reviews about products or service are based on a number of prominent review aspects. Conventionally, the prominent review aspects of a product type are determined manually. This costly approach cannot scale to large and cross-domain services such as Amazon.com, Taobao.com or Yelp.com where there are a large number of product types and new products emerge almost every day. In this paper, we propose a novel framework, for extracting the most prominent aspects of a given product type from textual reviews. The proposed framework, ExtRA, extracts K most prominent aspect terms or phrases which do not overlap semantically automatically without supervision. Extensive experiments show that ExtRA is effective and achieves the state-of-the-art performance on a dataset consisting of different product types.</p>
<p>【Keywords】:</p>
<h3 id="385. Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content.">385. Cross-Lingual Cross-Platform Rumor Verification Pivoting on Multimedia Content.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1385/">Paper Link</a>】    【Pages】:3487-3496</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wen:Weiming">Weiming Wen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Songwen">Songwen Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhou">Zhou Yu</a></p>
<p>【Abstract】:
With the increasing popularity of smart devices, rumors with multimedia content become more and more common on social networks. The multimedia information usually makes rumors look more convincing. Therefore, finding an automatic approach to verify rumors with multimedia content is a pressing task. Previous rumor verification research only utilizes multimedia as input features. We propose not to use the multimedia content but to find external information in other news platforms pivoting on it. We introduce a new features set, cross-lingual cross-platform features that leverage the semantic similarity between the rumors and the external information. When implemented, machine learning methods utilizing such features achieved the state-of-the-art rumor verification results.</p>
<p>【Keywords】:</p>
<h3 id="386. Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts.">386. Extractive Adversarial Networks: High-Recall Explanations for Identifying Personal Attacks in Social Media Posts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1386/">Paper Link</a>】    【Pages】:3497-3507</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Carton:Samuel">Samuel Carton</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mei:Qiaozhu">Qiaozhu Mei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Resnick:Paul">Paul Resnick</a></p>
<p>【Abstract】:
We introduce an adversarial method for producing high-recall explanations of neural text classifier decisions. Building on an existing architecture for extractive explanations via hard attention, we add an adversarial layer which scans the residual of the attention for remaining predictive signal. Motivated by the important domain of detecting personal attacks in social media comments, we additionally demonstrate the importance of manually setting a semantically appropriate “default” behavior for the model by explicitly manipulating its bias term. We develop a validation set of human-annotated personal attacks to evaluate the impact of these changes.</p>
<p>【Keywords】:</p>
<h3 id="387. Automatic Detection of Vague Words and Sentences in Privacy Policies.">387. Automatic Detection of Vague Words and Sentences in Privacy Policies.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1387/">Paper Link</a>】    【Pages】:3508-3517</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lebanoff:Logan">Logan Lebanoff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Fei">Fei Liu</a></p>
<p>【Abstract】:
Website privacy policies represent the single most important source of information for users to gauge how their personal data are collected, used and shared by companies. However, privacy policies are often vague and people struggle to understand the content. Their opaqueness poses a significant challenge to both users and policy regulators. In this paper, we seek to identify vague content in privacy policies. We construct the first corpus of human-annotated vague words and sentences and present empirical studies on automatic vagueness detection. In particular, we investigate context-aware and context-agnostic models for predicting vague words, and explore auxiliary-classifier generative adversarial networks for characterizing sentence vagueness. Our experimental results demonstrate the effectiveness of proposed approaches. Finally, we provide suggestions for resolving vagueness and improving the usability of privacy policies.</p>
<p>【Keywords】:</p>
<h3 id="388. Multi-view Models for Political Ideology Detection of News Articles.">388. Multi-view Models for Political Ideology Detection of News Articles.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1388/">Paper Link</a>】    【Pages】:3518-3527</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kulkarni:Vivek">Vivek Kulkarni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Junting">Junting Ye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Skiena:Steven">Steven Skiena</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
A news article’s title, content and link structure often reveal its political ideology. However, most existing works on automatic political ideology detection only leverage textual cues. Drawing inspiration from recent advances in neural inference, we propose a novel attention based multi-view model to leverage cues from all of the above views to identify the ideology evinced by a news article. Our model draws on advances in representation learning in natural language processing and network science to capture cues from both textual content and the network structure of news articles. We empirically evaluate our model against a battery of baselines and show that our model outperforms state of the art by 10 percentage points F1 score.</p>
<p>【Keywords】:</p>
<h3 id="389. Predicting Factuality of Reporting and Bias of News Media Sources.">389. Predicting Factuality of Reporting and Bias of News Media Sources.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1389/">Paper Link</a>】    【Pages】:3528-3539</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Baly:Ramy">Ramy Baly</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Karadzhov:Georgi">Georgi Karadzhov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Alexandrov:Dimitar">Dimitar Alexandrov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Glass:James_R=">James R. Glass</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nakov:Preslav">Preslav Nakov</a></p>
<p>【Abstract】:
We present a study on predicting the factuality of reporting and bias of news media. While previous work has focused on studying the veracity of claims or documents, here we are interested in characterizing entire news media. This is an under-studied, but arguably important research problem, both in its own right and as a prior for fact-checking systems. We experiment with a large list of news websites and with a rich set of features derived from (i) a sample of articles from the target news media, (ii) its Wikipedia page, (iii) its Twitter account, (iv) the structure of its URL, and (v) information about the Web traffic it attracts. The experimental results show sizable performance gains over the baseline, and reveal the importance of each feature type.</p>
<p>【Keywords】:</p>
<h3 id="390. Legal Judgment Prediction via Topological Learning.">390. Legal Judgment Prediction via Topological Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1390/">Paper Link</a>】    【Pages】:3540-3549</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Haoxi">Haoxi Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Zhipeng">Zhipeng Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Cunchao">Cunchao Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Chaojun">Chaojun Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>【Abstract】:
Legal Judgment Prediction (LJP) aims to predict the judgment result based on the facts of a case and becomes a promising application of artificial intelligence techniques in the legal field. In real-world scenarios, legal judgment usually consists of multiple subtasks, such as the decisions of applicable law articles, charges, fines, and the term of penalty. Moreover, there exist topological dependencies among these subtasks. While most existing works only focus on a specific subtask of judgment prediction and ignore the dependencies among subtasks, we formalize the dependencies among subtasks as a Directed Acyclic Graph (DAG) and propose a topological multi-task learning framework, TopJudge, which incorporates multiple subtasks and DAG dependencies into judgment prediction. We conduct experiments on several real-world large-scale datasets of criminal cases in the civil law system. Experimental results show that our model achieves consistent and significant improvements over baselines on all judgment prediction tasks. The source code can be obtained from <a href="https://github.com/thunlp/TopJudge">https://github.com/thunlp/TopJudge</a>.</p>
<p>【Keywords】:</p>
<h3 id="391. Hierarchical CVAE for Fine-Grained Hate Speech Classification.">391. Hierarchical CVAE for Fine-Grained Hate Speech Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1391/">Paper Link</a>】    【Pages】:3550-3559</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/q/Qian:Jing">Jing Qian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/ElSherief:Mai">Mai ElSherief</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Belding:Elizabeth_M=">Elizabeth M. Belding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Existing work on automated hate speech detection typically focuses on binary classification or on differentiating among a small set of categories. In this paper, we propose a novel method on a fine-grained hate speech classification task, which focuses on differentiating among 40 hate groups of 13 different hate group categories. We first explore the Conditional Variational Autoencoder (CVAE) as a discriminative model and then extend it to a hierarchical architecture to utilize the additional hate category information for more accurate prediction. Experimentally, we show that incorporating the hate category information for training can significantly improve the classification performance and our proposed model outperforms commonly-used discriminative models.</p>
<p>【Keywords】:</p>
<h3 id="392. Residualized Factor Adaptation for Community Social Media Prediction Tasks.">392. Residualized Factor Adaptation for Community Social Media Prediction Tasks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1392/">Paper Link</a>】    【Pages】:3560-3569</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zamani:Mohammadzaman">Mohammadzaman Zamani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:H=_Andrew">H. Andrew Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lynn:Veronica_E=">Veronica E. Lynn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Giorgi:Salvatore">Salvatore Giorgi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Balasubramanian:Niranjan">Niranjan Balasubramanian</a></p>
<p>【Abstract】:
Predictive models over social media language have shown promise in capturing community outcomes, but approaches thus far largely neglect the socio-demographic context (e.g. age, education rates, race) of the community from which the language originates. For example, it may be inaccurate to assume people in Mobile, Alabama, where the population is relatively older, will use words the same way as those from San Francisco, where the median age is younger with a higher rate of college education. In this paper, we present residualized factor adaptation, a novel approach to community prediction tasks which both (a) effectively integrates community attributes, as well as (b) adapts linguistic features to community attributes (factors). We use eleven demographic and socioeconomic attributes, and evaluate our approach over five different community-level predictive tasks, spanning health (heart disease mortality, percent fair/poor health), psychology (life satisfaction), and economics (percent housing price increase, foreclosure rate). Our evaluation shows that residualized factor adaptation significantly improves 4 out of 5 community-level outcome predictions over prior state-of-the-art for incorporating socio-demographic contexts.</p>
<p>【Keywords】:</p>
<h3 id="393. Framing and Agenda-Setting in Russian News: a Computational Analysis of Intricate Political Strategies.">393. Framing and Agenda-Setting in Russian News: a Computational Analysis of Intricate Political Strategies.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1393/">Paper Link</a>】    【Pages】:3570-3580</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Field:Anjalie">Anjalie Field</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kliger:Doron">Doron Kliger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wintner:Shuly">Shuly Wintner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pan:Jennifer">Jennifer Pan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurafsky:Dan">Dan Jurafsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsvetkov:Yulia">Yulia Tsvetkov</a></p>
<p>【Abstract】:
Amidst growing concern over media manipulation, NLP attention has focused on overt strategies like censorship and “fake news”. Here, we draw on two concepts from political science literature to explore subtler strategies for government media manipulation: agenda-setting (selecting what topics to cover) and framing (deciding how topics are covered). We analyze 13 years (100K articles) of the Russian newspaper Izvestia and identify a strategy of distraction: articles mention the U.S. more frequently in the month directly following an economic downturn in Russia. We introduce embedding-based methods for cross-lingually projecting English frames to Russian, and discover that these articles emphasize U.S. moral failings and threats to the U.S. Our work offers new ways to identify subtle media manipulation strategies at the intersection of agenda-setting and framing.</p>
<p>【Keywords】:</p>
<h3 id="394. Identifying the narrative styles of YouTube's vloggers.">394. Identifying the narrative styles of YouTube's vloggers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1394/">Paper Link</a>】    【Pages】:3581-3590</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kleinberg:Bennett">Bennett Kleinberg</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mozes:Maximilian">Maximilian Mozes</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vegt:Isabelle_van_der">Isabelle van der Vegt</a></p>
<p>【Abstract】:
Vlogs provide a rich public source of data in a novel setting. This paper examined the continuous sentiment styles employed in 27,333 vlogs using a dynamic intra-textual approach to sentiment analysis. Using unsupervised clustering, we identified seven distinct continuous sentiment trajectories characterized by fluctuations of sentiment throughout a vlog’s narrative time. We provide a taxonomy of these seven continuous sentiment styles and found that vlogs whose sentiment builds up towards a positive ending are the most prevalent in our sample. Gender was associated with preferences for different continuous sentiment trajectories. This paper discusses the findings with respect to previous work and concludes with an outlook towards possible uses of the corpus, method and findings of this paper for related areas of research.</p>
<p>【Keywords】:</p>
<h3 id="395. Native Language Identification with User Generated Content.">395. Native Language Identification with User Generated Content.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1395/">Paper Link</a>】    【Pages】:3591-3601</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goldin:Gili">Gili Goldin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rabinovich:Ella">Ella Rabinovich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wintner:Shuly">Shuly Wintner</a></p>
<p>【Abstract】:
We address the task of native language identification in the context of social media content, where authors are highly-fluent, advanced nonnative speakers (of English). Using both linguistically-motivated features and the characteristics of the social media outlet, we obtain high accuracy on this challenging task. We provide a detailed analysis of the features that sheds light on differences between native and nonnative speakers, and among nonnative speakers with different backgrounds.</p>
<p>【Keywords】:</p>
<h3 id="396. Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.">396. Beyond Error Propagation in Neural Machine Translation: Characteristics of Language Also Matter.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1396/">Paper Link</a>】    【Pages】:3602-3611</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lijun">Lijun Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Xu">Xu Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Di">Di He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Fei">Fei Tian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Tao">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lai:Jianhuang">Jianhuang Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a></p>
<p>【Abstract】:
Neural machine translation usually adopts autoregressive models and suffers from exposure bias as well as the consequent error propagation problem. Many previous works have discussed the relationship between error propagation and the accuracy drop (i.e., the left part of the translated sentence is often better than its right part in left-to-right decoding models) problem. In this paper, we conduct a series of analyses to deeply understand this problem and get several interesting findings. (1) The role of error propagation on accuracy drop is overstated in the literature, although it indeed contributes to the accuracy drop problem. (2) Characteristics of a language play a more important role in causing the accuracy drop: the left part of the translation result in a right-branching language (e.g., English) is more likely to be more accurate than its right part, while the right part is more accurate for a left-branching language (e.g., Japanese). Our discoveries are confirmed on different model structures including Transformer and RNN, and in other sequence generation tasks such as text summarization.</p>
<p>【Keywords】:</p>
<h3 id="397. A Study of Reinforcement Learning for Neural Machine Translation.">397. A Study of Reinforcement Learning for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1397/">Paper Link</a>】    【Pages】:3612-3621</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lijun">Lijun Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Fei">Fei Tian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Tao">Tao Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lai:Jianhuang">Jianhuang Lai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a></p>
<p>【Abstract】:
Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.</p>
<p>【Keywords】:</p>
<h3 id="398. Meta-Learning for Low-Resource Neural Machine Translation.">398. Meta-Learning for Low-Resource Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1398/">Paper Link</a>】    【Pages】:3622-3631</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Jiatao">Jiatao Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yong">Yong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0007:Yun">Yun Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Victor_O=_K=">Victor O. K. Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a></p>
<p>【Abstract】:
In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT’16 by seeing only 16,000 translated words (~600 parallel sentences)</p>
<p>【Keywords】:</p>
<h3 id="399. Unsupervised Statistical Machine Translation.">399. Unsupervised Statistical Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1399/">Paper Link</a>】    【Pages】:3632-3642</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Artetxe:Mikel">Mikel Artetxe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Labaka:Gorka">Gorka Labaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agirre:Eneko">Eneko Agirre</a></p>
<p>【Abstract】:
While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at <a href="https://github.com/artetxem/monoses">https://github.com/artetxem/monoses</a>.</p>
<p>【Keywords】:</p>
<h3 id="400. A Visual Attention Grounding Neural Model for Multimodal Machine Translation.">400. A Visual Attention Grounding Neural Model for Multimodal Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1400/">Paper Link</a>】    【Pages】:3643-3653</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Mingyang">Mingyang Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Runxiang">Runxiang Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Yong_Jae">Yong Jae Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhou">Zhou Yu</a></p>
<p>【Abstract】:
We introduce a novel multimodal machine translation model that utilizes parallel visual and textual information. Our model jointly optimizes the learning of a shared visual-language embedding and a translator. The model leverages a visual attention grounding mechanism that links the visual semantics with the corresponding textual semantics. Our approach achieves competitive state-of-the-art results on the Multi30K and the Ambiguous COCO datasets. We also collected a new multilingual multimodal product description dataset to simulate a real-world international online shopping scenario. On this dataset, our visual attention grounding model outperforms other methods by a large margin.</p>
<p>【Keywords】:</p>
<h3 id="401. Sentiment Classification towards Question-Answering with Hierarchical Matching Network.">401. Sentiment Classification towards Question-Answering with Hierarchical Matching Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1401/">Paper Link</a>】    【Pages】:3654-3663</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Chenlin">Chenlin Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Changlong">Changlong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jingjing">Jingjing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kang:Yangyang">Yangyang Kang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Shoushan">Shoushan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaozhong">Xiaozhong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>【Abstract】:
In an e-commerce environment, user-oriented question-answering (QA) text pair could carry rich sentiment information. In this study, we propose a novel task/method to address QA sentiment analysis. In particular, we create a high-quality annotated corpus with specially-designed annotation guidelines for QA-style sentiment classification. On the basis, we propose a three-stage hierarchical matching network to explore deep sentiment information in a QA text pair. First, we segment both the question and answer text into sentences and construct a number of [Q-sentence, A-sentence] units in each QA text pair. Then, by leveraging a QA bidirectional matching layer, the proposed approach can learn the matching vectors of each [Q-sentence, A-sentence] unit. Finally, we characterize the importance of the generated matching vectors via a self-matching attention layer. Experimental results, comparing with a number of state-of-the-art baselines, demonstrate the impressive effectiveness of the proposed approach for QA-style sentiment classification.</p>
<p>【Keywords】:</p>
<h3 id="402. Cross-topic Argument Mining from Heterogeneous Sources.">402. Cross-topic Argument Mining from Heterogeneous Sources.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1402/">Paper Link</a>】    【Pages】:3664-3674</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stab:Christian">Christian Stab</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Miller:Tristan">Tristan Miller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schiller:Benjamin">Benjamin Schiller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rai:Pranav">Pranav Rai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>【Abstract】:
Argument mining is a core technology for automating argument search in large document collections. Despite its usefulness for this task, most current approaches are designed for use only with specific text types and fall short when applied to heterogeneous texts. In this paper, we propose a new sentential annotation scheme that is reliably applicable by crowd workers to arbitrary Web texts. We source annotations for over 25,000 instances covering eight controversial topics. We show that integrating topic information into bidirectional long short-term memory networks outperforms vanilla BiLSTMs by more than 3 percentage points in F1 in two- and three-label cross-topic settings. We also show that these results can be further improved by leveraging additional data for topic relevance using multi-task learning.</p>
<p>【Keywords】:</p>
<h3 id="403. Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised.">403. Summarizing Opinions: Aspect Extraction Meets Sentiment Prediction and They Are Both Weakly Supervised.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1403/">Paper Link</a>】    【Pages】:3675-3686</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Angelidis:Stefanos">Stefanos Angelidis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
We present a neural framework for opinion summarization from online product reviews which is knowledge-lean and only requires light supervision (e.g., in the form of product domain labels and user-provided ratings). Our method combines two weakly supervised components to identify salient opinions and form extractive summaries from multiple reviews: an aspect extractor trained under a multi-task objective, and a sentiment predictor based on multiple instance learning. We introduce an opinion summarization dataset that includes a training set of product reviews from six diverse domains and human-annotated development and test sets with gold standard aspect annotations, salience labels, and opinion summaries. Automatic evaluation shows significant improvements over baselines, and a large-scale study indicates that our opinion summaries are preferred by human judges according to multiple criteria.</p>
<p>【Keywords】:</p>
<h3 id="404. CARER: Contextualized Affect Representations for Emotion Recognition.">404. CARER: Contextualized Affect Representations for Emotion Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1404/">Paper Link</a>】    【Pages】:3687-3697</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Saravia:Elvis">Elvis Saravia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Hsien=Chi_Toby">Hsien-Chi Toby Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Yen=Hao">Yen-Hao Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Junlin">Junlin Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yi=Shin">Yi-Shin Chen</a></p>
<p>【Abstract】:
Emotions are expressed in nuanced ways, which varies by collective or individual experiences, knowledge, and beliefs. Therefore, to understand emotion, as conveyed through text, a robust mechanism capable of capturing and modeling different linguistic nuances and phenomena is needed. We propose a semi-supervised, graph-based algorithm to produce rich structural descriptors which serve as the building blocks for constructing contextualized affect representations from text. The pattern-based representations are further enriched with word embeddings and evaluated through several emotion recognition tasks. Our experimental results demonstrate that the proposed method outperforms state-of-the-art techniques on emotion recognition tasks.</p>
<p>【Keywords】:</p>
<h3 id="405. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency.">405. Noise Contrastive Estimation and Negative Sampling for Conditional Models: Consistency and Statistical Efficiency.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1405/">Paper Link</a>】    【Pages】:3698-3707</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Zhuang">Zhuang Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collins:Michael">Michael Collins</a></p>
<p>【Abstract】:
Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and tradeoffs of both methods.</p>
<p>【Keywords】:</p>
<h3 id="406. CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization.">406. CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1406/">Paper Link</a>】    【Pages】:3708-3718</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yavuz:Semih">Semih Yavuz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chiu:Chung=Cheng">Chung-Cheng Chiu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Patrick">Patrick Nguyen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yonghui">Yonghui Wu</a></p>
<p>【Abstract】:
Maximum-likelihood estimation (MLE) is one of the most widely used approaches for training structured prediction models for text-generation based natural language processing applications. However, besides exposure bias, models trained with MLE suffer from wrong objective problem where they are trained to maximize the word-level correct next step prediction, but are evaluated with respect to sequence-level discrete metrics such as ROUGE and BLEU. Several variants of policy-gradient methods address some of these problems by optimizing for final discrete evaluation metrics and showing improvements over MLE training for downstream tasks like text summarization and machine translation. However, policy-gradient methods suffers from high sample variance, making the training process very difficult and unstable. In this paper, we present an alternative direction towards mitigating this problem by introducing a new objective (CaLcs) based on a differentiable surrogate of longest common subsequence (LCS) measure that captures sequence-level structure similarity. Experimental results on abstractive summarization and machine translation validate the effectiveness of the proposed approach.</p>
<p>【Keywords】:</p>
<h3 id="407. Pathologies of Neural Models Make Interpretation Difficult.">407. Pathologies of Neural Models Make Interpretation Difficult.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1407/">Paper Link</a>】    【Pages】:3719-3728</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Shi">Shi Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wallace:Eric">Eric Wallace</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Grissom_II:Alvin">Alvin Grissom II</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iyyer:Mohit">Mohit Iyyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rodriguez:Pedro">Pedro Rodriguez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a></p>
<p>【Abstract】:
One way to interpret neural model predictions is to highlight the most important input features—for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word’s importance is determined by either input perturbation—measuring the decrease in model confidence when that word is removed—or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.</p>
<p>【Keywords】:</p>
<h3 id="408. Phrase-level Self-Attention Networks for Universal Sentence Encoding.">408. Phrase-level Self-Attention Networks for Universal Sentence Encoding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1408/">Paper Link</a>】    【Pages】:3729-3738</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Wei">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyu">Tianyu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a></p>
<p>【Abstract】:
Universal sentence encoding is a hot topic in recent NLP research. Attention mechanism has been an integral part in many sentence encoding models, allowing the models to capture context dependencies regardless of the distance between the elements in the sequence. Fully attention-based models have recently attracted enormous interest due to their highly parallelizable computation and significantly less training time. However, the memory consumption of their models grows quadratically with the sentence length, and the syntactic information is neglected. To this end, we propose Phrase-level Self-Attention Networks (PSAN) that perform self-attention across words inside a phrase to capture context dependencies at the phrase level, and use the gated memory updating mechanism to refine each word’s representation hierarchically with longer-term context dependencies captured in a larger phrase. As a result, the memory consumption can be reduced because the self-attention is performed at the phrase level instead of the sentence level. At the same time, syntactic information can be easily integrated in the model. Experiment results show that PSAN can achieve the state-of-the-art performance across a plethora of NLP tasks including binary and multi-class classification, natural language inference and sentence similarity.</p>
<p>【Keywords】:</p>
<h3 id="409. BanditSum: Extractive Summarization as a Contextual Bandit.">409. BanditSum: Extractive Summarization as a Contextual Bandit.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1409/">Paper Link</a>】    【Pages】:3739-3748</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Yue">Yue Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Yikang">Yikang Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Crawford:Eric">Eric Crawford</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hoof:Herke_van">Herke van Hoof</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a></p>
<p>【Abstract】:
In this work, we propose a novel method for training neural networks to perform single-document extractive summarization without heuristically-generated extractive labels. We call our approach BanditSum as it treats extractive summarization as a contextual bandit (CB) problem, where the model receives a document to summarize (the context), and chooses a sequence of sentences to include in the summary (the action). A policy gradient reinforcement learning algorithm is used to train the model to select sequences of sentences that maximize ROUGE score. We perform a series of experiments demonstrating that BanditSum is able to achieve ROUGE scores that are better than or comparable to the state-of-the-art for extractive summarization, and converges using significantly fewer update steps than competing approaches. In addition, we show empirically that BanditSum performs significantly better than competing approaches when good summary sentences appear late in the source document.</p>
<p>【Keywords】:</p>
<h3 id="410. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification.">410. A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1410/">Paper Link</a>】    【Pages】:3749-3760</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Maddela:Mounica">Mounica Maddela</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Wei">Wei Xu</a></p>
<p>【Abstract】:
Current lexical simplification approaches rely heavily on heuristics and corpus level features that do not always align with human judgment. We create a human-rated word-complexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase. Our model performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB).</p>
<p>【Keywords】:</p>
<h3 id="411. Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.">411. Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1411/">Paper Link</a>】    【Pages】:3761-3771</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Guanghui">Guanghui Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jin=Ge">Jin-Ge Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Xuening">Xuening Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jinpeng">Jinpeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a></p>
<p>【Abstract】:
Previous work on grounded language learning did not fully capture the semantics underlying the correspondences between structured world state representations and texts, especially those between numerical values and lexical terms. In this paper, we attempt at learning explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of values and texts. We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance. As a by-product, we leverage the induced annotations to extract templates for language generation. Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework.</p>
<p>【Keywords】:</p>
<h3 id="412. Syntactic Scaffolds for Semantic Structures.">412. Syntactic Scaffolds for Semantic Structures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1412/">Paper Link</a>】    【Pages】:3772-3782</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Swayamdipta:Swabha">Swabha Swayamdipta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thomson:Sam">Sam Thomson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Kenton">Kenton Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dyer:Chris">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.</p>
<p>【Keywords】:</p>
<h3 id="413. Hierarchical Quantized Representations for Script Generation.">413. Hierarchical Quantized Representations for Script Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1413/">Paper Link</a>】    【Pages】:3783-3792</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Weber:Noah">Noah Weber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shekhar:Leena">Leena Shekhar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Balasubramanian:Niranjan">Niranjan Balasubramanian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chambers:Nate">Nate Chambers</a></p>
<p>【Abstract】:
Scripts define knowledge about how everyday scenarios (such as going to a restaurant) are expected to unfold. One of the challenges to learning scripts is the hierarchical nature of the knowledge. For example, a suspect arrested might plead innocent or guilty, and a very different track of events is then expected to happen. To capture this type of information, we propose an autoencoder model with a latent space defined by a hierarchy of categorical variables. We utilize a recently proposed vector quantization based approach, which allows continuous embeddings to be associated with each latent variable value. This permits the decoder to softly decide what portions of the latent hierarchy to condition on by attending over the value embeddings for a given setting. Our model effectively encodes and generates scripts, outperforming a recent language modeling-based method on several standard tasks, and allowing the autoencoder model to achieve substantially lower perplexity scores compared to the previous language modeling-based method.</p>
<p>【Keywords】:</p>
<h3 id="414. Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.">414. Semantic Role Labeling for Learner Chinese: the Importance of Syntactic Parsing and L2-L1 Parallel Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1414/">Paper Link</a>】    【Pages】:3793-3802</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Zi">Zi Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Yuguang">Yuguang Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Yuanyuan">Yuanyuan Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Weiwei">Weiwei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>【Abstract】:
This paper studies semantic parsing for interlanguage (L2), taking semantic role labeling (SRL) as a case task and learner Chinese as a case language. We first manually annotate the semantic roles for a set of learner texts to derive a gold standard for automatic SRL. Based on the new data, we then evaluate three off-the-shelf SRL systems, i.e., the PCFGLA-parser-based, neural-parser-based and neural-syntax-agnostic systems, to gauge how successful SRL for learner Chinese can be. We find two non-obvious facts: 1) the L1-sentence-trained systems performs rather badly on the L2 data; 2) the performance drop from the L1 data to the L2 data of the two parser-based systems is much smaller, indicating the importance of syntactic parsing in SRL for interlanguages. Finally, the paper introduces a new agreement-based model to explore the semantic coherency information in the large-scale L2-L1 parallel data. We then show such information is very effective to enhance SRL for learner texts. Our model achieves an F-score of 72.06, which is a 2.02 point improvement over the best baseline.</p>
<p>【Keywords】:</p>
<h3 id="415. A Teacher-Student Framework for Maintainable Dialog Manager.">415. A Teacher-Student Framework for Maintainable Dialog Manager.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1415/">Paper Link</a>】    【Pages】:3803-3812</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Weikang">Weikang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Han">Han Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwang:Mei=Yuh">Mei-Yuh Hwang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhifei">Zhifei Li</a></p>
<p>【Abstract】:
Reinforcement learning (RL) is an attractive solution for task-oriented dialog systems. However, extending RL-based systems to handle new intents and slots requires a system redesign. The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale. To address this issue, we propose a practical teacher-student framework to extend RL-based dialog systems without retraining from scratch. Specifically, the “student” is an extended dialog manager based on a new ontology, and the “teacher” is existing resources used for guiding the learning process of the “student”. By specifying constraints held in the new dialog manager, we transfer knowledge of the “teacher” to the “student” without additional resources. Experiments show that the performance of the extended system is comparable to the system trained from scratch. More importantly, the proposed framework makes no assumption about the unsupported intents and slots, which makes it possible to improve RL-based systems incrementally.</p>
<p>【Keywords】:</p>
<h3 id="416. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning.">416. Discriminative Deep Dyna-Q: Robust Planning for Dialogue Policy Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1416/">Paper Link</a>】    【Pages】:3813-3823</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Su:Shang=Yu">Shang-Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiujun">Xiujun Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jingjing">Jingjing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yun=Nung">Yun-Nung Chen</a></p>
<p>【Abstract】:
This paper presents a Discriminative Deep Dyna-Q (D3Q) approach to improving the effectiveness and robustness of Deep Dyna-Q (DDQ), a recently proposed framework that extends the Dyna-Q algorithm to integrate planning for task-completion dialogue policy learning. To obviate DDQ’s high dependency on the quality of simulated experiences, we incorporate an RNN-based discriminator in D3Q to differentiate simulated experience from real user experience in order to control the quality of training data. Experiments show that D3Q significantly outperforms DDQ by controlling the quality of simulated experience used for planning. The effectiveness and robustness of D3Q is further demonstrated in a domain extension setting, where the agent’s capability of adapting to a changing environment is tested.</p>
<p>【Keywords】:</p>
<h3 id="417. A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding.">417. A Self-Attentive Model with Gate Mechanism for Spoken Language Understanding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1417/">Paper Link</a>】    【Pages】:3824-3833</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Changliang">Changliang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Liang">Liang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi:Ji">Ji Qi</a></p>
<p>【Abstract】:
Spoken Language Understanding (SLU), which typically involves intent determination and slot filling, is a core component of spoken dialogue systems. Joint learning has shown to be effective in SLU given that slot tags and intents are supposed to share knowledge with each other. However, most existing joint learning methods only consider joint learning by sharing parameters on surface level rather than semantic level. In this work, we propose a novel self-attentive model with gate mechanism to fully utilize the semantic correlation between slot and intent. Our model first obtains intent-augmented embeddings based on neural network with self-attention mechanism. And then the intent semantic representation is utilized as the gate for labelling slot tags. The objectives of both tasks are optimized simultaneously via joint learning in an end-to-end way. We conduct experiment on popular benchmark ATIS. The results show that our model achieves state-of-the-art and outperforms other popular methods by a large margin in terms of both intent detection error rate and slot filling F1-score. This paper gives a new perspective for research on SLU.</p>
<p>【Keywords】:</p>
<h3 id="418. Learning End-to-End Goal-Oriented Dialog with Multiple Answers.">418. Learning End-to-End Goal-Oriented Dialog with Multiple Answers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1418/">Paper Link</a>】    【Pages】:3834-3843</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rajendran:Janarthanan">Janarthanan Rajendran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Ganhotra:Jatin">Jatin Ganhotra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh:Satinder">Satinder Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Polymenakos:Lazaros">Lazaros Polymenakos</a></p>
<p>【Abstract】:
In a dialog, there could be multiple valid next utterances at any point. The present end-to-end neural methods for dialog do not take this into account. They learn with the assumption that at any time there is only one correct next utterance. In this work, we focus on this problem in the goal-oriented dialog setting where there are different paths to reach a goal. We propose a new method, that uses a combination of supervised learning and reinforcement learning approaches to address this issue. We also propose a new and more effective testbed, permuted-bAbI dialog tasks, by introducing multiple valid next utterances to the original-bAbI dialog tasks, which allows evaluation of end-to-end goal-oriented dialog systems in a more realistic setting. We show that there is a significant drop in performance of existing end-to-end neural methods from 81.5% per-dialog accuracy on original-bAbI dialog tasks to 30.3% on permuted-bAbI dialog tasks. We also show that our proposed method improves the performance and achieves 47.3% per-dialog accuracy on permuted-bAbI dialog tasks. We also release permuted-bAbI dialog tasks, our proposed testbed, to the community for evaluating dialog systems in a goal-oriented setting.</p>
<p>【Keywords】:</p>
<h3 id="419. AirDialogue: An Environment for Goal-Oriented Dialogue Research.">419. AirDialogue: An Environment for Goal-Oriented Dialogue Research.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1419/">Paper Link</a>】    【Pages】:3844-3854</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Wei">Wei Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Le:Quoc_V=">Quoc V. Le</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Andrew_M=">Andrew M. Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jia">Jia Li</a></p>
<p>【Abstract】:
Recent progress in dialogue generation has inspired a number of studies on dialogue systems that are capable of accomplishing tasks through natural language interactions. A promising direction among these studies is the use of reinforcement learning techniques, such as self-play, for training dialogue agents. However, current datasets are limited in size, and the environment for training agents and evaluating progress is relatively unsophisticated. We present AirDialogue, a large dataset that contains 301,427 goal-oriented conversations. To collect this dataset, we create a context-generator which provides travel and flight restrictions. We then ask human annotators to play the role of a customer or an agent and interact with the goal of successfully booking a trip given the restrictions. Key to our environment is the ease of evaluating the success of the dialogue, which is achieved by using ground-truth states (e.g., the flight being booked) generated by the restrictions. Any dialogue agent that does not generate the correct states is considered to fail. Our experimental results indicate that state-of-the-art dialogue models can only achieve a score of 0.17 while humans can reach a score of 0.91, which suggests significant opportunities for future improvement.</p>
<p>【Keywords】:</p>
<h3 id="420. QuaSE: Sequence Editing under Quantifiable Guidance.">420. QuaSE: Sequence Editing under Quantifiable Guidance.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1420/">Paper Link</a>】    【Pages】:3855-3864</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liao:Yi">Yi Liao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Piji">Piji Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Tong">Tong Zhang</a></p>
<p>【Abstract】:
We propose the task of Quantifiable Sequence Editing (QuaSE): editing an input sequence to generate an output sequence that satisfies a given numerical outcome value measuring a certain property of the sequence, with the requirement of keeping the main content of the input sequence. For example, an input sequence could be a word sequence, such as review sentence and advertisement text. For a review sentence, the outcome could be the review rating; for an advertisement, the outcome could be the click-through rate. The major challenge in performing QuaSE is how to perceive the outcome-related wordings, and only edit them to change the outcome. In this paper, the proposed framework contains two latent factors, namely, outcome factor and content factor, disentangled from the input sentence to allow convenient editing to change the outcome and keep the content. Our framework explores the pseudo-parallel sentences by modeling their content similarity and outcome differences to enable a better disentanglement of the latent factors, which allows generating an output to better satisfy the desired outcome and keep the content. The dual reconstruction structure further enhances the capability of generating expected output by exploiting the couplings of latent factors of pseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review sentences with the ratings as outcome. Extensive experimental results are reported and discussed to elaborate the peculiarities of our framework.</p>
<p>【Keywords】:</p>
<h3 id="421. Paraphrase Generation with Deep Reinforcement Learning.">421. Paraphrase Generation with Deep Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1421/">Paper Link</a>】    【Pages】:3865-3878</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zichao">Zichao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Xin">Xin Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shang:Lifeng">Lifeng Shang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Hang">Hang Li</a></p>
<p>【Abstract】:
Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.</p>
<p>【Keywords】:</p>
<h3 id="422. Operation-guided Neural Networks for High Fidelity Data-To-Text Generation.">422. Operation-guided Neural Networks for High Fidelity Data-To-Text Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1422/">Paper Link</a>】    【Pages】:3879-3889</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nie:Feng">Feng Nie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jinpeng">Jinpeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jin=Ge">Jin-Ge Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pan:Rong">Rong Pan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a></p>
<p>【Abstract】:
Recent neural models for data-to-text generation are mostly based on data-driven end-to-end training over encoder-decoder networks. Even though the generated texts are mostly fluent and informative, they often generate descriptions that are not consistent with the input structured data. This is a critical issue especially in domains that require inference or calculations over raw data. In this paper, we attempt to improve the fidelity of neural data-to-text generation by utilizing pre-executed symbolic operations. We propose a framework called Operation-guided Attention-based sequence-to-sequence network (OpAtt), with a specifically designed gating mechanism as well as a quantization module for operation results to utilize information from pre-executed operations. Experiments on two sports datasets show our proposed method clearly improves the fidelity of the generated texts to the input structured data.</p>
<p>【Keywords】:</p>
<h3 id="423. Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training.">423. Generating Classical Chinese Poems via Conditional Variational Autoencoder and Adversarial Training.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1423/">Paper Link</a>】    【Pages】:3890-3900</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Juntao">Juntao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yan">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Haisong">Haisong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Dongmin">Dongmin Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan_0001:Rui">Rui Yan</a></p>
<p>【Abstract】:
It is a challenging task to automatically compose poems with not only fluent expressions but also aesthetic wording. Although much attention has been paid to this task and promising progress is made, there exist notable gaps between automatically generated ones with those created by humans, especially on the aspects of term novelty and thematic consistency. Towards filling the gap, in this paper, we propose a conditional variational autoencoder with adversarial training for classical Chinese poem generation, where the autoencoder part generates poems with novel terms and a discriminator is applied to adversarially learn their thematic consistency with their titles. Experimental results on a large poetry corpus confirm the validity and effectiveness of our model, where its automatic and human evaluation scores outperform existing models.</p>
<p>【Keywords】:</p>
<h3 id="424. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks.">424. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1424/">Paper Link</a>】    【Pages】:3901-3910</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Yao">Yao Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ni_0001:Xiaochuan">Xiaochuan Ni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Yuanyuan">Yuanyuan Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Ke:Qifa">Qifa Ke</a></p>
<p>【Abstract】:
Question generation, the task of automatically creating questions that can be answered by a certain span of text within a given passage, is important for question-answering and conversational systems in digital assistants such as Alexa, Cortana, Google Assistant and Siri. Recent sequence to sequence neural models have outperformed previous rule-based systems. Existing models mainly focused on using one or two sentences as the input. Long text has posed challenges for sequence to sequence neural models in question generation – worse performances were reported if using the whole paragraph (with multiple sentences) as the input. In reality, however, it often requires the whole paragraph as context in order to generate high quality questions. In this paper, we propose a maxout pointer mechanism with gated self-attention encoder to address the challenges of processing long text inputs for question generation. With sentence-level inputs, our model outperforms previous approaches with either sentence-level or paragraph-level inputs. Furthermore, our model can effectively utilize paragraphs as inputs, pushing the state-of-the-art result from 13.9 to 16.3 (BLEU_4).</p>
<p>【Keywords】:</p>
<h3 id="425. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.">425. Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1425/">Paper Link</a>】    【Pages】:3911-3921</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Tao">Tao Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0037:Rui">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Kai">Kai Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yasunaga:Michihiro">Michihiro Yasunaga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Dongxu">Dongxu Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zifan">Zifan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:James">James Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Irene">Irene Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Qingning">Qingning Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roman:Shanelle">Shanelle Roman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zilin">Zilin Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radev:Dragomir_R=">Dragomir R. Radev</a></p>
<p>【Abstract】:
We present Spider, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at <a href="https://yale-lily.github.io/seq2sql/spider">https://yale-lily.github.io/seq2sql/spider</a>.</p>
<p>【Keywords】:</p>
<h3 id="426. Unsupervised Natural Language Generation with Denoising Autoencoders.">426. Unsupervised Natural Language Generation with Denoising Autoencoders.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1426/">Paper Link</a>】    【Pages】:3922-3929</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Freitag:Markus">Markus Freitag</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roy:Scott">Scott Roy</a></p>
<p>【Abstract】:
Generating text from structured data is important for various tasks such as question answering and dialog systems. We show that in at least one domain, without any supervision and only based on unlabeled text, we are able to build a Natural Language Generation (NLG) system with higher performance than supervised approaches. In our approach, we interpret the structured data as a corrupt representation of the desired output and use a denoising auto-encoder to reconstruct the sentence. We show how to introduce noise into training examples that do not contain structured data, and that the resulting denoising auto-encoder generalizes to generate correct sentences when given structured data.</p>
<p>【Keywords】:</p>
<h3 id="427. Answer-focused and Position-aware Neural Question Generation.">427. Answer-focused and Position-aware Neural Question Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1427/">Paper Link</a>】    【Pages】:3930-3939</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Xingwu">Xingwu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0022:Jing">Jing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Yajuan">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Wei">Wei He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Yanjun">Yanjun Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shi">Shi Wang</a></p>
<p>【Abstract】:
In this paper, we focus on the problem of question generation (QG). Recent neural network-based approaches employ the sequence-to-sequence model which takes an answer and its context as input and generates a relevant question as output. However, we observe two major issues with these approaches: (1) The generated interrogative words (or question words) do not match the answer type. (2) The model copies the context words that are far from and irrelevant to the answer, instead of the words that are close and relevant to the answer. To address these two issues, we propose an answer-focused and position-aware neural question generation model. (1) By answer-focused, we mean that we explicitly model question word generation by incorporating the answer embedding, which can help generate an interrogative word matching the answer type. (2) By position-aware, we mean that we model the relative distance between the context words and the answer. Hence the model can be aware of the position of the context words when copying them to generate a question. We conduct extensive experiments to examine the effectiveness of our model. The experimental results show that our model significantly improves the baseline and outperforms the state-of-the-art system.</p>
<p>【Keywords】:</p>
<h3 id="428. Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation.">428. Diversity-Promoting GAN: A Cross-Entropy Based Generative Adversarial Network for Diversified Text Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1428/">Paper Link</a>】    【Pages】:3940-3949</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingjing">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Junyang">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
Existing text generation methods tend to produce repeated and ”boring” expressions. To tackle this problem, we propose a new text generation model, called Diversity-Promoting Generative Adversarial Network (DP-GAN). The proposed model assigns low reward for repeatedly generated text and high reward for ”novel” and fluent text, encouraging the generator to produce diverse and informative text. Moreover, we propose a novel language-model based discriminator, which can better distinguish novel text from repeated text without the saturation problem compared with existing classifier-based discriminators. The experimental results on review generation and dialogue generation tasks demonstrate that our model can generate substantially more diverse and informative text than existing baselines.</p>
<p>【Keywords】:</p>
<h3 id="429. Towards a Better Metric for Evaluating Question Generation Systems.">429. Towards a Better Metric for Evaluating Question Generation Systems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1429/">Paper Link</a>】    【Pages】:3950-3959</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nema:Preksha">Preksha Nema</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khapra:Mitesh_M=">Mitesh M. Khapra</a></p>
<p>【Abstract】:
There has always been criticism for using n-gram based similarity metrics, such as BLEU, NIST, etc, for evaluating the performance of NLG systems. However, these metrics continue to remain popular and are recently being used for evaluating the performance of systems which automatically generate questions from documents, knowledge graphs, images, etc. Given the rising interest in such automatic question generation (AQG) systems, it is important to objectively examine whether these metrics are suitable for this task. In particular, it is important to verify whether such metrics used for evaluating AQG systems focus on answerability of the generated question by preferring questions which contain all relevant information such as question type (Wh-types), entities, relations, etc. In this work, we show that current automatic evaluation metrics based on n-gram similarity do not always correlate well with human judgments about answerability of a question. To alleviate this problem and as a first step towards better evaluation metrics for AQG, we introduce a scoring function to capture answerability and show that when this scoring function is integrated with existing metrics, they correlate significantly better with human judgments. The scripts and data developed as a part of this work are made publicly available.</p>
<p>【Keywords】:</p>
<h3 id="430. Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement.">430. Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1430/">Paper Link</a>】    【Pages】:3960-3969</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Cheng">Cheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yi:Xiaoyuan">Xiaoyuan Yi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Wenhao">Wenhao Li</a></p>
<p>【Abstract】:
The ability to write diverse poems in different styles under the same poetic imagery is an important characteristic of human poetry writing. Most previous works on automatic Chinese poetry generation focused on improving the coherency among lines. Some work explored style transfer but suffered from expensive expert labeling of poem styles. In this paper, we target on stylistic poetry generation in a fully unsupervised manner for the first time. We propose a novel model which requires no supervised style labeling by incorporating mutual information, a concept in information theory, into modeling. Experimental results show that our model is able to generate stylistic poems without losing fluency and coherency.</p>
<p>【Keywords】:</p>
<h3 id="431. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints.">431. Generating More Interesting Responses in Neural Conversation Models with Distributional Constraints.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1431/">Paper Link</a>】    【Pages】:3970-3980</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Baheti:Ashutosh">Ashutosh Baheti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ritter:Alan">Alan Ritter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Jiwei">Jiwei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dolan:Bill">Bill Dolan</a></p>
<p>【Abstract】:
Neural conversation models tend to generate safe, generic responses for most inputs. This is due to the limitations of likelihood-based decoding objectives in generation tasks with diverse outputs, such as conversation. To address this challenge, we propose a simple yet effective approach for incorporating side information in the form of distributional constraints over the generated responses. We propose two constraints that help generate more content rich responses that are based on a model of syntax and topics (Griffiths et al., 2005) and semantic similarity (Arora et al., 2016). We evaluate our approach against a variety of competitive baselines, using both automatic metrics and human judgments, showing that our proposed approach generates responses that are much less generic without sacrificing plausibility. A working demo of our code can be found at <a href="https://github.com/abaheti95/DC-NeuralConversation">https://github.com/abaheti95/DC-NeuralConversation</a>.</p>
<p>【Keywords】:</p>
<h3 id="432. Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity.">432. Better Conversations by Modeling, Filtering, and Optimizing for Coherence and Diversity.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1432/">Paper Link</a>】    【Pages】:3981-3991</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Xinnuo">Xinnuo Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dusek:Ondrej">Ondrej Dusek</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Konstas:Ioannis">Ioannis Konstas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rieser:Verena">Verena Rieser</a></p>
<p>【Abstract】:
We present three enhancements to existing encoder-decoder models for open-domain conversational agents, aimed at effectively modeling coherence and promoting output diversity: (1) We introduce a measure of coherence as the GloVe embedding similarity between the dialogue context and the generated response, (2) we filter our training corpora based on the measure of coherence to obtain topically coherent and lexically diverse context-response pairs, (3) we then train a response generator using a conditional variational autoencoder model that incorporates the measure of coherence as a latent variable and uses a context gate to guarantee topical consistency with the context and promote lexical diversity. Experiments on the OpenSubtitles corpus show a substantial improvement over competitive neural models in terms of BLEU score as well as metrics of coherence and diversity.</p>
<p>【Keywords】:</p>
<h3 id="433. Incorporating Background Knowledge into Video Description Generation.">433. Incorporating Background Knowledge into Video Description Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1433/">Paper Link</a>】    【Pages】:3992-4001</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Whitehead:Spencer">Spencer Whitehead</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Shih=Fu">Shih-Fu Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Voss:Clare_R=">Clare R. Voss</a></p>
<p>【Abstract】:
Most previous efforts toward video captioning focus on generating generic descriptions, such as, “A man is talking.” We collect a news video dataset to generate enriched descriptions that include important background knowledge, such as named entities and related events, which allows the user to fully understand the video content. We develop an approach that uses video meta-data to retrieve topically related news documents for a video and extracts the events and named entities from these documents. Then, given the video as well as the extracted events and entities, we generate a description using a Knowledge-aware Video Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model’s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions.</p>
<p>【Keywords】:</p>
<h3 id="434. Multimodal Differential Network for Visual Question Generation.">434. Multimodal Differential Network for Visual Question Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1434/">Paper Link</a>】    【Pages】:4002-4012</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Patro:Badri_Narayana">Badri Narayana Patro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Sandeep">Sandeep Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurmi:Vinod_Kumar">Vinod Kumar Kurmi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Namboodiri:Vinay_P=">Vinay P. Namboodiri</a></p>
<p>【Abstract】:
Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr).</p>
<p>【Keywords】:</p>
<h3 id="435. Entity-aware Image Caption Generation.">435. Entity-aware Image Caption Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1435/">Paper Link</a>】    【Pages】:4013-4023</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Di">Di Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Whitehead:Spencer">Spencer Whitehead</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Lifu">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Shih=Fu">Shih-Fu Chang</a></p>
<p>【Abstract】:
Current image captioning approaches generate descriptions which lack specific information, such as named entities that are involved in the images. In this paper we propose a new task which aims to generate informative image captions, given images and hashtags as input. We propose a simple but effective approach to tackle this problem. We first train a convolutional neural networks - long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from Flickr show that our model generates news-style image descriptions with much richer information. Our model outperforms unimodal baselines significantly with various evaluation metrics.</p>
<p>【Keywords】:</p>
<h3 id="436. Learning to Describe Differences Between Pairs of Similar Images.">436. Learning to Describe Differences Between Pairs of Similar Images.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1436/">Paper Link</a>】    【Pages】:4024-4034</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jhamtani:Harsh">Harsh Jhamtani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg=Kirkpatrick:Taylor">Taylor Berg-Kirkpatrick</a></p>
<p>【Abstract】:
In this paper, we introduce the task of automatically generating text to describe the differences between two similar images. We collect a new dataset by crowd-sourcing difference descriptions for pairs of image frames extracted from video-surveillance footage. Annotators were asked to succinctly describe all the differences in a short paragraph. As a result, our novel dataset provides an opportunity to explore models that align language and vision, and capture visual salience. The dataset may also be a useful benchmark for coherent multi-sentence generation. We perform a first-pass visual analysis that exposes clusters of differing pixels as a proxy for object-level differences. We propose a model that captures visual salience by using a latent variable to align clusters of differing pixels with output sentences. We find that, for both single-sentence generation and as well as multi-sentence generation, the proposed model outperforms the models that use attention alone.</p>
<p>【Keywords】:</p>
<h3 id="437. Object Hallucination in Image Captioning.">437. Object Hallucination in Image Captioning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1437/">Paper Link</a>】    【Pages】:4035-4045</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rohrbach:Anna">Anna Rohrbach</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hendricks:Lisa_Anne">Lisa Anne Hendricks</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Burns:Kaylee">Kaylee Burns</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Darrell:Trevor">Trevor Darrell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saenko:Kate">Kate Saenko</a></p>
<p>【Abstract】:
Despite continuously improving performance, contemporary image captioning models are prone to “hallucinating” objects that are not actually in a scene. One problem is that standard metrics only measure similarity to ground truth captions and may not fully capture image relevance. In this work, we propose a new image relevance metric to evaluate current models with veridical visual labels and assess their rate of object hallucination. We analyze how captioning model architectures and learning objectives contribute to object hallucination, explore when hallucination is likely due to image misclassification or language priors, and assess how well current sentence metrics capture object hallucination. We investigate these questions on the standard image captioning benchmark, MSCOCO, using a diverse set of models. Our analysis yields several interesting findings, including that models which score best on standard sentence metrics do not always have lower hallucination and that models which hallucinate more tend to make errors driven by language priors.</p>
<p>【Keywords】:</p>
<h3 id="438. Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN.">438. Abstractive Text-Image Summarization Using Multi-Modal Attentional Hierarchical RNN.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1438/">Paper Link</a>】    【Pages】:4046-4056</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jingqiang">Jingqiang Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhuge:Hai">Hai Zhuge</a></p>
<p>【Abstract】:
Rapid growth of multi-modal documents on the Internet makes multi-modal summarization research necessary. Most previous research summarizes texts or images separately. Recent neural summarization research shows the strength of the Encoder-Decoder model in text summarization. This paper proposes an abstractive text-image summarization model using the attentional hierarchical Encoder-Decoder model to summarize a text document and its accompanying images simultaneously, and then to align the sentences and images in summaries. A multi-modal attentional mechanism is proposed to attend original sentences, images, and captions when decoding. The DailyMail dataset is extended by collecting images and captions from the Web. Experiments show our model outperforms the neural abstractive and extractive text summarization methods that do not consider images. In addition, our model can generate informative summaries of images.</p>
<p>【Keywords】:</p>
<h3 id="439. Keyphrase Generation with Correlation Constraints.">439. Keyphrase Generation with Correlation Constraints.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1439/">Paper Link</a>】    【Pages】:4057-4066</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jun">Jun Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiaoming">Xiaoming Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0006:Yu">Yu Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Zhao">Zhao Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhoujun">Zhoujun Li</a></p>
<p>【Abstract】:
In this paper, we study automatic keyphrase generation. Although conventional approaches to this task show promising results, they neglect correlation among keyphrases, resulting in duplication and coverage issues. To solve these problems, we propose a new sequence-to-sequence architecture for keyphrase generation named CorrRNN, which captures correlation among multiple keyphrases in two ways. First, we employ a coverage vector to indicate whether the word in the source document has been summarized by previous phrases to improve the coverage for keyphrases. Second, preceding phrases are taken into account to eliminate duplicate phrases and improve result coherence. Experiment results show that our model significantly outperforms the state-of-the-art method on benchmark datasets in terms of both accuracy and diversity.</p>
<p>【Keywords】:</p>
<h3 id="440. Closed-Book Training to Improve Summarization Encoder Memory.">440. Closed-Book Training to Improve Summarization Encoder Memory.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1440/">Paper Link</a>】    【Pages】:4067-4077</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yichen">Yichen Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
A good neural sequence-to-sequence summarization model should have a strong encoder that can distill and memorize the important information from long input texts so that the decoder can generate salient summaries based on the encoder’s memory. In this paper, we aim to improve the memorization capabilities of the encoder of a pointer-generator model by adding an additional ‘closed-book’ decoder without attention and pointer mechanisms. Such a decoder forces the encoder to be more selective in the information encoded in its memory state because the decoder can’t rely on the extra information provided by the attention and possibly copy modules, and hence improves the entire model. On the CNN/Daily Mail dataset, our 2-decoder model outperforms the baseline significantly in terms of ROUGE and METEOR metrics, for both cross-entropy and reinforced setups (and on human evaluation). Moreover, our model also achieves higher scores in a test-only DUC-2002 generalizability setup. We further present a memory ability test, two saliency metrics, as well as several sanity-check ablations (based on fixed-encoder, gradient-flow cut, and model capacity) to prove that the encoder of our 2-decoder model does in fact learn stronger memory representations than the baseline encoder.</p>
<p>【Keywords】:</p>
<h3 id="441. Improving Neural Abstractive Document Summarization with Structural Regularization.">441. Improving Neural Abstractive Document Summarization with Structural Regularization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1441/">Paper Link</a>】    【Pages】:4078-4087</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li_0107:Wei">Wei Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Xinyan">Xinyan Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Yajuan">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yuanzhuo">Yuanzhuo Wang</a></p>
<p>【Abstract】:
Recent neural sequence-to-sequence models have shown significant progress on short text summarization. However, for document summarization, they fail to capture the long-term structure of both documents and multi-sentence summaries, resulting in information loss and repetitions. In this paper, we propose to leverage the structural information of both documents and multi-sentence summaries to improve the document summarization performance. Specifically, we import both structural-compression and structural-coverage regularization into the summarization process in order to capture the information compression and information coverage properties, which are the two most important structural properties of document summarization. Experimental results demonstrate that the structural regularization improves the document summarization performance significantly, which enables our model to generate more informative and concise summaries, and thus significantly outperforms state-of-the-art neural abstractive methods.</p>
<p>【Keywords】:</p>
<h3 id="442. Iterative Document Representation Learning Towards Summarization with Polishing.">442. Iterative Document Representation Learning Towards Summarization with Polishing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1442/">Paper Link</a>】    【Pages】:4088-4097</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xiuying">Xiuying Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Shen">Shen Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tao:Chongyang">Chongyang Tao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yan">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan_0001:Rui">Rui Yan</a></p>
<p>【Abstract】:
In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.</p>
<p>【Keywords】:</p>
<h3 id="443. Bottom-Up Abstractive Summarization.">443. Bottom-Up Abstractive Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1443/">Paper Link</a>】    【Pages】:4098-4109</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gehrmann:Sebastian">Sebastian Gehrmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Yuntian">Yuntian Deng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rush:Alexander_M=">Alexander M. Rush</a></p>
<p>【Abstract】:
Neural summarization produces outputs that are fluent and readable, but which can be poor at content selection, for instance often copying full sentences from the source document. This work explores the use of data-efficient content selectors to over-determine phrases in a source document that should be part of the summary. We use this selector as a bottom-up attention step to constrain the model to likely phrases. We show that this approach improves the ability to compress text, while still generating fluent summaries. This two-step process is both simpler and higher performing than other end-to-end content selection models, leading to significant improvements on ROUGE for both the CNN-DM and NYT corpus. Furthermore, the content selector can be trained with as little as 1,000 sentences making it easy to transfer a trained summarizer to a new domain.</p>
<p>【Keywords】:</p>
<h3 id="444. Controlling Length in Abstractive Summarization Using a Convolutional Neural Network.">444. Controlling Length in Abstractive Summarization Using a Convolutional Neural Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1444/">Paper Link</a>】    【Pages】:4110-4119</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yizhu">Yizhu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Zhiyi">Zhiyi Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Kenny_Q=">Kenny Q. Zhu</a></p>
<p>【Abstract】:
Convolutional neural networks (CNNs) have met great success in abstractive summarization, but they cannot effectively generate summaries of desired lengths. Because generated summaries are used in difference scenarios which may have space or length constraints, the ability to control the summary length in abstractive summarization is an important problem. In this paper, we propose an approach to constrain the summary length by extending a convolutional sequence to sequence model. The results show that this approach generates high-quality summaries with user defined length, and outperforms the baselines consistently in terms of ROUGE score, length variations and semantic similarity.</p>
<p>【Keywords】:</p>
<h3 id="445. APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning.">445. APRIL: Interactively Learning to Summarise by Combining Active Preference Learning and Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1445/">Paper Link</a>】    【Pages】:4120-4130</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gao_0023:Yang">Yang Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meyer:Christian_M=">Christian M. Meyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>【Abstract】:
We propose a method to perform automatic document summarisation without using reference summaries. Instead, our method interactively learns from users’ preferences. The merit of preference-based interactive summarisation is that preferences are easier for users to provide than reference summaries. Existing preference-based interactive learning methods suffer from high sample complexity, i.e. they need to interact with the oracle for many rounds in order to converge. In this work, we propose a new objective function, which enables us to leverage active learning, preference learning and reinforcement learning techniques in order to reduce the sample complexity. Both simulation and real-user experiments suggest that our method significantly advances the state of the art. Our source code is freely available at <a href="https://github.com/UKPLab/emnlp2018-april">https://github.com/UKPLab/emnlp2018-april</a>.</p>
<p>【Keywords】:</p>
<h3 id="446. Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.">446. Adapting the Neural Encoder-Decoder Framework from Single to Multi-Document Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1446/">Paper Link</a>】    【Pages】:4131-4141</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lebanoff:Logan">Logan Lebanoff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Kaiqiang">Kaiqiang Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0004:Fei">Fei Liu</a></p>
<p>【Abstract】:
Generating a text abstract from a set of documents remains a challenging task. The neural encoder-decoder framework has recently been exploited to summarize single documents, but its success can in part be attributed to the availability of large parallel data automatically acquired from the Web. In contrast, parallel data for multi-document summarization are scarce and costly to obtain. There is a pressing need to adapt an encoder-decoder model trained on single-document summarization data to work with multiple-document input. In this paper, we present an initial investigation into a novel adaptation method. It exploits the maximal marginal relevance method to select representative sentences from multi-document input, and leverages an abstractive encoder-decoder model to fuse disparate sentences to an abstractive summary. The adaptation method is robust and itself requires no training data. Our system compares favorably to state-of-the-art extractive and abstractive approaches judged by automatic metrics and human assessors.</p>
<p>【Keywords】:</p>
<h3 id="447. Semi-Supervised Learning for Neural Keyphrase Generation.">447. Semi-Supervised Learning for Neural Keyphrase Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1447/">Paper Link</a>】    【Pages】:4142-4153</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Hai">Hai Ye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0008:Lu">Lu Wang</a></p>
<p>【Abstract】:
We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a self-learning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphrases as well as the titles of the articles. Experimental results show that our semi-supervised learning-based methods outperform a state-of-the-art model trained with labeled data only.</p>
<p>【Keywords】:</p>
<h3 id="448. MSMO: Multimodal Summarization with Multimodal Output.">448. MSMO: Multimodal Summarization with Multimodal Output.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1448/">Paper Link</a>】    【Pages】:4154-4164</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Junnan">Junnan Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Haoran">Haoran Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianshang">Tianshang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Yu">Yu Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jiajun">Jiajun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zong:Chengqing">Chengqing Zong</a></p>
<p>【Abstract】:
Multimodal summarization has drawn much attention due to the rapid growth of multimedia data. The output of the current multimodal summarization systems is usually represented in texts. However, we have found through experiments that multimodal output can significantly improve user satisfaction for informativeness of summaries. In this paper, we propose a novel task, multimodal summarization with multimodal output (MSMO). To handle this task, we first collect a large-scale dataset for MSMO research. We then propose a multimodal attention model to jointly generate text and select the most relevant image from the multimodal input. Finally, to evaluate multimodal outputs, we construct a novel multimodal automatic evaluation (MMAE) method which considers both intra-modality salience and inter-modality relevance. The experimental results show the effectiveness of MMAE.</p>
<p>【Keywords】:</p>
<h3 id="449. Frustratingly Easy Model Ensemble for Abstractive Summarization.">449. Frustratingly Easy Model Ensemble for Abstractive Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1449/">Paper Link</a>】    【Pages】:4165-4176</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Hayato">Hayato Kobayashi</a></p>
<p>【Abstract】:
Ensemble methods, which combine multiple models at decoding time, are now widely known to be effective for text-generation tasks. However, they generally increase computational costs, and thus, there have been many studies on compressing or distilling ensemble models. In this paper, we propose an alternative, simple but effective unsupervised ensemble method, post-ensemble, that combines multiple models by selecting a majority-like output in post-processing. We theoretically prove that our method is closely related to kernel density estimation based on the von Mises-Fisher kernel. Experimental results on a news-headline-generation task show that the proposed method performs better than the current ensemble methods.</p>
<p>【Keywords】:</p>
<h3 id="450. Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.">450. Automatic Pyramid Evaluation Exploiting EDU-based Extractive Reference Summaries.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1450/">Paper Link</a>】    【Pages】:4177-4186</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hirao:Tsutomu">Tsutomu Hirao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kamigaito:Hidetaka">Hidetaka Kamigaito</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nagata:Masaaki">Masaaki Nagata</a></p>
<p>【Abstract】:
This paper tackles automation of the pyramid method, a reliable manual evaluation framework. To construct a pyramid, we transform human-made reference summaries into extractive reference summaries that consist of Elementary Discourse Units (EDUs) obtained from source documents and then weight every EDU by counting the number of extractive reference summaries that contain the EDU. A summary is scored by the correspondences between EDUs in the summary and those in the pyramid. Experiments on DUC and TAC data sets show that our methods strongly correlate with various manual evaluations.</p>
<p>【Keywords】:</p>
<h3 id="451. Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks.">451. Learning to Encode Text as Human-Readable Summaries using Generative Adversarial Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1451/">Paper Link</a>】    【Pages】:4187-4195</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yau=Shian">Yau-Shian Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Hung=yi">Hung-yi Lee</a></p>
<p>【Abstract】:
Auto-encoders compress input data into a latent-space representation and reconstruct the original data from the representation. This latent representation is not easily interpreted by humans. In this paper, we propose training an auto-encoder that encodes input text into human-readable sentences, and unpaired abstractive summarization is thereby achieved. The auto-encoder is composed of a generator and a reconstructor. The generator encodes the input text into a shorter word sequence, and the reconstructor recovers the generator input from the generator output. To make the generator output human-readable, a discriminator restricts the output of the generator to resemble human-written sentences. By taking the generator output as the summary of the input text, abstractive summarization is achieved without document-summary pairs as training data. Promising results are shown on both English and Chinese corpora.</p>
<p>【Keywords】:</p>
<h3 id="452. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings.">452. Joint Multitask Learning for Community Question Answering Using Task-Specific Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1452/">Paper Link</a>】    【Pages】:4196-4207</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Joty:Shafiq_R=">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/M=agrave=rquez:Llu=iacute=s">Lluís Màrquez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nakov:Preslav">Preslav Nakov</a></p>
<p>【Abstract】:
We address jointly two important tasks for Question Answering in community forums: given a new question, (i) find related existing questions, and (ii) find relevant answers to this new question. We further use an auxiliary task to complement the previous two, i.e., (iii) find good answers with respect to the thread question in a question-comment thread. We use deep neural networks (DNNs) to learn meaningful task-specific embeddings, which we then incorporate into a conditional random field (CRF) model for the multitask setting, performing joint learning over a complex graph structure. While DNNs alone achieve competitive results when trained to produce the embeddings, the CRF, which makes use of the embeddings and the dependencies between the tasks, improves the results significantly and consistently across a variety of evaluation metrics, thus showing the complementarity of DNNs and structured learning.</p>
<p>【Keywords】:</p>
<h3 id="453. What Makes Reading Comprehension Questions Easier?">453. What Makes Reading Comprehension Questions Easier?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1453/">Paper Link</a>】    【Pages】:4208-4219</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sugawara:Saku">Saku Sugawara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inui:Kentaro">Kentaro Inui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sekine:Satoshi">Satoshi Sekine</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aizawa:Akiko">Akiko Aizawa</a></p>
<p>【Abstract】:
A challenge in creating a dataset for machine reading comprehension (MRC) is to collect questions that require a sophisticated understanding of language to answer beyond using superficial cues. In this work, we investigate what makes questions easier across recent 12 MRC datasets with three question styles (answer extraction, description, and multiple choice). We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. We then manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiple-choice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC.</p>
<p>【Keywords】:</p>
<h3 id="454. Commonsense for Generative Multi-Hop Question Answering Tasks.">454. Commonsense for Generative Multi-Hop Question Answering Tasks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1454/">Paper Link</a>】    【Pages】:4220-4230</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bauer:Lisa">Lisa Bauer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yicheng">Yicheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-finding extractive QA. We instead focus on a more challenging multi-hop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We first present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-the-art span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to fill in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model’s performance significantly (also verified via human evaluation), establishing a new state-of-the-art for the task. We also show that our background knowledge enhancements are generalizable and improve performance on QAngaroo-WikiHop, another multi-hop reasoning dataset.</p>
<p>【Keywords】:</p>
<h3 id="455. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.">455. Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1455/">Paper Link</a>】    【Pages】:4231-4242</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Haitian">Haitian Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dhingra:Bhuwan">Bhuwan Dhingra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zaheer:Manzil">Manzil Zaheer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mazaitis:Kathryn">Kathryn Mazaitis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a></p>
<p>【Abstract】:
Open Domain Question Answering (QA) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases (KBs) alone. In this paper we look at a more practical setting, namely QA over the combination of a KB and entity-linked text, which is appropriate when an incomplete KB is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, GRAFT-Net, for extracting answers from a question-specific subgraph containing text and KB entities and relations. We construct a suite of benchmark tasks for this problem, varying the difficulty of questions, the amount of training data, and KB completeness. We show that GRAFT-Net is competitive with the state-of-the-art when tested using either KBs or text alone, and vastly outperforms existing methods in the combined setting.</p>
<p>【Keywords】:</p>
<h3 id="456. A Nil-Aware Answer Extraction Framework for Question Answering.">456. A Nil-Aware Answer Extraction Framework for Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1456/">Paper Link</a>】    【Pages】:4243-4252</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kundu:Souvik">Souvik Kundu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a></p>
<p>【Abstract】:
Recently, there has been a surge of interest in reading comprehension-based (RC) question answering (QA). However, current approaches suffer from an impractical assumption that every question has a valid answer in the associated passage. A practical QA system must possess the ability to determine whether a valid answer exists in a given text passage. In this paper, we focus on developing QA systems that can extract an answer for a question if and only if the associated passage contains an answer. If the associated passage does not contain any valid answer, the QA system will correctly return Nil. We propose a novel nil-aware answer span extraction framework that is capable of returning Nil or a text span from the associated passage as an answer in a single step. We show that our proposed framework can be easily integrated with several recently proposed QA models developed for reading comprehension and can be trained in an end-to-end fashion. Our proposed nil-aware answer extraction neural network decomposes pieces of evidence into relevant and irrelevant parts and then combines them to infer the existence of any answer. Experiments on the NewsQA dataset show that the integration of our proposed framework significantly outperforms several strong baseline systems that use pipeline or threshold-based approaches.</p>
<p>【Keywords】:</p>
<h3 id="457. Exploiting Deep Representations for Neural Machine Translation.">457. Exploiting Deep Representations for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1457/">Paper Link</a>】    【Pages】:4253-4262</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dou:Zi=Yi">Zi-Yi Dou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0007:Xing">Xing Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Tong">Tong Zhang</a></p>
<p>【Abstract】:
Advanced neural machine translation (NMT) models generally implement encoder and decoder as multiple layers, which allows systems to model complex functions and capture complicated linguistic structures. However, only the top layers of encoder and decoder are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers. In this work, we propose to simultaneously expose all of these signals with layer aggregation and multi-layer attention mechanisms. In addition, we introduce an auxiliary regularization term to encourage different layers to capture diverse information. Experimental results on widely-used WMT14 English-German and WMT17 Chinese-English translation data demonstrate the effectiveness and universality of the proposed approach.</p>
<p>【Keywords】:</p>
<h3 id="458. Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.">458. Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1458/">Paper Link</a>】    【Pages】:4263-4272</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Gongbo">Gongbo Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/M=uuml=ller_0002:Mathias">Mathias Müller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rios:Annette">Annette Rios</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a></p>
<p>【Abstract】:
Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.</p>
<p>【Keywords】:</p>
<h3 id="459. Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.">459. Simplifying Neural Machine Translation with Addition-Subtraction Twin-Gated Recurrent Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1459/">Paper Link</a>】    【Pages】:4273-4283</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Biao">Biao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jinsong">Jinsong Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Qian">Qian Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Huiji">Huiji Zhang</a></p>
<p>【Abstract】:
In this paper, we propose an additionsubtraction twin-gated recurrent network (ATR) to simplify neural machine translation. The recurrent units of ATR are heavily simplified to have the smallest number of weight matrices among units of all existing gated RNNs. With the simple addition and subtraction operation, we introduce a twin-gated mechanism to build input and forget gates which are highly correlated. Despite this simplification, the essential non-linearities and capability of modeling long-distance dependencies are preserved. Additionally, the proposed ATR is more transparent than LSTM/GRU due to the simplification. Forward self-attention can be easily established in ATR, which makes the proposed network interpretable. Experiments on WMT14 translation tasks demonstrate that ATR-based neural machine translation can yield competitive performance on English-German and English-French language pairs in terms of both translation quality and speed. Further experiments on NIST Chinese-English translation, natural language inference and Chinese word segmentation verify the generality and applicability of ATR on different natural language processing tasks.</p>
<p>【Keywords】:</p>
<h3 id="460. Speeding Up Neural Machine Translation Decoding by Cube Pruning.">460. Speeding Up Neural Machine Translation Decoding by Cube Pruning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1460/">Paper Link</a>】    【Pages】:4284-4294</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Wen">Wen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang_0001:Liang">Liang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng_0004:Yang">Yang Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Lei">Lei Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a></p>
<p>【Abstract】:
Although neural machine translation has achieved promising results, it suffers from slow translation speed. The direct consequence is that a trade-off has to be made between translation quality and speed, thus its performance can not come into full play. We apply cube pruning, a popular technique to speed up dynamic programming, into neural machine translation to speed up the translation. To construct the equivalence class, similar target hidden states are combined, leading to less RNN expansion operations on the target side and less softmax operations over the large target vocabulary. The experiments show that, at the same or even better translation quality, our method can translate faster compared with naive beam search by 3.3x on GPUs and 3.5x on CPUs.</p>
<p>【Keywords】:</p>
<h3 id="461. Revisiting Character-Based Neural Machine Translation with Capacity and Compression.">461. Revisiting Character-Based Neural Machine Translation with Capacity and Compression.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1461/">Paper Link</a>】    【Pages】:4295-4305</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cherry:Colin">Colin Cherry</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Foster:George">George Foster</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bapna:Ankur">Ankur Bapna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Firat:Orhan">Orhan Firat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Macherey:Wolfgang">Wolfgang Macherey</a></p>
<p>【Abstract】:
Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.</p>
<p>【Keywords】:</p>
<h3 id="462. A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation.">462. A Skeleton-Based Model for Promoting Coherence Among Sentences in Narrative Story Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1462/">Paper Link</a>】    【Pages】:4306-4315</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingjing">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0050:Yi">Yi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Qi">Qi Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Xiaoyan">Xiaoyan Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
Narrative story generation is a challenging problem because it demands the generated sentences with tight semantic connections, which has not been well studied by most existing generative models. To address this problem, we propose a skeleton-based model to promote the coherence of generated stories. Different from traditional models that generate a complete sentence at a stroke, the proposed model first generates the most critical phrases, called skeleton, and then expands the skeleton to a complete and fluent sentence. The skeleton is not manually defined, but learned by a reinforcement learning method. Compared to the state-of-the-art models, our skeleton-based model can generate significantly more coherent text according to human evaluation and automatic evaluation. The G-score is improved by 20.1% in human evaluation.</p>
<p>【Keywords】:</p>
<h3 id="463. Nexus Network: Connecting the Preceding and the Following in Dialogue Generation.">463. Nexus Network: Connecting the Preceding and the Following in Dialogue Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1463/">Paper Link</a>】    【Pages】:4316-4327</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen_0001:Xiaoyu">Xiaoyu Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Hui">Hui Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klakow:Dietrich">Dietrich Klakow</a></p>
<p>【Abstract】:
Sequence-to-Sequence (seq2seq) models have become overwhelmingly popular in building end-to-end trainable dialogue systems. Though highly efficient in learning the backbone of human-computer communications, they suffer from the problem of strongly favoring short generic responses. In this paper, we argue that a good response should smoothly connect both the preceding dialogue history and the following conversations. We strengthen this connection by mutual information maximization. To sidestep the non-differentiability of discrete natural language tokens, we introduce an auxiliary continuous code space and map such code space to a learnable prior distribution for generation purpose. Experiments on two dialogue datasets validate the effectiveness of our model, where the generated responses are closely related to the dialogue context and lead to more interactive conversations.</p>
<p>【Keywords】:</p>
<h3 id="464. A Neural Local Coherence Model for Text Quality Assessment.">464. A Neural Local Coherence Model for Text Quality Assessment.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1464/">Paper Link</a>】    【Pages】:4328-4339</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mesgar:Mohsen">Mohsen Mesgar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Strube_0001:Michael">Michael Strube</a></p>
<p>【Abstract】:
We propose a local coherence model that captures the flow of what semantically connects adjacent sentences in a text. We represent the semantics of a sentence by a vector and capture its state at each word of the sentence. We model what relates two adjacent sentences based on the two most similar semantic states, each of which is in one of the sentences. We encode the perceived coherence of a text by a vector, which represents patterns of changes in salient information that relates adjacent sentences. Our experiments demonstrate that our approach is beneficial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other task-dependent features significantly improves the performance of a strong essay scorer.</p>
<p>【Keywords】:</p>
<h3 id="465. Deep Attentive Sentence Ordering Network.">465. Deep Attentive Sentence Ordering Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1465/">Paper Link</a>】    【Pages】:4340-4349</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cui:Baiyun">Baiyun Cui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yingming">Yingming Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Ming">Ming Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhongfei">Zhongfei Zhang</a></p>
<p>【Abstract】:
In this paper, we propose a novel deep attentive sentence ordering network (referred as ATTOrderNet) which integrates self-attention mechanism with LSTMs in the encoding of input sentences. It enables us to capture global dependencies among sentences regardless of their input order and obtains a reliable representation of the sentence set. With this representation, a pointer network is exploited to generate an ordered sequence. The proposed model is evaluated on Sentence Ordering and Order Discrimination tasks. The extensive experimental results demonstrate its effectiveness and superiority to the state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="466. Getting to "Hearer-old": Charting Referring Expressions Across Time.">466. Getting to "Hearer-old": Charting Referring Expressions Across Time.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1466/">Paper Link</a>】    【Pages】:4350-4359</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Staliunaite:Ieva">Ieva Staliunaite</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rohde:Hannah">Hannah Rohde</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Webber:Bonnie_L=">Bonnie L. Webber</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Louis:Annie">Annie Louis</a></p>
<p>【Abstract】:
When a reader is first introduced to an entity, its referring expression must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how expressions that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a model to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30% absolute improvement over a majority-class baseline.</p>
<p>【Keywords】:</p>
<h3 id="467. Making "fetch" happen: The influence of social and linguistic context on nonstandard word growth and decline.">467. Making "fetch" happen: The influence of social and linguistic context on nonstandard word growth and decline.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1467/">Paper Link</a>】    【Pages】:4360-4370</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Stewart:Ian">Ian Stewart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Eisenstein:Jacob">Jacob Eisenstein</a></p>
<p>【Abstract】:
In an online community, new words come and go: today’s “haha” may be replaced by tomorrow’s “lol.” Changes in online writing are usually studied as a social process, with innovations diffusing through a network of individuals in a speech community. But unlike other types of innovation, language change is shaped and constrained by the grammatical system in which it takes part. To investigate the role of social and structural factors in language change, we undertake a large-scale analysis of the frequencies of non-standard words in Reddit. Dissemination across many linguistic contexts is a predictor of success: words that appear in more linguistic contexts grow faster and survive longer. Furthermore, social dissemination plays a less important role in explaining word growth and decline than previously hypothesized.</p>
<p>【Keywords】:</p>
<h3 id="468. Analyzing Correlated Evolution of Multiple Features Using Latent Representations.">468. Analyzing Correlated Evolution of Multiple Features Using Latent Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1468/">Paper Link</a>】    【Pages】:4371-4382</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Murawaki:Yugo">Yugo Murawaki</a></p>
<p>【Abstract】:
Statistical phylogenetic models have allowed the quantitative analysis of the evolution of a single categorical feature and a pair of binary features, but correlated evolution involving multiple discrete features is yet to be explored. Here we propose latent representation-based analysis in which (1) a sequence of discrete surface features is projected to a sequence of independent binary variables and (2) phylogenetic inference is performed on the latent space. In the experiments, we analyze the features of linguistic typology, with a special focus on the order of subject, object and verb. Our analysis suggests that languages sharing the same word order are not necessarily a coherent group but exhibit varying degrees of diachronic stability depending on other features.</p>
<p>【Keywords】:</p>
<h3 id="469. Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting.">469. Capturing Regional Variation with Distributed Place Representations and Geographic Retrofitting.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1469/">Paper Link</a>】    【Pages】:4383-4394</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Dirk">Dirk Hovy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Purschke:Christoph">Christoph Purschke</a></p>
<p>【Abstract】:
Dialects are one of the main drivers of language variation, a major challenge for natural language processing tools. In most languages, dialects exist along a continuum, and are commonly discretized by combining the extent of several preselected linguistic variables. However, the selection of these variables is theory-driven and itself insensitive to change. We use Doc2Vec on a corpus of 16.8M anonymous online posts in the German-speaking area to learn continuous document representations of cities. These representations capture continuous regional linguistic distinctions, and can serve as input to downstream NLP tasks sensitive to regional variation. By incorporating geographic information via retrofitting and agglomerative clustering with structure, we recover dialect areas at various levels of granularity. Evaluating these clusters against an existing dialect map, we achieve a match of up to 0.77 V-score (harmonic mean of cluster completeness and homogeneity). Our results show that representation learning with retrofitting offers a robust general method to automatically expose dialectal differences and regional variation at a finer granularity than was previously possible.</p>
<p>【Keywords】:</p>
<h3 id="470. Characterizing Interactions and Relationships between People.">470. Characterizing Interactions and Relationships between People.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1470/">Paper Link</a>】    【Pages】:4395-4404</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rashid:Farzana">Farzana Rashid</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blanco_0002:Eduardo">Eduardo Blanco</a></p>
<p>【Abstract】:
This paper presents a set of dimensions to characterize the association between two people. We distinguish between interactions (when somebody refers to somebody in a conversation) and relationships (a sequence of interactions). We work with dialogue scripts from the TV show Friends, and do not impose any restrictions on the interactions and relationships. We introduce and analyze a new corpus, and present experimental results showing that the task can be automated.</p>
<p>【Keywords】:</p>
<h3 id="471. Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions.">471. Why Swear? Analyzing and Inferring the Intentions of Vulgar Expressions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1471/">Paper Link</a>】    【Pages】:4405-4414</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Holgate:Eric">Eric Holgate</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cachola:Isabel">Isabel Cachola</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Preotiuc=Pietro:Daniel">Daniel Preotiuc-Pietro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junyi_Jessy">Junyi Jessy Li</a></p>
<p>【Abstract】:
Vulgar words are employed in language use for several different functions, ranging from expressing aggression to signaling group identity or the informality of the communication. This versatility of usage of a restricted set of words is challenging for downstream applications and has yet to be studied quantitatively or using natural language processing techniques. We introduce a novel data set of 7,800 tweets from users with known demographic traits where all instances of vulgar words are annotated with one of the six categories of vulgar word use. Using this data set, we present the first analysis of the pragmatic aspects of vulgarity and how they relate to social factors. We build a model able to predict the category of a vulgar word based on the immediate context it appears in with 67.4 macro F1 across six classes. Finally, we demonstrate the utility of modeling the type of vulgar word use in context by using this information to achieve state-of-the-art performance in hate speech detection on a benchmark data set.</p>
<p>【Keywords】:</p>
<h3 id="472. Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks.">472. Is it Time to Swish? Comparing Deep Learning Activation Functions Across NLP tasks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1472/">Paper Link</a>】    【Pages】:4415-4424</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Eger:Steffen">Steffen Eger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Youssef:Paul">Paul Youssef</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurevych:Iryna">Iryna Gurevych</a></p>
<p>【Abstract】:
Activation functions play a crucial role in neural networks because they are the nonlinearities which have been attributed to the success story of deep learning. One of the currently most popular activation functions is ReLU, but several competitors have recently been proposed or ‘discovered’, including LReLU functions and swish. While most works compare newly proposed activation functions on few tasks (usually from image classification) and against few competitors (usually ReLU), we perform the first largescale comparison of 21 activation functions across eight different NLP tasks. We find that a largely unknown activation function performs most stably across all tasks, the so-called penalized tanh function. We also show that it can successfully replace the sigmoid and tanh gates in LSTM cells, leading to a 2 percentage point (pp) improvement over the standard choices on a challenging NLP task.</p>
<p>【Keywords】:</p>
<h3 id="473. Hard Non-Monotonic Attention for Character-Level Transduction.">473. Hard Non-Monotonic Attention for Character-Level Transduction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1473/">Paper Link</a>】    【Pages】:4425-4438</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Shijie">Shijie Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shapiro:Pamela">Pamela Shapiro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cotterell:Ryan">Ryan Cotterell</a></p>
<p>【Abstract】:
Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.</p>
<p>【Keywords】:</p>
<h3 id="474. Speed Reading: Learning to Read ForBackward via Shuttle.">474. Speed Reading: Learning to Read ForBackward via Shuttle.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1474/">Paper Link</a>】    【Pages】:4439-4448</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Tsu=Jui">Tsu-Jui Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Wei=Yun">Wei-Yun Ma</a></p>
<p>【Abstract】:
We present LSTM-Shuttle, which applies human speed reading techniques to natural language processing tasks for accurate and efficient comprehension. In contrast to previous work, LSTM-Shuttle not only reads shuttling forward but also goes back. Shuttling forward enables high efficiency, and going backward gives the model a chance to recover lost information, ensuring better prediction. We evaluate LSTM-Shuttle on sentiment analysis, news classification, and cloze on IMDB, Rotten Tomatoes, AG, and Children’s Book Test datasets. We show that LSTM-Shuttle predicts both better and more quickly. To demonstrate how LSTM-Shuttle actually behaves, we also analyze the shuttling operation and present a case study.</p>
<p>【Keywords】:</p>
<h3 id="475. Modeling Localness for Self-Attention Networks.">475. Modeling Localness for Self-Attention Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1475/">Paper Link</a>】    【Pages】:4449-4458</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Baosong">Baosong Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Derek_F=">Derek F. Wong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Fandong">Fandong Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chao:Lidia_S=">Lidia S. Chao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Tong">Tong Zhang</a></p>
<p>【Abstract】:
Self-attention networks have proven to be of profound value for its strength of capturing global dependencies. In this work, we propose to model localness for self-attention networks, which enhances the ability of capturing useful local context. We cast localness modeling as a learnable Gaussian bias, which indicates the central and scope of the local region to be paid more attention. The bias is then incorporated into the original attention distribution to form a revised distribution. To maintain the strength of capturing long distance dependencies while enhance the ability of capturing short-range dependencies, we only apply localness modeling to lower layers of self-attention networks. Quantitative and qualitative analyses on Chinese-English and English-German translation tasks demonstrate the effectiveness and universality of the proposed approach.</p>
<p>【Keywords】:</p>
<h3 id="476. Chargrid: Towards Understanding 2D Documents.">476. Chargrid: Towards Understanding 2D Documents.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1476/">Paper Link</a>】    【Pages】:4459-4469</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Katti:Anoop_R=">Anoop R. Katti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reisswig:Christian">Christian Reisswig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guder:Cordula">Cordula Guder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brarda:Sebastian">Sebastian Brarda</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bickel:Steffen">Steffen Bickel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/H=ouml=hne:Johannes">Johannes Höhne</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Faddoul:Jean_Baptiste">Jean Baptiste Faddoul</a></p>
<p>【Abstract】:
We introduce a novel type of text representation that preserves the 2D layout of a document. This is achieved by encoding each document page as a two-dimensional grid of characters. Based on this representation, we present a generic document understanding pipeline for structured documents. This pipeline makes use of a fully convolutional encoder-decoder network that predicts a segmentation mask and bounding boxes. We demonstrate its capabilities on an information extraction task from invoices and show that it significantly outperforms approaches based on sequential text or document images.</p>
<p>【Keywords】:</p>
<h3 id="477. Simple Recurrent Units for Highly Parallelizable Recurrence.">477. Simple Recurrent Units for Highly Parallelizable Recurrence.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1477/">Paper Link</a>】    【Pages】:4470-4481</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Tao">Tao Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yu">Yu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Sida_I=">Sida I. Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Hui">Hui Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Artzi:Yoav">Yoav Artzi</a></p>
<p>【Abstract】:
Common recurrent neural architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU), a light recurrent unit that balances model capacity and scalability. SRU is designed to provide expressive recurrence, enable highly parallelized implementation, and comes with careful initialization to facilitate training of deep models. We demonstrate the effectiveness of SRU on multiple NLP tasks. SRU achieves 5—9x speed-up over cuDNN-optimized LSTM on classification and question answering datasets, and delivers stronger results than LSTM and convolutional models. We also obtain an average of 0.7 BLEU improvement over the Transformer model (Vaswani et al., 2017) on translation by incorporating SRU into the architecture.</p>
<p>【Keywords】:</p>
<h3 id="478. NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval.">478. NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1478/">Paper Link</a>】    【Pages】:4482-4491</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Canjia">Canjia Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Yingfei">Yingfei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Le">Le Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Kai">Kai Hui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yates:Andrew">Andrew Yates</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jungang">Jungang Xu</a></p>
<p>【Abstract】:
Pseudo relevance feedback (PRF) is commonly used to boost the performance of traditional information retrieval (IR) models by using top-ranked documents to identify and weight new query terms, thereby reducing the effect of query-document vocabulary mismatches. While neural retrieval models have recently demonstrated strong results for ad-hoc retrieval, combining them with PRF is not straightforward due to incompatibilities between existing PRF approaches and neural architectures. To bridge this gap, we propose an end-to-end neural PRF framework that can be used with existing neural IR models by embedding different neural models as building blocks. Extensive experiments on two standard test collections confirm the effectiveness of the proposed NPRF framework in improving the performance of two state-of-the-art neural IR models.</p>
<p>【Keywords】:</p>
<h3 id="479. Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences.">479. Co-Stack Residual Affinity Networks with Multi-level Attention Refinement for Matching Text Sequences.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1479/">Paper Link</a>】    【Pages】:4492-4502</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tay:Yi">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luu:Anh_Tuan">Anh Tuan Luu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a></p>
<p>【Abstract】:
Learning a matching function between two text sequences is a long standing problem in NLP research. This task enables many potential applications such as question answering and paraphrase identification. This paper proposes Co-Stack Residual Affinity Networks (CSRAN), a new and universal neural architecture for this problem. CSRAN is a deep architecture, involving stacked (multi-layered) recurrent encoders. Stacked/Deep architectures are traditionally difficult to train, due to the inherent weaknesses such as difficulty with feature propagation and vanishing gradients. CSRAN incorporates two novel components to take advantage of the stacked architecture. Firstly, it introduces a new bidirectional alignment mechanism that learns affinity weights by fusing sequence pairs across stacked hierarchies. Secondly, it leverages a multi-level attention refinement component between stacked recurrent layers. The key intuition is that, by leveraging information across all network hierarchies, we can not only improve gradient flow but also improve overall performance. We conduct extensive experiments on six well-studied text sequence matching datasets, achieving state-of-the-art performance on all.</p>
<p>【Keywords】:</p>
<h3 id="480. Spherical Latent Spaces for Stable Variational Autoencoders.">480. Spherical Latent Spaces for Stable Variational Autoencoders.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1480/">Paper Link</a>】    【Pages】:4503-4513</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jiacheng">Jiacheng Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durrett:Greg">Greg Durrett</a></p>
<p>【Abstract】:
A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of “collapse” which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.</p>
<p>【Keywords】:</p>
<h3 id="481. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder.">481. Learning Universal Sentence Representations with Mean-Max Attention Autoencoder.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1481/">Paper Link</a>】    【Pages】:4514-4523</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Minghua">Minghua Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yunfang">Yunfang Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Weikang">Weikang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0101:Wei">Wei Li</a></p>
<p>【Abstract】:
In order to learn universal sentence representations, previous methods focus on complex recurrent neural networks or supervised learning. In this paper, we propose a mean-max attention autoencoder (mean-max AAE) within the encoder-decoder framework. Our autoencoder rely entirely on the MultiHead self-attention mechanism to reconstruct the input sequence. In the encoding we propose a mean-max strategy that applies both mean and max pooling operations over the hidden vectors to capture diverse information of the input. To enable the information to steer the reconstruction process dynamically, the decoder performs attention over the mean-max representation. By training our model on a large collection of unlabelled data, we obtain high-quality representations of sentences. Experimental results on a broad range of 10 transfer tasks demonstrate that our model outperforms the state-of-the-art unsupervised single methods, including the classical skip-thoughts and the advanced skip-thoughts+LN model. Furthermore, compared with the traditional recurrent neural network, our mean-max AAE greatly reduce the training time.</p>
<p>【Keywords】:</p>
<h3 id="482. Word Mover's Embedding: From Word2Vec to Document Embedding.">482. Word Mover's Embedding: From Word2Vec to Document Embedding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1482/">Paper Link</a>】    【Pages】:4524-4534</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Lingfei">Lingfei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yen:Ian_En=Hsu">Ian En-Hsu Yen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Kun">Kun Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Fangli">Fangli Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Balakrishnan:Avinash">Avinash Balakrishnan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Pin=Yu">Pin-Yu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ravikumar:Pradeep">Pradeep Ravikumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Witbrock:Michael_J=">Michael J. Witbrock</a></p>
<p>【Abstract】:
While the celebrated Word2Vec technique yields semantically rich representations for individual words, there has been relatively less success in extending to generate unsupervised sentences or documents embeddings. Recent work has demonstrated that a distance measure between documents called Word Mover’s Distance (WMD) that aligns semantically similar words, yields unprecedented KNN classification accuracy. However, WMD is expensive to compute, and it is hard to extend its use beyond a KNN classifier. In this paper, we propose the Word Mover’s Embedding (WME), a novel approach to building an unsupervised document (sentence) embedding from pre-trained word embeddings. In our experiments on 9 benchmark text classification datasets and 22 textual similarity tasks, the proposed technique consistently matches or outperforms state-of-the-art techniques, with significantly higher accuracy on problems of short length.</p>
<p>【Keywords】:</p>
<h3 id="483. Multilingual Clustering of Streaming News.">483. Multilingual Clustering of Streaming News.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1483/">Paper Link</a>】    【Pages】:4535-4544</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Miranda:Sebasti=atilde=o">Sebastião Miranda</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Znotins:Arturs">Arturs Znotins</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barzdins:Guntis">Guntis Barzdins</a></p>
<p>【Abstract】:
Clustering news across languages enables efficient media monitoring by aggregating articles from multilingual sources into coherent stories. Doing so in an online setting allows scalable processing of massive news streams. To this end, we describe a novel method for clustering an incoming stream of multilingual documents into monolingual and crosslingual clusters. Unlike typical clustering approaches that report results on datasets with a small and known number of labels, we tackle the problem of discovering an ever growing number of cluster labels in an online fashion, using real news datasets in multiple languages. In our formulation, the monolingual clusters group together documents while the crosslingual clusters group together monolingual clusters, one per language that appears in the stream. Our method is simple to implement, computationally efficient and produces state-of-the-art results on datasets in German, English and Spanish.</p>
<p>【Keywords】:</p>
<h3 id="484. Multi-Task Label Embedding for Text Classification.">484. Multi-Task Label Embedding for Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1484/">Paper Link</a>】    【Pages】:4545-4553</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Honglun">Honglun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Liqiang">Liqiang Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenqing">Wenqing Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yongkun">Yongkun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Yaohui">Yaohui Jin</a></p>
<p>【Abstract】:
Multi-task learning in text classification leverages implicit correlations among related tasks to extract common features and yield performance gains. However, a large body of previous work treats labels of each task as independent and meaningless one-hot vectors, which cause a loss of potential label information. In this paper, we propose Multi-Task Label Embedding to convert labels in text classification into semantic vectors, thereby turning the original tasks into vector matching tasks. Our model utilizes semantic correlations among tasks and makes it convenient to scale or transfer when new tasks are involved. Extensive experiments on five benchmark datasets for text classification show that our model can effectively improve the performances of related tasks with semantic representations of labels and additional information from each other.</p>
<p>【Keywords】:</p>
<h3 id="485. Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification.">485. Semantic-Unit-Based Dilated Convolution for Multi-Label Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1485/">Paper Link</a>】    【Pages】:4554-4564</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Junyang">Junyang Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Qi">Qi Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Pengcheng">Pengcheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Shuming">Shuming Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a></p>
<p>【Abstract】:
We propose a novel model for multi-label text classification, which is based on sequence-to-sequence learning. The model generates higher-level semantic unit representations with multi-level dilated convolution as well as a corresponding hybrid attention mechanism that extracts both the information at the word-level and the level of the semantic unit. Our designed dilated convolution effectively reduces dimension and supports an exponential expansion of receptive fields without loss of local information, and the attention-over-attention mechanism is able to capture more summary relevant information from the source context. Results of our experiments show that the proposed model has significant advantages over the baseline models on the dataset RCV1-V2 and Ren-CECps, and our analysis demonstrates that our model is competitive to the deterministic hierarchical models and it is more robust to classifying low-frequency labels</p>
<p>【Keywords】:</p>
<h3 id="486. MCapsNet: Capsule Network for Text with Multi-Task Learning.">486. MCapsNet: Capsule Network for Text with Multi-Task Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1486/">Paper Link</a>】    【Pages】:4565-4574</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Liqiang">Liqiang Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Honglun">Honglun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenqing">Wenqing Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yongkun">Yongkun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Yaohui">Yaohui Jin</a></p>
<p>【Abstract】:
Multi-task learning has an ability to share the knowledge among related tasks and implicitly increase the training data. However, it has long been frustrated by the interference among tasks. This paper investigates the performance of capsule network for text, and proposes a capsule-based multi-task learning architecture, which is unified, simple and effective. With the advantages of capsules for feature clustering, proposed task routing algorithm can cluster the features for each task in the network, which helps reduce the interference among tasks. Experiments on six text classification datasets demonstrate the effectiveness of our models and their characteristics for feature clustering.</p>
<p>【Keywords】:</p>
<h3 id="487. Uncertainty-aware generative models for inferring document class prevalence.">487. Uncertainty-aware generative models for inferring document class prevalence.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1487/">Paper Link</a>】    【Pages】:4575-4585</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Keith:Katherine_A=">Katherine A. Keith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/O=Connor:Brendan">Brendan O&apos;Connor</a></p>
<p>【Abstract】:
Prevalence estimation is the task of inferring the relative frequency of classes of unlabeled examples in a group—for example, the proportion of a document collection with positive sentiment. Previous work has focused on aggregating and adjusting discriminative individual classifiers to obtain prevalence point estimates. But imperfect classifier accuracy ought to be reflected in uncertainty over the predicted prevalence for scientifically valid inference. In this work, we present (1) a generative probabilistic modeling approach to prevalence estimation, and (2) the construction and evaluation of prevalence confidence intervals; in particular, we demonstrate that an off-the-shelf discriminative classifier can be given a generative re-interpretation, by backing out an implicit individual-level likelihood function, which can be used to conduct fast and simple group-level Bayesian inference. Empirically, we demonstrate our approach provides better confidence interval coverage than an alternative, and is dramatically more robust to shifts in the class prior between training and testing.</p>
<p>【Keywords】:</p>
<h3 id="488. Challenges of Using Text Classifiers for Causal Inference.">488. Challenges of Using Text Classifiers for Causal Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1488/">Paper Link</a>】    【Pages】:4586-4598</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wood=Doughty:Zach">Zach Wood-Doughty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shpitser:Ilya">Ilya Shpitser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dredze:Mark">Mark Dredze</a></p>
<p>【Abstract】:
Causal understanding is essential for many kinds of decision-making, but causal inference from observational data has typically only been applied to structured, low-dimensional datasets. While text classifiers produce low-dimensional outputs, their use in causal inference has not previously been studied. To facilitate causal analyses based on language data, we consider the role that text classifiers can play in causal inference through established modeling mechanisms from the causality literature on missing data and measurement error. We demonstrate how to conduct causal analyses using text classifiers on simulated and Yelp data, and discuss the opportunities and challenges of future work that uses text data in causal inference.</p>
<p>【Keywords】:</p>
<h3 id="489. Direct Output Connection for a High-Rank Language Model.">489. Direct Output Connection for a High-Rank Language Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1489/">Paper Link</a>】    【Pages】:4599-4609</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Takase:Sho">Sho Takase</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Suzuki:Jun">Jun Suzuki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nagata:Masaaki">Masaaki Nagata</a></p>
<p>【Abstract】:
This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation.</p>
<p>【Keywords】:</p>
<h3 id="490. Disfluency Detection using Auto-Correlational Neural Networks.">490. Disfluency Detection using Auto-Correlational Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1490/">Paper Link</a>】    【Pages】:4610-4619</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lou:Paria_Jamshid">Paria Jamshid Lou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Anderson_0001:Peter">Peter Anderson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Johnson_0001:Mark">Mark Johnson</a></p>
<p>【Abstract】:
In recent years, the natural language processing community has moved away from task-specific feature engineering, i.e., researchers discovering ad-hoc feature representations for various tasks, in favor of general-purpose methods that learn the input representation by themselves. However, state-of-the-art approaches to disfluency detection in spontaneous speech transcripts currently still depend on an array of hand-crafted features, and other representations derived from the output of pre-existing systems such as language models or dependency parsers. As an alternative, this paper proposes a simple yet effective model for automatic disfluency detection, called an auto-correlational neural network (ACNN). The model uses a convolutional neural network (CNN) and augments it with a new auto-correlation operator at the lowest layer that can capture the kinds of “rough copy” dependencies that are characteristic of repair disfluencies in speech. In experiments, the ACNN model outperforms the baseline CNN on a disfluency detection task with a 5% increase in f-score, which is close to the previous best result on this task.</p>
<p>【Keywords】:</p>
<h3 id="491. Pyramidal Recurrent Unit for Language Modeling.">491. Pyramidal Recurrent Unit for Language Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1491/">Paper Link</a>】    【Pages】:4620-4630</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mehta:Sachin">Sachin Mehta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koncel=Kedziorski:Rik">Rik Koncel-Kedziorski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rastegari:Mohammad">Mohammad Rastegari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajishirzi:Hannaneh">Hannaneh Hajishirzi</a></p>
<p>【Abstract】:
LSTMs are powerful tools for modeling contextual information, as evidenced by their success at the task of language modeling. However, modeling contexts in very high dimensional space can lead to poor generalizability. We introduce the Pyramidal Recurrent Unit (PRU), which enables learning representations in high dimensional space with more generalization power and fewer parameters. PRUs replace the linear transformation in LSTMs with more sophisticated interactions such as pyramidal or grouped linear transformations. This architecture gives strong results on word-level language modeling while reducing parameters significantly. In particular, PRU improves the perplexity of a recent state-of-the-art language model by up to 1.3 points while learning 15-20% fewer parameters. For similar number of model parameters, PRU outperforms all previous RNN models that exploit different gating mechanisms and transformations. We provide a detailed examination of the PRU and its behavior on the language modeling tasks. Our code is open-source and available at <a href="https://sacmehta.github.io/PRU/">https://sacmehta.github.io/PRU/</a>.</p>
<p>【Keywords】:</p>
<h3 id="492. On Tree-Based Neural Sentence Modeling.">492. On Tree-Based Neural Sentence Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1492/">Paper Link</a>】    【Pages】:4631-4641</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Haoyue">Haoyue Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Hao">Hao Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jiaze">Jiaze Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a></p>
<p>【Abstract】:
Neural networks with tree-based sentence encoders have shown better results on many downstream tasks. Most of existing tree-based encoders adopt syntactic parsing trees as the explicit structure prior. To study the effectiveness of different tree structures, we replace the parsing trees with trivial trees (i.e., binary balanced tree, left-branching tree and right-branching tree) in the encoders. Though trivial trees contain no syntactic information, those encoders get competitive or even better results on all of the ten downstream tasks we investigated. This surprising result indicates that explicit syntax guidance may not be the main contributor to the superior performances of tree-based neural sentence modeling. Further analysis show that tree modeling gives better results when crucial words are closer to the final representation. Additional experiments give more clues on how to design an effective tree-based encoder. Our code is open-source and available at <a href="https://github.com/ExplorerFreda/TreeEnc">https://github.com/ExplorerFreda/TreeEnc</a>.</p>
<p>【Keywords】:</p>
<h3 id="493. Language Modeling with Sparse Product of Sememe Experts.">493. Language Modeling with Sparse Product of Sememe Experts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1493/">Paper Link</a>】    【Pages】:4642-4651</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Yihong">Yihong Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Jun">Jun Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hao">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Ruobing">Ruobing Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Fen">Fen Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Leyu">Leyu Lin</a></p>
<p>【Abstract】:
Most language modeling methods rely on large-scale data to statistically learn the sequential patterns of words. In this paper, we argue that words are atomic language units but not necessarily atomic semantic units. Inspired by HowNet, we use sememes, the minimum semantic units in human languages, to represent the implicit semantics behind words for language modeling, named Sememe-Driven Language Model (SDLM). More specifically, to predict the next word, SDLM first estimates the sememe distribution given textual context. Afterwards, it regards each sememe as a distinct semantic expert, and these experts jointly identify the most probable senses and the corresponding word. In this way, SDLM enables language models to work beyond word-level manipulation to fine-grained sememe-level semantics, and offers us more powerful tools to fine-tune language models and improve the interpretability as well as the robustness of language models. Experiments on language modeling and the downstream application of headline generation demonstrate the significant effectiveness of SDLM.</p>
<p>【Keywords】:</p>
<h3 id="494. Siamese Network-Based Supervised Topic Modeling.">494. Siamese Network-Based Supervised Topic Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1494/">Paper Link</a>】    【Pages】:4652-4662</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Minghui">Minghui Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rao:Yanghui">Yanghui Rao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yuwei">Yuwei Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie_0001:Haoran">Haoran Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Fu_Lee">Fu Lee Wang</a></p>
<p>【Abstract】:
Label-specific topics can be widely used for supporting personality psychology, aspect-level sentiment analysis, and cross-domain sentiment classification. To generate label-specific topics, several supervised topic models which adopt likelihood-driven objective functions have been proposed. However, it is hard for them to get a precise estimation on both topic discovery and supervised learning. In this study, we propose a supervised topic model based on the Siamese network, which can trade off label-specific word distributions with document-specific label distributions in a uniform framework. Experiments on real-world datasets validate that our model performs competitive in topic discovery quantitatively and qualitatively. Furthermore, the proposed model can effectively predict categorical or real-valued labels for new documents by generating word embeddings from a label-specific topical space.</p>
<p>【Keywords】:</p>
<h3 id="495. GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model.">495. GraphBTM: Graph Enhanced Autoencoded Variational Inference for Biterm Topic Model.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1495/">Paper Link</a>】    【Pages】:4663-4672</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qile">Qile Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Zheng">Zheng Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0001:Xiaolin">Xiaolin Li</a></p>
<p>【Abstract】:
Discovering the latent topics within texts has been a fundamental task for many applications. However, conventional topic models suffer different problems in different settings. The Latent Dirichlet Allocation (LDA) may not work well for short texts due to the data sparsity (i.e. the sparse word co-occurrence patterns in short documents). The Biterm Topic Model (BTM) learns topics by modeling the word-pairs named biterms in the whole corpus. This assumption is very strong when documents are long with rich topic information and do not exhibit the transitivity of biterms. In this paper, we propose a novel way called GraphBTM to represent biterms as graphs and design a Graph Convolutional Networks (GCNs) with residual connections to extract transitive features from biterms. To overcome the data sparsity of LDA and the strong assumption of BTM, we sample a fixed number of documents to form a mini-corpus as a sample. We also propose a dataset called All News extracted from 15 news publishers, in which documents are much longer than 20 Newsgroups. We present an amortized variational inference method for GraphBTM. Our method generates more coherent topics compared with previous approaches. Experiments show that the sampling strategy improves performance by a large margin.</p>
<p>【Keywords】:</p>
<h3 id="496. Modeling Online Discourse with Coupled Distributed Topics.">496. Modeling Online Discourse with Coupled Distributed Topics.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1496/">Paper Link</a>】    【Pages】:4673-4682</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Srivatsan:Akshay">Akshay Srivatsan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wojtowicz:Zachary">Zachary Wojtowicz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg=Kirkpatrick:Taylor">Taylor Berg-Kirkpatrick</a></p>
<p>【Abstract】:
In this paper, we propose a deep, globally normalized topic model that incorporates structural relationships connecting documents in socially generated corpora, such as online forums. Our model (1) captures discursive interactions along observed reply links in addition to traditional topic information, and (2) incorporates latent distributed representations arranged in a deep architecture, which enables a GPU-based mean-field inference procedure that scales efficiently to large data. We apply our model to a new social media dataset consisting of 13M comments mined from the popular internet forum Reddit, a domain that poses significant challenges to models that do not account for relationships connecting user comments. We evaluate against existing methods across multiple metrics including perplexity and metadata prediction, and qualitatively analyze the learned interaction patterns.</p>
<p>【Keywords】:</p>
<h3 id="497. Learning Disentangled Representations of Texts with Application to Biomedical Abstracts.">497. Learning Disentangled Representations of Texts with Application to Biomedical Abstracts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1497/">Paper Link</a>】    【Pages】:4683-4693</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jain:Sarthak">Sarthak Jain</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Banner:Edward">Edward Banner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meent:Jan=Willem_van_de">Jan-Willem van de Meent</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marshall:Iain_James">Iain James Marshall</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wallace:Byron_C=">Byron C. Wallace</a></p>
<p>【Abstract】:
We propose a method for learning disentangled representations of texts that code for distinct and complementary aspects, with the aim of affording efficient model transfer and interpretability. To induce disentangled embeddings, we propose an adversarial objective based on the (dis)similarity between triplets of documents with respect to specific aspects. Our motivating application is embedding biomedical abstracts describing clinical trials in a manner that disentangles the populations, interventions, and outcomes in a given trial. We show that our method learns representations that encode these clinically salient aspects, and that these can be effectively used to perform aspect-specific retrieval. We demonstrate that the approach generalizes beyond our motivating application in experiments on two multi-aspect review corpora.</p>
<p>【Keywords】:</p>
<h3 id="498. Multi-Source Domain Adaptation with Mixture of Experts.">498. Multi-Source Domain Adaptation with Mixture of Experts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1498/">Paper Link</a>】    【Pages】:4694-4703</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Jiang">Jiang Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shah:Darsh_J=">Darsh J. Shah</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barzilay:Regina">Regina Barzilay</a></p>
<p>【Abstract】:
We propose a mixture-of-experts approach for unsupervised domain adaptation from multiple sources. The key idea is to explicitly capture the relationship between a target example and different source domains. This relationship, expressed by a point-to-set metric, determines how to combine predictors trained on various domains. The metric is learned in an unsupervised fashion using meta-training. Experimental results on sentiment analysis and part-of-speech tagging demonstrate that our approach consistently outperforms multiple baselines and can robustly handle negative transfer.</p>
<p>【Keywords】:</p>
<h3 id="499. A Neural Model of Adaptation in Reading.">499. A Neural Model of Adaptation in Reading.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1499/">Paper Link</a>】    【Pages】:4704-4710</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schijndel:Marten_Van">Marten Van Schijndel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Linzen:Tal">Tal Linzen</a></p>
<p>【Abstract】:
It has been argued that humans rapidly adapt their lexical and syntactic expectations to match the statistics of the current linguistic context. We provide further support to this claim by showing that the addition of a simple adaptation mechanism to a neural language model improves our predictions of human reading times compared to a non-adaptive model. We analyze the performance of the model on controlled materials from psycholinguistic experiments and show that it adapts not only to lexical items but also to abstract syntactic structures.</p>
<p>【Keywords】:</p>
<h3 id="500. Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study.">500. Understanding Deep Learning Performance through an Examination of Test Set Difficulty: A Psychometric Case Study.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1500/">Paper Link</a>】    【Pages】:4711-4716</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lalor:John">John Lalor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Hao">Hao Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Munkhdalai:Tsendsuren">Tsendsuren Munkhdalai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Hong">Hong Yu</a></p>
<p>【Abstract】:
Interpreting the performance of deep learning models beyond test set accuracy is challenging. Characteristics of individual data points are often not considered during evaluation, and each data point is treated equally. In this work we examine the impact of a test set question’s difficulty to determine if there is a relationship between difficulty and performance. We model difficulty using well-studied psychometric methods on human response patterns. Experiments on Natural Language Inference (NLI) and Sentiment Analysis (SA) show that the likelihood of answering a question correctly is impacted by the question’s difficulty. In addition, as DNNs are trained on larger datasets easy questions start to have a higher probability of being answered correctly than harder questions.</p>
<p>【Keywords】:</p>
<h3 id="501. Lexicosyntactic inference in neural models.">501. Lexicosyntactic inference in neural models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1501/">Paper Link</a>】    【Pages】:4717-4724</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/White:Aaron_Steven">Aaron Steven White</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rudinger:Rachel">Rachel Rudinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rawlins:Kyle">Kyle Rawlins</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durme:Benjamin_Van">Benjamin Van Durme</a></p>
<p>【Abstract】:
We investigate neural models’ ability to capture lexicosyntactic inferences: inferences triggered by the interaction of lexical and syntactic information. We take the task of event factuality prediction as a case study and build a factuality judgment dataset for all English clause-embedding verbs in various syntactic contexts. We use this dataset, which we make publicly available, to probe the behavior of current state-of-the-art neural systems, showing that these systems make certain systematic errors that are clearly visible through the lens of factuality prediction.</p>
<p>【Keywords】:</p>
<h3 id="502. Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models.">502. Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1502/">Paper Link</a>】    【Pages】:4725-4730</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Watcharawittayakul:Sedtawut">Sedtawut Watcharawittayakul</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Mingbin">Mingbin Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang_0001:Hui">Hui Jiang</a></p>
<p>【Abstract】:
In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE. The main idea of dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either a small or large values for the single forgetting factor. In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs. Our results on the challenging Google Billion word corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs. Furthermore, the proposed dual-FOFE method further gives over 10% improvement in perplexity over the original FOFE model.</p>
<p>【Keywords】:</p>
<h3 id="503. The importance of Being Recurrent for Modeling Hierarchical Structure.">503. The importance of Being Recurrent for Modeling Hierarchical Structure.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1503/">Paper Link</a>】    【Pages】:4731-4736</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tran:Ke_M=">Ke M. Tran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bisazza:Arianna">Arianna Bisazza</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Monz:Christof">Christof Monz</a></p>
<p>【Abstract】:
Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at <a href="https://github.com/">https://github.com/</a> ketranm/fan_vs_rnn</p>
<p>【Keywords】:</p>
<h3 id="504. Joint Learning for Targeted Sentiment Analysis.">504. Joint Learning for Targeted Sentiment Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1504/">Paper Link</a>】    【Pages】:4737-4742</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Dehong">Dehong Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a></p>
<p>【Abstract】:
Targeted sentiment analysis (TSA) aims at extracting targets and classifying their sentiment classes. Previous works only exploit word embeddings as features and do not explore more potentials of neural networks when jointly learning the two tasks. In this paper, we carefully design the hierarchical stack bidirectional gated recurrent units (HSBi-GRU) model to learn abstract features for both tasks, and we propose a HSBi-GRU based joint model which allows the target label to have influence on their sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HSBi-GRU in learning abstract features.</p>
<p>【Keywords】:</p>
<h3 id="505. Revisiting the Importance of Encoding Logic Rules in Sentiment Classification.">505. Revisiting the Importance of Encoding Logic Rules in Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1505/">Paper Link</a>】    【Pages】:4743-4751</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Krishna:Kalpesh">Kalpesh Krishna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jyothi:Preethi">Preethi Jyothi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Iyyer:Mohit">Mohit Iyyer</a></p>
<p>【Abstract】:
We analyze the performance of different sentiment classification models on syntactically complex inputs like A-but-B sentences. The first contribution of this analysis addresses reproducible research: to meaningfully compare different models, their accuracies must be averaged over far more random seeds than what has traditionally been reported. With proper averaging in place, we notice that the distillation model described in Hu et al. (2016), which incorporates explicit logic rules for sentiment classification, is ineffective. In contrast, using contextualized ELMo embeddings (Peters et al., 2018a) instead of logic rules yields significantly better performance. Additionally, we provide analysis and visualizations that demonstrate ELMo’s ability to implicitly learn logic rules. Finally, a crowdsourced analysis reveals how ELMo outperforms baseline models even on sentences with ambiguous sentiment labels.</p>
<p>【Keywords】:</p>
<h3 id="506. A Co-attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness.">506. A Co-attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1506/">Paper Link</a>】    【Pages】:4752-4757</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiangju">Xiangju Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Kaisong">Kaisong Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Shi">Shi Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Daling">Daling Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Yifei">Yifei Zhang</a></p>
<p>【Abstract】:
Emotion cause analysis has been a key topic in natural language processing. Existing methods ignore the contexts around the emotion word which can provide an emotion cause clue. Meanwhile, the clauses in a document play different roles on stimulating a certain emotion, depending on their content relevance. Therefore, we propose a co-attention neural network model for emotion cause analysis with emotional context awareness. The method encodes the clauses with a co-attention based bi-directional long short-term memory into high-level input representations, which are further fed into a convolutional layer for emotion cause analysis. Experimental results show that our approach outperforms the state-of-the-art baseline methods.</p>
<p>【Keywords】:</p>
<h3 id="507. Modeling Empathy and Distress in Reaction to News Stories.">507. Modeling Empathy and Distress in Reaction to News Stories.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1507/">Paper Link</a>】    【Pages】:4758-4765</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Buechel:Sven">Sven Buechel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buffone:Anneke">Anneke Buffone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slaff:Barry">Barry Slaff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ungar:Lyle_H=">Lyle H. Ungar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sedoc:Jo=atilde=o">João Sedoc</a></p>
<p>【Abstract】:
Computational detection and understanding of empathy is an important factor in advancing human-computer interaction. Yet to date, text-based empathy prediction has the following major limitations: It underestimates the psychological complexity of the phenomenon, adheres to a weak notion of ground truth where empathic states are ascribed by third parties, and lacks a shared corpus. In contrast, this contribution presents the first publicly available gold standard for empathy prediction. It is constructed using a novel annotation methodology which reliably captures empathy assessments by the writer of a statement using multi-item scales. This is also the first computational work distinguishing between multiple forms of empathy, empathic concern, and personal distress, as recognized throughout psychology. Finally, we present experimental results for three different predictive models, of which a CNN performs the best.</p>
<p>【Keywords】:</p>
<h3 id="508. Interpretable Emoji Prediction via Label-Wise Attention LSTMs.">508. Interpretable Emoji Prediction via Label-Wise Attention LSTMs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1508/">Paper Link</a>】    【Pages】:4766-4771</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Barbieri:Francesco">Francesco Barbieri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Anke:Luis_Espinosa">Luis Espinosa Anke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Camacho=Collados:Jos=eacute=">José Camacho-Collados</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schockaert:Steven">Steven Schockaert</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saggion:Horacio">Horacio Saggion</a></p>
<p>【Abstract】:
Human language has evolved towards newer forms of communication such as social media, where emojis (i.e., ideograms bearing a visual meaning) play a key role. While there is an increasing body of work aimed at the computational modeling of emoji semantics, there is currently little understanding about what makes a computational model represent or predict a given emoji in a certain way. In this paper we propose a label-wise attention mechanism with which we attempt to better understand the nuances underlying emoji prediction. In addition to advantages in terms of interpretability, we show that our proposed architecture improves over standard baselines in emoji prediction, and does particularly well when predicting infrequent emojis.</p>
<p>【Keywords】:</p>
<h3 id="509. A Tree-based Decoder for Neural Machine Translation.">509. A Tree-based Decoder for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1509/">Paper Link</a>】    【Pages】:4772-4777</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Xinyi">Xinyi Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pham:Hieu">Hieu Pham</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Pengcheng">Pengcheng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.</p>
<p>【Keywords】:</p>
<h3 id="510. Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation.">510. Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1510/">Paper Link</a>】    【Pages】:4778-4784</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shao:Chenze">Chenze Shao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Xilin">Xilin Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yang">Yang Feng</a></p>
<p>【Abstract】:
Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.</p>
<p>【Keywords】:</p>
<h3 id="511. Exploring Recombination for Efficient Decoding of Neural Machine Translation.">511. Exploring Recombination for Efficient Decoding of Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1511/">Paper Link</a>】    【Pages】:4785-4790</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhisong">Zhisong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0015:Rui">Rui Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Utiyama:Masao">Masao Utiyama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sumita:Eiichiro">Eiichiro Sumita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a></p>
<p>【Abstract】:
In Neural Machine Translation (NMT), the decoder can capture the features of the entire prediction history with neural connections and representations. This means that partial hypotheses with different prefixes will be regarded differently no matter how similar they are. However, this might be inefficient since some partial hypotheses can contain only local differences that will not influence future predictions. In this work, we introduce recombination in NMT decoding based on the concept of the “equivalence” of partial hypotheses. Heuristically, we use a simple n-gram suffix based equivalence function and adapt it into beam search decoding. Through experiments on large-scale Chinese-to-English and English-to-Germen translation tasks, we show that the proposed method can obtain similar translation quality with a smaller beam size, making NMT decoding more efficient.</p>
<p>【Keywords】:</p>
<h3 id="512. Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation.">512. Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1512/">Paper Link</a>】    【Pages】:4791-4796</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/L=auml=ubli:Samuel">Samuel Läubli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Volk_0001:Martin">Martin Volk</a></p>
<p>【Abstract】:
Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese–English news translation task. We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.</p>
<p>【Keywords】:</p>
<h3 id="513. Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point.">513. Automatic Reference-Based Evaluation of Pronoun Translation Misses the Point.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1513/">Paper Link</a>】    【Pages】:4797-4802</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guillou:Liane">Liane Guillou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hardmeier:Christian">Christian Hardmeier</a></p>
<p>【Abstract】:
We compare the performance of the APT and AutoPRF metrics for pronoun translation against a manually annotated dataset comprising human judgements as to the correctness of translations of the PROTEST test suite. Although there is some correlation with the human judgements, a range of issues limit the performance of the automated metrics. Instead, we recommend the use of semi-automatic metrics and test suites in place of fully automatic metrics.</p>
<p>【Keywords】:</p>
<h3 id="514. FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation.">514. FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1514/">Paper Link</a>】    【Pages】:4803-4809</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xu">Xu Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hao">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Pengfei">Pengfei Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Ziyun">Ziyun Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Yuan">Yuan Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>【Abstract】:
We present a Few-Shot Relation Classification Dataset (dataset), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research.</p>
<p>【Keywords】:</p>
<h3 id="515. A strong baseline for question relevancy ranking.">515. A strong baseline for question relevancy ranking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1515/">Paper Link</a>】    【Pages】:4810-4815</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gonz=aacute=lez=Gardu=ntilde=o:Ana_Valeria">Ana Valeria González-Garduño</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Augenstein:Isabelle">Isabelle Augenstein</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Søgaard</a></p>
<p>【Abstract】:
The best systems at the SemEval-16 and SemEval-17 community question answering shared tasks – a task that amounts to question relevancy ranking – involve complex pipelines and manual feature engineering. Despite this, many of these still fail at beating the IR baseline, i.e., the rankings provided by Google’s search engine. We present a strong baseline for question relevancy ranking by training a simple multi-task feed forward network on a bag of 14 distance measures for the input question pair. This baseline model, which is fast to train and uses only language-independent features, outperforms the best shared task systems on the task of retrieving relevant previously asked questions.</p>
<p>【Keywords】:</p>
<h3 id="516. Learning Sequence Encoders for Temporal Knowledge Graph Completion.">516. Learning Sequence Encoders for Temporal Knowledge Graph Completion.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1516/">Paper Link</a>】    【Pages】:4816-4821</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Garc=iacute=a=Dur=aacute=n:Alberto">Alberto García-Durán</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dumancic:Sebastijan">Sebastijan Dumancic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Niepert:Mathias">Mathias Niepert</a></p>
<p>【Abstract】:
Research on link prediction in knowledge graphs has mainly focused on static multi-relational data. In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time. In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations. To incorporate temporal information, we utilize recurrent neural networks to learn time-aware representations of relation types which can be used in conjunction with existing latent factorization methods. The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and heterogeneity of temporal expressions. Experiments show the benefits of our approach on four temporal KGs. The data sets are available under a permissive BSD-3 license.</p>
<p>【Keywords】:</p>
<h3 id="517. Similar but not the Same - Word Sense Disambiguation Improves Event Detection via Neural Representation Matching.">517. Similar but not the Same - Word Sense Disambiguation Improves Event Detection via Neural Representation Matching.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1517/">Paper Link</a>】    【Pages】:4822-4828</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Weiyi">Weiyi Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Thien_Huu">Thien Huu Nguyen</a></p>
<p>【Abstract】:
Event detection (ED) and word sense disambiguation (WSD) are two similar tasks in that they both involve identifying the classes (i.e. event types or word senses) of some word in a given sentence. It is thus possible to extract the knowledge hidden in the data for WSD, and utilize it to improve the performance on ED. In this work, we propose a method to transfer the knowledge learned on WSD to ED by matching the neural representations learned for the two tasks. Our experiments on two widely used datasets for ED demonstrate the effectiveness of the proposed method.</p>
<p>【Keywords】:</p>
<h3 id="518. Learning Word Representations with Cross-Sentence Dependencyfor End-to-End Co-reference Resolution.">518. Learning Word Representations with Cross-Sentence Dependencyfor End-to-End Co-reference Resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1518/">Paper Link</a>】    【Pages】:4829-4833</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Hongyin">Hongyin Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Glass:James_R=">James R. Glass</a></p>
<p>【Abstract】:
In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR). While the traditional E2E-CR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn cross-sentence dependency. Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word. With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document. Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.</p>
<p>【Keywords】:</p>
<h3 id="519. Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings.">519. Word Relation Autoencoder for Unseen Hypernym Extraction Using Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1519/">Paper Link</a>】    【Pages】:4834-4839</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hong=You">Hong-You Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Cheng=Syuan">Cheng-Syuan Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liao:Keng=Te">Keng-Te Liao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Shou=de">Shou-de Lin</a></p>
<p>【Abstract】:
Lexicon relation extraction given distributional representation of words is an important topic in NLP. We observe that the state-of-the-art projection-based methods cannot be generalized to handle unseen hypernyms. We propose to analyze it in the perspective of pollution and construct the corresponding indicator to measure it. We propose a word relation autoencoder (WRAE) model to address the challenge. Experiments on several hypernym-like lexicon datasets show that our model outperforms the competitors significantly.</p>
<p>【Keywords】:</p>
<h3 id="520. Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation.">520. Refining Pretrained Word Embeddings Using Layer-wise Relevance Propagation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1520/">Paper Link</a>】    【Pages】:4840-4846</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/u/Utsumi:Akira">Akira Utsumi</a></p>
<p>【Abstract】:
In this paper, we propose a simple method for refining pretrained word embeddings using layer-wise relevance propagation. Given a target semantic representation one would like word vectors to reflect, our method first trains the mapping between the original word vectors and the target representation using a neural network. Estimated target values are then propagated backward toward word vectors, and a relevance score is computed for each dimension of word vectors. Finally, the relevance score vectors are used to refine the original word vectors so that they are projected into the subspace that reflects the information relevant to the target representation. The evaluation experiment using binary classification of word pairs demonstrates that the refined vectors by our method achieve the higher performance than the original vectors.</p>
<p>【Keywords】:</p>
<h3 id="521. Learning Gender-Neutral Word Embeddings.">521. Learning Gender-Neutral Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1521/">Paper Link</a>】    【Pages】:4847-4853</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Jieyu">Jieyu Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Yichao">Yichao Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zeyu">Zeyu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0010:Wei">Wei Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Kai=Wei">Kai-Wei Chang</a></p>
<p>【Abstract】:
Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.</p>
<p>【Keywords】:</p>
<h3 id="522. Learning Concept Abstractness Using Weak Supervision.">522. Learning Concept Abstractness Using Weak Supervision.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1522/">Paper Link</a>】    【Pages】:4854-4859</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rabinovich:Ella">Ella Rabinovich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sznajder:Benjamin">Benjamin Sznajder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Spector:Artem">Artem Spector</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shnayderman:Ilya">Ilya Shnayderman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aharonov:Ranit">Ranit Aharonov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Konopnicki:David">David Konopnicki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Slonim:Noam">Noam Slonim</a></p>
<p>【Abstract】:
We introduce a weakly supervised approach for inferring the property of abstractness of words and expressions in the complete absence of labeled data. Exploiting only minimal linguistic clues and the contextual usage of a concept as manifested in textual data, we train sufficiently powerful classifiers, obtaining high correlation with human labels. The results imply the applicability of this approach to additional properties of concepts, additional languages, and resource-scarce scenarios.</p>
<p>【Keywords】:</p>
<h3 id="523. Word Sense Induction with Neural biLM and Symmetric Patterns.">523. Word Sense Induction with Neural biLM and Symmetric Patterns.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1523/">Paper Link</a>】    【Pages】:4860-4867</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Amrami:Asaf">Asaf Amrami</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldberg:Yoav">Yoav Goldberg</a></p>
<p>【Abstract】:
An established method for Word Sense Induction (WSI) uses a language model to predict probable substitutes for target words, and induces senses by clustering these resulting substitute vectors. We replace the ngram-based language model (LM) with a recurrent one. Beyond being more accurate, the use of the recurrent LM allows us to effectively query it in a creative way, using what we call dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.</p>
<p>【Keywords】:</p>
<h3 id="524. InferLite: Simple Universal Sentence Representations from Natural Language Inference Data.">524. InferLite: Simple Universal Sentence Representations from Natural Language Inference Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1524/">Paper Link</a>】    【Pages】:4868-4874</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kiros:Jamie_Ryan">Jamie Ryan Kiros</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chan:William">William Chan</a></p>
<p>【Abstract】:
Natural language inference has been shown to be an effective supervised task for learning generic sentence embeddings. In order to better understand the components that lead to effective representations, we propose a lightweight version of InferSent, called InferLite, that does not use any recurrent layers and operates on a collection of pre-trained word embeddings. We show that a simple instance of our model that makes no use of context, word ordering or position can still obtain competitive performance on the majority of downstream prediction tasks, with most performance gaps being filled by adding local contextual information through temporal convolutions. Our models can be trained in under 1 hour on a single GPU and allows for fast inference of new representations. Finally we describe a semantic hashing layer that allows our model to learn generic binary codes for sentences.</p>
<p>【Keywords】:</p>
<h3 id="525. Similarity-Based Reconstruction Loss for Meaning Representation.">525. Similarity-Based Reconstruction Loss for Meaning Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1525/">Paper Link</a>】    【Pages】:4875-4880</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kovaleva:Olga">Olga Kovaleva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Romanov:Alexey">Alexey Romanov</a></p>
<p>【Abstract】:
This paper addresses the problem of representation learning. Using an autoencoder framework, we propose and evaluate several loss functions that can be used as an alternative to the commonly used cross-entropy reconstruction loss. The proposed loss functions use similarities between words in the embedding space, and can be used to train any neural model for text generation. We show that the introduced loss functions amplify semantic diversity of reconstructed sentences, while preserving the original meaning of the input. We test the derived autoencoder-generated representations on paraphrase detection and language inference tasks and demonstrate performance improvement compared to the traditional cross-entropy loss.</p>
<p>【Keywords】:</p>
<h3 id="526. What can we learn from Semantic Tagging?">526. What can we learn from Semantic Tagging?</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1526/">Paper Link</a>】    【Pages】:4881-4889</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Abdou:Mostafa">Mostafa Abdou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kulmizev:Artur">Artur Kulmizev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ravishankar:Vinit">Vinit Ravishankar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abzianidze:Lasha">Lasha Abzianidze</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bos:Johan">Johan Bos</a></p>
<p>【Abstract】:
We investigate the effects of multi-task learning using the recently introduced task of semantic tagging. We employ semantic tagging as an auxiliary task for three different NLP tasks: part-of-speech tagging, Universal Dependency parsing, and Natural Language Inference. We compare full neural network sharing, partial neural network sharing, and what we term the learning what to share setting where negative transfer between tasks is less likely. Our findings show considerable improvements for all tasks, particularly in the learning what to share setting which shows consistent gains across all tasks.</p>
<p>【Keywords】:</p>
<h3 id="527. Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop.">527. Conditional Word Embedding and Hypothesis Testing via Bayes-by-Backprop.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1527/">Paper Link</a>】    【Pages】:4890-4895</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Rujun">Rujun Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gill:Michael">Michael Gill</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Spirling:Arthur">Arthur Spirling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a></p>
<p>【Abstract】:
Conventional word embedding models do not leverage information from document meta-data, and they do not model uncertainty. We address these concerns with a model that incorporates document covariates to estimate conditional word embedding distributions. Our model allows for (a) hypothesis tests about the meanings of terms, (b) assessments as to whether a word is near or far from another conditioned on different covariate values, and (c) assessments as to whether estimated differences are statistically significant.</p>
<p>【Keywords】:</p>
<h3 id="528. Classifying Referential and Non-referential It Using Gaze.">528. Classifying Referential and Non-referential It Using Gaze.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1528/">Paper Link</a>】    【Pages】:4896-4901</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yaneva:Victoria">Victoria Yaneva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Ha:Le_An">Le An Ha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Evans:Richard">Richard Evans</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitkov:Ruslan">Ruslan Mitkov</a></p>
<p>【Abstract】:
When processing a text, humans and machines must disambiguate between different uses of the pronoun it, including non-referential, nominal anaphoric or clause anaphoric ones. In this paper we use eye-tracking data to learn how humans perform this disambiguation and use this knowledge to improve the automatic classification of it. We show that by using gaze data and a POS-tagger we are able to significantly outperform a common baseline and classify between three categories of it with an accuracy comparable to that of linguistic-based approaches. In addition, the discriminatory power of specific gaze features informs the way humans process the pronoun, which, to the best of our knowledge, has not been explored using data from a natural reading task.</p>
<p>【Keywords】:</p>
<h3 id="529. State-of-the-art Chinese Word Segmentation with Bi-LSTMs.">529. State-of-the-art Chinese Word Segmentation with Bi-LSTMs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1529/">Paper Link</a>】    【Pages】:4902-4908</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Ji">Ji Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Ganchev:Kuzman">Kuzman Ganchev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weiss:David">David Weiss</a></p>
<p>【Abstract】:
A wide variety of neural-network architectures have been proposed for the task of Chinese word segmentation. Surprisingly, we find that a bidirectional LSTM model, when combined with standard deep learning techniques and best practices, can achieve better accuracy on many of the popular datasets as compared to models based on more complex neuralnetwork architectures. Furthermore, our error analysis shows that out-of-vocabulary words remain challenging for neural-network models, and many of the remaining errors are unlikely to be fixed through architecture changes. Instead, more effort should be made on exploring resources for further improvement.</p>
<p>【Keywords】:</p>
<h3 id="530. Sanskrit Sandhi Splitting using seq2(seq)2.">530. Sanskrit Sandhi Splitting using seq2(seq)2.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1530/">Paper Link</a>】    【Pages】:4909-4914</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Aralikatte:Rahul">Rahul Aralikatte</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gantayat:Neelamadhav">Neelamadhav Gantayat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Panwar:Naveen">Naveen Panwar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sankaran:Anush">Anush Sankaran</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mani:Senthil">Senthil Mani</a></p>
<p>【Abstract】:
In Sanskrit, small words (morphemes) are combined to form compound words through a process known as Sandhi. Sandhi splitting is the process of splitting a given compound word into its constituent morphemes. Although rules governing word splitting exists in the language, it is highly challenging to identify the location of the splits in a compound word. Though existing Sandhi splitting systems incorporate these pre-defined splitting rules, they have a low accuracy as the same compound word might be broken down in multiple ways to provide syntactically correct splits. In this research, we propose a novel deep learning architecture called Double Decoder RNN (DD-RNN), which (i) predicts the location of the split(s) with 95% accuracy, and (ii) predicts the constituent words (learning the Sandhi splitting rules) with 79.5% accuracy, outperforming the state-of-art by 20%. Additionally, we show the generalization capability of our deep learning model, by showing competitive results in the problem of Chinese word segmentation, as well.</p>
<p>【Keywords】:</p>
<h3 id="531. Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling.">531. Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1531/">Paper Link</a>】    【Pages】:4915-4920</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Zhiqing">Zhiqing Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Zhi=Hong">Zhi-Hong Deng</a></p>
<p>【Abstract】:
Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-the-art statistical models on four different datasets from SIGHAN 2005 bakeoff.</p>
<p>【Keywords】:</p>
<h3 id="532. LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs.">532. LemmaTag: Jointly Tagging and Lemmatizing for Morphologically Rich Languages with BRNNs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1532/">Paper Link</a>】    【Pages】:4921-4928</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kondratyuk:Daniel">Daniel Kondratyuk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gavenciak:Tomas">Tomas Gavenciak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Straka:Milan">Milan Straka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hajic:Jan">Jan Hajic</a></p>
<p>【Abstract】:
We present LemmaTag, a featureless neural network architecture that jointly generates part-of-speech tags and lemmas for sentences by using bidirectional RNNs with character-level and word-level embeddings. We demonstrate that both tasks benefit from sharing the encoding part of the network, predicting tag subcategories, and using the tagger output as an input to the lemmatizer. We evaluate our model across several languages with complex morphology, which surpasses state-of-the-art accuracy in both part-of-speech tagging and lemmatization in Czech, German, and Arabic.</p>
<p>【Keywords】:</p>
<h3 id="533. Recovering Missing Characters in Old Hawaiian Writing.">533. Recovering Missing Characters in Old Hawaiian Writing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1533/">Paper Link</a>】    【Pages】:4929-4934</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shillingford:Brendan">Brendan Shillingford</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jones:Oiwi_Parker">Oiwi Parker Jones</a></p>
<p>【Abstract】:
In contrast to the older writing system of the 19th century, modern Hawaiian orthography employs characters for long vowels and glottal stops. These extra characters account for about one-third of the phonemes in Hawaiian, so including them makes a big difference to reading comprehension and pronunciation. However, transliterating between older and newer texts is a laborious task when performed manually. We introduce two related methods to help solve this transliteration problem automatically. One approach is implemented, end-to-end, using finite state transducers (FSTs). The other is a hybrid deep learning approach, which approximately composes an FST with a recurrent neural network language model.</p>
<p>【Keywords】:</p>
<h3 id="534. When data permutations are pathological: the case of neural natural language inference.">534. When data permutations are pathological: the case of neural natural language inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1534/">Paper Link</a>】    【Pages】:4935-4939</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schluter:Natalie">Natalie Schluter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Varab:Daniel">Daniel Varab</a></p>
<p>【Abstract】:
Consider two competitive machine learning models, one of which was considered state-of-the art, and the other a competitive baseline. Suppose that by just permuting the examples of the training set, say by reversing the original order, by shuffling, or by mini-batching, you could report substantially better/worst performance for the system of your choice, by multiple percentage points. In this paper, we illustrate this scenario for a trending NLP task: Natural Language Inference (NLI). We show that for the two central NLI corpora today, the learning process of neural systems is far too sensitive to permutations of the data. In doing so we reopen the question of how to judge a good neural architecture for NLI, given the available dataset and perhaps, further, the soundness of the NLI task itself in its current state.</p>
<p>【Keywords】:</p>
<h3 id="535. Bridging Knowledge Gaps in Neural Entailment via Symbolic Models.">535. Bridging Knowledge Gaps in Neural Entailment via Symbolic Models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1535/">Paper Link</a>】    【Pages】:4940-4945</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kang:Dongyeop">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khot:Tushar">Tushar Khot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish">Ashish Sabharwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Peter">Peter Clark</a></p>
<p>【Abstract】:
Most textual entailment models focus on lexical gaps between the premise text and the hypothesis, but rarely on knowledge gaps. We focus on filling these knowledge gaps in the Science Entailment task, by leveraging an external structured knowledge base (KB) of science facts. Our new architecture combines standard neural entailment models with a knowledge lookup module. To facilitate this lookup, we propose a fact-level decomposition of the hypothesis, and verifying the resulting sub-facts against both the textual premise and the structured KB. Our model, NSNet, learns to aggregate predictions from these heterogeneous data formats. On the SciTail dataset, NSNet outperforms a simpler combination of the two predictions by 3% and the base entailment model by 5%.</p>
<p>【Keywords】:</p>
<h3 id="536. The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.">536. The BQ Corpus: A Large-scale Domain-specific Chinese Corpus For Sentence Semantic Equivalence Identification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1536/">Paper Link</a>】    【Pages】:4946-4951</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Jing">Jing Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Qingcai">Qingcai Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xin">Xin Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Haijun">Haijun Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Daohe">Daohe Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Buzhou">Buzhou Tang</a></p>
<p>【Abstract】:
This paper introduces the Bank Question (BQ) corpus, a Chinese corpus for sentence semantic equivalence identification (SSEI). The BQ corpus contains 120,000 question pairs from 1-year online bank custom service logs. To efficiently process and annotate questions from such a large scale of logs, this paper proposes a clustering based annotation method to achieve questions with the same intent. First, the deduplicated questions with the same answer are clustered into stacks by the Word Mover’s Distance (WMD) based Affinity Propagation (AP) algorithm. Then, the annotators are asked to assign the clustered questions into different intent categories. Finally, the positive and negative question pairs for SSEI are selected in the same intent category and between different intent categories respectively. We also present six SSEI benchmark performance on our corpus, including state-of-the-art algorithms. As the largest manually annotated public Chinese SSEI corpus in the bank domain, the BQ corpus is not only useful for Chinese question semantic matching research, but also a significant resource for cross-lingual and cross-domain SSEI research. The corpus is available in public.</p>
<p>【Keywords】:</p>
<h3 id="537. Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.">537. Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1537/">Paper Link</a>】    【Pages】:4952-4957</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ghaeini:Reza">Reza Ghaeini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli_Z=">Xiaoli Z. Fern</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tadepalli:Prasad">Prasad Tadepalli</a></p>
<p>【Abstract】:
Deep learning models have achieved remarkable success in natural language inference (NLI) tasks. While these models are widely explored, they are hard to interpret and it is often unclear how and why they actually work. In this paper, we take a step toward explaining such deep learning based models through a case study on a popular neural model for NLI. In particular, we propose to interpret the intermediate layers of NLI models by visualizing the saliency of attention and LSTM gating signals. We present several examples for which our methods are able to reveal interesting insights and identify the critical information contributing to the model decisions.</p>
<p>【Keywords】:</p>
<h3 id="538. Towards Semi-Supervised Learning for Deep Semantic Role Labeling.">538. Towards Semi-Supervised Learning for Deep Semantic Role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1538/">Paper Link</a>】    【Pages】:4958-4963</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mehta:Sanket_Vaibhav">Sanket Vaibhav Mehta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Jay_Yoon">Jay Yoon Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carbonell:Jaime_G=">Jaime G. Carbonell</a></p>
<p>【Abstract】:
Neural models have shown several state-of-the-art performances on Semantic Role Labeling (SRL). However, the neural models require an immense amount of semantic-role corpora and are thus not well suited for low-resource languages or domains. The paper proposes a semi-supervised semantic role labeling method that outperforms the state-of-the-art in limited SRL training corpora. The method is based on explicitly enforcing syntactic constraints by augmenting the training objective with a syntactic-inconsistency loss component and uses SRL-unlabeled instances to train a joint-objective LSTM. On CoNLL-2012 English section, the proposed semi-supervised training with 1%, 10% SRL-labeled data and varying amounts of SRL-unlabeled data achieves +1.58, +0.78 F1, respectively, over the pre-trained models that were trained on SOTA architecture with ELMo on the same SRL-labeled data. Additionally, by using the syntactic-inconsistency loss on inference time, the proposed model achieves +3.67, +2.1 F1 over pre-trained model on 1%, 10% SRL-labeled data, respectively.</p>
<p>【Keywords】:</p>
<h3 id="539. Identifying Domain Adjacent Instances for Semantic Parsers.">539. Identifying Domain Adjacent Instances for Semantic Parsers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1539/">Paper Link</a>】    【Pages】:4964-4969</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Ferguson:James">James Ferguson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Christensen:Janara">Janara Christensen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Edward">Edward Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gonz=agrave=lez:Edgar">Edgar Gonzàlez</a></p>
<p>【Abstract】:
When the semantics of a sentence are not representable in a semantic parser’s output schema, parsing will inevitably fail. Detection of these instances is commonly treated as an out-of-domain classification problem. However, there is also a more subtle scenario in which the test data is drawn from the same domain. In addition to formalizing this problem of domain-adjacency, we present a comparison of various baselines that could be used to solve it. We also propose a new simple sentence representation that emphasizes words which are unexpected. This approach improves the performance of a downstream semantic parser run on in-domain and domain-adjacent instances.</p>
<p>【Keywords】:</p>
<h3 id="540. Mapping natural language commands to web elements.">540. Mapping natural language commands to web elements.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1540/">Paper Link</a>】    【Pages】:4970-4976</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pasupat:Panupong">Panupong Pasupat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Tian=Shun">Tian-Shun Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Evan_Zheran">Evan Zheran Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guu:Kelvin">Kelvin Guu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>【Abstract】:
The web provides a rich, open-domain environment with textual, structural, and spatial properties. We propose a new task for grounding language in this environment: given a natural language command (e.g., “click on the second article”), choose the correct element on the web page (e.g., a hyperlink or text box). We collected a dataset of over 50,000 commands that capture various phenomena such as functional references (e.g. “find who made this site”), relational reasoning (e.g. “article by john”), and visual reasoning (e.g. “top-most article”). We also implemented and analyzed three baseline models that capture different phenomena present in the dataset.</p>
<p>【Keywords】:</p>
<h3 id="541. Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection.">541. Wronging a Right: Generating Better Errors to Improve Grammatical Error Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1541/">Paper Link</a>】    【Pages】:4977-4983</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kasewa:Sudhanshu">Sudhanshu Kasewa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stenetorp:Pontus">Pontus Stenetorp</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riedel_0001:Sebastian">Sebastian Riedel</a></p>
<p>【Abstract】:
Grammatical error correction, like other machine learning tasks, greatly benefits from large quantities of high quality training data, which is typically expensive to produce. While writing a program to automatically generate realistic grammatical errors would be difficult, one could learn the distribution of naturally-occurring errors and attempt to introduce them into other datasets. Initial work on inducing errors in this way using statistical machine translation has shown promise; we investigate cheaply constructing synthetic samples, given a small corpus of human-annotated data, using an off-the-rack attentive sequence-to-sequence model and a straight-forward post-processing procedure. Our approach yields error-filled artificial data that helps a vanilla bi-directional LSTM to outperform the previous state of the art at grammatical error detection, and a previously introduced model to gain further improvements of over 5% F0.5 score. When attempting to determine if a given sentence is synthetic, a human annotator at best achieves 39.39 F1 score, indicating that our model generates mostly human-like instances.</p>
<p>【Keywords】:</p>
<h3 id="542. Modeling Input Uncertainty in Neural Network Dependency Parsing.">542. Modeling Input Uncertainty in Neural Network Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1542/">Paper Link</a>】    【Pages】:4984-4991</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goot:Rob_van_der">Rob van der Goot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Noord:Gertjan_van">Gertjan van Noord</a></p>
<p>【Abstract】:
Recently introduced neural network parsers allow for new approaches to circumvent data sparsity issues by modeling character level information and by exploiting raw data in a semi-supervised setting. Data sparsity is especially prevailing when transferring to non-standard domains. In this setting, lexical normalization has often been used in the past to circumvent data sparsity. In this paper, we investigate whether these new neural approaches provide similar functionality as lexical normalization, or whether they are complementary. We provide experimental results which show that a separate normalization component improves performance of a neural network parser even if it has access to character level information as well as external word embeddings. Further improvements are obtained by a straightforward but novel approach in which the top-N best candidates provided by the normalization component are available to the parser.</p>
<p>【Keywords】:</p>
<h3 id="543. Parameter sharing between dependency parsers for related languages.">543. Parameter sharing between dependency parsers for related languages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1543/">Paper Link</a>】    【Pages】:4992-4997</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lhoneux:Miryam_de">Miryam de Lhoneux</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bjerva:Johannes">Johannes Bjerva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Augenstein:Isabelle">Isabelle Augenstein</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Søgaard</a></p>
<p>【Abstract】:
Previous work has suggested that parameter sharing between transition-based neural dependency parsers for related languages can lead to better performance, but there is no consensus on what parameters to share. We present an evaluation of 27 different parameter sharing strategies across 10 languages, representing five pairs of related languages, each pair from a different language family. We find that sharing transition classifier parameters always helps, whereas the usefulness of sharing word and/or character LSTM parameters varies. Based on this result, we propose an architecture where the transition classifier is shared, and the sharing of word and character parameters is controlled by a parameter that can be tuned on validation data. This model is linguistically motivated and obtains significant improvements over a monolingually trained baseline. We also find that sharing transition classifier parameters helps when training a parser on unrelated language pairs, but we find that, in the case of unrelated languages, sharing too many parameters does not help.</p>
<p>【Keywords】:</p>
<h3 id="544. Grammar Induction with Neural Language Models: An Unusual Replication.">544. Grammar Induction with Neural Language Models: An Unusual Replication.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1544/">Paper Link</a>】    【Pages】:4998-5003</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Htut:Phu_Mon">Phu Mon Htut</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bowman:Samuel_R=">Samuel R. Bowman</a></p>
<p>【Abstract】:
A substantial thread of recent work on latent tree learning has attempted to develop neural network models with parse-valued latent variables and train them on non-parsing tasks, in the hope of having them discover interpretable tree structure. In a recent paper, Shen et al. (2018) introduce such a model and report near-state-of-the-art results on the target task of language modeling, and the first strong latent tree learning result on constituency parsing. In an attempt to reproduce these results, we discover issues that make the original results hard to trust, including tuning and even training on what is effectively the test set. Here, we attempt to reproduce these results in a fair experiment and to extend them to two new datasets. We find that the results of this work are robust: All variants of the model under study outperform all latent tree learning baselines, and perform competitively with symbolic grammar induction systems. We find that this model represents the first empirical success for latent tree learning, and that neural network language modeling warrants further study as a setting for grammar induction.</p>
<p>【Keywords】:</p>
<h3 id="545. Data Augmentation via Dependency Tree Morphing for Low-Resource Languages.">545. Data Augmentation via Dependency Tree Morphing for Low-Resource Languages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1545/">Paper Link</a>】    【Pages】:5004-5009</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sahin:G=ouml=zde_G=uuml=l">Gözde Gül Sahin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Steedman:Mark">Mark Steedman</a></p>
<p>【Abstract】:
Neural NLP systems achieve high scores in the presence of sizable training dataset. Lack of such datasets leads to poor system performances in the case low-resource languages. We present two simple text augmentation techniques using dependency trees, inspired from image processing. We “crop” sentences by removing dependency links, and we “rotate” sentences by moving the tree fragments around the root. We apply these techniques to augment the training sets of low-resource languages in Universal Dependencies project. We implement a character-level sequence tagging model and evaluate the augmented datasets on part-of-speech tagging task. We show that crop and rotate provides improvements over the models trained with non-augmented data for majority of the languages, especially for languages with rich case marking systems.</p>
<p>【Keywords】:</p>
<h3 id="546. How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks.">546. How Much Reading Does Reading Comprehension Require? A Critical Investigation of Popular Benchmarks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1546/">Paper Link</a>】    【Pages】:5010-5015</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kaushik:Divyansh">Divyansh Kaushik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lipton:Zachary_C=">Zachary C. Lipton</a></p>
<p>【Abstract】:
Many recent papers address reading comprehension, where examples consist of (question, passage, answer) tuples. Presumably, a model must combine information from both questions and passages to predict corresponding answers. However, despite intense interest in the topic, with hundreds of published papers vying for leaderboard dominance, basic questions about the difficulty of many popular benchmarks remain unanswered. In this paper, we establish sensible baselines for the bAbI, SQuAD, CBT, CNN, and Who-did-What datasets, finding that question- and passage-only models often perform surprisingly well. On 14 out of 20 bAbI tasks, passage-only models achieve greater than 50% accuracy, sometimes matching the full model. Interestingly, while CBT provides 20-sentence passages, only the last is needed for accurate prediction. By comparison, SQuAD and CNN appear better-constructed.</p>
<p>【Keywords】:</p>
<h3 id="547. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.">547. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1547/">Paper Link</a>】    【Pages】:5016-5026</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Budzianowski:Pawel">Pawel Budzianowski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wen:Tsung=Hsien">Tsung-Hsien Wen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tseng:Bo=Hsiang">Bo-Hsiang Tseng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Casanueva:I=ntilde=igo">Iñigo Casanueva</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Ultes:Stefan">Stefan Ultes</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ramadan:Osman">Osman Ramadan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gasic:Milica">Milica Gasic</a></p>
<p>【Abstract】:
Even though machine learning has become the major scene in dialogue research community, the real breakthrough has been blocked by the scale of data available.To address this fundamental obstacle, we introduce the Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics.At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.The contribution of this work apart from the open-sourced dataset is two-fold:firstly, a detailed description of the data collection procedure along with a summary of data structure and analysis is provided. The proposed data-collection pipeline is entirely based on crowd-sourcing without the need of hiring professional annotators;secondly, a set of benchmark results of belief tracking, dialogue act and response generation is reported, which shows the usability of the data and sets a baseline for future studies.</p>
<p>【Keywords】:</p>
<h3 id="548. Linguistically-Informed Self-Attention for Semantic Role Labeling.">548. Linguistically-Informed Self-Attention for Semantic Role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1548/">Paper Link</a>】    【Pages】:5027-5038</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Strubell:Emma">Emma Strubell</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verga:Patrick">Patrick Verga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Andor:Daniel">Daniel Andor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weiss:David">David Weiss</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>【Abstract】:
Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate syntax using merely raw tokens as input, encoding the sequence only once to simultaneously perform parsing, predicate detection and role labeling for all predicates. Syntax is incorporated by training one attention head to attend to syntactic parents for each token. Moreover, if a high-quality syntactic parse is already available, it can be beneficially injected at test time without re-training our SRL model. In experiments on CoNLL-2005 SRL, LISA achieves new state-of-the-art performance for a model using predicted predicates and standard word embeddings, attaining 2.5 F1 absolute higher than the previous state-of-the-art on newswire and more than 3.5 F1 on out-of-domain data, nearly 10% reduction in error. On ConLL-2012 English SRL we also show an improvement of more than 2.5 F1. LISA also out-performs the state-of-the-art with contextually-encoded (ELMo) word representations, by nearly 1.0 F1 on news and more than 2.0 F1 on out-of-domain text.</p>
<p>【Keywords】:</p>
<h3 id="549. Phrase-Based & Neural Unsupervised Machine Translation.">549. Phrase-Based &amp; Neural Unsupervised Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/D18-1549/">Paper Link</a>】    【Pages】:5039-5049</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lample:Guillaume">Guillaume Lample</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Ott:Myle">Myle Ott</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Conneau:Alexis">Alexis Conneau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Denoyer:Ludovic">Ludovic Denoyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ranzato:Marc=Aurelio">Marc&apos;Aurelio Ranzato</a></p>
<p>【Abstract】:
Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT’14 English-French and WMT’16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.</p>
<p>【Keywords】:</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="https://github.com/huntercmd/ccf"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
