 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="light" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/light.css">
<link id="dark" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/dark.css" disabled/>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/skin.js"></script>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/classie.js"></script>

<!-- This is for Mathjax -->

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["$","$"] ],
			displayMath: [ ['$$','$$'], ["$$","$$"] ],
			processEscapes: true
			},
		TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
		"HTML-CSS": {linebreaks: {automatic: true}},
		SVG: {linebreaks: {automatic: true}}
	});
</script>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#21. CIKM 2012:Maui, HI, USA">21. CIKM 2012:Maui, HI, USA</a><ul>
<li><a href="#Paper Num: 451 || Session Num: 42">Paper Num: 451 || Session Num: 42</a></li>
<li><a href="#Keynote address    3">Keynote address    3</a><ul>
<li><a href="#1. User engagement: the network effect matters!">1. User engagement: the network effect matters!</a></li>
<li><a href="#2. Learning similarity measures based on random walks.">2. Learning similarity measures based on random walks.</a></li>
<li><a href="#3. Compressed data structures with relevance.">3. Compressed data structures with relevance.</a></li>
</ul>
</li>
<li><a href="#KM track: recommender systems    5">KM track: recommender systems    5</a><ul>
<li><a href="#4. LogUCB: an explore-exploit algorithm for comments recommendation.">4. LogUCB: an explore-exploit algorithm for comments recommendation.</a></li>
<li><a href="#5. DQR: a probabilistic approach to diversified query recommendation.">5. DQR: a probabilistic approach to diversified query recommendation.</a></li>
<li><a href="#6. Dynamic covering for recommendation systems.">6. Dynamic covering for recommendation systems.</a></li>
<li><a href="#7. MEET: a generalized framework for reciprocal recommender systems.">7. MEET: a generalized framework for reciprocal recommender systems.</a></li>
<li><a href="#8. Social contextual recommendation.">8. Social contextual recommendation.</a></li>
</ul>
</li>
<li><a href="#KM track: pattern mining    5">KM track: pattern mining    5</a><ul>
<li><a href="#9. Mining high utility itemsets without candidate generation.">9. Mining high utility itemsets without candidate generation.</a></li>
<li><a href="#10. A general framework to encode heterogeneous information sources for contextual pattern mining.">10. A general framework to encode heterogeneous information sources for contextual pattern mining.</a></li>
<li><a href="#11. Incorporating occupancy into frequent pattern mining for high quality pattern recommendation.">11. Incorporating occupancy into frequent pattern mining for high quality pattern recommendation.</a></li>
<li><a href="#12. PARMA: a parallel randomized algorithm for approximate association rules mining in MapReduce.">12. PARMA: a parallel randomized algorithm for approximate association rules mining in MapReduce.</a></li>
<li><a href="#13. Interactive pattern mining on hidden data: a sampling-based solution.">13. Interactive pattern mining on hidden data: a sampling-based solution.</a></li>
</ul>
</li>
<li><a href="#IR track: evaluation methodologies    5">IR track: evaluation methodologies    5</a><ul>
<li><a href="#14. An analysis of systematic judging errors in information retrieval.">14. An analysis of systematic judging errors in information retrieval.</a></li>
<li><a href="#15. On caption bias in interleaving experiments.">15. On caption bias in interleaving experiments.</a></li>
<li><a href="#16. Alternative assessor disagreement and retrieval depth.">16. Alternative assessor disagreement and retrieval depth.</a></li>
<li><a href="#17. Incorporating variability in user behavior into systems based evaluation.">17. Incorporating variability in user behavior into systems based evaluation.</a></li>
<li><a href="#18. Constructing test collections by inferring document relevance via extracted relevant information.">18. Constructing test collections by inferring document relevance via extracted relevant information.</a></li>
</ul>
</li>
<li><a href="#IR track: social media search    5">IR track: social media search    5</a><ul>
<li><a href="#19. Twevent: segment-based event detection from tweets.">19. Twevent: segment-based event detection from tweets.</a></li>
<li><a href="#20. Making your interests follow you on twitter.">20. Making your interests follow you on twitter.</a></li>
<li><a href="#21. Generating event storylines from microblogs.">21. Generating event storylines from microblogs.</a></li>
<li><a href="#22. Social book search: comparing topical relevance judgements and book suggestions for evaluation.">22. Social book search: comparing topical relevance judgements and book suggestions for evaluation.</a></li>
<li><a href="#23. Content-based crowd retrieval on the real-time web.">23. Content-based crowd retrieval on the real-time web.</a></li>
</ul>
</li>
<li><a href="#KM track: link and graph mining    5">KM track: link and graph mining    5</a><ul>
<li><a href="#24. Graph classification: a diversified discriminative feature selection approach.">24. Graph classification: a diversified discriminative feature selection approach.</a></li>
<li><a href="#25. Multi-scale link prediction.">25. Multi-scale link prediction.</a></li>
<li><a href="#26. An analysis of how ensembles of collective classifiers improve predictions in graphs.">26. An analysis of how ensembles of collective classifiers improve predictions in graphs.</a></li>
<li><a href="#27. Density index and proximity search in large graphs.">27. Density index and proximity search in large graphs.</a></li>
<li><a href="#28. Gelling, and melting, large graphs by edge manipulation.">28. Gelling, and melting, large graphs by edge manipulation.</a></li>
</ul>
</li>
<li><a href="#IR track: language technologies    5">IR track: language technologies    5</a><ul>
<li><a href="#29. One seed to find them all: mining opinion features via association.">29. One seed to find them all: mining opinion features via association.</a></li>
<li><a href="#30. Topic-driven reader comments summarization.">30. Topic-driven reader comments summarization.</a></li>
<li><a href="#31. Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams.">31. Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams.</a></li>
<li><a href="#32. Fast multi-task learning for query spelling correction.">32. Fast multi-task learning for query spelling correction.</a></li>
<li><a href="#33. Cross-argument inference for implicit discourse relation recognition.">33. Cross-argument inference for implicit discourse relation recognition.</a></li>
</ul>
</li>
<li><a href="#DB track: graph and knowledge base    5">DB track: graph and knowledge base    5</a><ul>
<li><a href="#34. Interpreting keyword queries over web knowledge bases.">34. Interpreting keyword queries over web knowledge bases.</a></li>
<li><a href="#35. RDF pattern matching using sortable views.">35. RDF pattern matching using sortable views.</a></li>
<li><a href="#36. Efficient algorithms for generalized subgraph query processing.">36. Efficient algorithms for generalized subgraph query processing.</a></li>
<li><a href="#37. G-SPARQL: a hybrid engine for querying large attributed graphs.">37. G-SPARQL: a hybrid engine for querying large attributed graphs.</a></li>
<li><a href="#38. A graph-based approach for ontology population with named entities.">38. A graph-based approach for ontology population with named entities.</a></li>
</ul>
</li>
<li><a href="#DB track: temporal, spatial and multimedia databases    5">DB track: temporal, spatial and multimedia databases    5</a><ul>
<li><a href="#39. Decomposition-by-normalization (DBN">39. Decomposition-by-normalization (DBN): leveraging approximate functional dependencies for efficient tensor decomposition.</a>: leveraging approximate functional dependencies for efficient tensor decomposition.)</li>
<li><a href="#40. A filter-based protocol for continuous queries over imprecise location data.">40. A filter-based protocol for continuous queries over imprecise location data.</a></li>
<li><a href="#41. Leveraging read rates of passive RFID tags for real-time indoor location tracking.">41. Leveraging read rates of passive RFID tags for real-time indoor location tracking.</a></li>
<li><a href="#42. Location-aware instant search.">42. Location-aware instant search.</a></li>
<li><a href="#43. Indexing uncertain spatio-temporal data.">43. Indexing uncertain spatio-temporal data.</a></li>
</ul>
</li>
<li><a href="#KM track: matrix methods and anomaly detection    4">KM track: matrix methods and anomaly detection    4</a><ul>
<li><a href="#44. Local anomaly descriptor: a robust unsupervised algorithm for anomaly detection based on diffusion space.">44. Local anomaly descriptor: a robust unsupervised algorithm for anomaly detection based on diffusion space.</a></li>
<li><a href="#45. Fast and reliable anomaly detection in categorical data.">45. Fast and reliable anomaly detection in categorical data.</a></li>
<li><a href="#46. TALMUD: transfer learning for multiple domains.">46. TALMUD: transfer learning for multiple domains.</a></li>
<li><a href="#47. Utilizing common substructures to speedup tensor factorization for mining dynamic graphs.">47. Utilizing common substructures to speedup tensor factorization for mining dynamic graphs.</a></li>
</ul>
</li>
<li><a href="#KM track: social networks    4">KM track: social networks    4</a><ul>
<li><a href="#48. Predicting emerging social conventions in online social networks.">48. Predicting emerging social conventions in online social networks.</a></li>
<li><a href="#49. Collective intelligence in the online social network of yahoo!answers and its implications.">49. Collective intelligence in the online social network of yahoo!answers and its implications.</a></li>
<li><a href="#50. From face-to-face gathering to social structure.">50. From face-to-face gathering to social structure.</a></li>
<li><a href="#51. Delineating social network data anonymization via random edge perturbation.">51. Delineating social network data anonymization via random edge perturbation.</a></li>
</ul>
</li>
<li><a href="#IR track: advertising    4">IR track: advertising    4</a><ul>
<li><a href="#52. Multiview hierarchical bayesian regression model andapplication to online advertising.">52. Multiview hierarchical bayesian regression model andapplication to online advertising.</a></li>
<li><a href="#53. Visual appearance of display ads and its effect on click through rate.">53. Visual appearance of display ads and its effect on click through rate.</a></li>
<li><a href="#54. The wisdom of advertisers: mining subgoals via query clustering.">54. The wisdom of advertisers: mining subgoals via query clustering.</a></li>
<li><a href="#55. Sequential selection of correlated ads by POMDPs.">55. Sequential selection of correlated ads by POMDPs.</a></li>
</ul>
</li>
<li><a href="#IR track: system architecture, distributed IR, scalability    4">IR track: system architecture, distributed IR, scalability    4</a><ul>
<li><a href="#56. Diversity in blog feed retrieval.">56. Diversity in blog feed retrieval.</a></li>
<li><a href="#57. Efficient retrieval of recommendations in a matrix factorization framework.">57. Efficient retrieval of recommendations in a matrix factorization framework.</a></li>
<li><a href="#58. KORE: keyphrase overlap relatedness for entity disambiguation.">58. KORE: keyphrase overlap relatedness for entity disambiguation.</a></li>
<li><a href="#59. Shard ranking and cutoff estimation for topically partitioned collections.">59. Shard ranking and cutoff estimation for topically partitioned collections.</a></li>
</ul>
</li>
<li><a href="#KM track: advertisement and products    5">KM track: advertisement and products    5</a><ul>
<li><a href="#60. Daily-deal selection for revenue maximization.">60. Daily-deal selection for revenue maximization.</a></li>
<li><a href="#61. Enabling direct interest-aware audience selection.">61. Enabling direct interest-aware audience selection.</a></li>
<li><a href="#62. Influence propagation in adversarial setting: how to defeat competition with least amount of investment.">62. Influence propagation in adversarial setting: how to defeat competition with least amount of investment.</a></li>
<li><a href="#63. Large-scale item categorization for e-commerce.">63. Large-scale item categorization for e-commerce.</a></li>
<li><a href="#64. Matching product titles using web-based enrichment.">64. Matching product titles using web-based enrichment.</a></li>
</ul>
</li>
<li><a href="#KM track: clustering    5">KM track: clustering    5</a><ul>
<li><a href="#65. Scalable clustering of signed networks using balance normalized cut.">65. Scalable clustering of signed networks using balance normalized cut.</a></li>
<li><a href="#66. Maximum margin clustering on evolutionary data.">66. Maximum margin clustering on evolutionary data.</a></li>
<li><a href="#67. Document-topic hierarchies from document graphs.">67. Document-topic hierarchies from document graphs.</a></li>
<li><a href="#68. Improving document clustering using automated machine translation.">68. Improving document clustering using automated machine translation.</a></li>
<li><a href="#69. Right-protected data publishing with hierarchical clustering preservation.">69. Right-protected data publishing with hierarchical clustering preservation.</a></li>
</ul>
</li>
<li><a href="#IR track: recommendation systems    5">IR track: recommendation systems    5</a><ul>
<li><a href="#70. Metaphor: a system for related search recommendations.">70. Metaphor: a system for related search recommendations.</a></li>
<li><a href="#71. Exploring personal impact for group recommendation.">71. Exploring personal impact for group recommendation.</a></li>
<li><a href="#72. The efficient imputation method for neighborhood-based collaborative filtering.">72. The efficient imputation method for neighborhood-based collaborative filtering.</a></li>
<li><a href="#73. Multi-faceted ranking of news articles using post-read actions.">73. Multi-faceted ranking of news articles using post-read actions.</a></li>
<li><a href="#74. A decentralized recommender system for effective web credibility assessment.">74. A decentralized recommender system for effective web credibility assessment.</a></li>
</ul>
</li>
<li><a href="#IR track: digital libraries and citation analysis    5">IR track: digital libraries and citation analysis    5</a><ul>
<li><a href="#75. Towards an effective and unbiased ranking of scientific literature through mutual reinforcement.">75. Towards an effective and unbiased ranking of scientific literature through mutual reinforcement.</a></li>
<li><a href="#76. A math-aware search engine for math question answering system.">76. A math-aware search engine for math question answering system.</a></li>
<li><a href="#77. Contextualization using hyperlinks and internal hierarchical structure of Wikipedia documents.">77. Contextualization using hyperlinks and internal hierarchical structure of Wikipedia documents.</a></li>
<li><a href="#78. Understanding book search behavior on the web.">78. Understanding book search behavior on the web.</a></li>
<li><a href="#79. Temporal corpus summarization using submodular word coverage.">79. Temporal corpus summarization using submodular word coverage.</a></li>
</ul>
</li>
<li><a href="#KM track: text mining    5">KM track: text mining    5</a><ul>
<li><a href="#80. TCSST: transfer classification of short & sparse text using external data.">80. TCSST: transfer classification of short &amp; sparse text using external data.</a></li>
<li><a href="#81. The generalized dirichlet distribution in enhanced topic detection.">81. The generalized dirichlet distribution in enhanced topic detection.</a></li>
<li><a href="#82. Modeling topic hierarchies with the recursive chinese restaurant process.">82. Modeling topic hierarchies with the recursive chinese restaurant process.</a></li>
<li><a href="#83. Two-part segmentation of text documents.">83. Two-part segmentation of text documents.</a></li>
<li><a href="#84. On the design of LDA models for aspect-based opinion mining.">84. On the design of LDA models for aspect-based opinion mining.</a></li>
</ul>
</li>
<li><a href="#IR track: formal retrieval models and learning to rank    5">IR track: formal retrieval models and learning to rank    5</a><ul>
<li><a href="#85. Predicting query performance for fusion-based retrieval.">85. Predicting query performance for fusion-based retrieval.</a></li>
<li><a href="#86. Back to the roots: a probabilistic framework for query-performance prediction.">86. Back to the roots: a probabilistic framework for query-performance prediction.</a></li>
<li><a href="#87. Learning to rank for robust question answering.">87. Learning to rank for robust question answering.</a></li>
<li><a href="#88. Learning to rank by aggregating expert preferences.">88. Learning to rank by aggregating expert preferences.</a></li>
<li><a href="#89. Learning to rank duplicate bug reports.">89. Learning to rank duplicate bug reports.</a></li>
</ul>
</li>
<li><a href="#DB track: probabilistic and uncertain data    5">DB track: probabilistic and uncertain data    5</a><ul>
<li><a href="#90. A model-based approach for RFID data stream cleansing.">90. A model-based approach for RFID data stream cleansing.</a></li>
<li><a href="#91. What is the IQ of your data transformation system?">91. What is the IQ of your data transformation system?</a></li>
<li><a href="#92. On the foundations of probabilistic information integration.">92. On the foundations of probabilistic information integration.</a></li>
<li><a href="#93. GPU acceleration of probabilistic frequent itemset mining from uncertain databases.">93. GPU acceleration of probabilistic frequent itemset mining from uncertain databases.</a></li>
<li><a href="#94. Completeness of queries over SQL databases.">94. Completeness of queries over SQL databases.</a></li>
</ul>
</li>
<li><a href="#DB track: top-k and nearest neighbor queries    5">DB track: top-k and nearest neighbor queries    5</a><ul>
<li><a href="#95. Being picky: processing top-k queries with set-defined selections.">95. Being picky: processing top-k queries with set-defined selections.</a></li>
<li><a href="#96. Finding top k most influential spatial facilities over uncertain objects.">96. Finding top k most influential spatial facilities over uncertain objects.</a></li>
<li><a href="#97. Efficient safe-region construction for moving top-K spatial keyword queries.">97. Efficient safe-region construction for moving top-K spatial keyword queries.</a></li>
<li><a href="#98. Monochromatic and bichromatic reverse nearest neighbor queries on land surfaces.">98. Monochromatic and bichromatic reverse nearest neighbor queries on land surfaces.</a></li>
<li><a href="#99. Pay-as-you-go maintenance of precomputed nearest neighbors in large graphs.">99. Pay-as-you-go maintenance of precomputed nearest neighbors in large graphs.</a></li>
</ul>
</li>
<li><a href="#KM track: spatial and temporal methods    4">KM track: spatial and temporal methods    4</a><ul>
<li><a href="#100. Spatial influence vs. community influence: modeling the global spread of social media.">100. Spatial influence vs. community influence: modeling the global spread of social media.</a></li>
<li><a href="#101. TUT: a statistical model for detecting trends, topics and user interests in social media.">101. TUT: a statistical model for detecting trends, topics and user interests in social media.</a></li>
<li><a href="#102. Predicting aggregate social activities using continuous-time stochastic process.">102. Predicting aggregate social activities using continuous-time stochastic process.</a></li>
<li><a href="#103. Acquiring temporal constraints between relations.">103. Acquiring temporal constraints between relations.</a></li>
</ul>
</li>
<li><a href="#IR track: web search    4">IR track: web search    4</a><ul>
<li><a href="#104. Towards optimum query segmentation: in doubt without.">104. Towards optimum query segmentation: in doubt without.</a></li>
<li><a href="#105. Leaving so soon?: understanding and predicting web search abandonment rationales.">105. Leaving so soon?: understanding and predicting web search abandonment rationales.</a></li>
<li><a href="#106. Click patterns: an empirical representation of complex query intents.">106. Click patterns: an empirical representation of complex query intents.</a></li>
<li><a href="#107. Domain dependent query reformulation for web search.">107. Domain dependent query reformulation for web search.</a></li>
</ul>
</li>
<li><a href="#DB track: web data management    4">DB track: web data management    4</a><ul>
<li><a href="#108. An automatic blocking mechanism for large-scale de-duplication tasks.">108. An automatic blocking mechanism for large-scale de-duplication tasks.</a></li>
<li><a href="#109. Processing continuous text queries featuring non-homogeneous scoring functions.">109. Processing continuous text queries featuring non-homogeneous scoring functions.</a></li>
<li><a href="#110. Comprehension-based result snippets.">110. Comprehension-based result snippets.</a></li>
<li><a href="#111. An effective rule miner for instance matching in a web of data.">111. An effective rule miner for instance matching in a web of data.</a></li>
</ul>
</li>
<li><a href="#KM track: information extraction    5">KM track: information extraction    5</a><ul>
<li><a href="#112. Non-stationary bayesian networks based on perfect simulation.">112. Non-stationary bayesian networks based on perfect simulation.</a></li>
<li><a href="#113. Active learning for relation type extension with local and global data views.">113. Active learning for relation type extension with local and global data views.</a></li>
<li><a href="#114. Segmenting web-domains and hashtags using length specific models.">114. Segmenting web-domains and hashtags using length specific models.</a></li>
<li><a href="#115. Crosslingual distant supervision for extracting relations of different complexity.">115. Crosslingual distant supervision for extracting relations of different complexity.</a></li>
<li><a href="#116. Labeling by landscaping: classifying tokens in context by pruning and decorating trees.">116. Labeling by landscaping: classifying tokens in context by pruning and decorating trees.</a></li>
</ul>
</li>
<li><a href="#IR track: topic modeling and content and sentiment analysis    4">IR track: topic modeling and content and sentiment analysis    4</a><ul>
<li><a href="#117. G-WSTD: a framework for geographic web search topic discovery.">117. G-WSTD: a framework for geographic web search topic discovery.</a></li>
<li><a href="#118. Supporting factual statements with evidence from the web.">118. Supporting factual statements with evidence from the web.</a></li>
<li><a href="#119. Role-explicit query identification and intent role annotation.">119. Role-explicit query identification and intent role annotation.</a></li>
<li><a href="#120. Joint topic modeling for event summarization across news and social media streams.">120. Joint topic modeling for event summarization across news and social media streams.</a></li>
</ul>
</li>
<li><a href="#DB track: query processing, optimization and performance    5">DB track: query processing, optimization and performance    5</a><ul>
<li><a href="#121. CGStream: continuous correlated graph query for data streams.">121. CGStream: continuous correlated graph query for data streams.</a></li>
<li><a href="#122. Efficient influence-based processing of market research queries.">122. Efficient influence-based processing of market research queries.</a></li>
<li><a href="#123. Deco: declarative crowdsourcing.">123. Deco: declarative crowdsourcing.</a></li>
<li><a href="#124. Predicting the effectiveness of keyword queries on databases.">124. Predicting the effectiveness of keyword queries on databases.</a></li>
<li><a href="#125. You can stop early with COLA: online processing of aggregate queries in the cloud.">125. You can stop early with COLA: online processing of aggregate queries in the cloud.</a></li>
</ul>
</li>
<li><a href="#KM track: classification and semantic methods    5">KM track: classification and semantic methods    5</a><ul>
<li><a href="#126. A novel local patch framework for fixing supervised learning models.">126. A novel local patch framework for fixing supervised learning models.</a></li>
<li><a href="#127. Automated feature weighting in naive bayes for high-dimensional data classification.">127. Automated feature weighting in naive bayes for high-dimensional data classification.</a></li>
<li><a href="#128. Learning to discover complex mappings from web forms to ontologies.">128. Learning to discover complex mappings from web forms to ontologies.</a></li>
<li><a href="#129. Modeling semantic relations between visual attributes and object categories via dirichlet forest prior.">129. Modeling semantic relations between visual attributes and object categories via dirichlet forest prior.</a></li>
<li><a href="#130. CoNet: feature generation for multi-view semi-supervised learning with partially observed views.">130. CoNet: feature generation for multi-view semi-supervised learning with partially observed views.</a></li>
</ul>
</li>
<li><a href="#IR track: multimedia and user feedback    5">IR track: multimedia and user feedback    5</a><ul>
<li><a href="#131. Generating facets for phone-based navigation of structured data.">131. Generating facets for phone-based navigation of structured data.</a></li>
<li><a href="#132. The effect of aggregated search coherence on search behavior.">132. The effect of aggregated search coherence on search behavior.</a></li>
<li><a href="#133. Improving bag-of-visual-words model with spatial-temporal correlation for video retrieval.">133. Improving bag-of-visual-words model with spatial-temporal correlation for video retrieval.</a></li>
<li><a href="#134. Exploring and predicting search task difficulty.">134. Exploring and predicting search task difficulty.</a></li>
<li><a href="#135. Iterative relevance feedback with adaptive exploration/exploitation trade-off.">135. Iterative relevance feedback with adaptive exploration/exploitation trade-off.</a></li>
</ul>
</li>
<li><a href="#DB track: emerging and advanced topics    5">DB track: emerging and advanced topics    5</a><ul>
<li><a href="#136. A practical concurrent index for solid-state drives.">136. A practical concurrent index for solid-state drives.</a></li>
<li><a href="#137. Robust distributed indexing for locality-skewed workloads.">137. Robust distributed indexing for locality-skewed workloads.</a></li>
<li><a href="#138. Efficient provenance storage for relational queries.">138. Efficient provenance storage for relational queries.</a></li>
<li><a href="#139. Generically extending anonymization algorithms to deal with successive queries.">139. Generically extending anonymization algorithms to deal with successive queries.</a></li>
<li><a href="#140. Authentication of moving range queries.">140. Authentication of moving range queries.</a></li>
</ul>
</li>
<li><a href="#KM track: novel applications    4">KM track: novel applications    4</a><ul>
<li><a href="#141. Model the complex dependence structures of financial variables by using canonical vine.">141. Model the complex dependence structures of financial variables by using canonical vine.</a></li>
<li><a href="#142. A unified learning framework for auto face annotation by mining web facial images.">142. A unified learning framework for auto face annotation by mining web facial images.</a></li>
<li><a href="#143. Efficient jaccard-based diversity analysis of large document collections.">143. Efficient jaccard-based diversity analysis of large document collections.</a></li>
<li><a href="#144. Knowing where and how criminal organizations operate using web content.">144. Knowing where and how criminal organizations operate using web content.</a></li>
</ul>
</li>
<li><a href="#IR track: social networks    4">IR track: social networks    4</a><ul>
<li><a href="#145. Social recommendation across multiple relational domains.">145. Social recommendation across multiple relational domains.</a></li>
<li><a href="#146. Mining competitive relationships by learning across heterogeneous networks.">146. Mining competitive relationships by learning across heterogeneous networks.</a></li>
<li><a href="#147. Evaluating geo-social influence in location-based social networks.">147. Evaluating geo-social influence in location-based social networks.</a></li>
<li><a href="#148. The walls have ears: optimize sharing for visibility and privacy in online social networks.">148. The walls have ears: optimize sharing for visibility and privacy in online social networks.</a></li>
</ul>
</li>
<li><a href="#Knowledge management short paper session    59">Knowledge management short paper session    59</a><ul>
<li><a href="#149. Influence and similarity on heterogeneous networks.">149. Influence and similarity on heterogeneous networks.</a></li>
<li><a href="#150. GRAFT: an approximate graphlet counting algorithm for large graph analysis.">150. GRAFT: an approximate graphlet counting algorithm for large graph analysis.</a></li>
<li><a href="#151. Hierarchical co-clustering based on entropy splitting.">151. Hierarchical co-clustering based on entropy splitting.</a></li>
<li><a href="#152. Mining long-lasting exploratory user interests from search history.">152. Mining long-lasting exploratory user interests from search history.</a></li>
<li><a href="#153. Feature selection based on term frequency and T-test for text categorization.">153. Feature selection based on term frequency and T-test for text categorization.</a></li>
<li><a href="#154. Adapting vector space model to ranking-based collaborative filtering.">154. Adapting vector space model to ranking-based collaborative filtering.</a></li>
<li><a href="#155. Joint relevance and answer quality learning for question routing in community QA.">155. Joint relevance and answer quality learning for question routing in community QA.</a></li>
<li><a href="#156. Fast approximation of steiner trees in large graphs.">156. Fast approximation of steiner trees in large graphs.</a></li>
<li><a href="#157. Automatically embedding newsworthy links to articles.">157. Automatically embedding newsworthy links to articles.</a></li>
<li><a href="#158. Learning spectral embedding via iterative eigenvalue thresholding.">158. Learning spectral embedding via iterative eigenvalue thresholding.</a></li>
<li><a href="#159. Measuring robustness of complex networks under MVC attack.">159. Measuring robustness of complex networks under MVC attack.</a></li>
<li><a href="#160. A simple approach to the design of site-level extractors using domain-centric principles.">160. A simple approach to the design of site-level extractors using domain-centric principles.</a></li>
<li><a href="#161. Extraction of topic evolutions from references in scientific articles and its GPU acceleration.">161. Extraction of topic evolutions from references in scientific articles and its GPU acceleration.</a></li>
<li><a href="#162. Graph-based workflow recommendation: on improving business process modeling.">162. Graph-based workflow recommendation: on improving business process modeling.</a></li>
<li><a href="#163. Reconciling ontologies and the web of data.">163. Reconciling ontologies and the web of data.</a></li>
<li><a href="#164. Efficient extraction of ontologies from domain specific text corpora.">164. Efficient extraction of ontologies from domain specific text corpora.</a></li>
<li><a href="#165. Effective and efficient?: bilingual sentiment lexicon extraction using collocation alignment.">165. Effective and efficient?: bilingual sentiment lexicon extraction using collocation alignment.</a></li>
<li><a href="#166. Exploiting latent relevance for relational learning of ubiquitous things.">166. Exploiting latent relevance for relational learning of ubiquitous things.</a></li>
<li><a href="#167. Discovering personally semantic places from GPS trajectories.">167. Discovering personally semantic places from GPS trajectories.</a></li>
<li><a href="#168. Mining coherent anomaly collections on web data.">168. Mining coherent anomaly collections on web data.</a></li>
<li><a href="#169. Mining topic-level opinion influence in microblog.">169. Mining topic-level opinion influence in microblog.</a></li>
<li><a href="#170. Meta path-based collective classification in heterogeneous information networks.">170. Meta path-based collective classification in heterogeneous information networks.</a></li>
<li><a href="#171. Discretionary social network data revelation with a user-centric utility guarantee.">171. Discretionary social network data revelation with a user-centric utility guarantee.</a></li>
<li><a href="#172. Empirical validation of the buckley-osthus model for the web host graph: degree and edge distributions.">172. Empirical validation of the buckley-osthus model for the web host graph: degree and edge distributions.</a></li>
<li><a href="#173. gSCorr: modeling geo-social correlations for new check-ins on location-based social networks.">173. gSCorr: modeling geo-social correlations for new check-ins on location-based social networks.</a></li>
<li><a href="#174. Swimming against the streamz: search and analytics over the enterprise activity stream.">174. Swimming against the streamz: search and analytics over the enterprise activity stream.</a></li>
<li><a href="#175. What is happening right now ... that interests me?: online topic discovery and recommendation in twitter.">175. What is happening right now ... that interests me?: online topic discovery and recommendation in twitter.</a></li>
<li><a href="#176. Frequent grams based embedding for privacy preserving record linkage.">176. Frequent grams based embedding for privacy preserving record linkage.</a></li>
<li><a href="#177. If you are happy and you know it... tweet.">177. If you are happy and you know it... tweet.</a></li>
<li><a href="#178. PRemiSE: personalized news recommendation via implicit social experts.">178. PRemiSE: personalized news recommendation via implicit social experts.</a></li>
<li><a href="#179. Hierarchical topic integration through semi-supervised hierarchical topic modeling.">179. Hierarchical topic integration through semi-supervised hierarchical topic modeling.</a></li>
<li><a href="#180. Exploiting enriched contextual information for mobile app classification.">180. Exploiting enriched contextual information for mobile app classification.</a></li>
<li><a href="#181. Incorporating word correlation into tag-topic model for semantic knowledge acquisition.">181. Incorporating word correlation into tag-topic model for semantic knowledge acquisition.</a></li>
<li><a href="#182. PriSM: discovering and prioritizing severe technical issues from product discussion forums.">182. PriSM: discovering and prioritizing severe technical issues from product discussion forums.</a></li>
<li><a href="#183. Preprocessing of informal mathematical discourse in context ofcontrolled natural language.">183. Preprocessing of informal mathematical discourse in context ofcontrolled natural language.</a></li>
<li><a href="#184. PathRank: a novel node ranking measure on a heterogeneous graph for recommender systems.">184. PathRank: a novel node ranking measure on a heterogeneous graph for recommender systems.</a></li>
<li><a href="#185. Unsupervised discovery of opposing opinion networks from forum discussions.">185. Unsupervised discovery of opposing opinion networks from forum discussions.</a></li>
<li><a href="#186. Exploring the existing category hierarchy to automatically label the newly-arising topics in cQA.">186. Exploring the existing category hierarchy to automatically label the newly-arising topics in cQA.</a></li>
<li><a href="#187. Query-focused multi-document summarization based on query-sensitive feature space.">187. Query-focused multi-document summarization based on query-sensitive feature space.</a></li>
<li><a href="#188. Time-aware topic recommendation based on micro-blogs.">188. Time-aware topic recommendation based on micro-blogs.</a></li>
<li><a href="#189. Topic-sensitive probabilistic model for expert finding in question answer communities.">189. Topic-sensitive probabilistic model for expert finding in question answer communities.</a></li>
<li><a href="#190. iSampling: framework for developing sampling methods considering user's interest.">190. iSampling: framework for developing sampling methods considering user's interest.</a></li>
<li><a href="#191. WiSeNet: building a wikipedia-based semantic network with ontologized relations.">191. WiSeNet: building a wikipedia-based semantic network with ontologized relations.</a></li>
<li><a href="#192. Shaping communities out of triangles.">192. Shaping communities out of triangles.</a></li>
<li><a href="#193. The early-adopter graph and its application to web-page recommendation.">193. The early-adopter graph and its application to web-page recommendation.</a></li>
<li><a href="#194. Relational co-clustering via manifold ensemble learning.">194. Relational co-clustering via manifold ensemble learning.</a></li>
<li><a href="#195. SemaFor: semantic document indexing using semantic forests.">195. SemaFor: semantic document indexing using semantic forests.</a></li>
<li><a href="#196. Measuring website similarity using an entity-aware click graph.">196. Measuring website similarity using an entity-aware click graph.</a></li>
<li><a href="#197. Community-based classification of noun phrases in twitter.">197. Community-based classification of noun phrases in twitter.</a></li>
<li><a href="#198. Real-time bid optimization for group-buying ads.">198. Real-time bid optimization for group-buying ads.</a></li>
<li><a href="#199. Degree relations of triangles in real-world networks and graph models.">199. Degree relations of triangles in real-world networks and graph models.</a></li>
<li><a href="#200. A probabilistic approach to mining geospatial knowledge from social annotations.">200. A probabilistic approach to mining geospatial knowledge from social annotations.</a></li>
<li><a href="#201. Providing grades and feedback for student summaries by ontology-based information extraction.">201. Providing grades and feedback for student summaries by ontology-based information extraction.</a></li>
<li><a href="#202. Joint bilingual name tagging for parallel corpora.">202. Joint bilingual name tagging for parallel corpora.</a></li>
<li><a href="#203. Using program synthesis for social recommendations.">203. Using program synthesis for social recommendations.</a></li>
<li><a href="#204. Web-scale multi-task feature selection for behavioral targeting.">204. Web-scale multi-task feature selection for behavioral targeting.</a></li>
<li><a href="#205. Balanced coverage of aspects for text summarization.">205. Balanced coverage of aspects for text summarization.</a></li>
<li><a href="#206. Dynamic effects of ad impressions on commercial actions in display advertising.">206. Dynamic effects of ad impressions on commercial actions in display advertising.</a></li>
<li><a href="#207. A hybrid approach for efficient provenance storage.">207. A hybrid approach for efficient provenance storage.</a></li>
</ul>
</li>
<li><a href="#Information retrieval short paper session    61">Information retrieval short paper session    61</a><ul>
<li><a href="#208. Content-based relevance estimation on the web using inter-document similarities.">208. Content-based relevance estimation on the web using inter-document similarities.</a></li>
<li><a href="#209. Trust prediction via aggregating heterogeneous social networks.">209. Trust prediction via aggregating heterogeneous social networks.</a></li>
<li><a href="#210. Estimating interleaved comparison outcomes from historical click data.">210. Estimating interleaved comparison outcomes from historical click data.</a></li>
<li><a href="#211. Automatic image annotation using tag-related random search over visual neighbors.">211. Automatic image annotation using tag-related random search over visual neighbors.</a></li>
<li><a href="#212. Diversionary comments under political blog posts.">212. Diversionary comments under political blog posts.</a></li>
<li><a href="#213. Discover breaking events with popular hashtags in twitter.">213. Discover breaking events with popular hashtags in twitter.</a></li>
<li><a href="#214. Query likelihood with negative query generation.">214. Query likelihood with negative query generation.</a></li>
<li><a href="#215. On the connections between explicit semantic analysis and latent semantic analysis.">215. On the connections between explicit semantic analysis and latent semantic analysis.</a></li>
<li><a href="#216. Variance maximization via noise injection for active sampling in learning to rank.">216. Variance maximization via noise injection for active sampling in learning to rank.</a></li>
<li><a href="#217. More than relevance: high utility query recommendation by mining users' search behaviors.">217. More than relevance: high utility query recommendation by mining users' search behaviors.</a></li>
<li><a href="#218. Finding nuggets in IP portfolios: core patent mining through textual temporal analysis.">218. Finding nuggets in IP portfolios: core patent mining through textual temporal analysis.</a></li>
<li><a href="#219. Interest-matching information propagation in multiple online social networks.">219. Interest-matching information propagation in multiple online social networks.</a></li>
<li><a href="#220. Customizing search results for non-native speakers.">220. Customizing search results for non-native speakers.</a></li>
<li><a href="#221. Quality models for microblog retrieval.">221. Quality models for microblog retrieval.</a></li>
<li><a href="#222. Do ads compete or collaborate?: designing click models with full relationship incorporated.">222. Do ads compete or collaborate?: designing click models with full relationship incorporated.</a></li>
<li><a href="#223. Exploiting concept hierarchy for result diversification.">223. Exploiting concept hierarchy for result diversification.</a></li>
<li><a href="#224. Ranking news events by influence decay and information fusion for media and users.">224. Ranking news events by influence decay and information fusion for media and users.</a></li>
<li><a href="#225. Leveraging tagging for neighborhood-aware probabilistic matrix factorization.">225. Leveraging tagging for neighborhood-aware probabilistic matrix factorization.</a></li>
<li><a href="#226. Semantic context learning with large-scale weakly-labeled image set.">226. Semantic context learning with large-scale weakly-labeled image set.</a></li>
<li><a href="#227. Sketch-based indexing of n-words.">227. Sketch-based indexing of n-words.</a></li>
<li><a href="#228. Interactive and context-aware tag spell check and correction.">228. Interactive and context-aware tag spell check and correction.</a></li>
<li><a href="#229. Federated search in the wild: the combined power of over a hundred search engines.">229. Federated search in the wild: the combined power of over a hundred search engines.</a></li>
<li><a href="#230. From sBoW to dCoT marginalized encoders for text representation.">230. From sBoW to dCoT marginalized encoders for text representation.</a></li>
<li><a href="#231. Task tours: helping users tackle complex search tasks.">231. Task tours: helping users tackle complex search tasks.</a></li>
<li><a href="#232. Structured query reformulations in commerce search.">232. Structured query reformulations in commerce search.</a></li>
<li><a href="#233. Towards jointly extracting aspects and aspect-specific sentiment knowledge.">233. Towards jointly extracting aspects and aspect-specific sentiment knowledge.</a></li>
<li><a href="#234. Collaborative ranking: improving the relevance for tail queries.">234. Collaborative ranking: improving the relevance for tail queries.</a></li>
<li><a href="#235. BiasTrust: teaching biased users about controversial topics.">235. BiasTrust: teaching biased users about controversial topics.</a></li>
<li><a href="#236. Recommending citations: translating papers into references.">236. Recommending citations: translating papers into references.</a></li>
<li><a href="#237. Query-biased learning to rank for real-time twitter search.">237. Query-biased learning to rank for real-time twitter search.</a></li>
<li><a href="#238. Discovering logical knowledge for deep question answering.">238. Discovering logical knowledge for deep question answering.</a></li>
<li><a href="#239. Mining noisy tagging from multi-label space.">239. Mining noisy tagging from multi-label space.</a></li>
<li><a href="#240. Learning from mistakes: towards a correctable learning algorithm.">240. Learning from mistakes: towards a correctable learning algorithm.</a></li>
<li><a href="#241. CONSENTO: a new framework for opinion based entity search and summarization.">241. CONSENTO: a new framework for opinion based entity search and summarization.</a></li>
<li><a href="#242. Search result presentation based on faceted clustering.">242. Search result presentation based on faceted clustering.</a></li>
<li><a href="#243. PolariCQ: polarity classification of political quotations.">243. PolariCQ: polarity classification of political quotations.</a></li>
<li><a href="#244. A comprehensive analysis of parameter settings for novelty-biased cumulative gain.">244. A comprehensive analysis of parameter settings for novelty-biased cumulative gain.</a></li>
<li><a href="#245. Entity centric query expansion for enterprise search.">245. Entity centric query expansion for enterprise search.</a></li>
<li><a href="#246. Location-sensitive resources recommendation in social tagging systems.">246. Location-sensitive resources recommendation in social tagging systems.</a></li>
<li><a href="#247. Differences in effectiveness across sub-collections.">247. Differences in effectiveness across sub-collections.</a></li>
<li><a href="#248. Map to humans and reduce error: crowdsourcing for deduplication applied to digital libraries.">248. Map to humans and reduce error: crowdsourcing for deduplication applied to digital libraries.</a></li>
<li><a href="#249. Full-text citation analysis: enhancing bibliometric and scientific publication ranking.">249. Full-text citation analysis: enhancing bibliometric and scientific publication ranking.</a></li>
<li><a href="#250. Detecting offensive tweets via topical feature discovery over a large scale twitter corpus.">250. Detecting offensive tweets via topical feature discovery over a large scale twitter corpus.</a></li>
<li><a href="#251. Automatic query expansion based on tag recommendation.">251. Automatic query expansion based on tag recommendation.</a></li>
<li><a href="#252. The downside of markup: examining the harmful effects of CSS and javascript on indexing today's web.">252. The downside of markup: examining the harmful effects of CSS and javascript on indexing today's web.</a></li>
<li><a href="#253. You should read this! let me explain you why: explaining news recommendations to users.">253. You should read this! let me explain you why: explaining news recommendations to users.</a></li>
<li><a href="#254. Characterizing web search queries that match very few or no results.">254. Characterizing web search queries that match very few or no results.</a></li>
<li><a href="#255. A unified optimization framework for auction and guaranteed delivery in online advertising.">255. A unified optimization framework for auction and guaranteed delivery in online advertising.</a></li>
<li><a href="#256. Query recommendation for children.">256. Query recommendation for children.</a></li>
<li><a href="#257. Modeling browsing behavior for click analysis in sponsored search.">257. Modeling browsing behavior for click analysis in sponsored search.</a></li>
<li><a href="#258. Sentiment-focused web crawling.">258. Sentiment-focused web crawling.</a></li>
<li><a href="#259. User guided entity similarity search using meta-path selection in heterogeneous information networks.">259. User guided entity similarity search using meta-path selection in heterogeneous information networks.</a></li>
<li><a href="#260. User activity profiling with multi-layer analysis.">260. User activity profiling with multi-layer analysis.</a></li>
<li><a href="#261. GTE: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets.">261. GTE: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets.</a></li>
<li><a href="#262. Stochastic simulation of time-biased gain.">262. Stochastic simulation of time-biased gain.</a></li>
<li><a href="#263. SonetRank: leveraging social networks to personalize search.">263. SonetRank: leveraging social networks to personalize search.</a></li>
<li><a href="#264. Predicting web search success with fine-grained interaction data.">264. Predicting web search success with fine-grained interaction data.</a></li>
<li><a href="#265. Multi-session re-search: in pursuit of repetition and diversification.">265. Multi-session re-search: in pursuit of repetition and diversification.</a></li>
<li><a href="#266. Mining sentiment terminology through time.">266. Mining sentiment terminology through time.</a></li>
<li><a href="#267. Theme chronicle model: chronicle consists of timestamp and topical words over each theme.">267. Theme chronicle model: chronicle consists of timestamp and topical words over each theme.</a></li>
<li><a href="#268. Fast top-k similarity queries via matrix compression.">268. Fast top-k similarity queries via matrix compression.</a></li>
</ul>
</li>
<li><a href="#Databases short paper session    33">Databases short paper session    33</a><ul>
<li><a href="#269. Top-k retrieval using conditional preference networks.">269. Top-k retrieval using conditional preference networks.</a></li>
<li><a href="#270. Sort-based query-adaptive loading of R-trees.">270. Sort-based query-adaptive loading of R-trees.</a></li>
<li><a href="#271. Efficient logging for enterprise workloads on column-oriented in-memory databases.">271. Efficient logging for enterprise workloads on column-oriented in-memory databases.</a></li>
<li><a href="#272. Schema-free structured querying of DBpedia data.">272. Schema-free structured querying of DBpedia data.</a></li>
<li><a href="#273. Discovering conditional inclusion dependencies.">273. Discovering conditional inclusion dependencies.</a></li>
<li><a href="#274. Diversifying query results on semi-structured data.">274. Diversifying query results on semi-structured data.</a></li>
<li><a href="#275. LINDA: distributed web-of-data-scale entity matching.">275. LINDA: distributed web-of-data-scale entity matching.</a></li>
<li><a href="#276. SliceSort: efficient sorting of hierarchical data.">276. SliceSort: efficient sorting of hierarchical data.</a></li>
<li><a href="#277. Efficient buffer management for piecewise linear representation of multiple data streams.">277. Efficient buffer management for piecewise linear representation of multiple data streams.</a></li>
<li><a href="#278. On skyline groups.">278. On skyline groups.</a></li>
<li><a href="#279. Finding the optimal path over multi-cost graphs.">279. Finding the optimal path over multi-cost graphs.</a></li>
<li><a href="#280. An efficient index for massive IOT data in cloud environment.">280. An efficient index for massive IOT data in cloud environment.</a></li>
<li><a href="#281. Clustering Wikipedia infoboxes to discover their types.">281. Clustering Wikipedia infoboxes to discover their types.</a></li>
<li><a href="#282. CloST: a hadoop-based storage system for big spatio-temporal data analytics.">282. CloST: a hadoop-based storage system for big spatio-temporal data analytics.</a></li>
<li><a href="#283. Keyword-based k-nearest neighbor search in spatial databases.">283. Keyword-based k-nearest neighbor search in spatial databases.</a></li>
<li><a href="#284. Credibility-based product ranking for C2C transactions.">284. Credibility-based product ranking for C2C transactions.</a></li>
<li><a href="#285. Location selection for utility maximization with capacity constraints.">285. Location selection for utility maximization with capacity constraints.</a></li>
<li><a href="#286. Efficient estimation of dynamic density functions with an application to outlier detection.">286. Efficient estimation of dynamic density functions with an application to outlier detection.</a></li>
<li><a href="#287. A positional access method for relational databases.">287. A positional access method for relational databases.</a></li>
<li><a href="#288. Real-time aggregate monitoring with differential privacy.">288. Real-time aggregate monitoring with differential privacy.</a></li>
<li><a href="#289. Efficient distributed locality sensitive hashing.">289. Efficient distributed locality sensitive hashing.</a></li>
<li><a href="#290. Author-conference topic-connection model for academic network search.">290. Author-conference topic-connection model for academic network search.</a></li>
<li><a href="#291. Impact neighborhood indexing (INI">291. Impact neighborhood indexing (INI) in diffusion graphs.</a> in diffusion graphs.)</li>
<li><a href="#292. Loyalty-based selection: retrieving objects that persistently satisfy criteria.">292. Loyalty-based selection: retrieving objects that persistently satisfy criteria.</a></li>
<li><a href="#293. Star-Join: spatio-textual similarity join.">293. Star-Join: spatio-textual similarity join.</a></li>
<li><a href="#294. Adapt: adaptive database schema design for multi-tenant applications.">294. Adapt: adaptive database schema design for multi-tenant applications.</a></li>
<li><a href="#295. Optimizing data migration for cloud-based key-value stores.">295. Optimizing data migration for cloud-based key-value stores.</a></li>
<li><a href="#296. Applying weighted queries on probabilistic databases.">296. Applying weighted queries on probabilistic databases.</a></li>
<li><a href="#297. A new tool for multi-level partitioning in teradata.">297. A new tool for multi-level partitioning in teradata.</a></li>
<li><a href="#298. Fast PCA computation in a DBMS with aggregate UDFs and LAPACK.">298. Fast PCA computation in a DBMS with aggregate UDFs and LAPACK.</a></li>
<li><a href="#299. Scaling multiple-source entity resolution using statistically efficient transfer learning.">299. Scaling multiple-source entity resolution using statistically efficient transfer learning.</a></li>
<li><a href="#300. A probabilistic approach to correlation queries in uncertain time series data.">300. A probabilistic approach to correlation queries in uncertain time series data.</a></li>
<li><a href="#301. On bundle configuration for viral marketing in social networks.">301. On bundle configuration for viral marketing in social networks.</a></li>
</ul>
</li>
<li><a href="#Knowledge management poster session    40">Knowledge management poster session    40</a><ul>
<li><a href="#302. Learning to rank for hybrid recommendation.">302. Learning to rank for hybrid recommendation.</a></li>
<li><a href="#303. Importance weighted passive learning.">303. Importance weighted passive learning.</a></li>
<li><a href="#304. A tag-centric discriminative model for web objects classification.">304. A tag-centric discriminative model for web objects classification.</a></li>
<li><a href="#305. Outlier detection using centrality and center-proximity.">305. Outlier detection using centrality and center-proximity.</a></li>
<li><a href="#306. An effective category classification method based on a language model for question category recommendation on a cQA service.">306. An effective category classification method based on a language model for question category recommendation on a cQA service.</a></li>
<li><a href="#307. Clustering short text using Ncut-weighted non-negative matrix factorization.">307. Clustering short text using Ncut-weighted non-negative matrix factorization.</a></li>
<li><a href="#308. Polygene-based evolution: a novel framework for evolutionary algorithms.">308. Polygene-based evolution: a novel framework for evolutionary algorithms.</a></li>
<li><a href="#309. A tensor encoding model for semantic processing.">309. A tensor encoding model for semantic processing.</a></li>
<li><a href="#310. Accelerating locality preserving nonnegative matrix factorization.">310. Accelerating locality preserving nonnegative matrix factorization.</a></li>
<li><a href="#311. The twitaholic next door.: scalable friend recommender system using a concept-sensitive hash function.">311. The twitaholic next door.: scalable friend recommender system using a concept-sensitive hash function.</a></li>
<li><a href="#312. Information propagation in social rating networks.">312. Information propagation in social rating networks.</a></li>
<li><a href="#313. Maximizing revenue from strategic recommendations under decaying trust.">313. Maximizing revenue from strategic recommendations under decaying trust.</a></li>
<li><a href="#314. Weighted linear kernel with tree transformed features for malware detection.">314. Weighted linear kernel with tree transformed features for malware detection.</a></li>
<li><a href="#315. Learning to predict the cost-per-click for your ad words.">315. Learning to predict the cost-per-click for your ad words.</a></li>
<li><a href="#316. Dual word and document seed selection for semi-supervised sentiment classification.">316. Dual word and document seed selection for semi-supervised sentiment classification.</a></li>
<li><a href="#317. On empirical tradeoffs in large scale hierarchical classification.">317. On empirical tradeoffs in large scale hierarchical classification.</a></li>
<li><a href="#318. An interaction framework of service-oriented ontology learning.">318. An interaction framework of service-oriented ontology learning.</a></li>
<li><a href="#319. Infobox suggestion for Wikipedia entities.">319. Infobox suggestion for Wikipedia entities.</a></li>
<li><a href="#320. Time feature selection for identifying active household members.">320. Time feature selection for identifying active household members.</a></li>
<li><a href="#321. Text classification with relatively small positive documents and unlabeled data.">321. Text classification with relatively small positive documents and unlabeled data.</a></li>
<li><a href="#322. On compressing weighted time-evolving graphs.">322. On compressing weighted time-evolving graphs.</a></li>
<li><a href="#323. Graph-based collective classification for tweets.">323. Graph-based collective classification for tweets.</a></li>
<li><a href="#324. A word-order based graph representation for relevance identification.">324. A word-order based graph representation for relevance identification.</a></li>
<li><a href="#325. Tracing clusters in evolving graphs with node attributes.">325. Tracing clusters in evolving graphs with node attributes.</a></li>
<li><a href="#326. Prediction of retweet cascade size over time.">326. Prediction of retweet cascade size over time.</a></li>
<li><a href="#327. An efficient and simple under-sampling technique for imbalanced time series classification.">327. An efficient and simple under-sampling technique for imbalanced time series classification.</a></li>
<li><a href="#328. Top-N recommendation through belief propagation.">328. Top-N recommendation through belief propagation.</a></li>
<li><a href="#329. Mining advices from weblogs.">329. Mining advices from weblogs.</a></li>
<li><a href="#330. Parallel proximal support vector machine for high-dimensional pattern classification.">330. Parallel proximal support vector machine for high-dimensional pattern classification.</a></li>
<li><a href="#331. On using category experts for improving the performance and accuracy in recommender systems.">331. On using category experts for improving the performance and accuracy in recommender systems.</a></li>
<li><a href="#332. Finding influential products on social domination game.">332. Finding influential products on social domination game.</a></li>
<li><a href="#333. Entity resolution using search engine results.">333. Entity resolution using search engine results.</a></li>
<li><a href="#334. Tweet classification based on their lifetime duration.">334. Tweet classification based on their lifetime duration.</a></li>
<li><a href="#335. Scalable collaborative filtering using incremental update and local link prediction.">335. Scalable collaborative filtering using incremental update and local link prediction.</a></li>
<li><a href="#336. Composing activity groups in social networks.">336. Composing activity groups in social networks.</a></li>
<li><a href="#337. A co-training based method for chinese patent semantic annotation.">337. A co-training based method for chinese patent semantic annotation.</a></li>
<li><a href="#338. Automatic labeling hierarchical topics.">338. Automatic labeling hierarchical topics.</a></li>
<li><a href="#339. An unsupervised method for author extraction from web pages containing user-generated content.">339. An unsupervised method for author extraction from web pages containing user-generated content.</a></li>
<li><a href="#340. Hierarchical target type identification for entity-oriented queries.">340. Hierarchical target type identification for entity-oriented queries.</a></li>
<li><a href="#341. Dictionary based sparse representation for domain adaptation.">341. Dictionary based sparse representation for domain adaptation.</a></li>
</ul>
</li>
<li><a href="#Information retrieval poster session    60">Information retrieval poster session    60</a><ul>
<li><a href="#342. Selecting expansion terms as a set via integer linear programming.">342. Selecting expansion terms as a set via integer linear programming.</a></li>
<li><a href="#343. An evaluation and enhancement of densitometric fragmentation for content slicing reuse.">343. An evaluation and enhancement of densitometric fragmentation for content slicing reuse.</a></li>
<li><a href="#344. Mathematical equation retrieval using plain words as a query.">344. Mathematical equation retrieval using plain words as a query.</a></li>
<li><a href="#345. Serial position effects of clicking behavior on result pages returned by search engines.">345. Serial position effects of clicking behavior on result pages returned by search engines.</a></li>
<li><a href="#346. Towards measuring the visualness of a concept.">346. Towards measuring the visualness of a concept.</a></li>
<li><a href="#347. Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters.">347. Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters.</a></li>
<li><a href="#348. Semantically coherent image annotation with a learning-based keyword propagation strategy.">348. Semantically coherent image annotation with a learning-based keyword propagation strategy.</a></li>
<li><a href="#349. Language processing for arabic microblog retrieval.">349. Language processing for arabic microblog retrieval.</a></li>
<li><a href="#350. Hierarchical image annotation using semantic hierarchies.">350. Hierarchical image annotation using semantic hierarchies.</a></li>
<li><a href="#351. On the inference of average precision from score distributions.">351. On the inference of average precision from score distributions.</a></li>
<li><a href="#352. An evaluation of corpus-driven measures of medical concept similarity for information retrieval.">352. An evaluation of corpus-driven measures of medical concept similarity for information retrieval.</a></li>
<li><a href="#353. A constraint to automatically regulate document-length normalisation.">353. A constraint to automatically regulate document-length normalisation.</a></li>
<li><a href="#354. Bridging offline and online social graph dynamics.">354. Bridging offline and online social graph dynamics.</a></li>
<li><a href="#355. Predicting the performance of passage retrieval for question answering.">355. Predicting the performance of passage retrieval for question answering.</a></li>
<li><a href="#356. Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context.">356. Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context.</a></li>
<li><a href="#357. Query-performance prediction and cluster ranking: two sides of the same coin.">357. Query-performance prediction and cluster ranking: two sides of the same coin.</a></li>
<li><a href="#358. Learning to rank search results for time-sensitive queries.">358. Learning to rank search results for time-sensitive queries.</a></li>
<li><a href="#359. On active learning in hierarchical classification.">359. On active learning in hierarchical classification.</a></li>
<li><a href="#360. Question-answer topic model for question retrieval in community question answering.">360. Question-answer topic model for question retrieval in community question answering.</a></li>
<li><a href="#361. How do humans distinguish different people with identical names on the web?">361. How do humans distinguish different people with identical names on the web?</a></li>
<li><a href="#362. Enhancing product search by best-selling prediction in e-commerce.">362. Enhancing product search by best-selling prediction in e-commerce.</a></li>
<li><a href="#363. Survival analysis for freshness in microblogging search.">363. Survival analysis for freshness in microblogging search.</a></li>
<li><a href="#364. Information preservation in static index pruning.">364. Information preservation in static index pruning.</a></li>
<li><a href="#365. Temporal models for microblogs.">365. Temporal models for microblogs.</a></li>
<li><a href="#366. I want what i need!: analyzing subjectivity of online forum threads.">366. I want what i need!: analyzing subjectivity of online forum threads.</a></li>
<li><a href="#367. Improving the performance of the reinforcement learning model for answering complex questions.">367. Improving the performance of the reinforcement learning model for answering complex questions.</a></li>
<li><a href="#368. Relation regularized subspace recommending for related scientific articles.">368. Relation regularized subspace recommending for related scientific articles.</a></li>
<li><a href="#369. Exploring the cluster hypothesis, and cluster-based retrieval, over the web.">369. Exploring the cluster hypothesis, and cluster-based retrieval, over the web.</a></li>
<li><a href="#370. A picture paints a thousand words: a method of generating image-text timelines.">370. A picture paints a thousand words: a method of generating image-text timelines.</a></li>
<li><a href="#371. Short-text domain specific key terms/phrases extraction using an n-gram model with wikipedia.">371. Short-text domain specific key terms/phrases extraction using an n-gram model with wikipedia.</a></li>
<li><a href="#372. A new probabilistic model for top-k ranking problem.">372. A new probabilistic model for top-k ranking problem.</a></li>
<li><a href="#373. Large scale analysis of changes in english vocabulary over recent time.">373. Large scale analysis of changes in english vocabulary over recent time.</a></li>
<li><a href="#374. Climbing the app wall: enabling mobile app discovery through context-aware recommendations.">374. Climbing the app wall: enabling mobile app discovery through context-aware recommendations.</a></li>
<li><a href="#375. TwiSent: a multistage system for analyzing sentiment in twitter.">375. TwiSent: a multistage system for analyzing sentiment in twitter.</a></li>
<li><a href="#376. Twitter hyperlink recommendation with user-tweet-hyperlink three-way clustering.">376. Twitter hyperlink recommendation with user-tweet-hyperlink three-way clustering.</a></li>
<li><a href="#377. Concavity in IR models.">377. Concavity in IR models.</a></li>
<li><a href="#378. Extracting interesting association rules from toolbar data.">378. Extracting interesting association rules from toolbar data.</a></li>
<li><a href="#379. Predicting CTR of new ads via click prediction.">379. Predicting CTR of new ads via click prediction.</a></li>
<li><a href="#380. An examination of content farms in web search using crowdsourcing.">380. An examination of content farms in web search using crowdsourcing.</a></li>
<li><a href="#381. Demographic context in web search re-ranking.">381. Demographic context in web search re-ranking.</a></li>
<li><a href="#382. On the usefulness of query features for learning to rank.">382. On the usefulness of query features for learning to rank.</a></li>
<li><a href="#383. Session-based query performance prediction.">383. Session-based query performance prediction.</a></li>
<li><a href="#384. A latent pairwise preference learning approach for recommendation from implicit feedback.">384. A latent pairwise preference learning approach for recommendation from implicit feedback.</a></li>
<li><a href="#385. Topic based pose relevance learning in dance archives.">385. Topic based pose relevance learning in dance archives.</a></li>
<li><a href="#386. PhotoFall: discovering weblog stories through photographs.">386. PhotoFall: discovering weblog stories through photographs.</a></li>
<li><a href="#387. RESQ: rank-energy selective query forwarding for distributed search systems.">387. RESQ: rank-energy selective query forwarding for distributed search systems.</a></li>
<li><a href="#388. The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy.">388. The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy.</a></li>
<li><a href="#389. Data filtering in humor generation: comparative analysis of hit rate and co-occurrence rankings as a method to choose usable pun candidates.">389. Data filtering in humor generation: comparative analysis of hit rate and co-occurrence rankings as a method to choose usable pun candidates.</a></li>
<li><a href="#390. Predicting primary categories of business listings for local search.">390. Predicting primary categories of business listings for local search.</a></li>
<li><a href="#391. Where do the query terms come from?: an analysis of query reformulation in collaborative web search.">391. Where do the query terms come from?: an analysis of query reformulation in collaborative web search.</a></li>
<li><a href="#392. Learning to recommend with social relation ensemble.">392. Learning to recommend with social relation ensemble.</a></li>
<li><a href="#393. A scalable approach for performing proximal search for verbose patent search queries.">393. A scalable approach for performing proximal search for verbose patent search queries.</a></li>
<li><a href="#394. Is wikipedia too difficult?: comparative analysis of readability of wikipedia, simple wikipedia and britannica.">394. Is wikipedia too difficult?: comparative analysis of readability of wikipedia, simple wikipedia and britannica.</a></li>
<li><a href="#395. Finding food entity relationships using user-generated data in recipe service.">395. Finding food entity relationships using user-generated data in recipe service.</a></li>
<li><a href="#396. SRGSIS: a novel framework based on social relationship graph for social image search.">396. SRGSIS: a novel framework based on social relationship graph for social image search.</a></li>
<li><a href="#397. Exploring simultaneous keyword and key sentence extraction: improve graph-based ranking using wikipedia.">397. Exploring simultaneous keyword and key sentence extraction: improve graph-based ranking using wikipedia.</a></li>
<li><a href="#398. Estimating query difficulty for news prediction retrieval.">398. Estimating query difficulty for news prediction retrieval.</a></li>
<li><a href="#399. Recency-sensitive model of web page authority.">399. Recency-sensitive model of web page authority.</a></li>
<li><a href="#400. Evaluating reward and risk for vertical selection.">400. Evaluating reward and risk for vertical selection.</a></li>
<li><a href="#401. Contextual evaluation of query reformulations in a search session by user simulation.">401. Contextual evaluation of query reformulations in a search session by user simulation.</a></li>
</ul>
</li>
<li><a href="#Databases poster session    8">Databases poster session    8</a><ul>
<li><a href="#402. Information-complete and redundancy-free keyword search over large data graphs.">402. Information-complete and redundancy-free keyword search over large data graphs.</a></li>
<li><a href="#403. Spatial-aware interest group queries in location-based social networks.">403. Spatial-aware interest group queries in location-based social networks.</a></li>
<li><a href="#404. Probabilistic ranking in fuzzy object databases.">404. Probabilistic ranking in fuzzy object databases.</a></li>
<li><a href="#405. Enabling ontology based semantic queries in biomedical database systems.">405. Enabling ontology based semantic queries in biomedical database systems.</a></li>
<li><a href="#406. Similarity search in 3D object-based video data.">406. Similarity search in 3D object-based video data.</a></li>
<li><a href="#407. Continuous top-k query for graph streams.">407. Continuous top-k query for graph streams.</a></li>
<li><a href="#408. Latent topics in graph-structured data.">408. Latent topics in graph-structured data.</a></li>
<li><a href="#409. Fast and accurate incremental entity resolution relative to an entity knowledge base.">409. Fast and accurate incremental entity resolution relative to an entity knowledge base.</a></li>
</ul>
</li>
<li><a href="#Knowledge management demonstration session    8">Knowledge management demonstration session    8</a><ul>
<li><a href="#410. LUKe and MIKe: learning from user knowledge and managing interactive knowledge extraction.">410. LUKe and MIKe: learning from user knowledge and managing interactive knowledge extraction.</a></li>
<li><a href="#411. PRAVDA-live: interactive knowledge harvesting.">411. PRAVDA-live: interactive knowledge harvesting.</a></li>
<li><a href="#412. 4Is of social bully filtering: identity, inference, influence, and intervention.">412. 4Is of social bully filtering: identity, inference, influence, and intervention.</a></li>
<li><a href="#413. Lonomics Atlas: a tool to explore interconnected ionomic, genomic and environmental data.">413. Lonomics Atlas: a tool to explore interconnected ionomic, genomic and environmental data.</a></li>
<li><a href="#414. CarbonDB: a semantic life cycle inventory database.">414. CarbonDB: a semantic life cycle inventory database.</a></li>
<li><a href="#415. Supporting temporal analytics for health-related events in microblogs.">415. Supporting temporal analytics for health-related events in microblogs.</a></li>
<li><a href="#416. InCaToMi: integrative causal topic miner between textual and non-textual time series data.">416. InCaToMi: integrative causal topic miner between textual and non-textual time series data.</a></li>
<li><a href="#417. A tool for automated evaluation of algorithms.">417. A tool for automated evaluation of algorithms.</a></li>
</ul>
</li>
<li><a href="#Information retrieval demonstration session    10">Information retrieval demonstration session    10</a><ul>
<li><a href="#418. A summarization tool for time-sensitive social media.">418. A summarization tool for time-sensitive social media.</a></li>
<li><a href="#419. CrowdTiles: presenting crowd-based information for event-driven information needs.">419. CrowdTiles: presenting crowd-based information for event-driven information needs.</a></li>
<li><a href="#420. ESA: emergency situation awareness via microbloggers.">420. ESA: emergency situation awareness via microbloggers.</a></li>
<li><a href="#421. Cager: a framework for cross-page search.">421. Cager: a framework for cross-page search.</a></li>
<li><a href="#422. Mixed-initiative conversational system using question-answer pairs mined from the web.">422. Mixed-initiative conversational system using question-answer pairs mined from the web.</a></li>
<li><a href="#423. PicAlert!: a system for privacy-aware image classification and retrieval.">423. PicAlert!: a system for privacy-aware image classification and retrieval.</a></li>
<li><a href="#424. TASE: a time-aware search engine.">424. TASE: a time-aware search engine.</a></li>
<li><a href="#425. Gumshoe quality toolkit: administering programmable search.">425. Gumshoe quality toolkit: administering programmable search.</a></li>
<li><a href="#426. Simultaneous realization of page-centric communication and search.">426. Simultaneous realization of page-centric communication and search.</a></li>
<li><a href="#427. MOUNA: mining opinions to unveil neglected arguments.">427. MOUNA: mining opinions to unveil neglected arguments.</a></li>
</ul>
</li>
<li><a href="#Databases demonstration session    9">Databases demonstration session    9</a><ul>
<li><a href="#428. MAGIK: managing completeness of data.">428. MAGIK: managing completeness of data.</a></li>
<li><a href="#429. Exploration of monte-carlo based probabilistic query processing in uncertain graphs.">429. Exploration of monte-carlo based probabilistic query processing in uncertain graphs.</a></li>
<li><a href="#430. The nautilus analyzer: understanding and debugging data transformations.">430. The nautilus analyzer: understanding and debugging data transformations.</a></li>
<li><a href="#431. Demonstrating ProApproX 2.0: a predictive query engine for probabilistic XML.">431. Demonstrating ProApproX 2.0: a predictive query engine for probabilistic XML.</a></li>
<li><a href="#432. HadoopXML: a suite for parallel processing of massive XML data with multiple twig pattern queries.">432. HadoopXML: a suite for parallel processing of massive XML data with multiple twig pattern queries.</a></li>
<li><a href="#433. MADden: query-driven statistical text analytics.">433. MADden: query-driven statistical text analytics.</a></li>
<li><a href="#434. STFMap: query- and feature-driven visualization of large time series data sets.">434. STFMap: query- and feature-driven visualization of large time series data sets.</a></li>
<li><a href="#435. Primates: a privacy management system for social networks.">435. Primates: a privacy management system for social networks.</a></li>
<li><a href="#436. AMADA: web data repositories in the amazon cloud.">436. AMADA: web data repositories in the amazon cloud.</a></li>
</ul>
</li>
<li><a href="#Workshop summaries    15">Workshop summaries    15</a><ul>
<li><a href="#437. DUBMMSM'12: international workshop on data-driven user behavioral modeling and mining from social media.">437. DUBMMSM'12: international workshop on data-driven user behavioral modeling and mining from social media.</a></li>
<li><a href="#438. CloudDB 2012: fourth international workshop on cloud data management.">438. CloudDB 2012: fourth international workshop on cloud data management.</a></li>
<li><a href="#439. CDMW 2012 - city data management workshop: workshop summary.">439. CDMW 2012 - city data management workshop: workshop summary.</a></li>
<li><a href="#440. Managing interoperability and compleXity in health systems - MIXHS'12.">440. Managing interoperability and compleXity in health systems - MIXHS'12.</a></li>
<li><a href="#441. The 2012 international workshop on web-scale knowledge representation, retrieval, and reasoning.">441. The 2012 international workshop on web-scale knowledge representation, retrieval, and reasoning.</a></li>
<li><a href="#442. SHB 2012: international workshop on smart health and wellbeing.">442. SHB 2012: international workshop on smart health and wellbeing.</a></li>
<li><a href="#443. Booksonline'12: 5th workshop on online books, complementary social media and their impact.">443. Booksonline'12: 5th workshop on online books, complementary social media and their impact.</a></li>
<li><a href="#444. DTMBIO 2012: international workshop on data and text mining in biomedical informatics.">444. DTMBIO 2012: international workshop on data and text mining in biomedical informatics.</a></li>
<li><a href="#445. PLEAD 2012: politics, elections and data.">445. PLEAD 2012: politics, elections and data.</a></li>
<li><a href="#446. Workshop on multimodal crowd sensing (CrowdSens 2012">446. Workshop on multimodal crowd sensing (CrowdSens 2012).</a>.)</li>
<li><a href="#447. Fifth workshop on exploiting semantic annotations in information retrieval: ESAIR"12">447. Fifth workshop on exploiting semantic annotations in information retrieval: ESAIR"12).</a>.)</li>
<li><a href="#448. First international workshop on information and knowledge management for developing region.">448. First international workshop on information and knowledge management for developing region.</a></li>
<li><a href="#449. PIKM 2012: 5th ACM workshop for PhD students in information and knowledge management.">449. PIKM 2012: 5th ACM workshop for PhD students in information and knowledge management.</a></li>
<li><a href="#450. WIDM 2012: the 12th international workshop on web information and data management.">450. WIDM 2012: the 12th international workshop on web information and data management.</a></li>
<li><a href="#451. DOLAP 2012 workshop summary.">451. DOLAP 2012 workshop summary.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="21. CIKM 2012:Maui, HI, USA">21. CIKM 2012:Maui, HI, USA</h1>
<p><a href="http://dl.acm.org/citation.cfm?id=2396761">21st ACM International Conference on Information and Knowledge Management, CIKM'12, Maui, HI, USA, October 29 - November 02, 2012.</a> ACM
【<a href="http://dblp.uni-trier.de/db/conf/cikm/cikm2012.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 451 || Session Num: 42">Paper Num: 451 || Session Num: 42</h2>
<h2 id="Keynote address    3">Keynote address    3</h2>
<h3 id="1. User engagement: the network effect matters!">1. User engagement: the network effect matters!</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396763">Paper Link</a>】    【Pages】:1-2</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Baeza=Yates:Ricardo_A=">Ricardo A. Baeza-Yates</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lalmas:Mounia">Mounia Lalmas</a></p>
<p>【Abstract】:
In the online world, user engagement refers to the quality of the user experience that emphasizes the positive aspects of the interaction with a web application and, in particular, the phenomena associated with wanting to use that application longer and frequently. This definition is motivated by the observation that successful web applications are not just used, but they are engaged with. Users invest time, attention, and emotion into them. Online providers aim not only to engage users with each service, but across all services in their network. They spend increasing effort to direct users to various services (e.g.~using hyperlinks to help users navigate to and explore other services), to increase user traffic between their services. Nothing is known for users engaging across such a network of Web sites, something we call networked user engagement. We address this problem by combining techniques from web analytics and mining, information retrieval evaluation, and existing works on user engagement coming from the domains of information science, multimodal human computer interaction and cognitive psychology. In this way, we can combine insights from big data with deep analysis of human behavior in the lab or through crowd-sourcing experiments.</p>
<p>【Keywords】:
metrics; network of services; user engagement; web sites</p>
<h3 id="2. Learning similarity measures based on random walks.">2. Learning similarity measures based on random walks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396764">Paper Link</a>】    【Pages】:3</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a></p>
<p>【Abstract】:
We describe a novel learnable proximity measure based on personalized PageRank (also known as "random walk with reset"). Instead of introducing one weight per edge label, as in most prior work, we introduce one weight for each edge label sequence. We show that this approach is advantageous for a number of real-world tasks, including querying graph databases, recommendation tasks, and inference in large, noisy knowledge bases.</p>
<p>【Keywords】:
machine learning; personalized pagerank</p>
<h3 id="3. Compressed data structures with relevance.">3. Compressed data structures with relevance.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396765">Paper Link</a>】    【Pages】:4-5</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vitter:Jeffrey_Scott">Jeffrey Scott Vitter</a></p>
<p>【Abstract】:
We describe recent breakthroughs in the field of compressed data structures, in which the data structure is stored in a compressed representation that still allows fast answers to queries. We focus in particular on compressed data structures to support the important application of pattern matching on massive document collections. Given an arbitrary query pattern in textual form, the job of the data structure is to report all the locations where the pattern appears. Another variant is to report all the documents that contain at least one instance of the pattern. We are particularly interested in reporting only the most relevant documents, using a variety of notions of relevance. We discuss recently developed techniques that support fast search in these contexts as well as under additional positional and temporal constraints.</p>
<p>【Keywords】:
compressed data structure; data compression; entropy; external memory; index; pattern matching; search</p>
<h2 id="KM track: recommender systems    5">KM track: recommender systems    5</h2>
<h3 id="4. LogUCB: an explore-exploit algorithm for comments recommendation.">4. LogUCB: an explore-exploit algorithm for comments recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396767">Paper Link</a>】    【Pages】:6-15</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mahajan:Dhruv_Kumar">Dhruv Kumar Mahajan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rastogi:Rajeev">Rajeev Rastogi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tiwari:Charu">Charu Tiwari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Adway">Adway Mitra</a></p>
<p>【Abstract】:
The highly dynamic nature of online commenting environments makes accurate ratings prediction for new comments challenging. In such a setting, in addition to exploiting comments with high predicted ratings, it is also critical to explore comments with high uncertainty in the predictions. In this paper, we propose a novel upper confidence bound (UCB) algorithm called LOGUCB that balances exploration with exploitation when the average rating of a comment is modeled using logistic regression on its features. At the core of our LOGUCB algorithm lies a novel variance approximation technique for the Bayesian logistic regression model that is used to compute the UCB value for each comment. In experiments with a real-life comments dataset from Yahoo! News, we show that LOGUCB with bag-of-words and topic features outperforms state-of-the-art explore-exploit algorithms.</p>
<p>【Keywords】:
comment ratings; explore-exploit; logistic regression; upper confidence bound</p>
<h3 id="5. DQR: a probabilistic approach to diversified query recommendation.">5. DQR: a probabilistic approach to diversified query recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396768">Paper Link</a>】    【Pages】:16-25</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Ruirui">Ruirui Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kao:Ben">Ben Kao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bi:Bin">Bin Bi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Reynold">Reynold Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lo:Eric">Eric Lo</a></p>
<p>【Abstract】:
Web search queries issued by casual users are often short and with limited expressiveness. Query recommendation is a popular technique employed by search engines to help users refine their queries. Traditional similarity-based methods, however, often result in redundant and monotonic recommendations. We identify five basic requirements of a query recommendation system. In particular, we focus on the requirements of redundancy-free and diversified recommendations. We propose the DQR framework, which mines a search log to achieve two goals: (1) It clusters search log queries to extract query concepts, based on which recommended queries are selected. (2) It employs a probabilistic model and a greedy heuristic algorithm to achieve recommendation diversification. Through a comprehensive user study we compare DQR against five other recommendation methods. Our experiment shows that DQR outperforms the other methods in terms of relevancy, diversity, and ranking performance of the recommendations.</p>
<p>【Keywords】:
diversification; query concept; query recommendation</p>
<h3 id="6. Dynamic covering for recommendation systems.">6. Dynamic covering for recommendation systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396769">Paper Link</a>】    【Pages】:26-34</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Antonellis:Ioannis">Ioannis Antonellis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarma:Anish_Das">Anish Das Sarma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dughmi:Shaddin">Shaddin Dughmi</a></p>
<p>【Abstract】:
In this paper, we identify a fundamental algorithmic problem that we term succinct dynamic covering (SDC), arising in many modern-day web applications, including ad-serving and online recommendation systems such as in eBay, Netflix, and Amazon. Roughly speaking, SDC applies two restrictions to the well-studied Max-Coverage problem [14]: Given an integer k, X={1,2,...,n}and I={S_1,...,S_m}, S_i subseteq X, find |J| subseteq I, such that |J| &lt; k and (union_S_in_J S) is as large as possible. The two restrictions applied by SDC are: (1)Dynamic: At query-time, we are given a query Q subseteq X, and our goal is to find J such that Q bigcap (union_S_J S) is as large as possible; Space-constrained: We don't have enough space to store (and process) the entire input; specifically, we have o(mn), and maybe as little as O((m+n)polylog(mn))space. A solution to SDC maintains a small data structure, and uses this datastructure to answer most dynamic queries with high accuracy. We call such a scheme a Coverage Oracle. We present algorithms and complexity results for coverage oracles. We present deterministic and probabilistic near-tight upper and lower bounds on the approximation ratio of SDC as a function of the amount of space available to the oracle. Our lower bound results show that to obtain constant-factor approximations we need Omega(mn) space. Fortunately, our upper bounds present an explicit tradeoff between space and approximation ratio, allowing us to determine the amount of space needed to guarantee certain accuracy.</p>
<p>【Keywords】:
dynamic covering; max-coverage problem; recommendation systems</p>
<h3 id="7. MEET: a generalized framework for reciprocal recommender systems.">7. MEET: a generalized framework for reciprocal recommender systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396770">Paper Link</a>】    【Pages】:35-44</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a></p>
<p>【Abstract】:
Reciprocal recommender systems refer to systems from which users can obtain recommendations of other individuals by satisfying preferences of both parties being involved. Different from the traditional user-item recommendation, reciprocal recommenders focus on the preferences of both parties simultaneously, as well as some special properties in terms of "reciprocal". In this paper, we propose MEET -- a generalized framework for reciprocal recommendation, in which we model the correlations of users as a bipartite graph that maintains both local and global "reciprocal" utilities. The local utility captures users' mutual preferences, whereas the global utility manages the overall quality of the entire reciprocal network. Extensive empirical evaluation on two real-world data sets (online dating and online recruiting) demonstrates the effectiveness of our proposed framework compared with existing recommendation algorithms. Our analysis also provides deep insights into the special aspects of reciprocal recommenders that differentiate them from user-item recommender systems.</p>
<p>【Keywords】:
bipartite graph; global and local regularization; reciprocal recommender</p>
<h3 id="8. Social contextual recommendation.">8. Social contextual recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396771">Paper Link</a>】    【Pages】:45-54</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Meng">Meng Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Peng">Peng Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Rui">Rui Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Fei">Fei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu_0001:Wenwu">Wenwu Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shiqiang">Shiqiang Yang</a></p>
<p>【Abstract】:
Exponential growth of information generated by online social networks demands effective recommender systems to give useful results. Traditional techniques become unqualified because they ignore social relation data; existing social recommendation approaches consider social network structure, but social context has not been fully considered. It is significant and challenging to fuse social contextual factors which are derived from users' motivation of social behaviors into social recommendation. In this paper, we investigate social recommendation on the basis of psychology and sociology studies, which exhibit two important factors: individual preference and interpersonal influence. We first present the particular importance of these two factors in online item adoption and recommendation. Then we propose a novel probabilistic matrix factorization method to fuse them in latent spaces. We conduct experiments on both Facebook style bidirectional and Twitter style unidirectional social network datasets in China. The empirical result and analysis on these two large datasets demonstrate that our method significantly outperform the existing approaches.</p>
<p>【Keywords】:
individual preference; interpersonal influence; matrix factorization; social recommendation</p>
<h2 id="KM track: pattern mining    5">KM track: pattern mining    5</h2>
<h3 id="9. Mining high utility itemsets without candidate generation.">9. Mining high utility itemsets without candidate generation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396773">Paper Link</a>】    【Pages】:55-64</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Mengchi">Mengchi Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qu:Jun=Feng">Jun-Feng Qu</a></p>
<p>【Abstract】:
High utility itemsets refer to the sets of items with high utility like profit in a database, and efficient mining of high utility itemsets plays a crucial role in many real-life applications and is an important research issue in data mining area. To identify high utility itemsets, most existing algorithms first generate candidate itemsets by overestimating their utilities, and subsequently compute the exact utilities of these candidates. These algorithms incur the problem that a very large number of candidates are generated, but most of the candidates are found out to be not high utility after their exact utilities are computed. In this paper, we propose an algorithm, called HUI-Miner (High Utility Itemset Miner), for high utility itemset mining. HUI-Miner uses a novel structure, called utility-list, to store both the utility information about an itemset and the heuristic information for pruning the search space of HUI-Miner. By avoiding the costly generation and utility computation of numerous candidate itemsets, HUI-Miner can efficiently mine high utility itemsets from the utility-lists constructed from a mined database. We compared HUI-Miner with the state-of-the-art algorithms on various databases, and experimental results show that HUI-Miner outperforms these algorithms in terms of both running time and memory consumption.</p>
<p>【Keywords】:
high utility itemset; mining algorithm</p>
<h3 id="10. A general framework to encode heterogeneous information sources for contextual pattern mining.">10. A general framework to encode heterogeneous information sources for contextual pattern mining.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396774">Paper Link</a>】    【Pages】:65-74</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dong:Weishan">Weishan Dong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Wei">Wei Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shi:Lei">Lei Shi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Changjin">Changjin Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a></p>
<p>【Abstract】:
Traditional pattern mining methods usually work on single data sources. However, in practice, there are often multiple and heterogeneous information sources. They collectively provide contextual information not available in any single source alone describing the same set of objects, and are useful for discovering hidden contextual patterns. One important challenge is to provide a general methodology to mine contextual patterns easily and efficiently. In this paper, we propose a general framework to encode contextual information from multiple sources into a coherent representation---Contextual Information Graph (CIG). The complexity of the encoding scheme is linear in both time and space. More importantly, CIG can be handled by any single-source pattern mining algorithms that accept taxonomies without any modification. We demonstrate by three applications of the contextual association rule, sequence and graph mining, that contextual patterns providing rich and insightful knowledge can be easily discovered by the proposed framework. It enables Contextual Pattern Mining (CPM) by reusing single-source methods, and is easy to deploy and use in real-world systems.</p>
<p>【Keywords】:
contextual pattern mining; heterogeneous sources</p>
<h3 id="11. Incorporating occupancy into frequent pattern mining for high quality pattern recommendation.">11. Incorporating occupancy into frequent pattern mining for high quality pattern recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396775">Paper Link</a>】    【Pages】:75-84</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Linpeng">Linpeng Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Lei">Lei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Ping">Ping Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Min">Min Wang</a></p>
<p>【Abstract】:
Mining interesting patterns from transaction databases has attracted a lot of research interest for more than a decade. Most of those studies use frequency, the number of times a pattern appears in a transaction database, as the key measure for pattern interestingness. In this paper, we introduce a new measure of pattern interestingness, occupancy. The measure of occupancy is motivated by some real-world pattern recommendation applications which require that any interesting pattern X should occupy a large portion of the transactions it appears in. Namely, for any supporting transaction t of pattern X, the number of items in X should be close to the total number of items in t. In these pattern recommendation applications, patterns with higher occupancy may lead to higher recall while patterns with higher frequency lead to higher precision. With the definition of occupancy we call a pattern dominant if its occupancy is above a user-specified threshold. Then, our task is to identify the qualified patterns which are both frequent and dominant. Additionally, we also formulate the problem of mining top-k qualified patterns: finding the qualified patterns with the top-k values of any function (e.g. weighted sum of both occupancy and support). The challenge to these tasks is that the monotone or anti-monotone property does not hold on occupancy. In other words, the value of occupancy does not increase or decrease monotonically when we add more items to a given itemset. Thus, we propose an algorithm called DOFIA (DOminant and Frequent Itemset mining Algorithm), which explores the upper bound properties on occupancy to reduce the search process. The tradeoff between bound tightness and computational complexity is also systematically addressed. Finally, we show the effectiveness of DOFIA in a real-world application on print-area recommendation for Web pages, and also demonstrate the efficiency of DOFIA on several large synthetic data sets.</p>
<p>【Keywords】:
DOFIA; constraint-based mining; frequent and dominant pattern; frequent pattern mining</p>
<h3 id="12. PARMA: a parallel randomized algorithm for approximate association rules mining in MapReduce.">12. PARMA: a parallel randomized algorithm for approximate association rules mining in MapReduce.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396776">Paper Link</a>】    【Pages】:85-94</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Riondato:Matteo">Matteo Riondato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/DeBrabant:Justin_A=">Justin A. DeBrabant</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fonseca:Rodrigo">Rodrigo Fonseca</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Upfal:Eli">Eli Upfal</a></p>
<p>【Abstract】:
Frequent Itemsets and Association Rules Mining (FIM) is a key task in knowledge discovery from data. As the dataset grows, the cost of solving this task is dominated by the component that depends on the number of transactions in the dataset. We address this issue by proposing PARMA, a parallel algorithm for the MapReduce framework, which scales well with the size of the dataset (as number of transactions) while minimizing data replication and communication cost. PARMA cuts down the dataset-size-dependent part of the cost by using a random sampling approach to FIM. Each machine mines a small random sample of the dataset, of size independent from the dataset size. The results from each machine are then filtered and aggregated to produce a single output collection. The output will be a very close approximation of the collection of Frequent Itemsets (FI's) or Association Rules (AR's) with their frequencies and confidence levels. The quality of the output is probabilistically guaranteed by our analysis to be within the user-specified accuracy and error probability parameters. The sizes of the random samples are independent from the size of the dataset, as is the number of samples. They depend on the user-chosen accuracy and error probability parameters and on the parallel computational model. We implemented PARMA in Hadoop MapReduce and show experimentally that it runs faster than previously introduced FIM algorithms for the same platform, while 1) scaling almost linearly, and 2) offering even higher accuracy and confidence than what is guaranteed by the analysis.</p>
<p>【Keywords】:
MapReduce; association rules; frequent itemsets; sampling</p>
<h3 id="13. Interactive pattern mining on hidden data: a sampling-based solution.">13. Interactive pattern mining on hidden data: a sampling-based solution.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396777">Paper Link</a>】    【Pages】:95-104</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bhuiyan:Mansurul">Mansurul Bhuiyan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mukhopadhyay:Snehasis">Snehasis Mukhopadhyay</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hasan:Mohammad_Al">Mohammad Al Hasan</a></p>
<p>【Abstract】:
Mining frequent patterns from a hidden dataset is an important task with 43 various real-life applications. In this research, we propose a solution to this problem that is based on Markov Chain Monte Carlo (MCMC) sampling of frequent patterns. Instead of returning all the frequent patterns, the proposed paradigm returns a small set of randomly selected patterns so that the clandestinity of the dataset can be maintained. Our solution also allows interactive sampling, so that the sampled patterns can fulfill the user's requirement effectively. We show experimental results from several real life datasets to validate the capability and usefulness of our solution; in particular, we show examples that by using our proposed solution, an eCommerce marketplace can allow pattern mining on user session data without disclosing the data to the public; such a mining paradigm helps the sellers of the marketplace, which eventually boost the marketplace's own revenue.</p>
<p>【Keywords】:
MCMC sampling; interactive pattern mining</p>
<h2 id="IR track: evaluation methodologies    5">IR track: evaluation methodologies    5</h2>
<h3 id="14. An analysis of systematic judging errors in information retrieval.">14. An analysis of systematic judging errors in information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396779">Paper Link</a>】    【Pages】:105-114</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Craswell:Nick">Nick Craswell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yilmaz:Emine">Emine Yilmaz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tahaghoghi:Seyed_M=_M=">Seyed M. M. Tahaghoghi</a></p>
<p>【Abstract】:
Test collections are powerful mechanisms for the evaluation and optimization of information retrieval systems. However, there is reported evidence that experiment outcomes can be affected by changes to the judging guidelines or changes in the judge population. This paper examines such effects in a web search setting, comparing the judgments of four groups of judges: NIST Web Track judges, untrained crowd workers and two groups of trained judges of a commercial search engine. Our goal is to identify systematic judging errors by comparing the labels contributed by the different groups, working under the same or different judging guidelines. In particular, we focus on detecting systematic differences in judging depending on specific characteristics of the queries and URLs. For example, we ask whether a given population of judges, working under a given set of judging guidelines, are more likely to consistently overrate Wikipedia pages than another group judging under the same instructions. Our approach is to identify judging errors with respect to a consensus set, a judged gold set and a set of user clicks. We further demonstrate how such biases can affect the training of retrieval systems.</p>
<p>【Keywords】:
bias; noise; relevence</p>
<h3 id="15. On caption bias in interleaving experiments.">15. On caption bias in interleaving experiments.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396780">Paper Link</a>】    【Pages】:115-124</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hofmann:Katja">Katja Hofmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Behr:Fritz">Fritz Behr</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Radlinski:Filip">Filip Radlinski</a></p>
<p>【Abstract】:
Information retrieval evaluation most often involves manually assessing the relevance of particular query-document pairs. In cases where this is difficult (such as personalized search), interleaved comparison methods are becoming increasingly common. These methods compare pairs of ranking functions based on user clicks on search results, thus better reflecting true user preferences. However, by depending on clicks, there is a potential for bias. For example, users have been previously shown to be more likely to click on results with attractive titles and snippets. An interleaving evaluation where one ranker tends to generate results that attract more clicks (without being more relevant) may thus be biased. We present an approach for detecting and compensating for this type of bias in interleaving evaluations. Introducing a new model of caption bias, we propose features that model bias based on (1) per-document effects, and (2) the (pairwise) relationships between a document and surrounding documents. We show that our model can effectively capture click behavior, with best results achieved by a model that combines both per-document and pairwise features. Applying this model to re-weight observed user clicks, we find a small overall effect on real interleaving comparisons, but also identify a case where initially detected preferences vanish after caption bias re-weighting is applied. Our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias.</p>
<p>【Keywords】:
evaluation; implicit feedback; interleaving</p>
<h3 id="16. Alternative assessor disagreement and retrieval depth.">16. Alternative assessor disagreement and retrieval depth.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396781">Paper Link</a>】    【Pages】:125-134</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Webber:William">William Webber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chandar:Praveen">Praveen Chandar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carterette:Ben">Ben Carterette</a></p>
<p>【Abstract】:
Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that a second assessor will disagree with the relevance assessment of the initial assessor, and find that there is a strong and consistent correlation between the two. We adopt a metarank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of second assessor disagreement given metarank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute system scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.</p>
<p>【Keywords】:
evaluation; retrieval experiment; sampling</p>
<h3 id="17. Incorporating variability in user behavior into systems based evaluation.">17. Incorporating variability in user behavior into systems based evaluation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396782">Paper Link</a>】    【Pages】:135-144</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Carterette:Ben">Ben Carterette</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kanoulas:Evangelos">Evangelos Kanoulas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yilmaz:Emine">Emine Yilmaz</a></p>
<p>【Abstract】:
Click logs present a wealth of evidence about how users interact with a search system. This evidence has been used for many things: learning rankings, personalizing, evaluating effectiveness, and more. But it is almost always distilled into point estimates of feature or parameter values, ignoring what may be the most salient feature of users---their variability. No two users interact with a system in exactly the same way, and even a single user may interact with results for the same query differently depending on information need, mood, time of day, and a host of other factors. We present a Bayesian approach to using logs to compute posterior distributions for probabilistic models of user interactions. Since they are distributions rather than point estimates, they naturally capture variability in the population. We show how to cluster posterior distributions to discover patterns of user interactions in logs, and discuss how to use the clusters to evaluate search engines according to a user model. Because the approach is Bayesian, our methods can be applied to very large logs (such as those possessed by Web search engines) as well as very small (such as those found in almost any other setting).</p>
<p>【Keywords】:
evaluation; test collections; user logs</p>
<h3 id="18. Constructing test collections by inferring document relevance via extracted relevant information.">18. Constructing test collections by inferring document relevance via extracted relevant information.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396783">Paper Link</a>】    【Pages】:145-154</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rajput:Shahzad">Shahzad Rajput</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ekstrand=Abueg:Matthew">Matthew Ekstrand-Abueg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pavlu:Virgiliu">Virgiliu Pavlu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aslam:Javed_A=">Javed A. Aslam</a></p>
<p>【Abstract】:
The goal of a typical information retrieval system is to satisfy a user's information need---e.g., by providing an answer or information "nugget"---while the actual search space of a typical information retrieval system consists of documents---i.e., collections of nuggets. In this paper, we characterize this relationship between nuggets and documents and discuss applications to system evaluation. In particular, for the problem of test collection construction for IR system evaluation, we demonstrate a highly efficient algorithm for simultaneously obtaining both relevant documents and relevant information. Our technique exploits the mutually reinforcing relationship between relevant documents and relevant information, yielding document-based test collections whose efficiency and efficacy exceed those of typical Cranfield-style test collections, while also generating sets of highly relevant information.</p>
<p>【Keywords】:
evaluation; information retrieval; nuggets; relevance assessment</p>
<h2 id="IR track: social media search    5">IR track: social media search    5</h2>
<h3 id="19. Twevent: segment-based event detection from tweets.">19. Twevent: segment-based event detection from tweets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396785">Paper Link</a>】    【Pages】:155-164</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chenliang">Chenliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Aixin">Aixin Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Datta:Anwitaman">Anwitaman Datta</a></p>
<p>【Abstract】:
Event detection from tweets is an important task to understand the current events/topics attracting a large number of common users. However, the unique characteristics of tweets (e.g. short and noisy content, diverse and fast changing topics, and large data volume) make event detection a challenging task. Most existing techniques proposed for well written documents (e.g. news articles) cannot be directly adopted. In this paper, we propose a segment-based event detection system for tweets, called Twevent. Twevent first detects bursty tweet segments as event segments and then clusters the event segments into events considering both their frequency distribution and content similarity. More specifically, each tweet is split into non-overlapping segments (i.e. phrases possibly refer to named entities or semantically meaningful information units). The bursty segments are identified within a fixed time window based on their frequency patterns, and each bursty segment is described by the set of tweets containing the segment published within that time window. The similarity between a pair of bursty segments is computed using their associated tweets. After clustering bursty segments into candidate events, Wikipedia is exploited to identify the realistic events and to derive the most newsworthy segments to describe the identified events. We evaluate Twevent and compare it with the state-of-the-art method using 4.3 million tweets published by Singapore-based users in June 2010. In our experiments, Twevent outperforms the state-of-the-art method by a large margin in terms of both precision and recall. More importantly, the events detected by Twevent can be easily interpreted with little background knowledge because of the newsworthy segments. We also show that Twevent is efficient and scalable, leading to a desirable solution for event detection from tweets.</p>
<p>【Keywords】:
event detection; microblogging; tweet segmentation; twitter</p>
<h3 id="20. Making your interests follow you on twitter.">20. Making your interests follow you on twitter.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396786">Paper Link</a>】    【Pages】:165-174</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pennacchiotti:Marco">Marco Pennacchiotti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silvestri:Fabrizio">Fabrizio Silvestri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vahabi:Hossein">Hossein Vahabi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Venturini:Rossano">Rossano Venturini</a></p>
<p>【Abstract】:
In this paper we introduce the task of "tweet recommendation", the problem of suggesting tweets that match a user's interests and likes. We propose an Information-Retrieval-like model that leverages the content of the user's tweets and those of her friends, and that effectively retrieves a set of tweets that is personalized and varied in nature. Our approach could be easily leveraged to build, for example, a Twitter or Facebook timeline that collects messages that are of interest for the user, but that are not posted by her friends. We compare to typical approaches used in similar tasks, reporting significant gains in terms of overall precision, up to about +20%, on both a corpus-based evaluation and real world user study.</p>
<p>【Keywords】:
information filtering; twitter recommendation</p>
<h3 id="21. Generating event storylines from microblogs.">21. Generating event storylines from microblogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396787">Paper Link</a>】    【Pages】:175-184</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chen">Chen Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chun">Chun Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jingxuan">Jingxuan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dingding">Dingding Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yang">Yang Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a></p>
<p>【Abstract】:
Microblogging service has emerged to be a dominant web medium for billions of individuals sharing and spreading instant news and information, therefore monitoring the event evolution on microblog sphere is crucial for providing both better user experience and deeper understanding on real-time events. In this paper we explore the problem of generating storylines from microblogs for user input queries. This problem is challenging due to the sparse, dynamic and social nature of microblogs. Given a query of an ongoing event, we propose to sketch the real-time storyline of the event by a two-level solution. We first propose a language model with dynamic pseudo relevance feedback to obtain relevant tweets, and then generate storylines via graph optimization. Comprehensive experiments on Twitter data sets demonstrate the effectiveness of the proposed methods in each level and the overall framework.</p>
<p>【Keywords】:
dynamic pseudo relevance feedback; language model; microblog; social media; storyline</p>
<h3 id="22. Social book search: comparing topical relevance judgements and book suggestions for evaluation.">22. Social book search: comparing topical relevance judgements and book suggestions for evaluation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396788">Paper Link</a>】    【Pages】:185-194</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Koolen:Marijn">Marijn Koolen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kamps:Jaap">Jaap Kamps</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a></p>
<p>【Abstract】:
The Web and social media give us access to a wealth of information, not only different in quantity but also in character---traditional descriptions from professionals are now supplemented with user generated content. This challenges modern search systems based on the classical model of topical relevance and ad hoc search: How does their effectiveness transfer to the changing nature of information and to the changing types of information needs and search tasks? We use the INEX 2011 Books and Social Search Track's collection of book descriptions from Amazon and social cataloguing site LibraryThing. We compare classical IR with social book search in the context of the LibraryThing discussion forums where members ask for book suggestions. Specifically, we compare book suggestions on the forum with Mechanical Turk judgements on topical relevance and recommendation, both the judgements directly and their resulting evaluation of retrieval systems. First, the book suggestions on the forum are a complete enough set of relevance judgements for system evaluation. Second, topical relevance judgements result in a different system ranking from evaluation based on the forum suggestions. Although it is an important aspect for social book search, topical relevance is not sufficient for evaluation. Third, professional metadata alone is often not enough to determine the topical relevance of a book. User reviews provide a better signal for topical relevance. Fourth, user-generated content is more effective for social book search than professional metadata. Based on our findings, we propose an experimental evaluation that better reflects the complexities of social book search.</p>
<p>【Keywords】:
book search; evaluation; user-generated content</p>
<h3 id="23. Content-based crowd retrieval on the real-time web.">23. Content-based crowd retrieval on the real-time web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396789">Paper Link</a>】    【Pages】:195-204</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kamath:Krishna_Yeswanth">Krishna Yeswanth Kamath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caverlee:James">James Caverlee</a></p>
<p>【Abstract】:
In this paper, we propose and evaluate a novel content-driven crowd discovery algorithm that can efficiently identify newly-formed communities of users from the real-time web. Short-lived crowds reflect the real-time interests of their constituents and provide a foundation for user-focused web monitoring. Three of the salient features of the algorithm are its: (i) prefix-tree based locality-sensitive hashing approach for discovering crowds from high-volume rapidly-evolving social media; (ii) efficient user profile updating for incorporating new user activities and fading older ones; and (iii) key dimension identification, so that crowd detection can be focused on the most active portions of the real-time web. Through extensive experimental study, we find significantly more efficient crowd discovery as compared to both a k-means clustering-based approach and a MapReduce-based implementation, while maintaining high-quality crowds as compared to an offline approach. Additionally, we find that expert crowds tend to be "stickier" and last longer in comparison to crowds of typical users.</p>
<p>【Keywords】:
clustering; community detection; real-time web; social media</p>
<h2 id="KM track: link and graph mining    5">KM track: link and graph mining    5</h2>
<h3 id="24. Graph classification: a diversified discriminative feature selection approach.">24. Graph classification: a diversified discriminative feature selection approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396791">Paper Link</a>】    【Pages】:205-214</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Yuanyuan">Yuanyuan Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Jeffrey_Xu">Jeffrey Xu Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Hong">Hong Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qin:Lu">Lu Qin</a></p>
<p>【Abstract】:
A graph models complex structural relationships among objects, and has been prevalently used in a wide range of applications. Building an automated graph classification model becomes very important for predicting unknown graphs or understanding complex structures between different classes. The graph classification framework being widely used consists of two steps, namely, feature selection and classification. The key issue is how to select important subgraph features from a graph database with a large number of graphs including positive graphs and negative graphs. Given the features selected, a generic classification approach can be used to build a classification model. In this paper, we focus on feature selection. We identify two main issues with the most widely used feature selection approach which is based on a discriminative score to select frequent subgraph features, and introduce a new diversified discriminative score to select features that have a higher diversity. We analyze the properties of the newly proposed diversified discriminative score, and conducted extensive performance studies to demonstrate that such a diversified discriminative score makes positive/negative graphs separable and leads to a higher classification accuracy.</p>
<p>【Keywords】:
diversity; feature selection; graph classification</p>
<h3 id="25. Multi-scale link prediction.">25. Multi-scale link prediction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396792">Paper Link</a>】    【Pages】:215-224</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shin:Donghyuk">Donghyuk Shin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Si">Si Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dhillon:Inderjit_S=">Inderjit S. Dhillon</a></p>
<p>【Abstract】:
The automated analysis of social networks has become an important problem due to the proliferation of social networks, such as LiveJournal, Flickr and Facebook. The scale of these social networks is massive and continues to grow rapidly. An important problem in social network analysis is proximity estimation that infers the closeness of different users. Link prediction, in turn, is an important application of proximity estimation. However, many methods for computing proximity measures have high computational complexity and are thus prohibitive for large-scale link prediction problems. One way to address this problem is to estimate proximity measures via low-rank approximation. However, a single low-rank approximation may not be sufficient to represent the behavior of the entire network. In this paper, we propose Multi-Scale Link Prediction (MSLP), a framework for link prediction, which can handle massive networks. The basic idea of MSLP is to construct low-rank approximations of the network at multiple scales in an efficient manner. To achieve this, we propose a fast tree-structured approximation algorithm. Based on this approach, MSLP combines predictions at multiple scales to make robust and accurate predictions. Experimental results on real-life datasets with more than a million nodes show the superior performance and scalability of our method.</p>
<p>【Keywords】:
hierarchical clustering; link prediction; low rank approximation; social network analysis</p>
<h3 id="26. An analysis of how ensembles of collective classifiers improve predictions in graphs.">26. An analysis of how ensembles of collective classifiers improve predictions in graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396793">Paper Link</a>】    【Pages】:225-234</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Eldardiry:Hoda">Hoda Eldardiry</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Neville:Jennifer">Jennifer Neville</a></p>
<p>【Abstract】:
We present a theoretical analysis framework that shows how ensembles of collective classifiers can improve predictions for graph data. We show how collective ensemble classification reduces errors due to variance in learning and more interestingly inference. We also present an empirical framework that includes various ensemble techniques for classifying relational data using collective inference. The methods span single- and multiple-graph network approaches, and are tested on both synthetic and real world classification tasks. Our experimental results, supported by our theoretical justifications, confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers.</p>
<p>【Keywords】:
collective classification; ensemble learning</p>
<h3 id="27. Density index and proximity search in large graphs.">27. Density index and proximity search in large graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396794">Paper Link</a>】    【Pages】:235-244</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Nan">Nan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wen:Zhen">Zhen Wen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Khan:Arijit">Arijit Khan</a></p>
<p>【Abstract】:
Given a large real-world graph where vertices are associated with labels, how do we quickly find interesting vertex sets according to a given query? In this paper, we study label-based proximity search in large graphs, which finds the top-k query-covering vertex sets with the smallest diameters. Each set has to cover all the labels in a query. Existing greedy algorithms only return approximate answers, and do not scale well to large graphs. We propose a novel framework, called gDensity, which uses density index and likelihood ranking to find vertex sets in an efficient and accurate manner. Promising vertices are ordered and examined according to their likelihood to produce answers, and the likelihood calculation is greatly facilitated by density indexing. Techniques such as progressive search and partial indexing are further proposed. Experiments on real-world graphs show the efficiency and scalability of gDensity.</p>
<p>【Keywords】:
graph mining; indexing; proximity search</p>
<h3 id="28. Gelling, and melting, large graphs by edge manipulation.">28. Gelling, and melting, large graphs by edge manipulation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396795">Paper Link</a>】    【Pages】:245-254</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tong:Hanghang">Hanghang Tong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Prakash:B=_Aditya">B. Aditya Prakash</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eliassi=Rad:Tina">Tina Eliassi-Rad</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faloutsos:Michalis">Michalis Faloutsos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faloutsos:Christos">Christos Faloutsos</a></p>
<p>【Abstract】:
Controlling the dissemination of an entity (e.g., meme, virus, etc) on a large graph is an interesting problem in many disciplines. Examples include epidemiology, computer security, marketing, etc. So far, previous studies have mostly focused on removing or inoculating nodes to achieve the desired outcome. We shift the problem to the level of edges and ask: which edges should we add or delete in order to speed-up or contain a dissemination? First, we propose effective and scalable algorithms to solve these dissemination problems. Second, we conduct a theoretical study of the two problems and our methods, including the hardness of the problem, the accuracy and complexity of our methods, and the equivalence between the different strategies and problems. Third and lastly, we conduct experiments on real topologies of varying sizes to demonstrate the effectiveness and scalability of our approaches.</p>
<p>【Keywords】:
edge manipulation; graph mining; immunization; scalability</p>
<h2 id="IR track: language technologies    5">IR track: language technologies    5</h2>
<h3 id="29. One seed to find them all: mining opinion features via association.">29. One seed to find them all: mining opinion features via association.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396797">Paper Link</a>】    【Pages】:255-264</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hai:Zhen">Zhen Hai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Kuiyu">Kuiyu Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cong:Gao">Gao Cong</a></p>
<p>【Abstract】:
Feature-based opinion analysis has attracted extensive attention recently. Identifying features associated with opinions expressed in reviews is essential for fine-grained opinion mining. One approach is to exploit the dependency relations that occur naturally between features and opinion words, and among features (or opinion words) themselves. In this paper, we propose a generalized approach to opinion feature extraction by incorporating robust statistical association analysis in a bootstrapping framework. The new approach starts with a small set of feature seeds, on which it iteratively enlarges by mining feature-opinion, feature-feature, and opinion-opinion dependency relations. Two association model types, namely likelihood ratio tests (LRT) and latent semantic analysis (LSA), are proposed for computing the pair-wise associations between terms (features or opinions). We accordingly propose two robust bootstrapping approaches, LRTBOOT and LSABOOT, both of which need just a handful of initial feature seeds to bootstrap opinion feature extraction. We benchmarked LRTBOOT and LSABOOT against existing approaches on a large number of real-life reviews crawled from the cellphone and hotel domains. Experimental results using varying number of feature seeds show that the proposed association-based bootstrapping approach significantly outperforms the competitors. In fact, one seed feature is all that is needed for LRTBOOT to significantly outperform the other methods. This seed feature can simply be the domain feature, e.g., "cellphone" or "hotel". The consequence of our discovery is far reaching: starting with just one feature seed, typically just the domain concept word, LRTBOOT can automatically extract a large set of high-quality opinion features from the corpus without any supervision or labeled features. This means that the automatic creation of a set of domain features is no longer a pipe dream!</p>
<p>【Keywords】:
aspect; association; bootstrapping; feature; opinion mining; seed; sentiment analysis</p>
<h3 id="30. Topic-driven reader comments summarization.">30. Topic-driven reader comments summarization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396798">Paper Link</a>】    【Pages】:265-274</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Zongyang">Zongyang Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Aixin">Aixin Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Quan">Quan Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cong:Gao">Gao Cong</a></p>
<p>【Abstract】:
Readers of a news article often read its comments contributed by other readers. By reading comments, readers obtain not only complementary information about this news article but also the opinions from other readers. However, the existing ranking mechanisms for comments (e.g., by recency or by user rating) fail to offer an overall picture of topics discussed in comments. In this paper, we first propose to study Topic-driven Reader Comments Summarization (Torcs) problem. We observe that many news articles from a news stream are related to each other; so are their comments. Hence, news articles and their associated comments provide context information for user commenting. To implicitly capture the context information, we propose two topic models to address the Torcs problem, namely, Master-Slave Topic Model (MSTM) and Extended Master-Slave Topic Model (EXTM). Both models treat a news article as a master document and each of its comments as a slave document. MSTM model constrains that the topics discussed in comments have to be derived from the commenting news article. On the other hand, EXTM model allows generating words of comments using both the topics derived from the commenting news article, and the topics derived from all comments themselves. Both models are used to group comments into topic clusters. We then use two ranking mechanisms Maximal Marginal Relevance (MMR) and Rating &amp; Length (RL) to select a few most representative comments from each comment cluster. To evaluate the two models, we conducted experiments on 1005 Yahoo! News articles with more than one million comments. Our experimental results show that EXTM significantly outperforms MSTM by perplexity. Through a user study, we also confirm that the comment summary generated by EXTM achieves better intra-cluster topic cohesion and inter-cluster topic diversity.</p>
<p>【Keywords】:
comments summarization; master-slave document; topic model</p>
<h3 id="31. Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams.">31. Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396799">Paper Link</a>】    【Pages】:275-284</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Rui">Rui Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wan:Xiaojun">Xiaojun Wan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Wayne_Xin">Wayne Xin Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Pu=Jen">Pu-Jen Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a></p>
<p>【Abstract】:
We present a novel graph-based framework for timeline summarization, the task of creating different summaries for different timestamps but for the same topic. Our work extends timeline summarization to a multimodal setting and creates timelines that are both textual and visual. Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common content. Our model optimizes local summary creation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking. In our algorithm, individual summaries are generated by taking into account the mutual dependencies between sentences and images, and are iteratively refined by considering how they contribute to the global timeline and its coherence. Experiments on real-world datasets show that the timelines produced by our model outperform several competitive baselines both in terms of ROUGE and when assessed by human evaluators.</p>
<p>【Keywords】:
evolutionary summarization; iterative reinforcement; text-to-image translation; visual timeline</p>
<h3 id="32. Fast multi-task learning for query spelling correction.">32. Fast multi-task learning for query spelling correction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396800">Paper Link</a>】    【Pages】:285-294</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Xu">Xu Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shrivastava:Anshumali">Anshumali Shrivastava</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li_0001:Ping">Ping Li</a></p>
<p>【Abstract】:
In this paper, we explore the use of a novel online multi-task learning framework for the task of search query spelling correction. In our procedure, correction candidates are initially generated by a ranker-based system and then re-ranked by our multi-task learning algorithm. With the proposed multi-task learning method, we are able to effectively transfer information from different and highly biased training datasets, for improving spelling correction on all datasets. Our experiments are conducted on three query spelling correction datasets including the well-known TREC benchmark dataset. The experimental results demonstrate that our proposed method considerably outperforms the existing baseline systems in terms of accuracy. Importantly, the proposed method is about one order of magnitude faster than baseline systems in terms of training speed. Compared to the commonly used online learning methods which typically require more than (e.g.,) 60 training passes, our proposed method is able to closely reach the empirical optimum in about 5 passes.</p>
<p>【Keywords】:
multi-task learning; querry spelling correction</p>
<h3 id="33. Cross-argument inference for implicit discourse relation recognition.">33. Cross-argument inference for implicit discourse relation recognition.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396801">Paper Link</a>】    【Pages】:295-304</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Xiaopei">Xiaopei Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Che:Tingting">Tingting Che</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yao:Jian=Min">Jian-Min Yao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>【Abstract】:
Motivated by the critical importance of connectives in recognizing discourse relations, we present an unsupervised cross-argument inference mechanism to implicit discourse relation recognition. The basic idea is to infer the implicit discourse relation of an argument pair from a large number of comparable argument pairs, which are automatically retrieved from the web in an unsupervised way. In this way, the inference proceeds from explicit relations to implicit ones via connective as bridge. This kind of pair-to-pair inference is based on the assumption that two argument pairs with high content similarity (i.e. comparable argument pairs) should have similar discourse relationship. Evaluation on PDTB proves the effectiveness of our inference mechanism in implicit relation recognition to the four level-1 relations. It also shows that our mechanism significantly outperforms other alternatives.</p>
<p>【Keywords】:
implicit discourse relation; pair-to-pair inference</p>
<h2 id="DB track: graph and knowledge base    5">DB track: graph and knowledge base    5</h2>
<h3 id="34. Interpreting keyword queries over web knowledge bases.">34. Interpreting keyword queries over web knowledge bases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396803">Paper Link</a>】    【Pages】:305-314</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pound:Jeffrey">Jeffrey Pound</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hudek:Alexander_K=">Alexander K. Hudek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ilyas:Ihab_F=">Ihab F. Ilyas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weddell:Grant_E=">Grant E. Weddell</a></p>
<p>【Abstract】:
Many keyword queries issued to Web search engines target information about real world entities, and interpreting these queries over Web knowledge bases can often enable the search system to provide exact answers to queries. Equally important is the problem of detecting when the reference knowledge base is not capable of answering the keyword query, due to lack of domain coverage. In this work we present an approach to computing structured representations of keyword queries over a reference knowledge base. We mine frequent query structures from a Web query log and map these structures into a reference knowledge base. Our approach exploits coarse linguistic structure in keyword queries, and combines it with rich structured query representations of information needs.</p>
<p>【Keywords】:
knowledge bases; query interpretation; query understanding; semantic query understanding</p>
<h3 id="35. RDF pattern matching using sortable views.">35. RDF pattern matching using sortable views.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396804">Paper Link</a>】    【Pages】:315-324</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chong:Zhihong">Zhihong Chong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:He">He Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhenjie">Zhenjie Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shu:Hu">Hu Shu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qi:Guilin">Guilin Qi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Aoying">Aoying Zhou</a></p>
<p>【Abstract】:
In the last few years, RDF is becoming the dominating data model used in semantic web for knowledge representation and inference. In this paper, we revisit the problem of pattern matching query in RDF model, which is usually expensive in efficiency due to the huge cost on join operations. To alleviate the efficiency pain, view materialization techniques are usually deployed to accelerate the query processing. However, given an arbitrary view, it remains difficult to identify how to reuse the view for a particular query, because of the NP-hardness behind the algorithm matching patterns and views. To fully exploit the benefit of the materialized views, we propose a new paradigm to enhance the effectiveness of the materialized view. Instead of choosing materialized views in arbitrary form, our paradigm aims to select the views only if they are sortable. The property of sortability raises huge gains on the pattern-view matching, bringing down the cost to linear complexity in terms of the pattern size. On the other side, the costs on identifying sortable views and searching over the views using inverted index are affordable. Moreover, sortable views generally improve the overall performance of pattern matching, by means of a cost model used to optimize the query rewriting on the most appropriate views. Finally, we demonstrate extensive experimental results to verify the superiority of our proposal on both efficiency and effectiveness.</p>
<p>【Keywords】:
query processing; rdf indexing; rdf query</p>
<h3 id="36. Efficient algorithms for generalized subgraph query processing.">36. Efficient algorithms for generalized subgraph query processing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396805">Paper Link</a>】    【Pages】:325-334</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Wenqing">Wenqing Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiao:Xiaokui">Xiaokui Xiao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:James">James Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhowmick:Sourav_S=">Sourav S. Bhowmick</a></p>
<p>【Abstract】:
We study a new type of graph queries, which injectively maps its edges to paths of the graphs in a given database, where the length of each path is constrained by a given threshold specified by the weight of the corresponding matching edge. We give important applications of the new graph query and identify new challenges of processing such a query. Then, we devise the cost model of the branch-and-bound algorithm framework for processing the graph query, and propose an efficient algorithm to minimize the cost overhead. We also develop three indexing techniques to efficiently answer the queries online. Finally, we verify the efficiency of our proposed indexes with extensive experiments on large real and synthetic datasets.</p>
<p>【Keywords】:
graph databases; graph indexing; graph matching algorithm; graph querying</p>
<h3 id="37. G-SPARQL: a hybrid engine for querying large attributed graphs.">37. G-SPARQL: a hybrid engine for querying large attributed graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396806">Paper Link</a>】    【Pages】:335-344</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sakr:Sherif">Sherif Sakr</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elnikety:Sameh">Sameh Elnikety</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Yuxiong">Yuxiong He</a></p>
<p>【Abstract】:
We propose a SPARQL-like language, G-SPARQL, for querying attributed graphs. The language expresses types of queries which of large interest for applications which model their data as large graphs such as: pattern matching, reachability and shortest path queries. Each query can combine both of structural predicates and value-based predicates (on the attributes of the graph nodes and edges). We describe an algebraic compilation mechanism for our proposed query language which is extended from the relational algebra and based on the basic construct of building SPARQL queries, the Triple Pattern. We describe a hybrid Memory/Disk representation of large attributed graphs where only the topology of the graph is maintained in memory while the data of the graph is stored in a relational database. The execution engine of our proposed query language splits parts of the query plan to be pushed inside the relational database while the execution of other parts of the query plan are processed using memory-based algorithms, as necessary. Experimental results on real datasets demonstrate the efficiency and the scalability of our approach and show that our approach outperforms native graph databases by several factors.</p>
<p>【Keywords】:
graphs; query; sparql</p>
<h3 id="38. A graph-based approach for ontology population with named entities.">38. A graph-based approach for ontology population with named entities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396807">Paper Link</a>】    【Pages】:345-354</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Wei">Wei Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jianyong">Jianyong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Ping">Ping Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Min">Min Wang</a></p>
<p>【Abstract】:
Automatically populating ontology with named entities extracted from the unstructured text has become a key issue for Semantic Web and knowledge management techniques. This issue naturally consists of two subtasks: (1) for the entity mention whose mapping entity does not exist in the ontology, attach it to the right category in the ontology (i.e., fine-grained named entity classification), and (2) for the entity mention whose mapping entity is contained in the ontology, link it with its mapping real world entity in the ontology (i.e., entity linking). Previous studies only focus on one of the two subtasks and cannot solve this task of populating ontology with named entities integrally. This paper proposes APOLLO, a grAph-based aPproach for pOpuLating ontoLOgy with named entities. APOLLO leverages the rich semantic knowledge embedded in the Wikipedia to resolve this task via random walks on graphs. Meanwhile, APOLLO can be directly applied to either of the two subtasks with minimal revision. We have conducted a thorough experimental study to evaluate the performance of APOLLO. The experimental results show that APOLLO achieves significant accuracy improvement for the task of ontology population with named entities, and outperforms the baseline methods for both subtasks.</p>
<p>【Keywords】:
entity linking; label propagation; named entity classification; ontology population</p>
<h2 id="DB track: temporal, spatial and multimedia databases    5">DB track: temporal, spatial and multimedia databases    5</h2>
<h3 id="39. Decomposition-by-normalization (DBN): leveraging approximate functional dependencies for efficient tensor decomposition.">39. Decomposition-by-normalization (DBN): leveraging approximate functional dependencies for efficient tensor decomposition.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396809">Paper Link</a>】    【Pages】:355-364</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Mijung">Mijung Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Candan:K=_Sel=ccedil=uk">K. Selçuk Candan</a></p>
<p>【Abstract】:
For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported throughout the data lifecycle. Although tensor decomposition is shown to be effective for multi-dimensional data analysis, the cost of tensor decomposition is often very high. We propose a novel decomposition-by-normalization scheme that first normalizes the given relation into smaller tensors based on the functional dependencies of the relation and then performs the decomposition using these smaller tensors. The decomposition and recombination steps of the decomposition-by- normalization scheme fit naturally in settings with multiple cores. This leads to a highly efficient, effective, and parallelized decomposition-by-normalization algorithm for both dense and sparse tensors. Experiments confirm the efficiency and effectiveness of the proposed decomposition-by-normalization scheme compared to the conventional nonnegative CP decomposition approach.</p>
<p>【Keywords】:
tensor decomposition; tensor-based relational data model</p>
<h3 id="40. A filter-based protocol for continuous queries over imprecise location data.">40. A filter-based protocol for continuous queries over imprecise location data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396810">Paper Link</a>】    【Pages】:365-374</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Yifan">Yifan Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Reynold">Reynold Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kao:Ben">Ben Kao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lam:Kam=yiu">Kam-yiu Lam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yinuo">Yinuo Zhang</a></p>
<p>【Abstract】:
In typical location-based services (LBS), moving objects (e.g., GPS-enabled mobile phones) report their locations through a wireless network. An LBS server can use the location information to answer various types of continuous queries. Due to hardware limitations, location data reported by the moving objects are often uncertain. In this paper, we study efficient methods for the execution of Continuous Possible Nearest Neighbor Query (CPoNNQ) that accesses imprecise location data. A CPoNNQ is a standing query (which is active during a period of time) such that, at any time point, all moving objects that have non-zero probabilities of being the nearest neighbor of a given query point are reported. To handle the continuous nature of a CPoNNQ, a simple solution is to require moving objects to continuously report their locations to the LBS server, which evaluates the query at every time step. To save communication bandwidth and mobile devices' batteries, we develop two filter-based protocols for CPoNNQ evaluation. Our protocols install "filter bounds" on moving objects, which suppress unnecessary location reporting and communication between the server and the moving objects. Through extensive experiments, we show that our protocols can effectively reduce communication costs while maintaining a high query quality.</p>
<p>【Keywords】:
communication cost; continuous queries; uncertain database</p>
<h3 id="41. Leveraging read rates of passive RFID tags for real-time indoor location tracking.">41. Leveraging read rates of passive RFID tags for real-time indoor location tracking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396811">Paper Link</a>】    【Pages】:375-384</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Da">Da Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Wilfred">Wilfred Ng</a></p>
<p>【Abstract】:
RFID (radio frequency identification) technology has been widely used for object tracking in many real-life applications, such as inventory monitoring and product flow tracking. These applications usually rely on passive RFID technologies rather than active ones, since passive RFID tags are more attractive than active ones in many aspects, such as lower tag cost and simpler maintenance. RFID technology is also important for indoor location tracking systems that require high degree of accuracy. However, most existing systems estimate object locations by using active RFID tags, which usually incur localization error of more than one meter. Although recent studies begin to investigate the application of passive tags for indoor location tracking, these methods are far from deployable and research of this application is still in its infancy. In this paper, we propose a new indoor location tracking system, named PassTrack, which relies on the read rates of passive RFID tags for location estimation. PassTrack is designed to tolerate noise arising from external environmental factors, by probabilistically modeling the relationship between tag read rate and tag-reader distance, and updating the model parameters based on the current readings of reference tags. Besides tolerance of noise, PassTrack is also outstanding in terms of localization accuracy and efficiency. Several new approaches for location inference are supported by PassTrack, and the best one incurs an average error of around 30 cm, and is able to carry out over 7500 location estimations per second on an ordinary machine. Furthermore, as a result of using passive RFID tags, PassTrack also enjoys the many other benefits of passive RFID tags mentioned before. We have conducted extensive experiments on both real and synthetic datasets, which demonstrate that our PassTrack system outperforms the previous localization approaches in localization accuracy, tracking efficiency and space applicability.</p>
<p>【Keywords】:
localization; passive; read rate; rfid; tag</p>
<h3 id="42. Location-aware instant search.">42. Location-aware instant search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396812">Paper Link</a>】    【Pages】:385-394</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhong:Ruicheng">Ruicheng Zhong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Ju">Ju Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Kian=Lee">Kian-Lee Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Lizhu">Lizhu Zhou</a></p>
<p>【Abstract】:
Location-Based Services (LBS) have been widely accepted by mobile users recently. Existing LBS-based systems require users to type in complete keywords. However for mobile users it is rather difficult to type in complete keywords on mobile devices. To alleviate this problem, in this paper we study the location-aware instant search problem, which returns users location-aware answers as users type in queries letter by letter. The main challenge is to achieve high interactive speed. To address this challenge, in this paper we propose a novel index structure, prefix-region tree (called PR-Tree), to efficiently support location-aware instant search. PR-Tree is a tree-based index structure which seamlessly integrates the textual description and spatial information to index the spatial data. Using the PR-Tree, we develop efficient algorithms to support single prefix queries and multi-keyword queries. Experiments show that our method achieves high performance and significantly outperforms state-of-the-art methods.</p>
<p>【Keywords】:
keywords search; spatial databases; type-ahead search</p>
<h3 id="43. Indexing uncertain spatio-temporal data.">43. Indexing uncertain spatio-temporal data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396813">Paper Link</a>】    【Pages】:395-404</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Emrich:Tobias">Tobias Emrich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kriegel:Hans=Peter">Hans-Peter Kriegel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mamoulis:Nikos">Nikos Mamoulis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Renz:Matthias">Matthias Renz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Z=uuml=fle:Andreas">Andreas Züfle</a></p>
<p>【Abstract】:
The advances in sensing and telecommunication technologies allow the collection and management of vast amounts of spatio-temporal data combining location and time information.Due to physical and resource limitations of data collection devices (e.g., RFID readers, GPS receivers and other sensors) data are typically collected only at discrete points of time. In-between these discrete time instances, the positions of tracked moving objects are uncertain. In this work, we propose novel approximation techniques in order to probabilistically bound the uncertain movement of objects; these techniques allow for efficient and effective filtering during query evaluation using an hierarchical index structure.To the best of our knowledge, this is the first approach that supports query evaluation on very large uncertain spatio-temporal databases, adhering to possible worlds semantics. We experimentally show that it accelerates the existing, scan-based approach by orders of magnitude.</p>
<p>【Keywords】:
indexing; uncertain spatio-temporal data; uncertain trajectory</p>
<h2 id="KM track: matrix methods and anomaly detection    4">KM track: matrix methods and anomaly detection    4</h2>
<h3 id="44. Local anomaly descriptor: a robust unsupervised algorithm for anomaly detection based on diffusion space.">44. Local anomaly descriptor: a robust unsupervised algorithm for anomaly detection based on diffusion space.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396815">Paper Link</a>】    【Pages】:405-414</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Hao">Hao Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qin:Hong">Hong Qin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yoo:Shinjae">Shinjae Yoo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Dantong">Dantong Yu</a></p>
<p>【Abstract】:
Current popular anomaly detection algorithms are capable of detecting global anomalies but oftentimes fail to distinguish local anomalies from normal instances. This paper aims to improve unsupervised anomaly detection via the exploration of physics-based diffusion space. Building upon the embedding manifold derived from diffusion maps, we devise Local Anomaly Descriptor (LAD) whose originality results from faithfully preserving intrinsic and informative density-relevant neighborhood information. This robust and effective algorithm is designed with a weighted umbrella Laplacian operator to bridge global and local properties. To further enhance the efficacy of our proposed algorithm, we explore the utility of anisotropic Gaussian kernel (AGK) which can offer better manifold-aware affinity information. Comprehensive experiments on both synthetic and UCI real datasets verify that our LAD outperforms existing anomaly detection algorithms.</p>
<p>【Keywords】:
LAD; anomaly detection; diffusion space</p>
<h3 id="45. Fast and reliable anomaly detection in categorical data.">45. Fast and reliable anomaly detection in categorical data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396816">Paper Link</a>】    【Pages】:415-424</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Akoglu:Leman">Leman Akoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tong:Hanghang">Hanghang Tong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vreeken:Jilles">Jilles Vreeken</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faloutsos:Christos">Christos Faloutsos</a></p>
<p>【Abstract】:
Spotting anomalies in large multi-dimensional databases is a crucial task with many applications in finance, health care, security, etc. We introduce COMPREX, a new approach for identifying anomalies using pattern-based compression. Informally, our method finds a collection of dictionaries that describe the norm of a database succinctly, and subsequently flags those points dissimilar to the norm---with high compression cost---as anomalies. Our approach exhibits four key features: 1) it is parameter-free; it builds dictionaries directly from data, and requires no user-specified parameters such as distance functions or density and similarity thresholds, 2) it is general; we show it works for a broad range of complex databases, including graph, image and relational databases that may contain both categorical and numerical features, 3) it is scalable; its running time grows linearly with respect to both database size as well as number of dimensions, and 4) it is effective; experiments on a broad range of datasets show large improvements in both compression, as well as precision in anomaly detection, outperforming its state-of-the-art competitors.</p>
<p>【Keywords】:
anomaly detection; categorical data; data encoding</p>
<h3 id="46. TALMUD: transfer learning for multiple domains.">46. TALMUD: transfer learning for multiple domains.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396817">Paper Link</a>】    【Pages】:425-434</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moreno:Orly">Orly Moreno</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shapira:Bracha">Bracha Shapira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rokach:Lior">Lior Rokach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shani:Guy">Guy Shani</a></p>
<p>【Abstract】:
Most collaborative Recommender Systems (RS) operate in a single domain (such as movies, books, etc.) and are capable of providing recommendations based on historical usage data which is collected in the specific domain only. Cross-domain recommenders address the sparsity problem by using Machine Learning (ML) techniques to transfer knowledge from a dense domain into a sparse target domain. In this paper we propose a transfer learning technique that extracts knowledge from multiple domains containing rich data (e.g., movies and music) and generates recommendations for a sparse target domain (e.g., games). Our method learns the relatedness between the different source domains and the target domain, without requiring overlapping users between domains. The model integrates the appropriate amount of knowledge from each domain in order to enrich the target domain data. Experiments with several datasets reveal that, using multiple sources and the relatedness between domains improves accuracy of results.</p>
<p>【Keywords】:
collaborative filtering; cross domains; recommender systems; transfer learning</p>
<h3 id="47. Utilizing common substructures to speedup tensor factorization for mining dynamic graphs.">47. Utilizing common substructures to speedup tensor factorization for mining dynamic graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396818">Paper Link</a>】    【Pages】:435-444</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Wei">Wei Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chan:Jeffrey">Jeffrey Chan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bailey:James">James Bailey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leckie:Christopher">Christopher Leckie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ramamohanarao:Kotagiri">Kotagiri Ramamohanarao</a></p>
<p>【Abstract】:
In large and complex graphs of social, chemical/biological, or other relations, frequent substructures are commonly shared by different graphs or by graphs evolving through different time periods. Tensors are natural representations of these complex time-evolving graph data. A factorization of a tensor provides a high-quality low-rank compact basis for each dimension of the tensor, which facilitates the interpretation of frequent substructures of the original graphs. However, the high computational cost of tensor factorization makes it infeasible for conventional tensor factorization methods to handle large graphs that evolve frequently with time. To address this problem, in this paper we propose a novel iterative tensor factorization (ITF) method whose time complexity is linear in the cardinalities of all dimensions of a tensor. This low time complexity means that when using tensors to represent dynamic graphs, the computational cost of ITF is linear in the size (number of edges/vertices) of graphs and is also linear in the number of time periods over which the graph evolves. More importantly, an error estimation of ITF suggests that its factorization correctness is comparable to that of the standard factorization method. We empirically evaluate our method on publication networks and chemical compound graphs, and demonstrate that ITF is an order of magnitude faster than the conventional method and at the same time preserves factorization quality. To the best of our knowledge, this research is the first work that uses important frequent substructures to speed up tensor factorizations for mining dynamic graphs.</p>
<p>【Keywords】:
dynamic graphs; scalability; tensor factorization</p>
<h2 id="KM track: social networks    4">KM track: social networks    4</h2>
<h3 id="48. Predicting emerging social conventions in online social networks.">48. Predicting emerging social conventions in online social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396820">Paper Link</a>】    【Pages】:445-454</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kooti:Farshad">Farshad Kooti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mason:Winter_A=">Winter A. Mason</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gummadi:P=_Krishna">P. Krishna Gummadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cha:Meeyoung">Meeyoung Cha</a></p>
<p>【Abstract】:
The way in which social conventions emerge in communities has been of interest to social scientists for decades. Here we report on the emergence of a particular social convention on Twitter---the way to indicate a tweet is being reposted and attributing the content to its source. Despite being invented at different times and having different adoption rates, only two variations became widely adopted. In this paper we describe this process in detail, highlighting the factors that come into play in deciding which variation individuals will adopt. Our classification analysis demonstrates that the date of adoption and the number of exposures are particularly important in the adoption process, while personal features (such as the number of followers and join date) and the number of adopter friends have less discriminative power in predicting adoptions. We discuss implications of these findings in the design of future Web applications and services.</p>
<p>【Keywords】:
microblog; prediction; social conventions</p>
<h3 id="49. Collective intelligence in the online social network of yahoo!answers and its implications.">49. Collective intelligence in the online social network of yahoo!answers and its implications.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396821">Paper Link</a>】    【Pages】:455-464</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Ze">Ze Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Haiying">Haiying Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grant:Joseph_Edward">Joseph Edward Grant</a></p>
<p>【Abstract】:
Question and Answer (Q&amp;A) websites such as Yahoo!Answers provide a platform where users can post questions and receive answers. These systems take advantage of the collective intelligence of users to find information. In this paper, we analyze the online social network (OSN) in Yahoo!Answers. Based on a large amount of our collected data, we studied the OSN's structural properties, which reveals strikingly distinct properties such as low link symmetry and weak correlation between indegree and outdegree. After studying the knowledge base and behaviors of the users, we find that a small number of top contributors answer most of the questions in the system. Also, each top contributor focuses on only a few knowledge categories. In addition, the knowledge categories of the users are highly clustered. We also study the knowledge base in a user's social network, which reveals that the members in a user's social network share only a few knowledge categories. Based on the findings, we provide guidance in the design of spammer detection algorithms and distributed Q&amp;A systems. We also propose a friendship-knowledge oriented Q&amp;A framework that synergically combines current OSN-based Q&amp;A and web Q&amp;A. We believe that the results presented in this paper are crucial in understanding the collective intelligence in the web Q&amp;A OSNs and lay a cornerstone for the evolution of next-generation Q&amp;A systems.</p>
<p>【Keywords】:
collective intelligence; knowledge networks; on-line social networks</p>
<h3 id="50. From face-to-face gathering to social structure.">50. From face-to-face gathering to social structure.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396822">Paper Link</a>】    【Pages】:465-474</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chunyan">Chunyan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Mao">Mao Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a></p>
<p>【Abstract】:
The rapid development of on-line social networking sites has dramatically changed the way people live and communicate. One particularly interesting phenomena came along with this development is the prominent role of various on-line networking portals played in scheduling and organizing off-line group events and activities. In this paper, we focus on studying the face-to-face(f2f) group formed through, or facilitated by, on-line portals. We first show the distinct characteristics of such f2f groups by analyzing datasets collected from Whrrl and Meetup. Next, we propose a dynamic model for group gathering based on the process of friend invitation to interpret how a f2f group is formed on-line. The results of our model are confirmed by empirical observations. Finally, we demonstrate that using such group information can effectively improve the accuracies of social tie inference and friend recommendation.</p>
<p>【Keywords】:
group-gathering; social networks; social tie inference</p>
<h3 id="51. Delineating social network data anonymization via random edge perturbation.">51. Delineating social network data anonymization via random edge perturbation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396823">Paper Link</a>】    【Pages】:475-484</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Mingqiang">Mingqiang Xue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karras:Panagiotis">Panagiotis Karras</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ra=iuml=ssi:Chedy">Chedy Raïssi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kalnis:Panos">Panos Kalnis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pung:Hung_Keng">Hung Keng Pung</a></p>
<p>【Abstract】:
Social network data analysis raises concerns about the privacy of related entities or individuals. To address this issue, organizations can publish data after simply replacing the identities of individuals with pseudonyms, leaving the overall structure of the social network unchanged. However, it has been shown that attacks based on structural identification (e.g., a walk-based attack) enable an adversary to re-identify selected individuals in an anonymized network. In this paper we explore the capacity of techniques based on random edge perturbation to thwart such attacks. We theoretically establish that any kind of structural identification attack can effectively be prevented using random edge perturbation and show that, surprisingly, important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated and hence data analysis tasks performed on the perturbed data, given that the legitimate data recipient knows the perturbation probability as well. Yet we also examine ways to enhance the walk-based attack, proposing a variant we call probabilistic attack. Nevertheless, we demonstrate that such probabilistic attacks can also be prevented under sufficient perturbation. Eventually, we conduct a thorough theoretical study of the probability of success of any}structural attack as a function of the perturbation probability. Our analysis provides a powerful tool for delineating the identification risk of perturbed social network data; our extensive experiments with synthetic and real datasets confirm our expectations.</p>
<p>【Keywords】:
graph utility; privacy; random perturbation; social network</p>
<h2 id="IR track: advertising    4">IR track: advertising    4</h2>
<h3 id="52. Multiview hierarchical bayesian regression model andapplication to online advertising.">52. Multiview hierarchical bayesian regression model andapplication to online advertising.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396825">Paper Link</a>】    【Pages】:485-494</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Tianbing">Tianbing Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ruofei">Ruofei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Zhen">Zhen Guo</a></p>
<p>【Abstract】:
With the development of Web applications, large scale data are popular; and they are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about multi-view data with implicit structure. In this paper, we propose a novel hierarchical Bayesian mixture regression model, which discovers and then exploits the relationships among multiple views of the data to perform various machine learning tasks. A stochastic EM inference and learning algorithm is derived; and a parallel implementation in Hadoop MapReduce [9] paradigm is developed to scale up the learning. We apply the developed model and algorithm on click-through-rate (CTR) prediction and campaign targeting recommendation in online advertising to measure its effectiveness. The experiments on both synthetic data and large scale ads serving data from a real world online advertising exchange demonstrate the superior CTR prediction accuracy of our method compared to existing state-of-the-art methods. The results also show that our model can recommend high performance targeting features for online advertising campaigns.</p>
<p>【Keywords】:
hierarchical bayesian regression; online advertising</p>
<h3 id="53. Visual appearance of display ads and its effect on click through rate.">53. Visual appearance of display ads and its effect on click through rate.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396826">Paper Link</a>】    【Pages】:495-504</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Azimi:Javad">Javad Azimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ruofei">Ruofei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Yang">Yang Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Navalpakkam:Vidhya">Vidhya Navalpakkam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Jianchang">Jianchang Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli">Xiaoli Fern</a></p>
<p>【Abstract】:
One of the most important categories of online advertising is display advertising which provides publishers with significant revenue. Similar to other categories, the main goal in display advertising is to maximize user response rate for advertising campaigns, such as click through rates (CTR) or conversion rates. Previous studies have tried to optimize these parameters using objectives such as behavioral targeting. However, there is no published work so far to address the effect of the visual appearance of ads (creatives) on user response rate via a systematic data-driven approach. In this paper, we quantitatively study the relationship between the visual appearance and performance of creatives using large scale data in the world's largest display ads exchange system, RightMedia. We designed a set of 43 visual features, some of which are novel and others are inspired by related work. We extracted these features from real creatives served on RightMedia. We also designed and conducted a series of experiments to evaluate the effectiveness of visual features for CTR prediction, ranking and performance classification. Based on the evaluation results, we selected a subset of features that have the highest impact on CTR. We believe that the findings presented in this paper will be very useful for the online advertising industry in designing high-performance creatives. It also provides the research community with the first ever data set, initial insights into visual appearance's effect on user response propensity, and evaluation benchmarks for further study.</p>
<p>【Keywords】:
creative recommendation; online advertising; visual features</p>
<h3 id="54. The wisdom of advertisers: mining subgoals via query clustering.">54. The wisdom of advertisers: mining subgoals via query clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396827">Paper Link</a>】    【Pages】:505-514</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yamamoto:Takehiro">Takehiro Yamamoto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sakai:Tetsuya">Tetsuya Sakai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iwata:Mayu">Mayu Iwata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Chen">Chen Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wen:Ji=Rong">Ji-Rong Wen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tanaka:Katsumi">Katsumi Tanaka</a></p>
<p>【Abstract】:
This paper tackles the problem of mining subgoals of a given search goal from data. For example, when a searcher wants to travel to London, she may need to accomplish several subtasks such as "book flights," "book a hotel," "find good restaurants" and "decide which sightseeing spots to visit." As another example, if a searcher wants to lose weight, there may exist several alternative solutions such as "do physical exercise," "take diet pills," and "control calorie intake." In this paper, we refer to such subtasks or solutions as subgoals, and propose to utilize sponsored search data for finding subgoals of a given query by means of query clustering. Advertisements (ads) reflect advertisers' tremendous efforts in trying to match a given query with implicit user needs. Moreover, ads are usually associated with a particular action or transaction. We therefore hypothesized that they are useful for subgoal mining. To our knowledge, our work is the first to use sponsored search data for this purpose. Our experimental results show that sponsored search data is a good resource for obtaining related queries and for identifying subgoals via query clustering. In particular, our method that combines ad impressions from sponsored search data and query co-occurrences from session data outperforms a state-of-the-art query clustering method that relies on document clicks rather than ad impressions in terms of purity, NMI, Rand Index, F1-measure and subgoal recall.</p>
<p>【Keywords】:
query clustering; sponsored search; user intent</p>
<h3 id="55. Sequential selection of correlated ads by POMDPs.">55. Sequential selection of correlated ads by POMDPs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396828">Paper Link</a>】    【Pages】:515-524</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Shuai">Shuai Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jun">Jun Wang</a></p>
<p>【Abstract】:
Online advertising has become a key source of revenue for both web search engines and online publishers. For them, the ability of allocating right ads to right webpages is critical because any mismatched ads would not only harm web users' satisfactions but also lower the ad income. In this paper, we study how online publishers could optimally select ads to maximize their ad incomes over time. The conventional offline, content-based matching between webpages and ads is a fine start but cannot solve the problem completely because good matching does not necessarily lead to good payoff. Moreover, with the limited display impressions, we need to balance the need of selecting ads to learn true ad payoffs (exploration) with that of allocating ads to generate high immediate payoffs based on the current belief (exploitation). In this paper, we address the problem by employing Partially observable Markov decision processes (POMDPs) and discuss how to utilize the correlation of ads to improve the efficiency of the exploration and increase ad incomes in a long run. Our mathematical derivation shows that the belief states of correlated ads can be naturally updated using a formula similar to collaborative filtering. To test our model, a real world ad dataset from a major search engine is collected and categorized. Experimenting over the data, we provide an analyse of the effect of the underlying parameters, and demonstrate that our algorithms significantly outperform other strong baselines.</p>
<p>【Keywords】:
computational advertising; correlation; pomdps; revenue optimisation; value iteration</p>
<h2 id="IR track: system architecture, distributed IR, scalability    4">IR track: system architecture, distributed IR, scalability    4</h2>
<h3 id="56. Diversity in blog feed retrieval.">56. Diversity in blog feed retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396830">Paper Link</a>】    【Pages】:525-534</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Keikha:Mostafa">Mostafa Keikha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crestani:Fabio">Fabio Crestani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
Blog distillation (blog feed retrieval) is a task in blog retrieval where the goal is to rank blogs according to their recurrent relevance to a query topic. One of the main properties of blog feed retrieval is that the unit of retrieval is a collection of documents as opposed to a single document as in other IR tasks. This collection retrieval nature of blog distillation introduces new challenges and requires new investigations specific to this problem. Researchers have addressed this problem by considering a wide range of evidence and information resources. However, previous work has not studied the effect of on-topic diversity of blog posts in blog relevance. By on-topic diversity of blog posts we mean that those posts that are about the query topic need to have high diversity and cover different sub-topics of the query. In this study, we investigate three types of on-topic diversity and their effect on retrieval performance: topical diversity, temporal diversity and hybrid diversity. Our experiments over different blog collections and different baseline methods show that on-topic diversity can improve the performance of the retrieval system. Among the three types of diversity, hybrid diversity, that considers both topical and temporal diversities, achieves the best performance.</p>
<p>【Keywords】:
blog retrieval; diversity; novelty</p>
<h3 id="57. Efficient retrieval of recommendations in a matrix factorization framework.">57. Efficient retrieval of recommendations in a matrix factorization framework.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396831">Paper Link</a>】    【Pages】:535-544</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Koenigstein:Noam">Noam Koenigstein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ram:Parikshit">Parikshit Ram</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shavitt:Yuval">Yuval Shavitt</a></p>
<p>【Abstract】:
Low-rank Matrix Factorization (MF) methods provide one of the simplest and most effective approaches to collaborative filtering. This paper is the first to investigate the problem of efficient retrieval of recommendations in a MF framework. We reduce the retrieval in a MF model to an apparently simple task of finding the maximum dot-product for the user vector over the set of item vectors. However, to the best of our knowledge the problem of efficiently finding the maximum dot-product in the general case has never been studied. To this end, we propose two techniques for efficient search -- (i) We index the item vectors in a binary spatial-partitioning metric tree and use a simple branch and-bound algorithm with a novel bounding scheme to efficiently obtain exact solutions. (ii) We use spherical clustering to index the users on the basis of their preferences and pre-compute recommendations only for the representative user of each cluster to obtain extremely efficient approximate solutions. We obtain a theoretical error bound which determines the quality of any approximate result and use it to control the approximation. Both these simple techniques are fairly independent of each other and hence are easily combined to further improve recommendation retrieval efficiency. We evaluate our algorithms on real-world collaborative-filtering datasets, demonstrating more than ×7 speedup (with respect to the naive linear search) for the exact solution and over ×250 speedup for approximate solutions by combining both techniques.</p>
<p>【Keywords】:
collaborative filtering; fast retrieval; inner-product</p>
<h3 id="58. KORE: keyphrase overlap relatedness for entity disambiguation.">58. KORE: keyphrase overlap relatedness for entity disambiguation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396832">Paper Link</a>】    【Pages】:545-554</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hoffart:Johannes">Johannes Hoffart</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seufert:Stephan">Stephan Seufert</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Dat_Ba">Dat Ba Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Theobald:Martin">Martin Theobald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.</p>
<p>【Keywords】:
entity disambiguation; entity relatedness; locality-sensitive hashing; semantic relatedness</p>
<h3 id="59. Shard ranking and cutoff estimation for topically partitioned collections.">59. Shard ranking and cutoff estimation for topically partitioned collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396833">Paper Link</a>】    【Pages】:555-564</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kulkarni:Anagha">Anagha Kulkarni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tigelaar:Almer_S=">Almer S. Tigelaar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hiemstra:Djoerd">Djoerd Hiemstra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Callan:Jamie">Jamie Callan</a></p>
<p>【Abstract】:
Large document collections can be partitioned into 'topical shards' to facilitate distributed search. In a low-resource search environment only a few of the shards can be searched in parallel. Such a search environment faces two intertwined challenges. First, determining which shards to consult for a given query: shard ranking. Second, how many shards to consult from the ranking: cutoff estimation. In this paper we present a family of three algorithms that address both of these problems. As a basis we employ a commonly used data structure, the central sample index (CSI), to represent the shard contents. Running a query against the CSI yields a flat document ranking that each of our algorithms transforms into a tree structure. A bottom up traversal of the tree is used to infer a ranking of shards and also to estimate a stopping point in this ranking that yields cost-effective selective distributed search. As compared to a state-of-the-art shard ranking approach the proposed algorithms provide substantially higher search efficiency while providing comparable search effectiveness.</p>
<p>【Keywords】:
distributed information retrieval; selective search</p>
<h2 id="KM track: advertisement and products    5">KM track: advertisement and products    5</h2>
<h3 id="60. Daily-deal selection for revenue maximization.">60. Daily-deal selection for revenue maximization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396835">Paper Link</a>】    【Pages】:565-574</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lappas:Theodoros">Theodoros Lappas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Terzi:Evimaria">Evimaria Terzi</a></p>
<p>【Abstract】:
Daily-Deal Sites (DDS) like Groupon, LivingSocial, Amazon's Goldbox, and many more, have become particularly popular over the last three years, providing discounted offers to customers for restaurants, ticketed events, services etc. In this paper, we study the following problem: among a set of candidate deals, which are the ones that a DDS should feature as daily-deals in order to maximize its revenue? Our first contribution lies in providing two combinatorial formulations of this problem. Both formulations take into account factors like the diversification of daily deals and the limited consuming capacity of the userbase. We prove that our problems are NP-hard and devise pseudopolynomial -- time approximation algorithms for their solution. We also propose a set of heuristics, and demonstrate their efficiency in our experiments. In the context of deal selection and scheduling, we acknowledge the importance of the ability to estimate the expected revenue of a candidate deal. We explore the nature of this task in the context of real data, and propose a framework for revenue-estimation. We demonstrate the effectiveness of our entire methodology in an experimental evaluation on a large dataset of daily-deals from Groupon.</p>
<p>【Keywords】:
daily deals; deal selection; e-commerce; revenue maximization</p>
<h3 id="61. Enabling direct interest-aware audience selection.">61. Enabling direct interest-aware audience selection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396836">Paper Link</a>】    【Pages】:575-584</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fuxman:Ariel">Ariel Fuxman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kannan:Anitha">Anitha Kannan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Zhenhui">Zhenhui Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsaparas:Panayiotis">Panayiotis Tsaparas</a></p>
<p>【Abstract】:
Advertisers typically have a fairly accurate idea of the interests of their target audience. However, today's online advertising systems are unable to leverage this information. The reasons are two-fold. First, there is no agreed upon vocabulary of interests for advertisers and advertising systems to communicate. More importantly, advertising systems lack a mechanism for mapping users to the interest vocabulary. In this paper, we tackle both problems. We present a system for direct interest-aware audience selection. This system takes the query histories of search engine users as input, extracts their interests, and describes them with interpretable labels. The labels are not drawn from a predefined taxonomy, but rather dynamically generated from the query histories, and are thus easy for the advertisers to interpret and use for targeting users. In addition, the system enables seamless addition of interest labels that may be provided by the advertiser.</p>
<p>【Keywords】:
audience selection; user interests</p>
<h3 id="62. Influence propagation in adversarial setting: how to defeat competition with least amount of investment.">62. Influence propagation in adversarial setting: how to defeat competition with least amount of investment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396837">Paper Link</a>】    【Pages】:585-594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shirazipourazad:Shahrzad">Shahrzad Shirazipourazad</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bogard:Brian">Brian Bogard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vachhani:Harsh">Harsh Vachhani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sen:Arunabha">Arunabha Sen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Horn:Paul">Paul Horn</a></p>
<p>【Abstract】:
It has been observed that individuals' decisions to adopt a product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization in social networks. The primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these studies is that they focus on a non-adversarial environment, where only one player is engaged in influencing the nodes. However, in a realistic scenario multiple players attempt to influence the nodes in a competitive fashion. The proposed model considers a competitive environment where a node that has not yet adopted an innovation, can adopt only one of the several competing innovations and once it adopts an innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k nodes and the second player, with the knowledge of the choice of the first, attempts to identify a smallest set of nodes (excluding the ones already chosen by the first) so that when the influence propagation process ends, the number of nodes influenced by the second player is larger than the number of nodes influenced by the first. The paper studies two propagation models and shows that in both the models, the identification of the smallest set of nodes to defeat the adversary is NP-Hard. It provides an approximation algorithm and proves that the performance bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily defeat the first with this algorithm, if the first utilizes the node degree or closeness centrality based algorithms for the selection of influential nodes. The proposed algorithm also provides better performance if the second player utilizes it instead of the greedy algorithm to maximize its influence.</p>
<p>【Keywords】:
adversarial environment; influence maximization; social networks</p>
<h3 id="63. Large-scale item categorization for e-commerce.">63. Large-scale item categorization for e-commerce.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396838">Paper Link</a>】    【Pages】:595-604</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Dan">Dan Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ruvini:Jean=David">Jean-David Ruvini</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarwar:Badrul">Badrul Sarwar</a></p>
<p>【Abstract】:
This paper studies the problem of leveraging computationally intensive classification algorithms for large scale text categorization problems. We propose a hierarchical approach which decomposes the classification problem into a coarse level task and a fine level task. A simple yet scalable classifier is applied to perform the coarse level classification while a more sophisticated model is used to separate classes at the fine level. However, instead of relying on a human-defined hierarchy to decompose the problem, we we use a graph algorithm to discover automatically groups of highly similar classes. As an illustrative example, we apply our approach to real-world industrial data from eBay, a major e-commerce site where the goal is to classify live items into a large taxonomy of categories. In such industrial setting, classification is very challenging due to the number of classes, the amount of training data, the size of the feature space and the real-world requirements on the response time. We demonstrate through extensive experimental evaluation that (1) the proposed hierarchical approach is superior to flat models, and (2) the data-driven extraction of latent groups works significantly better than the existing human-defined hierarchy.</p>
<p>【Keywords】:
classification; text</p>
<h3 id="64. Matching product titles using web-based enrichment.">64. Matching product titles using web-based enrichment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396839">Paper Link</a>】    【Pages】:605-614</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gopalakrishnan:Vishrawas">Vishrawas Gopalakrishnan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iyengar:Suresh_Parthasarathy">Suresh Parthasarathy Iyengar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Madaan:Amit">Amit Madaan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rastogi:Rajeev">Rajeev Rastogi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sengamedu:Srinivasan_H=">Srinivasan H. Sengamedu</a></p>
<p>【Abstract】:
Matching product titles from different data feeds that refer to the same underlying product entity is a key problem in online shopping. This matching problem is challenging because titles across the feeds have diverse representations with some missing important keywords like brand and others containing extraneous keywords related to product specifications. In this paper, we propose a novel unsupervised matching algorithm that leverages web earch engines to (1) enrich product titles by adding important missing tokens that occur frequently in search results, and (2) compute importance scores for tokens based on their ability to retrieve other (enriched title) tokens in search results. Our matching scheme calculates the Cosine similarity between enriched title pairs with tokens weighted by their importance scores. We propose an optimization that exploits the templatized structure of product titles to reduce the number of search queries. In experiments with real-life shopping datasets, we found that our matching algorithm has superior F1 scores compared to IDF-based cosine similarity.</p>
<p>【Keywords】:
entity resolution; web-based enrichment</p>
<h2 id="KM track: clustering    5">KM track: clustering    5</h2>
<h3 id="65. Scalable clustering of signed networks using balance normalized cut.">65. Scalable clustering of signed networks using balance normalized cut.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396841">Paper Link</a>】    【Pages】:615-624</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chiang:Kai=Yang">Kai-Yang Chiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Whang:Joyce_Jiyoung">Joyce Jiyoung Whang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dhillon:Inderjit_S=">Inderjit S. Dhillon</a></p>
<p>【Abstract】:
We consider the general $k$-way clustering problem in signed social networks where relationships between entities can be either positive or negative. Motivated by social balance theory, the clustering problem in signed networks aims to find mutually antagonistic groups such that entities within the same group are friends with each other. A recent method proposed in [13] extended the spectral clustering algorithm to the signed network setting by considering the signed graph Laplacian. This has been shown to be equivalent to finding clusters that minimize the 2-way signed ratio cut. In this paper, we show that there is a fundamental weakness when we directly extend the signed Laplacian to the k-way clustering problem. To overcome this weakness, we formulate new k-way objectives for signed networks. In particular, we propose a criterion that is analogous to the normalized cut, called balance normalized cut, which is not only theoretically sound but also experimentally effective in k-way clustering. In addition, we prove that these objectives are equivalent to weighted kernel k-means objectives by choosing an appropriate kernel matrix. Employing this equivalence, we develop a multilevel clustering framework for signed networks. In this framework, we coarsen the graph level by level and refine the clustering results at each level via a k-means based algorithm so that the signed clustering objectives are optimized. This approach gives good quality clustering results, and is also highly efficient and scalable. In experiments, we see that our multilevel approach is competitive to other state-of-the-art methods, while it is much faster and more scalable. In particular, the largest graph we have considered in our experiments contains 1 million nodes and 100 million edges --- this graph can be clustered in less than four hundred seconds using our algorithm.</p>
<p>【Keywords】:
clustering; signed graph kernels; signed networks; social balance theory;</p>
<h3 id="66. Maximum margin clustering on evolutionary data.">66. Maximum margin clustering on evolutionary data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396842">Paper Link</a>】    【Pages】:625-634</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Xuhui">Xuhui Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Lin">Lin Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Longbing">Longbing Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Xia">Xia Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ong:Yew=Soon">Yew-Soon Ong</a></p>
<p>【Abstract】:
Evolutionary data, such as topic changing blogs and evolving trading behaviors in capital market, is widely seen in business and social applications. The time factor and intrinsic change embedded in evolutionary data greatly challenge evolutionary clustering. To incorporate the time factor, existing methods mainly regard the evolutionary clustering problem as a linear combination of snapshot cost and temporal cost, and reflect the time factor through the temporal cost. It still faces accuracy and scalability challenge though promising results gotten. This paper proposes a novel evolutionary clustering approach, evolutionary maximum margin clustering (e-MMC), to cluster large-scale evolutionary data from the maximum margin perspective. e-MMC incorporates two frameworks: Data Integration from the data changing perspective and Model Integration corresponding to model adjustment to tackle the time factor and change, with an adaptive label allocation mechanism. Three e-MMC clustering algorithms are proposed based on the two frameworks. Extensive experiments are performed on synthetic data, UCI data and real-world blog data, which confirm that e-MMC outperforms the state-of-the-art clustering algorithms in terms of accuracy, computational cost and scalability. It shows that e-MMC is particularly suitable for clustering large-scale evolving data.</p>
<p>【Keywords】:
evolutionary data.; maximum margin clustering</p>
<h3 id="67. Document-topic hierarchies from document graphs.">67. Document-topic hierarchies from document graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396843">Paper Link</a>】    【Pages】:635-644</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weninger:Tim">Tim Weninger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bisk:Yonatan">Yonatan Bisk</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
Topic taxonomies present a multi-level view of a document collection, where general topics live towards the top of the taxonomy and more specific topics live towards the bottom. Topic taxonomies allow users to quickly drill down into their topic of interest to find documents. We show that hierarchies of documents, where documents live at the inner nodes of the hierarchy-tree can also be inferred by combining document text with inter-document links. We present a Bayesian generative model by which an explicit hierarchy of documents is created. Experiments on three document-graph data sets shows that the generated document hierarchies are able to fit the observed data, and that the levels in the constructed document hierarchy represent practical groupings.</p>
<p>【Keywords】:
bayesian generative models; hierarchical clustering; model evaluation; topic models</p>
<h3 id="68. Improving document clustering using automated machine translation.">68. Improving document clustering using automated machine translation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396844">Paper Link</a>】    【Pages】:645-653</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xiang">Xiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qian:Buyue">Buyue Qian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davidson:Ian">Ian Davidson</a></p>
<p>【Abstract】:
With the development of statistical machine translation, we have ready-to-use tools that can translate documents from one language to many other languages. These translations provide different yet correlated views of the same set of documents. This gives rise to an intriguing question: can we use the extra information to achieve a better clustering of the documents? Some recent work on multiview clustering provided positive answers to this question. In this work, we propose an alternative approach to address this problem using the constrained clustering framework. Unlike traditional Must-Link and Cannot-Link constraints, the constraints generated from machine translation are dense yet noisy. We show how to incorporate this type of constraints by presenting two algorithms, one parametric and one non-parametric. Our algorithms are easy to implement, efficient, and can consistently improve the clustering of real data, namely the Reuters RCV1/RCV2 Multilingual Dataset. In contrast to existing multiview clustering algorithms, our technique does not need the compatibility or the conditional independence assumption, nor does it involve subtle parameter tuning.</p>
<p>【Keywords】:
constrained spectral clustering; document clustering; machine translation</p>
<h3 id="69. Right-protected data publishing with hierarchical clustering preservation.">69. Right-protected data publishing with hierarchical clustering preservation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396845">Paper Link</a>】    【Pages】:654-663</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vlachos:Michail">Michail Vlachos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wieczorek:Aleksander">Aleksander Wieczorek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schneider:Johannes">Johannes Schneider</a></p>
<p>【Abstract】:
The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms for establishing ownership in case of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on the dataset utility after the protection process. This work presents techniques that explicitly address this shortcoming and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. We encode data ownership using watermarking principles. In the process, we derive fundamental bounds on the distortion incurred by the watermarking. We leverage our theoretical analysis to design fast algorithms for right protection without exhaustively searching the vast design space.</p>
<p>【Keywords】:
watermarking</p>
<h2 id="IR track: recommendation systems    5">IR track: recommendation systems    5</h2>
<h3 id="70. Metaphor: a system for related search recommendations.">70. Metaphor: a system for related search recommendations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396847">Paper Link</a>】    【Pages】:664-673</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Reda:Azarias">Azarias Reda</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Yubin">Yubin Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tiwari:Mitul">Mitul Tiwari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Posse:Christian">Christian Posse</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shah:Sam">Sam Shah</a></p>
<p>【Abstract】:
Search plays an important role in online social networks as it provides an essential mechanism for discovering members and content on the network. Related search recommendation is one of several mechanisms used for improving members' search experience in finding relevant results to their queries. This paper describes the design, implementation, and deployment of Metaphor, the related search recommendation system on LinkedIn, a professional social networking site with over 175~million members worldwide. Metaphor builds on a number of signals and filters that capture several dimensions of relatedness across member search activity. The system, which has been in live operation for over a year, has gone through multiple iterations and evaluation cycles. This paper makes three contributions. First, we provide a discussion of a large-scale related search recommendation system. Second, we describe a mechanism for effectively combining several signals in building a unified dataset for related search recommendations. Third, we introduce a query length model for capturing bias in recommendation click behavior. We also discuss some of the practical concerns in deploying related search recommendations.</p>
<p>【Keywords】:
log analysis; query suggestions; recommender system</p>
<h3 id="71. Exploring personal impact for group recommendation.">71. Exploring personal impact for group recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396848">Paper Link</a>】    【Pages】:674-683</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xingjie">Xingjie Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tian:Yuan">Yuan Tian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Mao">Mao Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a></p>
<p>【Abstract】:
Group activities are essential ingredients of people's social life. The rapid growth of online social networking services has greatly boosted group activities by providing convenient platform for users to organize and participate in such activities. Therefore, recommender systems, as a critical component in social networking services, now face new challenges in supporting group activities. In this paper, we study the group recommendation problem, i.e., making recommendations to a group of people in social networking services. We analyze the decision making process in a group to propose a personal impact topic (PIT) model for group recommendations. The PIT model effectively identifies the group preference profile for a given group by considering the personal preferences and personal impacts of group members. Moreover, we further enhance the discovery of personal impact with social network information to obtain an extended personal impact topic (E-PIT) model. We have conducted comprehensive data analysis and evaluations on three real datasets. The results show that our proposed group recommendation techniques outperform baseline approaches.</p>
<p>【Keywords】:
group recommendation; probabilistic generative model; recommender systems</p>
<h3 id="72. The efficient imputation method for neighborhood-based collaborative filtering.">72. The efficient imputation method for neighborhood-based collaborative filtering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396849">Paper Link</a>】    【Pages】:684-693</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ren:Yongli">Yongli Ren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Gang">Gang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0010:Jun">Jun Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Wanlei">Wanlei Zhou</a></p>
<p>【Abstract】:
As each user tends to rate a small proportion of available items, the resulted Data Sparsity issue brings significant challenges to the research of recommender systems. This issue becomes even more severe for neighborhood-based collaborative filtering methods, as there are even lower numbers of ratings available in the neighborhood of the query item. In this paper, we aim to address the Data Sparsity issue in the context of the neighborhood-based collaborative filtering. Given the (user, item) query, a set of key ratings are identified, and an auto-adaptive imputation method is proposed to fill the missing values in the set of key ratings. The proposed method can be used with any similarity metrics, such as the Pearson Correlation Coefficient and Cosine-based similarity, and it is theoretically guaranteed to outperform the neighborhood-based collaborative filtering approaches. Results from experiments prove that the proposed method could significantly improve the accuracy of recommendations for neighborhood-based Collaborative Filtering algorithms.</p>
<p>【Keywords】:
collaborative filtering; imputation; recommender systems</p>
<h3 id="73. Multi-faceted ranking of news articles using post-read actions.">73. Multi-faceted ranking of news articles using post-read actions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396850">Paper Link</a>】    【Pages】:694-703</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Deepak">Deepak Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Bee=Chung">Bee-Chung Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xuanhui">Xuanhui Wang</a></p>
<p>【Abstract】:
Personalized article recommendation is important for news portals to improve user engagement. Existing work quantifies engagement primarily through click rates. We suggest that quality of recommendations may be improved by exploiting different types of "post-read" engagement signals like sharing, commenting, printing and e-mailing article links. Specifically, we propose a multi-faceted ranking problem for recommending articles, where each facet corresponds to a ranking task that seeks to maximize actions of a particular post-read type (e.g., ranking articles to maximize sharing actions). Our approach is to predict the probability that a user would take a post-read action on an article, so that articles can be ranked according to such probabilities. However, post-read actions are rare events --- enormous data sparsity makes the problem challenging. We meet the challenge by exploiting correlations across different post-read action types through a novel locally augmented tensor (LAT) model, so that the ranking performance of a particular action type can be improved by leveraging data from all other action types. Through extensive experiments, we show that our LAT model significantly outperforms a variety of state-of-the-art factor models, logistic regression and IR models.</p>
<p>【Keywords】:
multi-faceted; post-read; tensor model</p>
<h3 id="74. A decentralized recommender system for effective web credibility assessment.">74. A decentralized recommender system for effective web credibility assessment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396851">Paper Link</a>】    【Pages】:704-713</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Papaioannou:Thanasis_G=">Thanasis G. Papaioannou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ranvier:Jean=Eudes">Jean-Eudes Ranvier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Olteanu:Alexandra">Alexandra Olteanu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aberer:Karl">Karl Aberer</a></p>
<p>【Abstract】:
An overwhelming and growing amount of data is available online. The problem of untrustworthy online information is augmented by its high economic potential and its dynamic nature, e.g. transient domain names, dynamic content, etc. In this paper, we address the problem of assessing the credibility of web pages by a decentralized social recommender system. Specifically, we concurrently employ i) item-based collaborative filtering (CF) based on specific web page features, ii) user-based CF based on friend ratings and iii) the ranking of the page in search results. These factors are appropriately combined into a single assessment based on adaptive weights that depend on their effectiveness for different topics and different fractions of malicious ratings. Simulation experiments with real traces of web page credibility evaluations suggest that our hybrid approach outperforms both its constituent components and classical content-based classification approaches.</p>
<p>【Keywords】:
collaborative filtering; similarity metrics; social networks</p>
<h2 id="IR track: digital libraries and citation analysis    5">IR track: digital libraries and citation analysis    5</h2>
<h3 id="75. Towards an effective and unbiased ranking of scientific literature through mutual reinforcement.">75. Towards an effective and unbiased ranking of scientific literature through mutual reinforcement.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396853">Paper Link</a>】    【Pages】:714-723</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Xiaorui">Xiaorui Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Xiaoping">Xiaoping Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhuge:Hai">Hai Zhuge</a></p>
<p>【Abstract】:
It is important to help researchers find valuable scientific papers from a large literature collection containing information of authors, papers and venues. Graph-based algorithms have been proposed to rank papers based on networks formed by citation and co-author relationships. This paper proposes a new graph-based ranking framework MutualRank that integrates mutual reinforcement relationships among networks of papers, researchers and venues to achieve a more synthetic, accurate and fair ranking result than previous graph-based methods. MutualRank leverages the network structure information among papers, authors, and their venues available from a literature collection dataset and sets up a unified mutual reinforcement model that involves both intra- and inter-network information for ranking papers, authors and venues simultaneously. To evaluate, we collect a set of recommended papers from websites of graduate-level computational linguistics courses of 15 top universities as the benchmark and apply different methods to estimate paper importance. The results show that MutualRank greatly outperforms the competitors including Pag-eRank, HITS and CoRank in ranking papers as well as researchers. The experimental results also demonstrate that venues ranked by MutualRank are reasonable.</p>
<p>【Keywords】:
iterative ranking; mutual reinforcement; time distortion</p>
<h3 id="76. A math-aware search engine for math question answering system.">76. A math-aware search engine for math question answering system.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396854">Paper Link</a>】    【Pages】:724-733</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Tam_T=">Tam T. Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Kuiyu">Kuiyu Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a></p>
<p>【Abstract】:
We propose a math-aware search engine that is capable of handling both textual keywords as well as mathematical expressions. Our math feature extraction and representation framework captures the semantics of math expressions via a Finite State Machine model. We adapt the passive aggressive online learning binary classifier as the ranking model. We benchmarked our approach against three classical information retrieval (IR) strategies on math documents crawled from Math Overflow, a well-known online math question answering system. Experimental results show that our proposed approach can perform better than other methods by more than 9%.</p>
<p>【Keywords】:
learning to rank; math document retrieval; math-aware search engine</p>
<h3 id="77. Contextualization using hyperlinks and internal hierarchical structure of Wikipedia documents.">77. Contextualization using hyperlinks and internal hierarchical structure of Wikipedia documents.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396855">Paper Link</a>】    【Pages】:734-743</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Norozi:Muhammad_Ali">Muhammad Ali Norozi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Arvola:Paavo">Paavo Arvola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vries:Arjen_P=_de">Arjen P. de Vries</a></p>
<p>【Abstract】:
Context surrounding hyperlinked semi-structured documents, externally in the form of citations and internally in the form of hierarchical structure, contains a wealth of useful but implicit evidence about a document's relevance. These rich sources of information should be exploited as contextual evidence. This paper proposes various methods of accumulating evidence from the context, and measures the effect of contextual evidence on retrieval effectiveness for document and focused retrieval of hyperlinked semi-structured documents. We propose a re-weighting model to contextualize (a) evidence from citations in a query-independent and query-dependent fashion (based on Markovian random walks) and (b) evidence accumulated from the internal tree structure of documents. The in-links and out-links of a node in the citation graph are used as external context, while the internal document structure provides internal, within-document context. We hypothesize that documents in a good context (having strong contextual evidence) should be good candidates to be relevant to the posed query, and vice versa. We tested several variants of contextualization and verified notable improvements in comparison with the baseline system and gold standards in the retrieval of full documents and focused elements.</p>
<p>【Keywords】:
contextualization; random walks; re-weighting; schema agnostic search; semi-structured data; structural indices; xml retrieval</p>
<h3 id="78. Understanding book search behavior on the web.">78. Understanding book search behavior on the web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396856">Paper Link</a>】    【Pages】:744-753</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Jin_Young">Jin Young Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feild:Henry_Allen">Henry Allen Feild</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cartright:Marc=Allen">Marc-Allen Cartright</a></p>
<p>【Abstract】:
With the increased availability of e-books and digitized book collections, more users are searching the web for information about books. There are many online digital libraries containing book, author and subject data, which are accessed via internal search services as well as external web sites, such as Google. Although this is a common yet complex information-seeking behavior involving multiple search systems with different characteristics, little is known about how users find information in this scenario. In this work, we analyze web-based book search behavior using three months of logs from the Open Library, a globally accessible digital library. Our study encompasses the user behavior on web search engines and the digital library, unlike previous work which focused on institution-level digital libraries. Among our findings are (1) query characteristics and session-level behaviors are drastically different between internal and external searchers; (2) the field usage is different based on the modes of interaction---keyword search, advanced search interface and faceted filtering; (3) users go through with more iterations of faceted filtering than query reformulation. To facilitate future research on book search, we also create a book search test collection based on the log data. We then perform an evaluation of several retrieval methods, finding that field-based retrieval models have advantages over document-based models.</p>
<p>【Keywords】:
book search; query log analysis; user modeling</p>
<h3 id="79. Temporal corpus summarization using submodular word coverage.">79. Temporal corpus summarization using submodular word coverage.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396857">Paper Link</a>】    【Pages】:754-763</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sipos:Ruben">Ruben Sipos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Swaminathan:Adith">Adith Swaminathan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shivaswamy:Pannaga">Pannaga Shivaswamy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Joachims:Thorsten">Thorsten Joachims</a></p>
<p>【Abstract】:
In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people's personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole. In this paper, we explore methods that provide a temporal summary of such corpora in terms of landmark documents, authors, and topics. In particular, we explicitly model the temporal nature of influence between documents and re-interpret summarization as a coverage problem over words anchored in time. The resulting models provide monotone sub-modular objectives for computing informative and non-redundant summaries over time, which can be efficiently optimized with greedy algorithms. Our empirical study shows the effectiveness of our approach over several baselines.</p>
<p>【Keywords】:
submodular; summarization; temporal</p>
<h2 id="KM track: text mining    5">KM track: text mining    5</h2>
<h3 id="80. TCSST: transfer classification of short & sparse text using external data.">80. TCSST: transfer classification of short &amp; sparse text using external data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396859">Paper Link</a>】    【Pages】:764-772</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Long:Guodong">Guodong Long</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ling">Ling Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xingquan">Xingquan Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Chengqi">Chengqi Zhang</a></p>
<p>【Abstract】:
Short &amp; sparse text is becoming more prevalent on the web, such as search snippets, micro-blogs and product reviews. Accurately classifying short &amp; sparse text has emerged as an important while challenging task. Existing work has considered utilizing external data (e.g. Wikipedia) to alleviate data sparseness, by appending topics detected from external data as new features. However, training a classifier on features concatenated from different spaces is not easy considering the features have different physical meanings and different significance to the classification task. Moreover, it exacerbates the "curse of dimensionality" problem. In this study, we propose a transfer classification method, TCSST, to exploit the external data to tackle the data sparsity issue. The transfer classifier will be learned in the original feature space. Considering that the labels of the external data may not be readily available or sufficiently enough, TCSST further exploits the unlabeled external data to aid the transfer classification. We develop novel strategies to allow TCSST to iteratively select high quality unlabeled external data to help with the classification. We evaluate the performance of TCSST on both benchmark as well as real-world data sets. Our experimental results demonstrate that the proposed method is effective in classifying very short &amp; sparse text, consistently outperforming existing and baseline methods.</p>
<p>【Keywords】:
classification; external data; short &amp; sparse text mining; transfer learning; wikipedia</p>
<h3 id="81. The generalized dirichlet distribution in enhanced topic detection.">81. The generalized dirichlet distribution in enhanced topic detection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396860">Paper Link</a>】    【Pages】:773-782</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Espinosa:Karla_L=_Caballero">Karla L. Caballero Espinosa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Barajas:Joel">Joel Barajas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Akella:Ram">Ram Akella</a></p>
<p>【Abstract】:
We present a new, robust and computationally efficient Hierarchical Bayesian model for effective topic correlation modeling. We model the prior distribution of topics by a Generalized Dirichlet distribution (GD) rather than a Dirichlet distribution as in Latent Dirichlet Allocation (LDA). We define this model as GD-LDA. This framework captures correlations between topics, as in the Correlated Topic Model (CTM) and Pachinko Allocation Model (PAM), and is faster to infer than CTM and PAM. GD-LDA is effective to avoid over-fitting as the number of topics is increased. As a tree model, it accommodates the most important set of topics in the upper part of the tree based on their probability mass. Thus, GD-LDA provides the ability to choose significant topics effectively. To discover topic relationships, we perform hyper-parameter estimation based on Monte Carlo EM Estimation. We provide results using Empirical Likelihood(EL) in 4 public datasets from TREC and NIPS. Then, we present the performance of GD-LDA in ad hoc information retrieval (IR) based on MAP, P@10, and Discounted Gain. We discuss an empirical comparison of the fitting time. We demonstrate significant improvement over CTM, LDA, and PAM for EL estimation. For all the IR measures, GD-LDA shows higher performance than LDA, the dominant topic model in IR. All these improvements with a small increase in fitting time than LDA, as opposed to CTM and PAM.</p>
<p>【Keywords】:
document representation; statistical topic modeling</p>
<h3 id="82. Modeling topic hierarchies with the recursive chinese restaurant process.">82. Modeling topic hierarchies with the recursive chinese restaurant process.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396861">Paper Link</a>】    【Pages】:783-792</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Joon_Hee">Joon Hee Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Dongwoo">Dongwoo Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Suin">Suin Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Oh:Alice_H=">Alice H. Oh</a></p>
<p>【Abstract】:
Topic models such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet processes (HDP) are simple solutions to discover topics from a set of unannotated documents. While they are simple and popular, a major shortcoming of LDA and HDP is that they do not organize the topics into a hierarchical structure which is naturally found in many datasets. We introduce the recursive Chinese restaurant process (rCRP) and a nonparametric topic model with rCRP as a prior for discovering a hierarchical topic structure with unbounded depth and width. Unlike previous models for discovering topic hierarchies, rCRP allows the documents to be generated from a mixture over the entire set of topics in the hierarchy. We apply rCRP to a corpus of New York Times articles, a dataset of MovieLens ratings, and a set of Wikipedia articles and show the discovered topic hierarchies. We compare the predictive power of rCRP with LDA, HDP, and nested Chinese restaurant process (nCRP) using heldout likelihood to show that rCRP outperforms the others. We suggest two metrics that quantify the characteristics of a topic hierarchy to compare the discovered topic hierarchies of rCRP and nCRP. The results show that rCRP discovers a hierarchy in which the topics become more specialized toward the leaves, and topics in the immediate family exhibit more affinity than topics beyond the immediate family.</p>
<p>【Keywords】:
bayesian nonparametric models; hierarchical topic modeling</p>
<h3 id="83. Two-part segmentation of text documents.">83. Two-part segmentation of text documents.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396862">Paper Link</a>】    【Pages】:793-802</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Deepak:P=">P. Deepak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Visweswariah:Karthik">Karthik Visweswariah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wiratunga:Nirmalie">Nirmalie Wiratunga</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sani:Sadiq">Sadiq Sani</a></p>
<p>【Abstract】:
We consider the problem of segmenting text documents that have a two-part structure such as a problem part and a solution part. Documents of this genre include incident reports that typically involve description of events relating to a problem followed by those pertaining to the solution that was tried. Segmenting such documents into the component two parts would render them usable in knowledge reuse frameworks such as Case-Based Reasoning. This segmentation problem presents a hard case for traditional text segmentation due to the lexical inter-relatedness of the segments. We develop a two-part segmentation technique that can harness a corpus of similar documents to model the behavior of the two segments and their inter-relatedness using language models and translation models respectively. In particular, we use separate language models for the problem and solution segment types, whereas the inter-relatedness between segment types is modeled using an IBM Model 1 translation model. We model documents as being generated starting from the problem part that comprises of words sampled from the problem language model, followed by the solution part whose words are sampled either from the solution language model or from a translation model conditioned on the words already chosen in the problem part. We show, through an extensive set of experiments on real-world data, that our approach outperforms the state-of-the-art text segmentation algorithms in the accuracy of segmentation, and that such improved accuracy translates well to improved usability in Case-based Reasoning systems. We also analyze the robustness of our technique to varying amounts and types of noise and empirically illustrate that our technique is quite noise tolerant, and degrades gracefully with increasing amounts of noise.</p>
<p>【Keywords】:
language models; segmentation; text; translation models</p>
<h3 id="84. On the design of LDA models for aspect-based opinion mining.">84. On the design of LDA models for aspect-based opinion mining.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396863">Paper Link</a>】    【Pages】:803-812</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moghaddam:Samaneh">Samaneh Moghaddam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ester:Martin">Martin Ester</a></p>
<p>【Abstract】:
Aspect-based opinion mining, which aims to extract aspects and their corresponding ratings from customers reviews, provides very useful information for customers to make purchase decisions. In the past few years several probabilistic graphical models have been proposed to address this problem, most of them based on Latent Dirichlet Allocation (LDA). While these models have a lot in common, there are some characteristics that distinguish them from each other. These fundamental differences correspond to major decisions that have been made in the design of the LDA models. While research papers typically claim that a new model outperforms the existing ones, there is normally no "one-size-fits-all" model. In this paper, we present a set of design guidelines for aspect-based opinion mining by discussing a series of increasingly sophisticated LDA models. We argue that these models represent the essence of the major published methods and allow us to distinguish the impact of various design decisions. We conduct extensive experiments on a very large real life dataset from Epinions.com (500K reviews) and compare the performance of different models in terms of the likelihood of the held-out test set and in terms of the accuracy of aspect identification and rating prediction.</p>
<p>【Keywords】:
aspect identification; aspect-based opinion mining; latent dirichlet allocation; rating prediction; variational methods</p>
<h2 id="IR track: formal retrieval models and learning to rank    5">IR track: formal retrieval models and learning to rank    5</h2>
<h3 id="85. Predicting query performance for fusion-based retrieval.">85. Predicting query performance for fusion-based retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396865">Paper Link</a>】    【Pages】:813-822</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Markovits:Gad">Gad Markovits</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shtok:Anna">Anna Shtok</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carmel:David">David Carmel</a></p>
<p>【Abstract】:
Estimating the effectiveness of a search performed in response to a query in the absence of relevance judgments is the goal of query-performance prediction methods. Post-retrieval predictors analyze the result list of the most highly ranked documents. We address the prediction challenge for retrieval approaches wherein the final result list is produced by fusing document lists that were retrieved in response to a query. To that end, we present a novel fundamental prediction framework that accounts for this special characteristics of the fusion setting; i.e., the use of intermediate retrieved lists. The framework is based on integrating prediction performed upon the final result list with that performed upon the lists that were fused to create it; prediction integration is controlled based on inter-list similarities. We empirically demonstrate the merits of various predictors instantiated from the framework. A case in point, their prediction quality substantially transcends that of applying state-of-the-art predictors upon the final result list.</p>
<p>【Keywords】:
fusion; query-performance prediction</p>
<h3 id="86. Back to the roots: a probabilistic framework for query-performance prediction.">86. Back to the roots: a probabilistic framework for query-performance prediction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396866">Paper Link</a>】    【Pages】:823-832</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shtok:Anna">Anna Shtok</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hummel:Shay">Shay Hummel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raiber:Fiana">Fiana Raiber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carmel:David">David Carmel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rom:Ofri">Ofri Rom</a></p>
<p>【Abstract】:
The query-performance prediction task is estimating the effectiveness of a search performed in response to a query when no relevance judgments are available. Although there exist many effective prediction methods, these differ substantially in their basic principles, and rely on diverse hypotheses about the characteristics of effective retrieval. We present a novel fundamental probabilistic prediction framework. Using the framework, we derive and explain various previously proposed prediction methods that might seem completely different, but turn out to share the same formal basis. The derivations provide new perspectives on several predictors (e.g., Clarity). The framework is also used to devise new prediction approaches that outperform the state-of-the-art.</p>
<p>【Keywords】:
query-performance prediction</p>
<h3 id="87. Learning to rank for robust question answering.">87. Learning to rank for robust question answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396867">Paper Link</a>】    【Pages】:833-842</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Arvind">Arvind Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raghavan:Hema">Hema Raghavan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Subbian:Karthik">Karthik Subbian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Melville:Prem">Prem Melville</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lawrence:Richard_D=">Richard D. Lawrence</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gondek:David">David Gondek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:James">James Fan</a></p>
<p>【Abstract】:
This paper aims to solve the problem of improving the ranking of answer candidates for factoid based questions in a state-of-the-art Question Answering system. We first provide an extensive comparison of 5 ranking algorithms on two datasets -- from the Jeopardy quiz show and a medical domain. We then show the effectiveness of a cascading approach, where the ranking produced by one ranker is used as input to the next stage. The cascading approach shows sizeable gains on both datasets. We finally evaluate several rank aggregation techniques to combine these algorithms, and find that Supervised Kemeny aggregation is a robust technique that always beats the baseline ranking approach used by Watson for the Jeopardy competition. We further corroborate our results on TREC Question Answering datasets.</p>
<p>【Keywords】:
question-answering; rank-aggregation; ranking</p>
<h3 id="88. Learning to rank by aggregating expert preferences.">88. Learning to rank by aggregating expert preferences.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396868">Paper Link</a>】    【Pages】:843-851</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Volkovs:Maksims">Maksims Volkovs</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Larochelle:Hugo">Hugo Larochelle</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zemel:Richard_S=">Richard S. Zemel</a></p>
<p>【Abstract】:
We present a general treatment of the problem of aggregating preferences from several experts into a consensus ranking, in the context where information about a target ranking is available. Specifically, we describe how such problems can be converted into a standard learning-to-rank one on which existing learning solutions can be invoked. This transformation allows us to optimize the aggregating function for any target IR metric, such as Normalized Discounted Cumulative Gain, or Expected Reciprocal Rank. When applied to crowdsourcing and meta-search benchmarks, our new algorithm improves on state-of-the-art preference aggregation methods.</p>
<p>【Keywords】:
crowdsourcing; meta-search; preference aggregation</p>
<h3 id="89. Learning to rank duplicate bug reports.">89. Learning to rank duplicate bug reports.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396869">Paper Link</a>】    【Pages】:852-861</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Jian">Jian Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Hongyu">Hongyu Zhang</a></p>
<p>【Abstract】:
For a large and complex software system, the project team could receive a large number of bug reports. Some bug reports could be duplicates as they essentially report the same problem. It is often tedious and costly to manually check if a newly reported bug is a duplicate of an already reported bug. In this paper, we propose BugSim, a method that can automatically retrieve duplicate bug reports given a new bug report. BugSim is based on learning to rank concepts. We identify textual and statistical features of bug reports and propose a similarity function for bug reports based on the features. We then construct a training set by assembling pairs of duplicate and non-duplicate bug reports. We train the weights of features by applying the stochastic gradient descent algorithm over the training set. For a new bug report, we retrieve candidate duplicate reports using the trained model. We evaluate BugSim using more than 45,100 real bug reports of twelve Eclipse projects. The evaluation results show that the proposed method is effective. On average, the recall rate for the top 10 retrieved reports is 76.11%. Furthermore, BugSim outperforms the previous state-of-art methods that are implemented using SVM and BM25Fext.</p>
<p>【Keywords】:
bug reports; duplicate bug retrieval; duplicate documents; learning to rank; software maintenance</p>
<h2 id="DB track: probabilistic and uncertain data    5">DB track: probabilistic and uncertain data    5</h2>
<h3 id="90. A model-based approach for RFID data stream cleansing.">90. A model-based approach for RFID data stream cleansing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396871">Paper Link</a>】    【Pages】:862-871</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Wilfred">Wilfred Ng</a></p>
<p>【Abstract】:
In recent years, RFID technologies have been used in many applications, such as inventory checking and object tracking. However, raw RFID data are inherently unreliable due to physical device limitations and different kinds of environmental noise. Currently, existing work mainly focuses on RFID data cleansing in a static environment (e.g. inventory checking). It is therefore difficult to cleanse RFID data streams in a mobile environment (e.g. object tracking) using the existing solutions, which do not address the data missing issue effectively. In this paper, we study how to cleanse RFID data streams for object tracking, which is a challenging problem, since a significant percentage of readings are routinely dropped. We propose a probabilistic model for object tracking in a mobile environment. We develop a Bayesian inference based approach for cleansing RFID data using the model. In order to sample data from the movement distribution, we devise a sequential sampler that cleans RFID data with high accuracy and efficiency. We validate the effectiveness and robustness of our solution through extensive simulations and demonstrate its performance by using two real RFID applications of human tracking and conveyor belt monitoring.</p>
<p>【Keywords】:
data cleaning; probabilistic algorithms; uncertainty</p>
<h3 id="91. What is the IQ of your data transformation system?">91. What is the IQ of your data transformation system?</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396872">Paper Link</a>】    【Pages】:872-881</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mecca:Giansalvatore">Giansalvatore Mecca</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Papotti:Paolo">Paolo Papotti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raunich:Salvatore">Salvatore Raunich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Santoro:Donatello">Donatello Santoro</a></p>
<p>【Abstract】:
Mapping and translating data across different representations is a crucial problem in information systems. Many formalisms and tools are currently used for this purpose, to the point that developers typically face a difficult question: "what is the right tool for my translation task?" In this paper, we introduce several techniques that contribute to answer this question. Among these, a fairly general definition of a data transformation system, a new and very efficient similarity measure to evaluate the outputs produced by such a system, and a metric to estimate user efforts. Based on these techniques, we are able to compare a wide range of systems on many translation tasks, to gain interesting insights about their effectiveness, and, ultimately, about their "intelligence".</p>
<p>【Keywords】:
ETL; benchmarks; data transformation; schema mappings</p>
<h3 id="92. On the foundations of probabilistic information integration.">92. On the foundations of probabilistic information integration.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396873">Paper Link</a>】    【Pages】:882-891</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sadri:Fereidoon">Fereidoon Sadri</a></p>
<p>【Abstract】:
Information integration has been a subject of research for several decades and still remains a very active research area. Many new applications depend or benefit from large scale integration. Examples include large research projects in life sciences, need for data sharing among government agencies, reliance of corporations on business intelligence (which requires data integration from many heterogeneous sources), and integration of information on the web. The importance of information integration with uncertainty has been observed in recent years. Frequently, information from multiple sources are uncertain and possibly inconsistent. Further the process of integration often depends on approximate schema mappings, another source of uncertainty. An integration system is useful only to the extent that the information it produces can be trusted. Hence, providing a measure of certainty for integrated information is of crucial importance in many important applications. In this paper we study the problem of integration of uncertain information. We present a simple and intuitive approach to the representation and integration of uncertain information from multiple sources, and show that our integration approach coincides with a recent formalism for uncertain information integration. We extend the model to probabilistic possible-worlds, and show certain unintuitive constraints are imposed upon probabilities of possible-worlds of sources. In particular, we show the probabilities of possible worlds of a source are not independent, rather, they are dependent on probabilities of other sources. We study the problem of determining the probabilities for the result of integration. Finally, we present a practical approach to relaxing probabilistic constraints in integration.</p>
<p>【Keywords】:
information integration; probabilistic data; uncertain data</p>
<h3 id="93. GPU acceleration of probabilistic frequent itemset mining from uncertain databases.">93. GPU acceleration of probabilistic frequent itemset mining from uncertain databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396874">Paper Link</a>】    【Pages】:892-901</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kozawa:Yusuke">Yusuke Kozawa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amagasa:Toshiyuki">Toshiyuki Amagasa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kitagawa:Hiroyuki">Hiroyuki Kitagawa</a></p>
<p>【Abstract】:
Uncertain databases have been widely developed to deal with the vast amount of data that contain uncertainty. To extract valuable information from the uncertain databases, several methods of frequent itemset mining, one of the major data mining techniques, have been proposed. However, their performance is not satisfactory because handling uncertainty incurs high processing costs. In order to address this problem, we utilize GPGPU (General-Purpose computation on GPU). GPGPU implies using a GPU (Graphics Processing Unit), which is originally designed for processing graphics, to accelerate general purpose computation. In this paper, we propose a method of frequent itemset mining from uncertain databases using GPGPU. The main idea is to speed up probability computations by making the best use of GPU's high parallelism and low-latency memory. We also employ an algorithm to manipulate a bitstring and data-parallel primitives to improve performance in the other parts of the method. Extensive experiments show that our proposed method is up to two orders of magnitude faster than existing methods.</p>
<p>【Keywords】:
frequent itemset mining; gpgpu; uncertain databases</p>
<h3 id="94. Completeness of queries over SQL databases.">94. Completeness of queries over SQL databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396875">Paper Link</a>】    【Pages】:902-911</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nutt:Werner">Werner Nutt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Razniewski:Simon">Simon Razniewski</a></p>
<p>【Abstract】:
Data completeness is an important aspect of data quality. We consider a setting, where databases can be incomplete in two ways: records may be missing and records may contain null values. We (i) formalize when the answer set of a query is complete in spite of such incompleteness, and (ii) we introduce table completeness statements, by which one can express that certain parts of a database are complete. We then study how to deduce from a set of table-completeness statements that a query can be answered completely. Null values as used in SQL are ambiguous. They can indicate either that no attribute value exists or that a value exists, but is unknown. We study completeness reasoning for the different interpretations. We show that in the combined case it is necessary to syntactically distinguish between different kinds of null values and present an encoding for doing that in standard SQL databases. With this technique, any SQL DBMS evaluates complete queries correctly with respect to the different meanings that nulls can carry. We study the complexity of completeness reasoning and provide algorithms that in most cases agree with the worst-case lower bounds.</p>
<p>【Keywords】:
data completeness; data quality; metadata management</p>
<h2 id="DB track: top-k and nearest neighbor queries    5">DB track: top-k and nearest neighbor queries    5</h2>
<h3 id="95. Being picky: processing top-k queries with set-defined selections.">95. Being picky: processing top-k queries with set-defined selections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396877">Paper Link</a>】    【Pages】:912-921</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Stupar:Aleksandar">Aleksandar Stupar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Michel:Sebastian">Sebastian Michel</a></p>
<p>【Abstract】:
Focusing on the top-K items according to a ranking criterion constitutes an important functionality in many different query answering scenarios. The idea is to read only the necessary information---mostly from secondary storage---with the ultimate goal to achieve low latency. In this work, we consider processing such top-K queries under the constraint that the result items are members of a specific set, which is provided at query time. We call this restriction a set-defined selection criterion. Set-defined selections drastically influence the pros and cons of an id-ordered index vs. a score-ordered index. We present a mathematical model that allows to decide at runtime which index to choose, leading to a combined index. To improve the latency around the break even point of the two indices, we show how to benefit from a partitioned score-ordered index and present an algorithm to create such partitions based on analyzing query logs. Further performance gains can be enjoyed using approximate top-K results, with tunable result quality. The presented approaches are evaluated using both real-world and synthetic data.</p>
<p>【Keywords】:
index partitioning; top-k query processing</p>
<h3 id="96. Finding top k most influential spatial facilities over uncertain objects.">96. Finding top k most influential spatial facilities over uncertain objects.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396878">Paper Link</a>】    【Pages】:922-931</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhan:Liming">Liming Zhan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ying">Ying Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Wenjie">Wenjie Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Xuemin">Xuemin Lin</a></p>
<p>【Abstract】:
Uncertainty is inherent in many important applications, such as location-based services (LBS), sensor monitoring and radio-frequency identification (RFID). Recently, considerable research efforts have been put into the field of uncertainty-aware spatial query processing. In this paper, we study the problem of finding top k most influential facilities over a set of uncertain objects, which is an important spatial query in the above applications. Based on the maximal utility principle, we propose a new ranking model to identify the top k most influential facilities, which carefully captures influence of facilities on the uncertain objects. By utilizing two uncertain object indexing techniques, R-tree and U-Quadtree, effective and efficient algorithms are proposed following the filtering and verification paradigm, which significantly improves the performance of the algorithms in terms of CPU and I/O costs. Comprehensive experiments on real datasets demonstrate the effectiveness and efficiency of our techniques.</p>
<p>【Keywords】:
spatial; uncertain</p>
<h3 id="97. Efficient safe-region construction for moving top-K spatial keyword queries.">97. Efficient safe-region construction for moving top-K spatial keyword queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396879">Paper Link</a>】    【Pages】:932-941</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Weihuang">Weihuang Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Kian=Lee">Kian-Lee Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Jianhua">Jianhua Feng</a></p>
<p>【Abstract】:
Many real-world applications have requirements to support moving spatial keyword queries. For example a tourist looks for top-k "seafood restaurants" while walking in a city. She will continuously issue moving queries. However existing spatial keyword search methods focus on static queries and it calls for new effective techniques to support moving queries efficiently. In this paper we propose an effective method to support moving top-k spatial keyword queries. In addition to finding top-k answers of a moving query, we also calculate a safe region such that if a new query with a location falling in the safe region, we can directly use the answer set to answer the query. To this end, we propose an effective model to represent the safe region and devise efficient search algorithms to compute the safe region. We have implemented our method and experimental results on real datasets show that our method achieves high efficiency and outperforms existing methods significantly.</p>
<p>【Keywords】:
moving top-k spatial keyword queries; safe region</p>
<h3 id="98. Monochromatic and bichromatic reverse nearest neighbor queries on land surfaces.">98. Monochromatic and bichromatic reverse nearest neighbor queries on land surfaces.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396880">Paper Link</a>】    【Pages】:942-951</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Da">Da Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Wilfred">Wilfred Ng</a></p>
<p>【Abstract】:
Finding reverse nearest neighbors (RNNs) is an important operation in spatial databases. The problem of evaluating RNN queries has already received considerable attention due to its importance in many real-world applications, such as resource allocation and disaster response. While RNN query processing has been extensively studied in Euclidean space, no work ever studies this problem on land surfaces. However, practical applications of RNN queries involve terrain surfaces that constrain object movements, which rendering the existing algorithms inapplicable. In this paper, we investigate the evaluation of two types of RNN queries on land surfaces: monochromatic RNN (MRNN) queries and bichromatic RNN (BRNN) queries. On a land surface, the distance between two points is calculated as the length of the shortest path along the surface. However, the computational cost of the state-of-the-art shortest path algorithm on a land surface is quadratic to the size of the surface model, which is usually quite huge. As a result, surface RNN query processing is a challenging problem. Leveraging some newly-discovered properties of Voronoi cell approximation structures, we make use of standard index structures such as an R-tree to design efficient algorithms that accelerate the evaluation of MRNN and BRNN queries on land surfaces. Our proposed algorithms are able to localize query evaluation by accessing just a small fraction of the surface data near the query point, which helps avoid shortest path evaluation on a large surface. Extensive experiments are conducted on large real-world datasets to demonstrate the efficiency of our algorithms.</p>
<p>【Keywords】:
land surface; reverse nearest neighbor; terrain</p>
<h3 id="99. Pay-as-you-go maintenance of precomputed nearest neighbors in large graphs.">99. Pay-as-you-go maintenance of precomputed nearest neighbors in large graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396881">Paper Link</a>】    【Pages】:952-961</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Crecelius:Tom">Tom Crecelius</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schenkel:Ralf">Ralf Schenkel</a></p>
<p>【Abstract】:
An important building block of many graph applications such as searching in social networks, keyword search in graphs, and retrieval of linked documents is retrieving the transitive neighbors of a node in ascending order of their distances. Since large graphs cannot be kept in memory and graph traversals at query time would be prohibitively expensive, the list of neighbors for each node is usually precomputed and stored in a compact form. While the problem of precomputing all-pairs shortest distances has been well studied for decades, efficiently maintaining this information when the graph changes is not as well understood. This paper presents an algorithm for maintaining nearest neighbor lists in weighted graphs under node insertions and decreasing edge weights. It considers the important case where queries are a lot more frequent than updates, and presents two approaches for transparently performing necessary index updates while executing queries. Extensive experiments with large graphs, including a subset of Twitter's user graph, demonstrate that the overhead for this maintenance is small.</p>
<p>【Keywords】:
databases; incremental apsd; shortest paths</p>
<h2 id="KM track: spatial and temporal methods    4">KM track: spatial and temporal methods    4</h2>
<h3 id="100. Spatial influence vs. community influence: modeling the global spread of social media.">100. Spatial influence vs. community influence: modeling the global spread of social media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396883">Paper Link</a>】    【Pages】:962-971</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kamath:Krishna_Yeswanth">Krishna Yeswanth Kamath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caverlee:James">James Caverlee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Zhiyuan">Zhiyuan Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sui:Daniel_Z=">Daniel Z. Sui</a></p>
<p>【Abstract】:
In this paper we seek to understand and model the global spread of social media. How does social media spread from location to location across the globe? Can we model this spread and predict where social media will be popular in the future? Toward answering these questions, we develop a probabilistic model that synthesizes two conflicting hypotheses about the nature of online information spread: (i) the spatial influence model, which asserts that social media spreads to locations that are close by; and (ii) the community affinity influence model, which asserts that social media spreads between locations that are culturally connected, even if they are distant. Based on the geospatial footprint of 755 million geo-tagged hashtags spread through Twitter, we evaluate these models at predicting locations that will adopt hashtags in the future. We find that distance is the single most important explanation of future hashtag adoption since hashtags are fundamentally local. We also find that community affinities (like culture, language, and common interests) enhance the quality of purely spatial models, indicating the necessity of incorporating non-spatial features into models of global social media spread.</p>
<p>【Keywords】:
information diffusion models; social media; virtual communities</p>
<h3 id="101. TUT: a statistical model for detecting trends, topics and user interests in social media.">101. TUT: a statistical model for detecting trends, topics and user interests in social media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396884">Paper Link</a>】    【Pages】:972-981</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Xuning">Xuning Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Christopher_C=">Christopher C. Yang</a></p>
<p>【Abstract】:
The rapid development of online social media sites is accompanied by the generation of tremendous web contents. Web users are shifting from data consumers to data producers. As a result, topic detection and tracking without taking users' interests into account is not enough. This paper presents a statistical model that can detect interpretable trends and topics from document streams, where each trend (short for trending story) corresponds to a series of continuing events or a storyline. A topic is represented by a cluster of words frequently co-occurred. A trend can contain multiple topics and a topic can be shared by different trends. In addition, by leveraging a Recurrent Chinese Restaurant Process (RCRP), the number of trends in our model can be determined automatically without human intervention, so that our model can better generalize to unseen data. Furthermore, our proposed model incorporates user interest to fully simulate the generation process of web contents, which offers the opportunity for personalized recommendation in online social media. Experiments on three different datasets indicated that our proposed model can capture meaningful topics and trends, monitor rise and fall of detected trends, outperform baseline approach in terms of perplexity on held-out dataset, and improve the result of user participation prediction by leveraging users' interests to different trends.</p>
<p>【Keywords】:
evolution; modeling; topic; trend; user interest</p>
<h3 id="102. Predicting aggregate social activities using continuous-time stochastic process.">102. Predicting aggregate social activities using continuous-time stochastic process.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396885">Paper Link</a>】    【Pages】:982-991</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Shu">Shu Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Min">Min Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Bo">Bo Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Dongwon">Dongwon Lee</a></p>
<p>【Abstract】:
How to accurately model and predict the future status of social networks has become an important problem in recent years. Conventional solutions to such a problem often employ topological structure of the sociogram, i.e., friendship links. However, they often disregard different levels of activeness of social actors and become insufficient to deal with complex dynamics of user behaviors. In this paper, to address this issue, we first refine the notion of social activity to better describe dynamic user behaviors in social networks. We then propose a Parameterized Social Activity Model (PSAM) using continuous-time stochastic process for predicting aggregate social activities. With social activities evolving over time, PSAM itself also evolves and therefore dynamically captures the real-time characteristics of the current active population. Our experiments using two real social networks (Facebook and CiteSeer) reveal that the proposed PSAM model is effective in simulating social activity evolution and predicting aggregate social activities accurately at different time scales.</p>
<p>【Keywords】:
aggregate social activity; continuous-time stochastic process</p>
<h3 id="103. Acquiring temporal constraints between relations.">103. Acquiring temporal constraints between relations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2396886">Paper Link</a>】    【Pages】:992-1001</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_Pratim">Partha Pratim Talukdar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wijaya:Derry_Tanti">Derry Tanti Wijaya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>【Abstract】:
We consider the problem of automatically acquiring knowledge about the typical temporal orderings among relations (e.g., actedIn(person, film) typically occurs before wonPrize (film, award)), given only a database of known facts (relation instances) without time information, and a large document collection. Our approach is based on the conjecture that the narrative order of verb mentions within documents correlates with the temporal order of the relations they represent. We propose a family of algorithms based on this conjecture, utilizing a corpus of 890m dependency parsed sentences to obtain verbs that represent relations of interest, and utilizing Wikipedia documents to gather statistics on narrative order of verb mentions. Our proposed algorithm, GraphOrder, is a novel and scalable graph-based label propagation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions. This algorithm achieves as high as 38.4% absolute improvement in F1 over a random baseline. Finally, we demonstrate the utility of this learned general knowledge about typical temporal orderings among relations, by showing that these temporal constraints can be successfully used by a joint inference framework to assign specific temporal scopes to individual facts.</p>
<p>【Keywords】:
graph-based semi-supervised learning; knowledge bases; label propagation; narrative ordering; temporal ordering; temporal scoping</p>
<h2 id="IR track: web search    4">IR track: web search    4</h2>
<h3 id="104. Towards optimum query segmentation: in doubt without.">104. Towards optimum query segmentation: in doubt without.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398398">Paper Link</a>】    【Pages】:1015-1024</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hagen:Matthias">Matthias Hagen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Potthast:Martin">Martin Potthast</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Beyer:Anna">Anna Beyer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Stein:Benno">Benno Stein</a></p>
<p>【Abstract】:
Query segmentation is the problem of identifying those keywords in a query, which together form compound concepts or phrases like "new york times". Such segments can help a search engine to better interpret a user's intents and to tailor the search results more appropriately. Our contributions to this problem are threefold. (1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations. (2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures. (3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to (partially) leave queries without any segmentation. This new in-doubt-without approach chooses different segmentation strategies depending on query types. A large-scale evaluation shows substantial improvement upon the state of the art in terms of segmentation accuracy. To draw a complete picture, we also evaluate the impact of segmentation strategies on retrieval performance in a TREC setting. It turns out that more accurate segmentation not necessarily yields better retrieval performance. Based on this insight, we propose an in-doubt-without variant which achieves the best retrieval performance despite leaving many queries unsegmented. But there is still room for improvement: the optimum segmentation strategy which always chooses the segmentation that maximizes retrieval performance, significantly outperforms all other tested approaches.</p>
<p>【Keywords】:
query segmentation</p>
<h3 id="105. Leaving so soon?: understanding and predicting web search abandonment rationales.">105. Leaving so soon?: understanding and predicting web search abandonment rationales.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398399">Paper Link</a>】    【Pages】:1025-1034</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Diriye:Abdigani">Abdigani Diriye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen">Ryen White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Buscher:Georg">Georg Buscher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a></p>
<p>【Abstract】:
Users of search engines often abandon their searches. Despite the high frequency of Web search abandonment and its importance to Web search engines, little is known about why searchers abandon beyond that it can be for good or bad reasons. In this paper, we ex-tend previous work by studying search abandonment using both a retrospective survey and an in-situ method that captures aban-donment rationales at abandonment time. We show that although satisfaction is a common motivator for abandonment, one-in-five abandonment instances does not relate to satisfaction. We also studied the automatic prediction of the underlying reason for ob-served abandonment. We used features of the query and the results, interaction with the result page (e.g., cursor movements, scrolling, clicks), and the full search session. We show that our classifiers can learn to accurately predict the reasons for observed search abandonment. Such accurate predictions help search providers estimate user satisfaction for queries without clicks, affording a more complete understanding of search engine performance.</p>
<p>【Keywords】:
abandonment rationales; web search abandonment</p>
<h3 id="106. Click patterns: an empirical representation of complex query intents.">106. Click patterns: an empirical representation of complex query intents.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398400">Paper Link</a>】    【Pages】:1035-1044</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Duan:Huizhong">Huizhong Duan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kiciman:Emre">Emre Kiciman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
Understanding users' search intents is critical component of modern search engines. A key limitation made by most query log analyses is the assumption that each clicked web result represents one unique intent. However, there are many search tasks, such as comparison shopping or in-depth research, where a user's intent is to explore many documents. In these cases, the assumption of a one-to-one correspondence between clicked documents and user intent breaks down. To capture and understand such behaviors, we propose the use of click patterns. Click patterns capture the relationship among clicks on search results by treating the set of clicks made by a user as a single unit. We aggregate click patterns together using a hierarchical clustering algorithm to discover the common click patterns. By using click patterns as an empirical representation of user intent, we are able to create a rich representation of mixtures of multiple navigational and informational intents. We analyze real search logs and demonstrate that such complex mixtures of intents do occur in the wild and can be identified using click patterns. We further demonstrate the usefulness of click patterns by integrating them into a measure of query ambiguity and into a query recommendation task. We show that calculating query ambiguity as the entropy over the distribution of click patterns provides a measure of ambiguity with improved discriminative power, consistency and temporal stability as compared to previous measures of ambiguity. We explore the use of click pattern similarity and click pattern entropy in generating query recommendations and show promising results.</p>
<p>【Keywords】:
click pattern; click profile; entropy; query ambiguity</p>
<h3 id="107. Domain dependent query reformulation for web search.">107. Domain dependent query reformulation for web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398401">Paper Link</a>】    【Pages】:1045-1054</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dang:Van">Van Dang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kumaran:Giridhar">Giridhar Kumaran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Troy:Adam_D=">Adam D. Troy</a></p>
<p>【Abstract】:
Query reformulation has been studied as a domain independent task. Existing work attempts to expand a query or substitute its terms with the same set of candidates regardless of the domain of this query. Since terms might be semantically related in one domain but not in others, it is more effective to provide candidates for queries with respect to their domain. This paper demonstrates the advantage of this domain dependent query reformulation approach, which learns its candidates, using a standard technique, for each domain from a separate sample of data derived automatically from a generic query log. Our results show that our approach statistically significantly outperforms the domain independent approach, which learns to reformulate from the same log using the same technique, on a large query set consisting of both health and commerce queries. Our results have very practical interpretation: while building different reformulation systems to handle queries from different domains does not require additional manual effort, it provides substantially better retrieval effectiveness than having a single system handling all queries. Additionally, we show that leveraging domain specific manually labelled data leads to further improvement.</p>
<p>【Keywords】:
domain dependent; query log; query reformulation</p>
<h2 id="DB track: web data management    4">DB track: web data management    4</h2>
<h3 id="108. An automatic blocking mechanism for large-scale de-duplication tasks.">108. An automatic blocking mechanism for large-scale de-duplication tasks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398403">Paper Link</a>】    【Pages】:1055-1064</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sarma:Anish_Das">Anish Das Sarma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jain:Ankur">Ankur Jain</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Machanavajjhala:Ashwin">Ashwin Machanavajjhala</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bohannon:Philip">Philip Bohannon</a></p>
<p>【Abstract】:
De-duplication - identification of distinct records referring to the same real-world entity - is a well-known challenge in data integration. Since very large datasets prohibit the comparison of every pair of records, blocking has been identified as a technique of dividing the dataset for pairwise comparisons, thereby trading off recall of identified duplicates for efficiency. Traditional de-duplication tasks, while challenging, typically involved a fixed schema such as Census data or medical records. However, with the presence of large, diverse sets of structured data on the web and the need to organize it effectively on content portals, de-duplication systems need to scale in a new dimension to handle a large number of schemas, tasks and data sets, while handling ever larger problem sizes. In addition, when working in a map-reduce framework it is important that canopy formation be implemented as a hash function, making the canopy design problem more challenging. We present CBLOCK, a system that addresses these challenges. CBLOCK learns hash functions automatically from attribute domains and a labeled dataset consisting of duplicates. Subsequently, CBLOCK expresses blocking functions using a hierarchical tree structure composed of atomic hash functions. The application may guide the automated blocking process based on architectural constraints, such as by specifying a maximum size of each block (based on memory requirements), impose disjointness of blocks (in a grid environment), or specify a particular objective function trading off recall for efficiency. As a post-processing step to automatically generated blocks, CBLOCK rolls-up smaller blocks to increase recall. We present experimental results on two large-scale de-duplication datasets from a commercial search engine - consisting of over 140K movies and 40K restaurants respectively - and demonstrate the utility of CBLOCK.</p>
<p>【Keywords】:
blocking; canopy formation; de-duplication</p>
<h3 id="109. Processing continuous text queries featuring non-homogeneous scoring functions.">109. Processing continuous text queries featuring non-homogeneous scoring functions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398404">Paper Link</a>】    【Pages】:1065-1074</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vouzoukidou:Nelly">Nelly Vouzoukidou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amann:Bernd">Bernd Amann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Christophides:Vassilis">Vassilis Christophides</a></p>
<p>【Abstract】:
In this work we are interested in the scalable processing of content filtering queries over text item streams. In particular, we are aiming to generalize state of the art solutions with non-homogeneous scoring functions combining query-independent item importance with query-dependent content relevance. While such complex ranking functions are widely used in web search engines this is to our knowledge the first scientific work studying their usage in a continuous query scenario. Our main contribution consists in the definition and the evaluation of new efficient in-memory data structures for indexing continuous top-k queries based on an original two-dimensional representation of text queries. We are exploring locally-optimal score bounds and heuristics that efficiently prune the search space of candidate top-k query results which have to be updated at the arrival of new stream items. Finally, we experimentally evaluate memory/matching time trade-offs of these index structures. In particular we experimentally illustrate their linear scaling behavior with respect to the number of indexed queries.</p>
<p>【Keywords】:
continuous top-k query processing; non-homogeneous scoring functions; text streams</p>
<h3 id="110. Comprehension-based result snippets.">110. Comprehension-based result snippets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398405">Paper Link</a>】    【Pages】:1075-1084</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kashyap:Abhijith">Abhijith Kashyap</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hristidis:Vagelis">Vagelis Hristidis</a></p>
<p>【Abstract】:
Result snippets are used by most search interfaces to preview query results. Snippets help users quickly decide the relevance of the results, thereby reducing the overall search time and effort. Most work on snippets have focused on text snippets for Web pages in Web search. However, little work has studied the problem of snippets for structured data, e.g., product catalogs. Furthermore, all works have focused on the important goal of creating informative snippets, but have ignored the amount of user effort required to comprehend, i.e., read and digest, the displayed snippets. In particular, they implicitly assume that the comprehension effort or cost only depends on the length of the snippet, which we show is incorrect for structured data. We propose novel techniques to construct snippets of structured heterogeneous results, which not only select the most informative attributes for each result, but also minimize the expected user effort (time) to comprehend these snippets. We create a comprehension model to quantify the effort incurred by users in comprehending a list of result snippets. Our model is supported by an extensive user-study. A key observation is that the user effort for comprehending an attribute across multiple snippets only depends on the number of unique positions (e.g., indentations) where this attribute is displayed and not on the number of occurrences. We analyze the complexity of the snippet construction problem and show that the problem is NP-hard, even when we only consider the comprehension cost. We present efficient approximate algorithms, and experimentally demonstrate their effectiveness and efficiency.</p>
<p>【Keywords】:
information overload; query interfaces; result snippets</p>
<h3 id="111. An effective rule miner for instance matching in a web of data.">111. An effective rule miner for instance matching in a web of data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398406">Paper Link</a>】    【Pages】:1085-1094</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Niu:Xing">Xing Niu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rong:Shu">Shu Rong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Haofen">Haofen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:
Publishing structured data and linking them to Linking Open Data (LOD) is an ongoing effort to create a Web of data. Each newly involved data source may contain duplicated instances (entities) whose descriptions or schemata differ from those of the existing sources in LOD. To tackle this heterogeneity issue, several matching methods have been developed to link equivalent entities together. Many general-purpose matching methods which focus on similarity metrics suffer from very diverse matching results for different data source pairs. On the other hand, the dataset-specific ones leverage heuristic rules or even manual efforts to ensure the quality, which makes it impossible to apply them to other sources or domains. In this paper, we offer a third choice, a general method of automatically discovering dataset-specific matching rules. In particular, we propose a semi-supervised learning algorithm to iteratively refine matching rules and find new matches of high confidence based on these rules. This dramatically relieves the burden on users of defining rules but still gives high-quality matching results. We carry out experiments on real-world large scale data sources in LOD; the results show the effectiveness of our approach in terms of the precision of discovered matches and the number of missing matches found. Furthermore, we discuss several extensions (like similarity embedded rules, class restriction and SPARQL rewriting) to fit various applications with different requirements.</p>
<p>【Keywords】:
association rule mining; em algorithm; instance matching; semi-supervised learning</p>
<h2 id="KM track: information extraction    5">KM track: information extraction    5</h2>
<h3 id="112. Non-stationary bayesian networks based on perfect simulation.">112. Non-stationary bayesian networks based on perfect simulation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398408">Paper Link</a>】    【Pages】:1095-1104</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jia:Yi">Yi Jia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zeng:Wenrong">Wenrong Zeng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huan:Jun">Jun Huan</a></p>
<p>【Abstract】:
Non-stationary Dynamic Bayesian Networks (Non-stationary DBNs) are widely used to model the temporal changes of directed dependency structures from multivariate time series data. However, the existing change-points based non-stationary DBNs methods have several drawbacks including excessive computational cost, and low convergence speed. In this paper we proposed a novel non-stationary DBNs method. Our method is based on the perfect simulation model. We applied this approach for network structure inference from synthetic data and biological microarray gene expression data and compared it with other two state-of-the-art non-stationary DBNs methods. The experimental results demonstrated that our method outperformed two other state-of-the-art methods in both computational cost and structure prediction accuracy. The further sensitivity analysis showed that once converged our model is robust to large parameter ranges, which reduces the uncertainty of the model behavior.</p>
<p>【Keywords】:
dynamic bayesian networks; markov chain monte carlo; perfect simulation</p>
<h3 id="113. Active learning for relation type extension with local and global data views.">113. Active learning for relation type extension with local and global data views.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398409">Paper Link</a>】    【Pages】:1105-1112</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Ang">Ang Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grishman:Ralph">Ralph Grishman</a></p>
<p>【Abstract】:
Relation extraction is the process of identifying instances of specified types of semantic relations in text; relation type extension involves extending a relation extraction system to recognize a new type of relation. We present LGCo-Testing, an active learning system for relation type extension based on local and global views of relation instances. Locally, we extract features from the sentence that contains the instance. Globally, we measure the distributional similarity between instances from a 2 billion token corpus. Evaluation on the ACE 2004 corpus shows that LGCo-Testing can reduce annotation cost by 97% while maintaining the performance level of supervised learning.</p>
<p>【Keywords】:
co-testing; co-training; distributional similarity; infactive learning; information extraction; inrelation extraction</p>
<h3 id="114. Segmenting web-domains and hashtags using length specific models.">114. Segmenting web-domains and hashtags using length specific models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398410">Paper Link</a>】    【Pages】:1113-1122</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Srinivasan:Sriram">Sriram Srinivasan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhattacharya:Sourangshu">Sourangshu Bhattacharya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chakraborty:Rudrasis">Rudrasis Chakraborty</a></p>
<p>【Abstract】:
Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends.</p>
<p>【Keywords】:
compound splitting; hashtag segmentation; structured learning; web domain segmentation; word segmentation</p>
<h3 id="115. Crosslingual distant supervision for extracting relations of different complexity.">115. Crosslingual distant supervision for extracting relations of different complexity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398411">Paper Link</a>】    【Pages】:1123-1132</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blessing:Andr=eacute=">André Blessing</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
We propose crosslingual distant supervision (crosslingual DS) for relation extraction, an approach that automatically extracts labels from a pivot language for labeling one or more target languages. The approach has two benefits compared to standard DS: (i) increased coverage if target language labels are not available; and (ii) higher accuracy of automatically generated labels because noisy labels are eliminated in crosslingual filtering. An evaluation for two relations of different complexity shows that crosslingual DS increases the accuracy of relation extraction. Our approach is language independent; we successfully apply it to four different languages: Chinese, English, French and German.</p>
<p>【Keywords】:
crosslingual distant supervision; relation extraction</p>
<h3 id="116. Labeling by landscaping: classifying tokens in context by pruning and decorating trees.">116. Labeling by landscaping: classifying tokens in context by pruning and decorating trees.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398412">Paper Link</a>】    【Pages】:1133-1142</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Patwardhan:Siddharth">Siddharth Patwardhan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boguraev:Branimir">Branimir Boguraev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Apoorv">Apoorv Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moschitti:Alessandro">Alessandro Moschitti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chu=Carroll:Jennifer">Jennifer Chu-Carroll</a></p>
<p>【Abstract】:
State-of-the-art approaches to token labeling within text documents typically cast the problem either as a classification task, without using complex structural characteristics of the input, or as a sequential labeling task, carried out by a Conditional Random Field (CRF) classifier. Here we explore principled ways for structure to be brought to bear on the task. In line with recent trends in statistical learning of structured natural language input, we use a Support Vector Machine (SVM) classification framework deploying tree kernels. We then propose tree transformations and decorations, as a methodology for modeling complex linguistic phenomena in highly multi-dimensional feature spaces. We develop a general purpose tree engineering framework, which enables us to transcend the typically complex and laborious process of feature engineering. We build kernel based classifiers for two token labeling tasks: fine-grained event recognition, and lexical answer type detection in questions. For both, we show that in comparison with a corresponding linear kernel SVM, our method of using tree kernels improves recognition, thanks to appropriately engineering tree structures for use by the tree kernel. We also observe significant improvements when comparing with a CRF-based realization of structured prediction, itself performing at levels comparable to state-of-the-art.</p>
<p>【Keywords】:
support vector machines; token classification; tree kernels</p>
<h2 id="IR track: topic modeling and content and sentiment analysis    4">IR track: topic modeling and content and sentiment analysis    4</h2>
<h3 id="117. G-WSTD: a framework for geographic web search topic discovery.">117. G-WSTD: a framework for geographic web search topic discovery.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398414">Paper Link</a>】    【Pages】:1143-1152</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Di">Di Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vosecky:Jan">Jan Vosecky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leung:Kenneth_Wai=Ting">Kenneth Wai-Ting Leung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Wilfred">Wilfred Ng</a></p>
<p>【Abstract】:
Search engine query log is an important information source that contains millions of users' interests and information needs. In this paper, we tackle the problem of discovering latent geographic search topics via mining search engine query logs. A novel framework G-WSTD that contains search session derivation, geographic information extraction and geographic search topic discovery is developed to support a variety of downstream web applications. The core components of the framework are two topic models, which discover geographic search topics from two different perspectives. The first one is the Discrete Search Topic Model (DSTM), which aims to capture the semantic commonalities across discrete geographic locations. The second one is the Regional Search Topic Model (RSTM), which focuses on a specific region on the map and discovers web search topics that demonstrate geographic locality. We evaluate our framework against several strong baselines on a real-life query log. The framework demonstrates improved data interpretability, better prediction performance and higher topic distinctiveness in the experimentation. The effectiveness of the framework is also verified by applications such as user profiling and URL annotation.</p>
<p>【Keywords】:
geographic; query log; search engine</p>
<h3 id="118. Supporting factual statements with evidence from the web.">118. Supporting factual statements with evidence from the web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398415">Paper Link</a>】    【Pages】:1153-1162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Leong:Chee_Wee">Chee Wee Leong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cucerzan:Silviu">Silviu Cucerzan</a></p>
<p>【Abstract】:
Fact verification has become an important task due to the increased popularity of blogs, discussion groups, and social sites, as well as of encyclopedic collections that aggregate content from many contributors. We investigate the task of automatically retrieving supporting evidence from the Web for factual statements. Using Wikipedia as a starting point, we derive a large corpus of statements paired with supporting Web documents, which we employ further as training and test data under the assumption that the contributed references to Wikipedia represent some of the most relevant Web documents for supporting the corresponding statements. Given a factual statement, the proposed system first transforms it into a set of semantic terms by using machine learning techniques. It then employs a quasi-random strategy for selecting subsets of the semantic terms according to topical likelihood. These semantic terms are used to construct queries for retrieving Web documents via a Web search API. Finally, the retrieved documents are aggregated and re-ranked by employing additional measures of their suitability to support the factual statement. To gauge the quality of the retrieved evidence, we conduct a user study through Amazon Mechanical Turk, which shows that our system is capable of retrieving supporting Web documents comparable to those chosen by Wikipedia contributors.</p>
<p>【Keywords】:
fact verification; semantic term extraction; supporting evidence; web references; web search; wikipedia</p>
<h3 id="119. Role-explicit query identification and intent role annotation.">119. Role-explicit query identification and intent role annotation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398416">Paper Link</a>】    【Pages】:1163-1172</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Haitao">Haitao Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ren:Fuji">Fuji Ren</a></p>
<p>【Abstract】:
Understanding the information need or intent encoded within a query has long been regarded as an essential factor of effective information retrieval. For better query representation and understanding, two intent roles (kernel-object and modifier) are introduced to structurally parse a class of role-explicit queries, which constitute a majority of common user queries. Furthermore, we focus on two research problems: RP-1: Given a role-explicit query, how to identify the kernel-object and modifier, namely intent role annotation; RP-2: How to determine whether an arbitrary query is role-explicit or not. To solve RP-1, we propose a simplified word n-gram role model (SWNR), which quantifies the generating probability of a role-explicit query and performs intent role annotation effectively. Using a set of discriminative features, we build classifiers to address RP-2 in a supervised manner. The experimental results show that: (1) SWNR can achieve a satisfactory performance, more than 73% in terms of different metrics; (2) The classifiers can achieve more than 90% precision in identifying role-explicit queries; (3) Compared with traditional techniques for query representation and understanding, e.g., name entity recognition in query and class-level query intent inference, intent role annotation provides a more flexible framework and a number of applications can benefit from annotating role-explicit queries, such as intent mining and diversified document ranking.</p>
<p>【Keywords】:
kernel-object; modifier; query understanding; role-explicit</p>
<h3 id="120. Joint topic modeling for event summarization across news and social media streams.">120. Joint topic modeling for event summarization across news and social media streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398417">Paper Link</a>】    【Pages】:1173-1182</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Wei">Wei Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Peng">Peng Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Darwish:Kareem">Kareem Darwish</a></p>
<p>【Abstract】:
Social media streams such as Twitter are regarded as faster first-hand sources of information generated by massive users. The content diffused through this channel, although noisy, provides important complement and sometimes even a substitute to the traditional news media reporting. In this paper, we propose a novel unsupervised approach based on topic modeling to summarize trending subjects by jointly discovering the representative and complementary information from news and tweets. Our method captures the content that enriches the subject matter by reinforcing the identification of complementary sentence-tweet pairs. To valuate the complementarity of a pair, we leverage topic modeling formalism by combining a two-dimensional topic-aspect model and a cross-collection approach in the multi-document summarization literature. The final summaries are generated by co-ranking the news sentences and tweets in both sides simultaneously. Experiments give promising results as compared to state-of-the-art baselines.</p>
<p>【Keywords】:
complementary summary; cross-collection topic-aspect model; gibbs sampling; lda</p>
<h2 id="DB track: query processing, optimization and performance    5">DB track: query processing, optimization and performance    5</h2>
<h3 id="121. CGStream: continuous correlated graph query for data streams.">121. CGStream: continuous correlated graph query for data streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398419">Paper Link</a>】    【Pages】:1183-1192</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pan:Shirui">Shirui Pan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xingquan">Xingquan Zhu</a></p>
<p>【Abstract】:
In this paper, we propose to query correlated graph in a data stream scenario, where given a query graph q an algorithm is required to retrieve all the subgraphs whose Pearson's correlation coefficients with q are greater than a threshold Θ over some graph data flowing in a stream fashion. Due to the dynamic changing nature of the stream data and the inherent complexity of the graph query process, treating graph streams as static datasets is computationally infeasible or ineffective. In the paper, we propose a novel algorithm, CGStream, to identify correlated graphs from data stream, by using a sliding window which covers a number of consecutive batches of stream data records. Our theme is to regard stream query as the traversing along a data stream and the query is achieved at a number of outlooks over the data stream. For each outlook, we derive a lower frequency bound to mine a set of frequent subgraph candidates, where the lower bound guarantees that no pattern is missing from the current outlook to the next outlook. On top of that, we derive an upper correlation bound and a heuristic rule to prune the candidate size, which helps reduce the computation cost at each outlook. Experimental results demonstrate that the proposed algorithm is several times, or even an order of magnitude, more efficient than the straightforward algorithm. Meanwhile, our algorithm achieves good performance in terms of query precision.</p>
<p>【Keywords】:
correlated graph; data stream; pearson's correlation coefficient</p>
<h3 id="122. Efficient influence-based processing of market research queries.">122. Efficient influence-based processing of market research queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398420">Paper Link</a>】    【Pages】:1193-1202</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Arvanitis:Anastasios">Anastasios Arvanitis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Deligiannakis:Antonios">Antonios Deligiannakis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vassiliou:Yannis">Yannis Vassiliou</a></p>
<p>【Abstract】:
The rapid growth of social web has contributed vast amounts of user preference data. Analyzing this data and its relationships with products could have several practical applications, such as personalized advertising, market segmentation, product feature promotion etc. In this work we develop novel algorithms for efficiently processing two important classes of queries involving user preferences, i.e. potential customers identification and product positioning. With regards to the first problem, we formulate product attractiveness based on the notion of reverse skyline queries. We then present a new algorithm, termed as RSA, that significantly reduces the I/O cost, as well as the computation cost, when compared to the state-of-the-art reverse skyline algorithm, while at the same time being able to quickly report the first results. Several real-world applications require processing of a large number of queries, in order to identify the product characteristics that maximize the number of potential customers. Motivated by this problem, we also develop a batched extension of our RSA algorithm that significantly improves upon processing multiple queries individually, by grouping contiguous candidates, exploiting I/O commonalities and enabling shared processing. Our experimental study using both real and synthetic data sets demonstrates the superiority of our proposed algorithms for the studied classes of queries.</p>
<p>【Keywords】:
market research; preferences; reverse skylines</p>
<h3 id="123. Deco: declarative crowdsourcing.">123. Deco: declarative crowdsourcing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398421">Paper Link</a>】    【Pages】:1203-1212</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Parameswaran:Aditya_G=">Aditya G. Parameswaran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Hyunjung">Hyunjung Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Garcia=Molina:Hector">Hector Garcia-Molina</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Polyzotis:Neoklis">Neoklis Polyzotis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Widom:Jennifer">Jennifer Widom</a></p>
<p>【Abstract】:
Crowdsourcing enables programmers to incorporate "human computation" as a building block in algorithms that cannot be fully automated, such as text analysis and image recognition. Similarly, humans can be used as a building block in data-intensive applications--providing, comparing, and verifying data used by applications. Building upon the decades-long success of declarative approaches to conventional data management, we use a similar approach for data-intensive applications that incorporate humans. Specifically, declarative queries are posed over stored relational data as well as data computed on-demand from the crowd, and the underlying system orchestrates the computation of query answers. We present Deco, a database system for declarative crowdsourcing. We describe Deco's data model, query language, and our prototype. Deco's data model was designed to be general (it can be instantiated to other proposed models), flexible (it allows methods for data cleansing and external access to be plugged in), and principled (it has a precisely-defined semantics). Syntactically, Deco's query language is a simple extension to SQL. Based on Deco's data model, we define a precise semantics for arbitrary queries involving both stored data and data obtained from the crowd. We then describe the Deco query processor which uses a novel push-pull hybrid execution model to respect the Deco semantics while coping with the unique combination of latency, monetary cost, and uncertainty introduced in the crowdsourcing environment. Finally, we experimentally explore the query processing alternatives provided by Deco using our current prototype.</p>
<p>【Keywords】:
declarative crowdsourcing; human computation</p>
<h3 id="124. Predicting the effectiveness of keyword queries on databases.">124. Predicting the effectiveness of keyword queries on databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398422">Paper Link</a>】    【Pages】:1213-1222</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Shiwen">Shiwen Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Termehchy:Arash">Arash Termehchy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hristidis:Vagelis">Vagelis Hristidis</a></p>
<p>【Abstract】:
Keyword query interfaces (KQIs) for databases provide easy access to data, but often suffer from low ranking quality, i.e. low precision and/or recall, as shown in recent benchmarks. It would be useful to be able to identify queries that are likely to have low ranking quality to improve the user satisfaction. For instance, the system may suggest to the user alternative queries for such hard queries. In this paper, we analyze the characteristics of hard queries and propose a novel framework to measure the degree of difficulty for a keyword query over a database, considering both the structure and the content of the database and the query results. We evaluate our query difficulty prediction model against two relevance judgment benchmarks for keyword search on databases, INEX and SemSearch. Our study shows that our model predicts the hard queries with high accuracy. Further, our prediction algorithms incur minimal time overhead.</p>
<p>【Keywords】:
(semi-)structured data; database; keyword query; query performance</p>
<h3 id="125. You can stop early with COLA: online processing of aggregate queries in the cloud.">125. You can stop early with COLA: online processing of aggregate queries in the cloud.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398423">Paper Link</a>】    【Pages】:1223-1232</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shi:Yingjie">Yingjie Shi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meng:Xiaofeng">Xiaofeng Meng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Fusheng">Fusheng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gan:Yantao">Yantao Gan</a></p>
<p>【Abstract】:
Cloud-based data management systems are emerging as scalable, fault-tolerant, and efficient solutions to manage large volumes of data with cost effective infrastructures, and more and more data analysis applications are migrated to the cloud. As an attractive solution to provide a quick sketch of massive data before a long wait of the final accurate query result, online processing of aggregate queries in the cloud is of paramount importance. This problem is challenging to solve because of the large block based data organization and distributed processing mode in the cloud. In this paper, we present COLA, a system for Cloud Online Aggregation to provide progressive approximate answers for both single tables and joined multiple tables. We develop an online query processing algorithm for MapReduce to support incremental and continuous computing of aggregations on joins which minimizes the waiting time before an acceptable estimate is achieved. We formulate a statistical foundation that supports block-level sampling for single-table online aggregations and effective estimation of approximate results and confidence intervals of statistical significance. We also develop a two-phase stratified sampling method to support multi-table aggregations to improve the approximate query answers and speed up the convergence of confidence intervals. We implement COLA in Hadoop, and our experiments demonstrate that COLA can deliver reasonable precise online estimates within a time period two orders of magnitude shorter than that used to produce exact answers.</p>
<p>【Keywords】:
cloud computing; mapreduce; online aggregation</p>
<h2 id="KM track: classification and semantic methods    5">KM track: classification and semantic methods    5</h2>
<h3 id="126. A novel local patch framework for fixing supervised learning models.">126. A novel local patch framework for fixing supervised learning models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398425">Paper Link</a>】    【Pages】:1233-1242</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yilei">Yilei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Bingzheng">Bingzheng Wei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Jun">Jun Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Yang">Yang Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Zhi=Hong">Zhi-Hong Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a></p>
<p>【Abstract】:
In the past decades, machine learning models, especially supervised learning algorithms, have been widely used in various real world applications. However, no matter how strong a learning model is, it will suffer from the prediction errors when it is applied to real world problems. Due to the black box nature of supervised learning models, it is a challenging problem to fix the supervised learning models by further learning from the failure cases it generates. In this paper, we propose a novel Local Patch Framework (LPF) to locally fix supervised learning models by learning from its predicted failure cases. Since the learning models are generally globally optimized during training process, our proposed LPF assumes that most of the learning errors are led by local errors in the model. Thus we aim to break the black boxes of learning models by identifying and fixing the local errors of various models automatically. The proposed LPF has two key steps, which are local error region subspace learning and local patch model learning. Through this way, we aim to fix the errors of learning models locally and automatically with certain generalization ability on unseen testing data. Experiments on both classification and ranking problems show that the proposed LPF is effective and outperforms the original algorithms and the incremental learning model.</p>
<p>【Keywords】:
local patch; metric learning; model fixing; supervised learning model</p>
<h3 id="127. Automated feature weighting in naive bayes for high-dimensional data classification.">127. Automated feature weighting in naive bayes for high-dimensional data classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398426">Paper Link</a>】    【Pages】:1243-1252</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Lifei">Lifei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shengrui">Shengrui Wang</a></p>
<p>【Abstract】:
Naive Bayes (NB for short) is one of the popular methods for supervised classification in a knowledge management system. Currently, in many real-world applications, high-dimensional data pose a major challenge to conventional NB classifiers, due to noisy or redundant features and local relevance of these features to classes. In this paper, an automated feature weighting solution is proposed to result in a NB method effective in dealing with high-dimensional data. We first propose a locally weighted probability model, for Bayesian modeling in high-dimensional spaces, to implement a soft feature selection scheme. Then we propose an optimization algorithm to find the weights in linear time complexity, based on the Logitnormal priori distribution and the Maximum a Posteriori principle. Experimental studies show the effectiveness and suitability of the proposed model for high-dimensional data classification.</p>
<p>【Keywords】:
classification; high-dimensionality; maximum a posteriori; naive bayes; soft feature weighting</p>
<h3 id="128. Learning to discover complex mappings from web forms to ontologies.">128. Learning to discover complex mappings from web forms to ontologies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398427">Paper Link</a>】    【Pages】:1253-1262</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/An:Yuan">Yuan An</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xiaohua">Xiaohua Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Il=Yeol">Il-Yeol Song</a></p>
<p>【Abstract】:
In order to realize the Semantic Web, various structures on the Web including Web forms need to be annotated with and mapped to domain ontologies. We present a machine learning-based automatic approach for discovering complex mappings from Web forms to ontologies. A complex mapping associates a set of semantically related elements on a form to a set of semantically related elements in an ontology. Existing schema mapping solutions mainly rely on integrity constraints to infer complex schema mappings. However, it is difficult to extract rich integrity constraints from forms. We show how machine learning techniques can be used to automatically discover complex mappings between Web forms and ontologies. The challenge is how to capture and learn the complicated knowledge encoded in existing complex mappings. We develop an initial solution that takes a naive Bayesian approach. We evaluated the performance of the solution on various domains. Our experimental results show that the solution returns the expected mappings as the top-1 results usually among several hundreds candidate mappings for more than 80% of the test cases. Furthermore, the expected mappings are always returned as the top-k results with k&lt;4. The experiments have demonstrated that the approach is effective and has the potential to save significant human efforts.</p>
<p>【Keywords】:
mapping discovery; ontologies; semantic mapping; web forms</p>
<h3 id="129. Modeling semantic relations between visual attributes and object categories via dirichlet forest prior.">129. Modeling semantic relations between visual attributes and object categories via dirichlet forest prior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398428">Paper Link</a>】    【Pages】:1263-1272</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Xin">Xin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xiaohua">Xiaohua Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Zhongna">Zhongna Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/An:Yuan">Yuan An</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Tingting">Tingting He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:E=_K=">E. K. Park</a></p>
<p>【Abstract】:
In this paper, we deal with two research issues: the automation of visual attribute identification and semantic relation learning between visual attributes and object categories. The contribution is two-fold, firstly, we provide uniform framework to reliably extract both categorical attributes and depictive attributes. Secondly, we incorporate the obtained semantic associations between visual attributes and object categories into a text-based topic model and extract descriptive latent topics from external textual knowledge sources. Specifically, we show that in mining natural language descriptions from external knowledge sources, the relation between semantic visual attributes and object categories can be encoded as Must-Links and Cannot-Links, which can be represented by Dirichlet-Forest prior. To alleviate the workload of manual supervision and labeling in image categorization process, we introduce a semi-supervised training framework using soft-margin semi-supervised SVM classifier. We also show that the large-scale image categorization results can be significantly improved by combining automatically acquired visual attributes. Experimental results show that the proposed model achieves better ability in describing object-related attributes and makes the inferred latent topics more descriptive.</p>
<p>【Keywords】:
dirichlet-forest prior; topic model; visual attribute identification</p>
<h3 id="130. CoNet: feature generation for multi-view semi-supervised learning with partially observed views.">130. CoNet: feature generation for multi-view semi-supervised learning with partially observed views.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398429">Paper Link</a>】    【Pages】:1273-1282</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Quanz:Brian">Brian Quanz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huan:Jun">Jun Huan</a></p>
<p>【Abstract】:
Multi-view semi-supervised learning methods try to exploit the combination of multiple views along with large amounts of unlabeled data in order to learn better predictive functions when limited labeled data is available. However, lack of complete view data limits the applicability of multi-view semi-supervised learning to real world data. Commonly, one data view is readily and cheaply available, but additionally views may be costly or only available in some cases. This work aims to make multi-view semi-supervised learning approaches more applicable to real world data specifically by addressing the issue of missing views. We introduce CoNet, a feature generation method that learns a mapping from one view to another that is specifically designed to produce features that are useful for multi-view semi-supervised learning algorithms. The mapping is then used to fill in views as pre-processing. Our comprehensive experimental study demonstrates the utility of our method as compared to the state-of-the-art multi-view semi-supervised learning methods for this scenario of partially observed views.</p>
<p>【Keywords】:
missing data; multi-view learning; semi-supervised learning</p>
<h2 id="IR track: multimedia and user feedback    5">IR track: multimedia and user feedback    5</h2>
<h3 id="131. Generating facets for phone-based navigation of structured data.">131. Generating facets for phone-based navigation of structured data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398431">Paper Link</a>】    【Pages】:1283-1292</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kummamuru:Krishna">Krishna Kummamuru</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jujjuru:Ajith">Ajith Jujjuru</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Duggirala:Mayuri">Mayuri Duggirala</a></p>
<p>【Abstract】:
Designing interactive voice systems that have optimum cognitive load on callers has been an active research topic for quite some time. There have been many studies comparing the user preferences on navigation trees with higher depths over higher breadths. In this paper, we consider the navigation of structured data containing various types of attributes using phone-based interactions. This problem is particularly relevant to emerging economies in which innovative voice-based applications are being built to address semi-literate population. We address the problem of identifying the right sequence of facets to be presented to the user for phone-based navigation of the data in two stages. Firstly, we perform extensive user studies in the target population to understand the relation between the nature of facets (attributes) of the data and the cognitive load. Secondly, we propose an algorithm to design optimum navigation trees based on the inferences made in the first phase. We compare the proposed algorithm with the traditional facet generation algorithms with respect to various factors and discuss the optimality of the proposed algorithm.</p>
<p>【Keywords】:
cognitive load; navigation of structured data; phone-based interaction</p>
<h3 id="132. The effect of aggregated search coherence on search behavior.">132. The effect of aggregated search coherence on search behavior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398432">Paper Link</a>】    【Pages】:1293-1302</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Arguello:Jaime">Jaime Arguello</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Capra:Robert">Robert Capra</a></p>
<p>【Abstract】:
Aggregated search is the task of blending results from different specialized search services, or verticals, into the web search results. Aggregated search coherence refers to the degree to which results from different systems focus on similar senses of the query. While cross-component coherence has been cited as an important criterion for whole-page evaluation, its effect on search behavior has not been deeply investigated in prior research. In this work, we focus on the coherence between two aggregated search components: images and web results. In particular, we investigate whether the query-senses associated with the blended image results can influence user interaction with the web results. For example, if a user wants web results about "jaguar" the animal, are they more likely to examine the web results if the image results contain pictures of the animal instead of pictures of the car? Based on two large user studies, our results show that the image results can systematically affect user interaction with the web results. If the web results are largely consistent with the search task, then the effect of the image results is small. However, if the web results are only marginally consistent with the search task, such as when they are highly diversified across query-senses, the image results have a significant effect on user interaction with the web results. Our findings have implications on current research in whole-page evaluation, aggregated search, and diversity ranking.</p>
<p>【Keywords】:
aggregated search; aggregated search coherence; assimilation effects; evaluation; search behavior; user study</p>
<h3 id="133. Improving bag-of-visual-words model with spatial-temporal correlation for video retrieval.">133. Improving bag-of-visual-words model with spatial-temporal correlation for video retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398433">Paper Link</a>】    【Pages】:1303-1312</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Lei">Lei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Dawei">Dawei Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elyan:Eyad">Eyad Elyan</a></p>
<p>【Abstract】:
Most of the state-of-art approaches to Query-by-Example (QBE) video retrieval are based on the Bag-of-visual-Words (BovW) representation of visual content. It, however, ignores the spatial-temporal information, which is important for similarity measurement between videos. Direct incorporation of such information into the video data representation for a large scale data set is computationally expensive in terms of storage and similarity measurement. It is also static regardless of the change of discriminative power of visual words for different queries. To tackle these limitations, in this paper, we propose to discover Spatial-Temporal Correlations (STC) imposed by the query example to improve the BovW model for video retrieval. The STC, in terms of spatial proximity and relative motion coherence between different visual words, is crucial to identify the discriminative power of the visual words. We develop a novel technique to emphasize the most discriminative visual words for similarity measurement, and incorporate this STC-based approach into the standard inverted index architecture. Our approach is evaluated on the TRECVID2002 and CC_WEB_VIDEO datasets for two typical QBE video retrieval tasks respectively. The experimental results demonstrate that it substantially improves the BovW model as well as a state of the art method that also utilizes spatial-temporal information for QBE video retrieval.</p>
<p>【Keywords】:
bag-of-visual-word; content based video retrieval; discriminative visual word; query-by-example; spatial-temporal correlation</p>
<h3 id="134. Exploring and predicting search task difficulty.">134. Exploring and predicting search task difficulty.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398434">Paper Link</a>】    【Pages】:1313-1322</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Jingjing">Jingjing Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Chang">Chang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cole:Michael_J=">Michael J. Cole</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Belkin:Nicholas_J=">Nicholas J. Belkin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiangmin">Xiangmin Zhang</a></p>
<p>【Abstract】:
We report on an investigation of behavioral differences between users in difficult and easy search tasks. Behavioral factors that can be used in real-time to predict task difficulty are identified. User data was collected in a controlled lab experiment (n=38) where each participant completed four search tasks in the genomics domain. We looked at user behaviors that can be obtained by systems at three levels, distinguished by the time point when the measurements can be done. They are: 1) first-round level at the beginning of the search, 2) accumulated level during the search, and 3) whole-session level by the end of the search. Results show that a number of user behaviors at all three levels differed between easy and difficult tasks. Models predicting task difficulty at all three levels were developed and evaluated. A real-time model incorporating first-round and accumulated levels of behaviors (FA) had fairly good prediction performance (accuracy 83%; precision 88%), which is comparable with the model using the whole-session level behaviors which are not real-time (accuracy 75%; precision 92%). We also found that for efficiency purpose, using only a limited number of significant variables (FC_FA) can obtain a prediction accuracy of 75%, with a precision of 88%. Our findings can help search systems predict task difficulty and adapt search results to users.</p>
<p>【Keywords】:
accumulated level; difficulty prediction; first-round level; task difficulty; user behavior; user modeling; whole-session level</p>
<h3 id="135. Iterative relevance feedback with adaptive exploration/exploitation trade-off.">135. Iterative relevance feedback with adaptive exploration/exploitation trade-off.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398435">Paper Link</a>】    【Pages】:1323-1331</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Suditu:Nicolae">Nicolae Suditu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fleuret:Fran=ccedil=ois">François Fleuret</a></p>
<p>【Abstract】:
Content-based image retrieval systems have to cope with two different regimes: understanding broadly the categories of interest to the user, and refining the search in this or these categories to converge to specific images among them. Here, in contrast with other types of retrieval systems, these two regimes are of great importance since the search initialization is hardly optimal (i.e. the page-zero problem) and the relevance feedback must tolerate the semantic gap of the image's visual features. We present a new approach that encompasses these two regimes, and infers from the user actions a seamless transition between them. Starting from a query-free approach meant to solve the page-zero problem, we propose an adaptive exploration/exploitation trade-off that transforms the original framework into a versatile retrieval framework with full searching capabilities. Our approach is compared to the state-of-the-art it extends by conducting user evaluations on a collection of 60,000 images from the ImageNet database.</p>
<p>【Keywords】:
bayesian framework; iterative relevance feedback; query-free interactive image retrieval; user-based evaluation</p>
<h2 id="DB track: emerging and advanced topics    5">DB track: emerging and advanced topics    5</h2>
<h3 id="136. A practical concurrent index for solid-state drives.">136. A practical concurrent index for solid-state drives.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398437">Paper Link</a>】    【Pages】:1332-1341</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Thonangi:Risi">Risi Thonangi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Babu:Shivnath">Shivnath Babu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Jun">Jun Yang</a></p>
<p>【Abstract】:
Solid-state drives are becoming a viable alternative to magnetic disks in database systems, but their performance characteristics, particularly those caused by their erase-before-write behavior, make conventional database indexes a poor fit. There have been various proposals of indexes specialized for these devices, but to make such indexes practical, we must address the issue of concurrency control. Good concurrency control is especially critical to indexes on solid-state drives, because they typically rely on batch updates, which may take long and block concurrent index accesses. We design, implement, and evaluate an index structure called FD+tree and an associated concurrency control scheme called FD+FC. Our evaluation confirms significant performance advantages of our approach over less sophisticated ones, and brings ou insights on data structure design and OLTP performance tuning on solid-state drives.</p>
<p>【Keywords】:
concurrency control; performance evaluation; ssd indexes</p>
<h3 id="137. Robust distributed indexing for locality-skewed workloads.">137. Robust distributed indexing for locality-skewed workloads.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398438">Paper Link</a>】    【Pages】:1342-1351</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Mu=Woong">Mu-Woong Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hwang:Seung=won">Seung-won Hwang</a></p>
<p>【Abstract】:
Multidimensional indexing is crucial for enabling a fast search over large-scale data. Owing to the unprecedented scale of data, extending such indexing technology has recently gained attention in distributed environments. The goal of existing efforts in distributed indexing has been the localization of queries to data residing at a small number of nodes (i.e., locality-preserving indexing) to minimize communication cost. However, considering that workloads often correlate with data locality, such indexing often generates hotspots. Location-based queries are typically skewed to disaster areas during certain periods of time, e.g., during Hurricane Irene, search traffic increased by more than 2000%. To alleviate such hotspots, we propose workload-balancing as an optimization goal. A cost model analytically supporting the need for load balancing is first developed, then a distributed index that evenly distributes the workload is presented. Our empirical study suggests that hotspots degrading search performance can be effectively alleviated. Specifically, when deployed to Amazon EC2, our proposed scheme showed maximum speed-up of 127.7%. Even in hostile settings where workload is not at all correlated with the search criteria, the proposed scheme's performance is comparable to existing approaches optimized for such settings.</p>
<p>【Keywords】:
distributed indxing</p>
<h3 id="138. Efficient provenance storage for relational queries.">138. Efficient provenance storage for relational queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398439">Paper Link</a>】    【Pages】:1352-1361</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bao:Zhifeng">Zhifeng Bao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/K=ouml=hler:Henning">Henning Köhler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Liwei">Liwei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Xiaofang">Xiaofang Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sadiq:Shazia_Wasim">Shazia Wasim Sadiq</a></p>
<p>【Abstract】:
Provenance information is vital in many application areas as it helps explain data lineage and derivation. However, storing fine-grained provenance information can be expensive. In this paper, we present a framework for storing provenance information relating to data derived via database queries. In particular, we first propose a provenance tree data structure which matches the query structure and thereby presents a possibility to avoid redundant storage of information regarding the derivation process. Then we investigate two approaches for reducing storage costs. The first approach utilizes two ingenious rules to achieve reduction on provenance trees. The second one is a dynamic programming solution, which provides a way of optimizing the selection of query tree nodes where provenance information should be stored. The optimization algorithm runs in polynomial time in the query size and is linear in the size of the provenance information, thus enabling provenance tracking and optimization without incurring large overheads. Experiments show that our approaches guarantee significantly lower storage costs than existing approaches.</p>
<p>【Keywords】:
provenance storage</p>
<h3 id="139. Generically extending anonymization algorithms to deal with successive queries.">139. Generically extending anonymization algorithms to deal with successive queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398440">Paper Link</a>】    【Pages】:1362-1371</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Barbosa:Manuel">Manuel Barbosa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pinto:Alexandre">Alexandre Pinto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gomes:Bruno">Bruno Gomes</a></p>
<p>【Abstract】:
This paper addresses the scenario of multi-release anonymization of datasets. We consider dynamic datasets where data can be inserted and deleted, and view this scenario as a case where each release is a small subset of the dataset corresponding, for example, to the results of a query. Compared to multiple releases of the full database, this has the obvious advantage of faster anonymization. We present an algorithm for post-processing anonymized queries that prevents anonymity attacks using multiple released queries. This algorithm can be used with several distinct protection principles and anonymization algorithms, which makes it generic and flexible. We give an experimental evaluation of the algorithm and compare it to $m$-invariance both in terms of efficiency and data quality. To this end, we propose two data quality metrics based on Shannon's entropy, and show that they can be seen as a refinement of existing metrics.</p>
<p>【Keywords】:
anonymization; dynamic datasets; multiple-releases</p>
<h3 id="140. Authentication of moving range queries.">140. Authentication of moving range queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398441">Paper Link</a>】    【Pages】:1372-1381</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yung:Duncan">Duncan Yung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lo:Eric">Eric Lo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yiu:Man_Lung">Man Lung Yiu</a></p>
<p>【Abstract】:
A moving range query continuously reports the query result (e.g., restaurants) that are within radius $r$ from a moving query point (e.g., moving tourist). To minimize the communication cost with the mobile clients, a service provider that evaluates moving range queries also returns a safe region that bounds the validity of query results. However, an untrustworthy service provider may report incorrect safe regions to mobile clients. In this paper, we present efficient techniques for authenticating the safe regions of moving range queries. We theoretically proved that our methods for authenticating moving range queries can minimize the data sent between the service provider and the mobile clients. Extensive experiments are carried out using both real and synthetic datasets and results show that our methods incur small communication costs and overhead.</p>
<p>【Keywords】:
authentication; moving queries</p>
<h2 id="KM track: novel applications    4">KM track: novel applications    4</h2>
<h3 id="141. Model the complex dependence structures of financial variables by using canonical vine.">141. Model the complex dependence structures of financial variables by using canonical vine.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398443">Paper Link</a>】    【Pages】:1382-1391</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Wei">Wei Wei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Xuhui">Xuhui Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jinyan">Jinyan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Longbing">Longbing Cao</a></p>
<p>【Abstract】:
Financial variables such as asset returns in the massive market contain various hierarchical and horizontal relationships forming complicated dependence structures. Modeling and mining of these structures is challenging due to their own high structural complexities as well as the stylized facts of the market data. This paper introduces a new canonical vine dependence model to identify the asymmetric and non-linear dependence structures of asset returns without any prior independence assumptions. To simplify the model while maintaining its merit, a partial correlation based method is proposed to optimize the canonical vine. Compared with the original canonical vine, the new model can still maintain the most important dependence but many unimportant nodes are removed to simplify the canonical vine structure. Our model is applied to construct and analyze dependence structures of European stocks as case studies. Its performance is evaluated by measuring portfolio of Value at Risk, a widely used risk management measure. In comparison to a very recent canonical vine model and the 'full' model, our experimental results demonstrate that our model has a much better quality of Value at Risk, providing insightful knowledge for investors to control and reduce the aggregation risk of the portfolio.</p>
<p>【Keywords】:
canonical vine; dependence structure; financial variables</p>
<h3 id="142. A unified learning framework for auto face annotation by mining web facial images.">142. A unified learning framework for auto face annotation by mining web facial images.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398444">Paper Link</a>】    【Pages】:1392-1401</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dayong">Dayong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hoi:Steven_Chu=Hong">Steven Chu-Hong Hoi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He_0001:Ying">Ying He</a></p>
<p>【Abstract】:
Auto face annotation plays an important role in many real-world multimedia information and knowledge management systems. Recently there is a surge of research interests in mining weakly-labeled facial images on the internet to tackle this long-standing research challenge in computer vision and image understanding. In this paper, we present a novel unified learning framework for face annotation by mining weakly labeled web facial images through interdisciplinary efforts of combining sparse feature representation, content-based image retrieval, transductive learning and inductive learning techniques. In particular, we first introduce a new search-based face annotation paradigm using transductive learning, and then propose an effective inductive learning scheme for training classification-based annotators from weakly labeled facial images, and finally unify both transductive and inductive learning approaches to maximize the learning efficacy. We conduct extensive experiments on a real-world web facial image database, in which encouraging results show that the proposed unified learning scheme outperforms the state-of-the-art approaches.</p>
<p>【Keywords】:
face annotation; image retrieval; inductive learning; sparse coding; transductive learning; web facial images</p>
<h3 id="143. Efficient jaccard-based diversity analysis of large document collections.">143. Efficient jaccard-based diversity analysis of large document collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398445">Paper Link</a>】    【Pages】:1402-1411</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Fan">Fan Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Siersdorfer:Stefan">Stefan Siersdorfer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zerr:Sergej">Sergej Zerr</a></p>
<p>【Abstract】:
We propose two efficient algorithms for exploring topic diversity in large document corpora such as user generated content on the social web, bibliographic data, or other web repositories. Analyzing diversity is useful for obtaining insights into knowledge evolution, trends, periodicities, and topic heterogeneity of such collections. Calculating diversity statistics requires averaging over the similarity of all object pairs, which, for large corpora, is prohibitive from a computational point of view. Our proposed algorithms overcome the quadratic complexity of the average pair-wise similarity computation, and allow for constant time (depending on dataset properties) or linear time approximation with probabilistic guarantees. We show examples of diversity-based studies on large samples from corpora such as the social photo sharing site Flickr, the DBLP bibliography, and US Census data.</p>
<p>【Keywords】:
diversity; jaccard;clustering</p>
<h3 id="144. Knowing where and how criminal organizations operate using web content.">144. Knowing where and how criminal organizations operate using web content.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398446">Paper Link</a>】    【Pages】:1412-1421</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Coscia:Michele">Michele Coscia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rios:Viridiana">Viridiana Rios</a></p>
<p>【Abstract】:
We develop a framework that uses Web content to obtain quantitative information about a phenomenon that would otherwise require the operation of large scale, expensive intelligence exercises. Exploiting indexed reliable sources such as online newspapers and blogs, we use unambiguous query terms to characterize a complex evolving phenomena and solve a security policy problem: identifying the areas of operation and modus operandi of criminal organizations, in particular, Mexican drug trafficking organizations over the last two decades. We validate our methodology by comparing information that is known with certainty with the one we extracted using our framework. We show that our framework is able to use information available on the web to efficiently extract implicit knowledge about criminal organizations. In the scenario of Mexican drug trafficking, our findings provide evidence that criminal organizations are more strategic and operate in more differentiated ways than current academic literature thought.</p>
<p>【Keywords】:
data retrieval; knowledge discovery process; query</p>
<h2 id="IR track: social networks    4">IR track: social networks    4</h2>
<h3 id="145. Social recommendation across multiple relational domains.">145. Social recommendation across multiple relational domains.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398448">Paper Link</a>】    【Pages】:1422-1431</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Meng">Meng Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Peng">Peng Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Fei">Fei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu_0001:Wenwu">Wenwu Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shiqiang">Shiqiang Yang</a></p>
<p>【Abstract】:
Social networks enable users to create different types of personal items. In dealing with serious information overload, the major problems of social recommendation are sparsity and cold start. In existing approaches, relational and heterogeneous domains can not be effectively utilized for social recommendation, which brings a challenge to model users and multiple types of items together on social networks. In this paper, we consider how to represent social networks with multiple relational domains and alleviate the major problems in an individual domain by transferring knowledge from other domains. We propose a novel Hybrid Random Walk (HRW), which can integrate multiple heterogeneous domains including directed/undirected links, signed/unsigned links and within-domain/cross-domain links into a star-structured hybrid graph with user graph at the center. We perform random walk until convergence and use the steady state distribution for recommendation. We conduct experiments on a real social network dataset and show that our method can significantly outperform existing social recommendation approaches.</p>
<p>【Keywords】:
hybrid random walk; relational domains; social recommendation; star-structured graph; transfer learning</p>
<h3 id="146. Mining competitive relationships by learning across heterogeneous networks.">146. Mining competitive relationships by learning across heterogeneous networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398449">Paper Link</a>】    【Pages】:1432-1441</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Yang">Yang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keomany:Jacklyne">Jacklyne Keomany</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Yanting">Yanting Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Ying">Ying Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tian">Tian Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Liangwei">Liangwei Wang</a></p>
<p>【Abstract】:
Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this paper, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Our proposed model, Topical Factor Graph Model (TFGM), defines a latent topic layer to bridge the two networks and learns a semi-supervised learning model to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46\% improvement over alternative methods.</p>
<p>【Keywords】:
competitive relationship; social network; web mining</p>
<h3 id="147. Evaluating geo-social influence in location-based social networks.">147. Evaluating geo-social influence in location-based social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398450">Paper Link</a>】    【Pages】:1442-1451</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Chao">Chao Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shou:Lidan">Lidan Shou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0005:Ke">Ke Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0001:Gang">Gang Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bei:Yijun">Yijun Bei</a></p>
<p>【Abstract】:
The emerging location-based social network (LBSN) services not only allow people to maintain cyber links with their friends, but also enable them to share the events happening on them at different locations. The geo-social correlations among event participants make it possible to quantify mutual user influence for various events. Such a quantification of influence could benefit a wide spectrum of real-life applications such as targeted advertising and viral marketing. In this paper, we perform an in-depth analysis of the geo-social correlations among LBSN users at event level, based on which we address two problems: user influence evaluation and influential events discovery. To capture the geo-social closeness between LBSN users, we propose a unified influence metric. This metric combines a novel social proximity measure named penalized hitting time, with a geographical weight function modeled by power law distribution. We propose two approximate algorithms, namely global iteration (GI) and dynamic neighborhood expansion (DNE), to efficiently evaluate user influence with tight theoretical error bounds. We then adopt the sampling technique and the threshold algorithm to support efficient retrieval of top-K influential events. Extensive experiments on both real-life and synthetic LBSN data sets confirm that the proposed algorithms are effective, efficient, and scalable.</p>
<p>【Keywords】:
information extraction; social network; structural analysis</p>
<h3 id="148. The walls have ears: optimize sharing for visibility and privacy in online social networks.">148. The walls have ears: optimize sharing for visibility and privacy in online social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398451">Paper Link</a>】    【Pages】:1452-1461</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dinh:Thang_N=">Thang N. Dinh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Yilin">Yilin Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thai:My_T=">My T. Thai</a></p>
<p>【Abstract】:
With a rapid expansion of online social networks (OSNs), millions of users are tweeting and sharing their personal status daily without being aware of where that information eventually travels to. Likewise, with a huge magnitude of data available on OSNs, it poses a substantial challenge to track how a piece of information leaks to specific targets. In this paper, we study the problem of smartly sharing information to control the propagation of sensitive information in OSNs. In particular, we formulate and investigate the Maximum Circle of Trust problem of which we seek to construct a circle of trust on the fly so that OSN users can safely share their information knowing that it will not be propagated to their unwanted targets (whom they are not willing to share with). Since most of messages in OSNs are propagated within 2 to 5 hops, we first investigate this problem under 2-hop information propagation by showing the hardness of obtaining an optimal solution, along with an algorithm with proven performance guarantee. In a general case where information can be propagated more than two hops, the problem is #P-hard i.e. the problem cannot be solved in a polynomial time. Thus we propose a novel greedy algorithm, hybridizing the handy but costly sampling method with a novel cut-based estimation. The quality of the hybrid algorithm is comparable to that of the sampling method while taking only a tiny fraction of the time. We have validated the effectiveness of our solutions in many real-world traces. Such an extensive experiment also highlights several important observations on information leakage which help to sharpen the security of OSNs in the future.</p>
<p>【Keywords】:
algorithms; circle of trust; complexity; social networks</p>
<h2 id="Knowledge management short paper session    59">Knowledge management short paper session    59</h2>
<h3 id="149. Influence and similarity on heterogeneous networks.">149. Influence and similarity on heterogeneous networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398453">Paper Link</a>】    【Pages】:1462-1466</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Guan">Guan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Qingbo">Qingbo Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Philip_S=">Philip S. Yu</a></p>
<p>【Abstract】:
In the social network research, the studies on social influence maximization and entity similarity are two important and orthogonal tasks. On homogeneous networks, social influence maximization research tries to identify an initial influential set that maximizes the spread of the information, while similarity studies focus on designing meaningful ways to quantify entities' similarities. When heterogeneous networks are becoming ubiquitous and entities of different types are related to each other, we observe the possibility of merging the two directions together to improve the performance for both of them. In fact, we found that influence values among one type of nodes and similarity scores among the other type of nodes reinforce each other towards better and more meaningful results. Therefore, we introduce a framework that computes social influence for one type of nodes and simultaneously measures similarity of the other type of nodes in a heterogeneous network. First, we decouple the target heterogeneous network (or we call it Influence Similarity (IS) network) into three different parts: Influence network, Similarity network and information tunnels (IT) between them. Through IT, we exchange the influence scores and the similarity scores to calculate more precise similarity and influence scores in order to improve both of their qualities. The experiment results on real world data shows that our framework enables influence maximization framework to identify more influential seeds in Influence network and similarity measures to produce more meaningful similarity scores in Similarity network simultaneously.</p>
<p>【Keywords】:
influence; similarity; social networks</p>
<h3 id="150. GRAFT: an approximate graphlet counting algorithm for large graph analysis.">150. GRAFT: an approximate graphlet counting algorithm for large graph analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398454">Paper Link</a>】    【Pages】:1467-1471</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rahman:Mahmudur">Mahmudur Rahman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhuiyan:Mansurul">Mansurul Bhuiyan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hasan:Mohammad_Al">Mohammad Al Hasan</a></p>
<p>【Abstract】:
Graphlet frequency distribution (GFD) is an analysis tool for understanding the variance of local structure in a graph. Many recent works use GFD for comparing, and characterizing real-life networks. However, the main bottleneck for graph analysis using GFD is the excessive computation cost for obtaining the frequency of each of the graphlets in a large network. To overcome this, we propose a simple, yet powerful algorithm, called GRAFT, that obtains the approximate graphlet frequency for all graphlets that have upto 5 vertices. Comparing to an exact counting algorithm, our algorithm achieves a speedup factor between 10 and 100 for a negligible counting error, which is, on average, less than 5%; For example, exact graphlet counting for ca-AstroPh takes approximately 3 days; but, GRAFT runs for 45 minutes to perform the same task with a counting accuracy of 95.6%.</p>
<p>【Keywords】:
approximate graphlet counting; graph analysis; graphlet frequency distribution</p>
<h3 id="151. Hierarchical co-clustering based on entropy splitting.">151. Hierarchical co-clustering based on entropy splitting.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398455">Paper Link</a>】    【Pages】:1472-1476</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Wei">Wei Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiang">Xiang Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pan:Feng">Feng Pan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0010:Wei">Wei Wang</a></p>
<p>【Abstract】:
Two dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analysis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingency table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hierarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchial co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background. In this paper, we present a new co-clustering algorithm with solid information theoretic background. It simultaneously constructs a hierarchical structure of both row and column clusters which retains sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed which grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms.</p>
<p>【Keywords】:
co-clustering; contingency table; entropy; text analysis</p>
<h3 id="152. Mining long-lasting exploratory user interests from search history.">152. Mining long-lasting exploratory user interests from search history.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398456">Paper Link</a>】    【Pages】:1477-1481</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Bin">Bin Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Yuanhua">Yuanhua Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
A user's web search history contains many valuable search patterns. In this paper, we study search patterns that represent a user's long-lasting and exploratory search interests. By focusing on long-lastingness and exploratoriness, we are able to discover search patterns that are most useful for recommending new and relevant information to the user. Our approach is based on language modeling and clustering, and specifically designed to handle web search logs. We run our algorithm on a real web search log collection, and evaluate its performance using a novel simulated study on the same search log dataset. Experiment results support our hypothesis that long-lastingness and exploratoriness are necessary for generating successful recommendation. Our algorithm is shown to effectively discover such search interest patterns, and thus directly useful for making recommendation based on personal search history.</p>
<p>【Keywords】:
recommendation system; search log mining; user modeling</p>
<h3 id="153. Feature selection based on term frequency and T-test for text categorization.">153. Feature selection based on term frequency and T-test for text categorization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398457">Paper Link</a>】    【Pages】:1482-1486</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Deqing">Deqing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Hui">Hui Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Rui">Rui Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Weifeng">Weifeng Lv</a></p>
<p>【Abstract】:
Much work has been done on feature selection. Existing methods are based on document frequency, such as Chi-Square Statistic, Information Gain etc. However, these methods have two shortcomings: one is that they are not reliable for low-frequency terms, and the other is that they only count whether one term occurs in a document and ignore the term frequency. Actually, high-frequency terms within a specific category are often regards as discriminators. This paper focuses on how to construct the feature selection function based on term frequency, and proposes a new approach based on t-test, which is used to measure the diversity of the distributions of a term between the specific category and the entire corpus. Extensive comparative experiments on two text corpora using three classifiers show that our new approach is comparable to or or slightly better than the state-of-the-art feature selection methods (i.e., chi2, and IG) in terms of macro-F1 and micro-F1</p>
<p>【Keywords】:
T-test; feature selection; term frequency; text classification</p>
<h3 id="154. Adapting vector space model to ranking-based collaborative filtering.">154. Adapting vector space model to ranking-based collaborative filtering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398458">Paper Link</a>】    【Pages】:1487-1491</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Jiankai">Jiankai Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Jun">Jun Ma</a></p>
<p>【Abstract】:
Collaborative filtering (CF) is an effective technique addressing the information overload problem. Recently ranking-based CF methods have shown advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we seek accuracy improvement of ranking-based CF through adaptation of the vector space model, where we consider each user as a document and her pairwise relative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Then we use cosine similarity to select a neighborhood of users for the target user to make recommendations. Experiments on benchmarks in comparison with the state-of-the-art methods demonstrate the promise of our approach.</p>
<p>【Keywords】:
collaborative filtering;; ranking-based collaborative filtering; recommender systems; term weighting; vector space model</p>
<h3 id="155. Joint relevance and answer quality learning for question routing in community QA.">155. Joint relevance and answer quality learning for question routing in community QA.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398459">Paper Link</a>】    【Pages】:1492-1496</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Guangyou">Guangyou Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Kang">Kang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>【Abstract】:
Community question answering (cQA) has become a popular service for users to ask and answer questions. In recent years, the efficiency of cQA service is hindered by a sharp increase of questions in the community. This paper is concerned with the problem of question routing. Question routing in cQA aims to route new questions to the eligible answerers who can give high quality answers. However, the traditional methods suffer from the following two problems: (1) word mismatch between the new questions and the users' answering history; (2) high variance in perceived answer quality. To solve the above two problems, this paper proposes a novel joint learning method by taking both word mismatch and answer quality into a unified framework for question routing. We conduct experiments on large-scale real world data set from Yahoo! Answers. Experimental results show that our proposed method significantly outperforms the traditional query likelihood language model (QLLM) as well as state-of-the-art cluster-based language model (CBLM) and category-sensitive query likelihood language model (TCSLM).</p>
<p>【Keywords】:
answer quality; language model; question routing; translation model</p>
<h3 id="156. Fast approximation of steiner trees in large graphs.">156. Fast approximation of steiner trees in large graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398460">Paper Link</a>】    【Pages】:1497-1501</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gubichev:Andrey">Andrey Gubichev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Neumann_0001:Thomas">Thomas Neumann</a></p>
<p>【Abstract】:
Finding the minimum connected subtree of a graph that contains a given set of nodes (i.e., the Steiner tree problem) is a fundamental operation in keyword search in graphs, yet it is known to be NP-hard. Existing approximation techniques either make use of the heavy indexing of the graph, or entirely rely on online heuristics. In this paper we bridge the gap between these two extremes and present a scalable landmark-based index structure that, combined with a few lightweight online heuristics, yields a fast and accurate approximation of the Steiner tree. Our solution handles real-world graphs with millions of nodes and provides an approximation error of less than 5% on average.</p>
<p>【Keywords】:
graph databases; keyword search; steiner trees</p>
<h3 id="157. Automatically embedding newsworthy links to articles.">157. Automatically embedding newsworthy links to articles.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398461">Paper Link</a>】    【Pages】:1502-1506</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Ceylan:Hakan">Hakan Ceylan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Arapakis:Ioannis">Ioannis Arapakis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Donmez:Pinar">Pinar Donmez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lalmas:Mounia">Mounia Lalmas</a></p>
<p>【Abstract】:
It is of great interest to news providers such as Yahoo! News to attain higher visitor rates by promoting greater engagement with their content. One aspect of engagement deals with keeping users on the site longer by allowing them to navigate through content with enhanced, click-through experiences. News portals have invested in ways to provide embedded links within news stories. So far these links have been manually curated by professional editors, and due to the manual effort involved, the use of such links has been limited. In this paper we propose an automated approach to detecting and linking newsworthy events to associated articles. Our analysis, conducted on Amazon's Mechanical Turk, reveals that our system's performance is comparable to that of professional editors, and that users find the automatically generated highlights interesting and the associated articles worthy of reading.</p>
<p>【Keywords】:
automatic linking; engagement strategy; newsworthiness</p>
<h3 id="158. Learning spectral embedding via iterative eigenvalue thresholding.">158. Learning spectral embedding via iterative eigenvalue thresholding.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398462">Paper Link</a>】    【Pages】:1507-1511</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shang:Fanhua">Fanhua Shang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiao:Licheng">Licheng Jiao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yuanyuan">Yuanyuan Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Fei">Fei Wang</a></p>
<p>【Abstract】:
Learning data representation is a fundamental problem in data mining and machine learning. Spectral embedding is one popular method for learning effective data representations. In this paper we propose a novel framework to learn enhanced spectral embedding, which not only considers the geometrical structure of the data space, but also takes advantage of the given pairwise constraints. The proposed formulation can be solved by an iterative eigenvalue thresholding (IET) algorithm. Specially, we convert the problem of learning spectral embedding with pairwise constraints into the one of completing an "ideal" kernel matrix. And we introduce the spectral embedding of graph Laplacian as the auxiliary information and cast it as a small-scale positive semidefinite (PSD) matrix optimization problem with nuclear norm regularization. Then, we develop an IET algorithm to solve it efficiently. Moreover, we also present an effective semi-supervised clustering (SSC) approach with learned spectral embedding (LSE). Finally, we validate the proposed IET algorithm and LSE approach by extensive experiments on real-world data sets.</p>
<p>【Keywords】:
fixed point method; learning spectral embedding; matrix completion; nuclear norm minimization</p>
<h3 id="159. Measuring robustness of complex networks under MVC attack.">159. Measuring robustness of complex networks under MVC attack.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398463">Paper Link</a>】    【Pages】:1512-1516</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Rong=Hua">Rong-Hua Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Jeffrey_Xu">Jeffrey Xu Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Xin">Xin Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Hong">Hong Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shang:Zechao">Zechao Shang</a></p>
<p>【Abstract】:
Measuring robustness of complex networks is a fundamental task for analyzing the structure and function of complex networks. In this paper, we study the network robustness under the maximal vertex coverage (MVC) attack, where the attacker aims to delete as many edges of the network as possible by attacking a small fraction of nodes. First, we present two robustness metrics of complex networks based on MVC attack. We then propose an efficient randomized greedy algorithm with near-optimal performance guarantee for computing the proposed metrics. Finally, we conduct extensive experiments on 20 real datasets. The results show that P2P and co-authorship networks are extremely robust under the MVC attack while both the online social networks and the Email communication networks exhibit vulnerability under the MVC attack. In addition, the results demonstrate the efficiency and effectiveness of our proposed algorithms for computing the corresponding robustness metrics.</p>
<p>【Keywords】:
fm sketch; mvc attack; robustness; submodular function</p>
<h3 id="160. A simple approach to the design of site-level extractors using domain-centric principles.">160. A simple approach to the design of site-level extractors using domain-centric principles.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398464">Paper Link</a>】    【Pages】:1517-1521</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Long:Chong">Chong Long</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Geng:Xiubo">Xiubo Geng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Chang">Chang Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keerthi:Sathiya">Sathiya Keerthi</a></p>
<p>【Abstract】:
We consider the problem of extracting, in a domain-centric fashion, a given set of attributes from a large number of semi-structured websites. Previous approaches [7, 5] to solve this problem are based on page level inference. We propose a distinct new approach that directly chooses attribute extractors for a site using a scoring mechanism that is designed at the domain level via simple classification methods using a training set from a small number of sites. To keep the number of candidate extractors in each site manageably small we use two observations that hold in most domains: (a) imprecise annotators can be used to identify a small set of candidate extractors for a few attributes (anchors); and (b) non-anchor attributes lie in close proximity to the anchor attributes. Experiments on three domains (Events, Books and Restaurants) show that our approach is very effective in spite of its simplicity.</p>
<p>【Keywords】:
information extraction; text mining</p>
<h3 id="161. Extraction of topic evolutions from references in scientific articles and its GPU acceleration.">161. Extraction of topic evolutions from references in scientific articles and its GPU acceleration.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398465">Paper Link</a>】    【Pages】:1522-1526</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Masada:Tomonari">Tomonari Masada</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Takasu:Atsuhiro">Atsuhiro Takasu</a></p>
<p>【Abstract】:
This paper provides a topic model for extracting topic evolutions as a corpus-wide transition matrix among latent topics. Recent trends in text mining point to a high demand for exploiting metadata. Especially, exploitation of reference relationships among documents induced by hyperlinking Web pages, citing scientific articles, tumblring blog posts, retweeting tweets, etc., is put in the foreground of the effort for an effective mining. We focus on scholarly activities and propose a topic model for obtaining a corpus-wide view on how research topics evolve along citation relationships. Our model, called TERESA, extends latent Dirichlet allocation (LDA) by introducing a corpus-wide topic transition probability matrix, which models reference relationships as transitions among topics. Our approximated variational inference updates LDA posteriors and topic transition posteriors alternately. The main issue is execution time amounting to O(MK2), where K is the number of topics and M is that of links in citation network. Therefore, we accelerate the inference with Nvidia CUDA compatible GPUs. We compare the effectiveness of TERESA with that of LDA by introducing a new measure called diversity plus focusedness (D+F). We also present topic evolution examples our method gives.</p>
<p>【Keywords】:
citation analysis; gpu; topic modeling</p>
<h3 id="162. Graph-based workflow recommendation: on improving business process modeling.">162. Graph-based workflow recommendation: on improving business process modeling.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398466">Paper Link</a>】    【Pages】:1527-1531</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Bin">Bin Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Jianwei">Jianwei Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:ShuiGuang">ShuiGuang Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dongjing">Dongjing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Zhaohui">Zhaohui Wu</a></p>
<p>【Abstract】:
How to improve the modeling efficiency and accuracy has become a burning problem. The popularization of recommendation technique in E-Commerce provide us new trajectories that can be used for addressing the problem. In this paper, we propose a graph-based workflow recommendation for improving business process modeling. The start point is so-called "workflow repository" including a set of already developed process models. Graph mining method is used to extract the process patterns from the repository. Based on graph edit distance (GED) [2], we calculate the distance between patterns and the partial business process, viewed as reference model, which is under modeling and select the candidate nodes with smaller distances for recommendation. The performance study show its feasibility for practical uses.</p>
<p>【Keywords】:
business process modeling; graph edit distance; graph-based; workflow recommendation</p>
<h3 id="163. Reconciling ontologies and the web of data.">163. Reconciling ontologies and the web of data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398467">Paper Link</a>】    【Pages】:1532-1536</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Abedjan:Ziawasch">Ziawasch Abedjan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lorey:Johannes">Johannes Lorey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naumann:Felix">Felix Naumann</a></p>
<p>【Abstract】:
To integrate Linked Open Data, which originates from various and heterogeneous sources, the use of well-defined ontologies is essential. However, oftentimes the utilization of these ontologies by data publishers differs from the intended application envisioned by ontology engineers. This may lead to unspecified properties being used ad-hoc as predicates in RDF triples or it may result in infrequent usage of specified properties. These mismatches impede the goals and propagation of the Web of Data as data consumers face difficulties when trying to discover and integrate domain-specific information. In this work, we identify and classify common misusage patterns by employing frequency analysis and rule mining. Based on this analysis, we introduce an algorithm to propose suggestions for a data-driven ontology re-engineering workflow, which we evaluate on two large-scale RDF datasets.</p>
<p>【Keywords】:
data mining; linked data; ontology engineering</p>
<h3 id="164. Efficient extraction of ontologies from domain specific text corpora.">164. Efficient extraction of ontologies from domain specific text corpora.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398468">Paper Link</a>】    【Pages】:1537-1541</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tianyu">Tianyu Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chubak:Pirooz">Pirooz Chubak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lakshmanan:Laks_V=_S=">Laks V. S. Lakshmanan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pottinger:Rachel">Rachel Pottinger</a></p>
<p>【Abstract】:
Extracting ontological relationships (e.g., ISA and HASA) from free-text repositories (e.g., engineering documents and instruction manuals) can improve users' queries, as well as benefit applications built for these domains. Current methods to extract ontologies from text usually miss many meaningful relationships because they either concentrate on single-word terms and short phrases or neglect syntactic relationships between concepts in sentences. We propose a novel pattern-based algorithm to find ontological relationships between complex concepts by exploiting parsing information to extract multi-word concepts and nested concepts. Our procedure is iterative: we tailor the constrained sequential pattern mining framework to discover new patterns. Our experiments on three real data sets show that our algorithm consistently and significantly outperforms previous representative ontology extraction algorithms.</p>
<p>【Keywords】:
ontologies; ontology extraction</p>
<h3 id="165. Effective and efficient?: bilingual sentiment lexicon extraction using collocation alignment.">165. Effective and efficient?: bilingual sentiment lexicon extraction using collocation alignment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398469">Paper Link</a>】    【Pages】:1542-1546</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Zheng">Zheng Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Songbo">Songbo Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Xueke">Xueke Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shi:Weisong">Weisong Shi</a></p>
<p>【Abstract】:
Bilingual sentiment lexicon is fundamental resource for cross-language sentiment analysis but its compilation remains a major bottleneck in computational linguistics. Traditional word alignment algorithm faces with the status of large alignment space, which may introduce redundant computations as well as alignment errors. In this paper, we use collocation alignment to extract bilingual sentiment lexicon overcoming the drawbacks of word alignment. The idea of collocation alignment is inspired by the strong cohesion between feature words and opinion words in sentiment corpus. Experimental results show that our approach not only decreases the computing time dramatically but also improves the precision of extracted bilingual word pairs due to the smaller alignment space.</p>
<p>【Keywords】:
bilingual sentiment lexicon; collocation alignment; word alignment</p>
<h3 id="166. Exploiting latent relevance for relational learning of ubiquitous things.">166. Exploiting latent relevance for relational learning of ubiquitous things.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398470">Paper Link</a>】    【Pages】:1547-1551</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yao:Lina">Lina Yao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sheng:Quan_Z=">Quan Z. Sheng</a></p>
<p>【Abstract】:
With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. While this integration offers many exciting opportunities such as efficient supply chains and improved environmental monitoring, it also presents many significant challenges. One such challenge lies in how to classify, discover, and manage ubiquitous things, which is critical for efficient and effective object search, recommendation, and composition. In this paper, we focus on automatically classifying ubiquitous things into manageable semantic category labels by exploiting the information hidden in interactions between users and ubiquitous things. We develop a novel approach to extract latent relevance by building a relational network of ubiquitous things (RNUbiT) where similar things are linked via virtual edges according to their latent relevance. A discriminative learning algorithm is also developed to automatically determine category labels for ubiquitous things. We conducted experiments using real-world data and the experimental results demonstrate the feasibility and validity of our proposed approach.</p>
<p>【Keywords】:
modularity; multi-label classification; relational learning; ubiquitous things discovery; web of things</p>
<h3 id="167. Discovering personally semantic places from GPS trajectories.">167. Discovering personally semantic places from GPS trajectories.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398471">Paper Link</a>】    【Pages】:1552-1556</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Mingqi">Mingqi Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ling">Ling Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Gencai">Gencai Chen</a></p>
<p>【Abstract】:
A place is a locale that is frequently visited by an individual user and carries important semantic meanings (e.g. home, work, etc.). Many location-aware applications will be greatly enhanced with the ability of the automatic discovery of personally semantic places. The discovery of a user's personally semantic places involves obtaining the physical locations and semantic meanings of these places. In this paper, we propose approaches to address both of the problems. For the physical place extraction problem, a hierarchical clustering algorithm is proposed to firstly extract visit points from the GPS trajectories, and then these visit points can be clustered to form physical places. For the semantic place recognition problem, Bayesian networks (encoding the temporal patterns in which the places are visited) are used in combination with a customized POI (i.e. place of interest) database (containing the spatial features of the places) to categorize the extracted physical places into pre-defined types. An extensive set of experiments have been conducted to demonstrate the effectiveness of the proposed approaches based on a dataset of real-world GPS trajectories.</p>
<p>【Keywords】:
gps trajectory; location-aware computing; place discovery; place recognition; spatial-temporal data mining</p>
<h3 id="168. Mining coherent anomaly collections on web data.">168. Mining coherent anomaly collections on web data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398472">Paper Link</a>】    【Pages】:1557-1561</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dai:Hanbo">Hanbo Dai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Feida">Feida Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lim:Ee=Peng">Ee-Peng Lim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pang:HweeHwa">HweeHwa Pang</a></p>
<p>【Abstract】:
The recent boom of weblogs and social media has attached increasing importance to the identification of suspicious users with unusual behavior, such as spammers or fraudulent reviewers. A typical spamming strategy is to employ multiple dummy accounts to collectively promote a target, be it a URL or a product. Consequently, these suspicious accounts exhibit certain coherent anomalous behavior identifiable as a collection. In this paper, we propose the concept of Coherent Anomaly Collection (CAC) to capture this kind of collections, and put forward an efficient algorithm to simultaneously find the top-K disjoint CACs together with their anomalous behavior patterns. Compared with existing approaches, our new algorithm can find disjoint anomaly collections with coherent extreme behavior without having to specify either their number or sizes. Results on real Twitter data show that our approach discovers meaningful and informative hashtag spammer groups of various sizes which are hard to detect by clustering-based methods.</p>
<p>【Keywords】:
anomaly collection/cluster; anomaly/outlier detection</p>
<h3 id="169. Mining topic-level opinion influence in microblog.">169. Mining topic-level opinion influence in microblog.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398473">Paper Link</a>】    【Pages】:1562-1566</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Daifeng">Daifeng Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shuai:Xin">Xin Shuai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Guozheng">Guozheng Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Ying">Ying Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Zhipeng">Zhipeng Luo</a></p>
<p>【Abstract】:
This paper proposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously incorporates topic factor, user opinions and social influence in a unified probabilistic model with two stages learning processes. In the first stage, topic factor and user influence are integrated to generate users' influential relationship based on different topics; in the second stage, users' historical messages and social interaction records are leveraged by TOIM to construct their historical opinions and neighbors' opinion influence through a statistical learning process, which can be further utilized to predict users' future opinions on some specific topics. We evaluate our TOIM on a large-scaled dataset from Tencent Weibo, one of the largest microbloggings website in China. The experimental results show that TOIM can better predict users' opinion than other baseline methods.</p>
<p>【Keywords】:
opinion mining; sentiment analysis; social influence; topic modeling</p>
<h3 id="170. Meta path-based collective classification in heterogeneous information networks.">170. Meta path-based collective classification in heterogeneous information networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398474">Paper Link</a>】    【Pages】:1567-1571</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kong:Xiangnan">Xiangnan Kong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Philip_S=">Philip S. Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Ying">Ying Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wild:David_J=">David J. Wild</a></p>
<p>【Abstract】:
Collective classification approaches exploit the dependencies of a group of linked objects whose class labels are correlated and need to be predicted simultaneously. In this paper, we focus on studying the collective classification problem in heterogeneous networks, which involves multiple types of data objects interconnected by multiple types of links. Intuitively, two objects are correlated if they are linked by many paths in the network. By considering different linkage paths in the network, one can capture the subtlety of different types of dependencies among objects. We introduce the concept of meta-path based dependencies among objects, where a meta path is a path consisting a certain sequence of linke types. We show that the quality of collective classification results strongly depends upon the meta paths used. To accommodate the large network size, a novel solution, called HCC (meta-path based Heterogenous Collective Classification), is developed to effectively assign labels to a group of instances that are interconnected through different meta-paths. The proposed HCC model can capture different types of dependencies among objects with respect to different meta paths. Empirical studies on real-world networks demonstrate that effectiveness of the proposed meta path-based collective classification approach.</p>
<p>【Keywords】:
heterogeneous information networks; meta path</p>
<h3 id="171. Discretionary social network data revelation with a user-centric utility guarantee.">171. Discretionary social network data revelation with a user-centric utility guarantee.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398475">Paper Link</a>】    【Pages】:1572-1576</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Song:Yi">Yi Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karras:Panagiotis">Panagiotis Karras</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nobari:Sadegh">Sadegh Nobari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheliotis:Giorgos">Giorgos Cheliotis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Mingqiang">Mingqiang Xue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bressan:St=eacute=phane">Stéphane Bressan</a></p>
<p>【Abstract】:
The proliferation of online social networks has created intense interest in studying their nature and revealing information of interest to the end user. At the same time, such revelation raises privacy concerns. Existing research addresses this problem following an approach popular in the database community: a model of data privacy is defined, and the data is rendered in a form that satisfies the constraints of that model while aiming to maximize some utility measure. Still, these is no consensus on a clear and quantifiable utility measure over graph data. In this paper, we take a different approach: we define a utility guarantee, in terms of certain graph properties being preserved, that should be respected when releasing data, while otherwise distorting the graph to an extend desired for the sake of confidentiality. We propose a form of data release which builds on current practice in social network platforms: A user may want to see a subgraph of the network graph, in which that user as well as connections and affiliates participate. Such a snapshot should not allow malicious users to gain private information, yet provide useful information for benevolent users. We propose a mechanism to prepare data for user view under this setting. In an experimental study with real data, we demonstrate that our method preserves several properties of interest more successfully than methods that randomly distort the graph to an equal extent, while withstanding structural attacks proposed in the literature.</p>
<p>【Keywords】:
data utility; security and privacy; social network</p>
<h3 id="172. Empirical validation of the buckley-osthus model for the web host graph: degree and edge distributions.">172. Empirical validation of the buckley-osthus model for the web host graph: degree and edge distributions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398476">Paper Link</a>】    【Pages】:1577-1581</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhukovskiy:Maxim">Maxim Zhukovskiy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vinogradov:Dmitry">Dmitry Vinogradov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pritykin:Yuri">Yuri Pritykin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ostroumova:Liudmila">Liudmila Ostroumova</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grechnikov:Evgeny">Evgeny Grechnikov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gusev:Gleb">Gleb Gusev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raigorodskii:Andrei_M=">Andrei M. Raigorodskii</a></p>
<p>【Abstract】:
We consider the Buckley-Osthus implementation of preferential attachment and its ability to model the web host graph in two aspects. One is the degree distribution that we observe to follow the power law, as often being the case for real-world graphs. Another one is the two-dimensional edge distribution, the number of edges between vertices of given degrees. We fit a single "initial attractiveness" parameter a of the model, first with respect to the degree distribution of the web host graph, and then, absolutely independently, with respect to the edge distribution. Surprisingly, the values of a we obtain turn out to be nearly the same. Therefore the same model with the same value of the parameter a fits very well the two independent and basic aspects of the web host graph. In addition, we demonstrate that other models completely lack the asymptotic behavior of the edge distribution of the web host graph, even when accurately capturing the degree distribution. To the best of our knowledge, this is the first study confirming the ability of preferential attachment models to reflect the distribution of edges between vertices with respect to their degrees in a real graph of Internet.</p>
<p>【Keywords】:
assortative mixing; buckley-osthus random graphs; edge distribution with respect to vertex degrees; power law degree distribution; preferential attachment; web host graph</p>
<h3 id="173. gSCorr: modeling geo-social correlations for new check-ins on location-based social networks.">173. gSCorr: modeling geo-social correlations for new check-ins on location-based social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398477">Paper Link</a>】    【Pages】:1582-1586</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Huiji">Huiji Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jiliang">Jiliang Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Huan">Huan Liu</a></p>
<p>【Abstract】:
Location-based social networks (LBSNs) have attracted an increasing number of users in recent years. The availability of geographical and social information of online LBSNs provides an unprecedented opportunity to study the human movement from their socio-spatial behavior, enabling a variety of location-based services. Previous work on LBSNs reported limited improvements from using the social network information for location prediction; as users can check-in at new places, traditional work on location prediction that relies on mining a user's historical trajectories is not designed for this "cold start" problem of predicting new check-ins. In this paper, we propose to utilize the social network information for solving the "cold start" location prediction problem, with a geo-social correlation model to capture social correlations on LBSNs considering social networks and geographical distance. The experimental results on a real-world LBSN demonstrate that our approach properly models the social correlations of a user's new check-ins by considering various correlation strengths and correlation measures.</p>
<p>【Keywords】:
geo-social correlation; location prediction; location recommendation; location-based social networks</p>
<h3 id="174. Swimming against the streamz: search and analytics over the enterprise activity stream.">174. Swimming against the streamz: search and analytics over the enterprise activity stream.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398478">Paper Link</a>】    【Pages】:1587-1591</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guy:Ido">Ido Guy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Steier:Tal">Tal Steier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Barnea:Maya">Maya Barnea</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ronen:Inbal">Inbal Ronen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Daniel:Tal">Tal Daniel</a></p>
<p>【Abstract】:
Activity streams have become prevalent on the web and are starting to emerge in enterprises. In this work, we present Streamz, a novel application that uses a faceted search approach to provide employees with advanced capabilities of search, navigation, attention management, and other types of analytics on top of an enterprise activity stream. We provide a detailed description of the Streamz tool as well as usage analysis based on user interface logs and interviews of active users.</p>
<p>【Keywords】:
activity streams; enterprise; social media</p>
<h3 id="175. What is happening right now ... that interests me?: online topic discovery and recommendation in twitter.">175. What is happening right now ... that interests me?: online topic discovery and recommendation in twitter.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398479">Paper Link</a>】    【Pages】:1592-1596</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Diaz=Aviles:Ernesto">Ernesto Diaz-Aviles</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Drumond:Lucas">Lucas Drumond</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gantner:Zeno">Zeno Gantner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schmidt=Thieme:Lars">Lars Schmidt-Thieme</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nejdl:Wolfgang">Wolfgang Nejdl</a></p>
<p>【Abstract】:
Users engaged in the Social Web increasingly rely upon continuous streams of Twitter messages (tweets) for real-time access to information and fresh knowledge about current affairs. However, given the deluge of tweets, it is a challenge for individuals to find relevant and appropriately ranked information. We propose to address this knowledge management problem by going beyond the general perspective of information finding in Twitter, that asks: "What is happening right now?", towards an individual user perspective, and ask: "What is interesting to me right now?" In this paper, we consider collaborative filtering as an online ranking problem and present RMFO, a method that creates, in real-time, user-specific rankings for a set of tweets based on individual preferences that are inferred from the user's past system interactions. Experiments on the 476 million Twitter tweets dataset show that our online approach largely outperforms recommendations based on Twitter's global trend and Weighted Regularized Matrix Factorization (WRMF), a highly competitive state-of-the-art Collaborative Filtering technique, demonstrating the efficacy of our approach.</p>
<p>【Keywords】:
collaborative filtering; online ranking; twitter</p>
<h3 id="176. Frequent grams based embedding for privacy preserving record linkage.">176. Frequent grams based embedding for privacy preserving record linkage.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398480">Paper Link</a>】    【Pages】:1597-1601</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bonomi:Luca">Luca Bonomi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiong:Li">Li Xiong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Rui">Rui Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fung:Benjamin_C=_M=">Benjamin C. M. Fung</a></p>
<p>【Abstract】:
In this paper, we study the problem of privacy preserving record linkage which aims to perform record linkage without revealing anything about the non-linked records. We propose a new secure embedding strategy based on frequent variable length grams which allows record linkage on the embedded space. The frequent grams used for constructing the embedding base are mined from the original database under the framework of differential privacy. Compared with the state-of-the-art secure matching schema [15], our approach provides formal, provable privacy guarantees and achieves better scalability while providing comparable utility.</p>
<p>【Keywords】:
differential privacy; privacy; record linkage; security</p>
<h3 id="177. If you are happy and you know it... tweet.">177. If you are happy and you know it... tweet.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398481">Paper Link</a>】    【Pages】:1602-1606</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/T=:Amir_Asiaee">Amir Asiaee T.</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tepper:Mariano">Mariano Tepper</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Banerjee:Arindam">Arindam Banerjee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sapiro:Guillermo">Guillermo Sapiro</a></p>
<p>【Abstract】:
Extracting sentiment from Twitter data is one of the fundamental problems in social media analytics. Twitter's length constraint renders determining the positive/negative sentiment of a tweet difficult, even for a human judge. In this work we present a general framework for per-tweet (in contrast with batches of tweets) sentiment analysis which consists of: (1) extracting tweets about a desired target subject, (2) separating tweets with sentiment, and (3) setting apart positive from negative tweets. For each step, we study the performance of a number of classical and new machine learning algorithms. We also show that the intrinsic sparsity of tweets allows performing classification in a low dimensional space, via random projections, without losing accuracy. In addition, we present weighted variants of all employed algorithms, exploiting the available labeling uncertainty, which further improve classification accuracy. Finally, we show that spatially aggregating our per-tweet classification results produces a very satisfactory outcome, making our approach a good candidate for batch tweet sentiment analysis.</p>
<p>【Keywords】:
bayes classification; compressed learning; sparse modeling; supervised learning; svm; twitter sentiment analysis</p>
<h3 id="178. PRemiSE: personalized news recommendation via implicit social experts.">178. PRemiSE: personalized news recommendation via implicit social experts.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398482">Paper Link</a>】    【Pages】:1607-1611</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chen">Chen Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Runquan">Runquan Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Zhenhua">Zhenhua Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a></p>
<p>【Abstract】:
A variety of news recommender systems based on different strategies have been proposed to provide news personalization services for online news readers. However, little research work has been reported on utilizing the implicit "social" factors (i.e., the potential influential experts in news reading community) among news readers to facilitate news personalization. In this paper, we investigate the feasibility of integrating content-based methods, collaborative filtering and information diffusion models by employing probabilistic matrix factorization techniques. We propose PRemiSE, a novel Personalized news Recommendation framework via implicit Social Experts, in which the opinions of potential influencers on virtual social networks extracted from implicit feedbacks are treated as auxiliary resources for recommendation. Empirical results demonstrate the efficacy and effectiveness of our method, particularly, on handling the so-called cold-start problem.</p>
<p>【Keywords】:
expert; matrix factorization; news recommendation; social network</p>
<h3 id="179. Hierarchical topic integration through semi-supervised hierarchical topic modeling.">179. Hierarchical topic integration through semi-supervised hierarchical topic modeling.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398483">Paper Link</a>】    【Pages】:1612-1616</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Xianling">Xianling Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Jing">Jing He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Hongfei">Hongfei Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a></p>
<p>【Abstract】:
Lots of document collections are well organized in hierarchical structure, and such structure can help users browse and understand these collections. Meanwhile, there are a large number of plain document collections loosely organized, and it is difficult for users to understand them effectively. In this paper we study how to automatically integrate latent topics in a plain collection with the topics in a hierarchical structured collection. We propose to use semi-supervised topic modeling to solve the problem in a principled way. The experiments show that the proposed method can generate both meaningful latent topics and expand high quality hierarchical topic structures.</p>
<p>【Keywords】:
hierarchical topic modeling; topical integration</p>
<h3 id="180. Exploiting enriched contextual information for mobile app classification.">180. Exploiting enriched contextual information for mobile app classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398484">Paper Link</a>】    【Pages】:1617-1621</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Hengshu">Hengshu Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Huanhuan">Huanhuan Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Enhong">Enhong Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiong:Hui">Hui Xiong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tian:Jilei">Jilei Tian</a></p>
<p>【Abstract】:
A key step for the mobile app usage analysis is to classify apps into some predefined categories. However, it is a nontrivial task to effectively classify mobile apps due to the limited contextual information available for the analysis. To this end, in this paper, we propose an approach to first enrich the contextual information of mobile apps by exploiting the additional Web knowledge from the Web search engine. Then, inspired by the observation that different types of mobile apps may be relevant to different real-world contexts, we also extract some contextual features for mobile apps from the context-rich device logs of mobile users. Finally, we combine all the enriched contextual information into a Maximum Entropy model for training a mobile app classifier. The experimental results based on 443 mobile users' device logs clearly show that our approach outperforms two state-of-the-art benchmark methods with a significant margin.</p>
<p>【Keywords】:
automatic mobile app classification; real-world contexts; web knowledge</p>
<h3 id="181. Incorporating word correlation into tag-topic model for semantic knowledge acquisition.">181. Incorporating word correlation into tag-topic model for semantic knowledge acquisition.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398485">Paper Link</a>】    【Pages】:1622-1626</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Fang">Fang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Tingting">Tingting He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tu:Xinhui">Xinhui Tu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xiaohua">Xiaohua Hu</a></p>
<p>【Abstract】:
This paper presents a tag-topic model with Dirichlet Forest prior (TTM-DF) for semantic knowledge acquisition from blog. The TTM-DF model extends the tag-topic model (TTM) by replacing the Dirichlet prior with the Dirichlet Forest prior over the topic-word multinomial. The correlation between words are calculated to generate a set of Must-Links and Cannot-Links, then the structures of Dirichlet trees are obtained though encoding the constraints of Must-Links and Cannot-Links. Words under the same subtrees are expected to be more correlated than words under different subtrees. We conduct experiments on a synthetic and a blog dataset. Both of the experimental results show that the TTM-DF model performs much better than the TTM model. It can improve the coherence of the underlying topics and the tag-topic distributions, and capture semantic knowledge effectively.</p>
<p>【Keywords】:
blog; dirichlet forest prior; tag; topic model</p>
<h3 id="182. PriSM: discovering and prioritizing severe technical issues from product discussion forums.">182. PriSM: discovering and prioritizing severe technical issues from product discussion forums.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398486">Paper Link</a>】    【Pages】:1627-1631</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gangadharaiah:Rashmi">Rashmi Gangadharaiah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Catherine:Rose">Rose Catherine</a></p>
<p>【Abstract】:
Online forums provide a channel for users to report and discuss problems related to products and troubleshooting, for faster resolution. These could garner negative publicity if left unattended by the companies. Manually monitoring these massive amounts of discussions is laborious. This paper makes the first attempt at collecting issues that require immediate action by the product supplier by analyzing the immense information on forums. Features that are specific to forum discussions, in conjunction with linguistic cues help in capturing and better prioritizing issues. Any attempt to collect training data for learning a classifier for this task will require enormous labeling effort. Hence, this paper adopts a co-training approach, which uses minimal manual labeling, coupled with linguistic features extracted using a set-expansion algorithm to discover severe problems. Further, most distinct and recent issues are obtained by incorporating a measure of 'centrality', 'diversity' and temporal aspect of the forum threads. We show that this helps in better prioritizing longstanding issues and identify issues that need to be addressed immediately.</p>
<p>【Keywords】:
discovering and prioritizing severe technical issues</p>
<h3 id="183. Preprocessing of informal mathematical discourse in context ofcontrolled natural language.">183. Preprocessing of informal mathematical discourse in context ofcontrolled natural language.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398487">Paper Link</a>】    【Pages】:1632-1636</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Reyes:Ra=uacute=l_Ernesto_Guti=eacute=rrez_de_Pi=ntilde=erez">Raúl Ernesto Gutiérrez de Piñerez Reyes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/D=iacute=az=Fr=iacute=as:Juan_Francisco">Juan Francisco Díaz-Frías</a></p>
<p>【Abstract】:
Informal Mathematical Discourse (IMD) is characterized by the mixture of natural language and symbolic expressions in the context of textbooks, publications in mathematics and mathematical proof. We focused the IMD processing at the low level of discourse. In this paper, we proposed the preprocessing phase before the IMD structure analysis within the context of Controlled Natural Language (CNL). Our contribution is defined in context of the IMD processing and the use of machine learning; first, we present a CNL, a pure corpus and Matemathical Treebank for processing IMD; second, we present a preprocessing phase for IMD analysis with connectives disambiguation and verbs treatment, finally, we found a satisfactory result on input text parsing using a statistical parsing model. We will propagate these results for classification of argumentative informal practices via the low level discourse in IMD processing.</p>
<p>【Keywords】:
connective tagging; controlled natural language; corpus; informal mathematical discourse; statistical parsing</p>
<h3 id="184. PathRank: a novel node ranking measure on a heterogeneous graph for recommender systems.">184. PathRank: a novel node ranking measure on a heterogeneous graph for recommender systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398488">Paper Link</a>】    【Pages】:1637-1641</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Sangkeun">Sangkeun Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Sungchan">Sungchan Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kahng:Minsuk">Minsuk Kahng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Sang=goo">Sang-goo Lee</a></p>
<p>【Abstract】:
In this paper, we present a novel random-walk based node ranking measure, PathRank, which is defined on a heterogeneous graph by extending the Personalized PageRank algorithm. Not only can our proposed measure exploit the semantics behind the different types of nodes and edges in a heterogeneous graph, but also it can emulate various recommendation semantics such as collaborative filtering, content-based filtering, and their combinations. The experimental results show that PathRank can produce more various and effective recommendation results compared to existing approaches.</p>
<p>【Keywords】:
flexibility; graph; heterogeneity; network; pagerank; personalized pagerank; ranking; recommender systems</p>
<h3 id="185. Unsupervised discovery of opposing opinion networks from forum discussions.">185. Unsupervised discovery of opposing opinion networks from forum discussions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398489">Paper Link</a>】    【Pages】:1642-1646</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Yue">Yue Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Hongning">Hongning Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
With more and more people freely express opinions as well as actively interact with each other in discussion threads, online forums are becoming a gold mine with rich information about people's opinions and social behaviors. In this paper, we study an interesting new problem of automatically discovering opposing opinion networks of users from forum discussions, which are subset of users who are strongly against each other on some topic. Toward this goal, we propose to use signals from both textual content (e.g., who says what) and social interactions (e.g., who talks to whom) which are both abundant in online forums. We also design an optimization formulation to combine all the signals in an unsupervised way. We created a data set by manually annotating forum data on five controversial topics and our experimental results show that the proposed optimization method outperforms several baselines and existing approaches, demonstrating the power of combining both text analysis and social network analysis in analyzing and generating the opposing opinion networks.</p>
<p>【Keywords】:
linear programming; online forums; opinion analysis; optimization; social network analysis</p>
<h3 id="186. Exploring the existing category hierarchy to automatically label the newly-arising topics in cQA.">186. Exploring the existing category hierarchy to automatically label the newly-arising topics in cQA.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398490">Paper Link</a>】    【Pages】:1647-1651</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Guangyou">Guangyou Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Li">Li Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Kang">Kang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>【Abstract】:
This work investigates selecting concise labels for the newly-arising topics in community question answer. Previous methods of generating labels do not take the information of the existing category hierarchy into consideration. The main motivation of our paper is to utilize this information into the label generation process. We propose a general framework to address this problem. Firstly, we map the questions into Wikipedia concept sets, which are more meaningful than terms. Secondly, important concepts are identified to represent the main focus of the newly-arising topics. Thirdly, candidate labels are extracted from Wikipedia category graph. Finally, candidate labels are filtered and reranked by combination of structure information of existing category hierarchy and Wikipedia category graph. The experiments show that in our test collections, about 80% "correct" labels appear in the top ten labels recommended by our system.</p>
<p>【Keywords】:
category hierarchy; community question answering; newly-arising topics</p>
<h3 id="187. Query-focused multi-document summarization based on query-sensitive feature space.">187. Query-focused multi-document summarization based on query-sensitive feature space.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398491">Paper Link</a>】    【Pages】:1652-1656</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Wenpeng">Wenpeng Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pei:Yulong">Yulong Pei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Fan">Fan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Lian=en">Lian'en Huang</a></p>
<p>【Abstract】:
Query-oriented relevance, information richness and novelty are important requirements in query-focused summarization, which, to a considerable extent, determine the summary quality. Previous work either rarely took into account all above demands simultaneously or dealt with part of them in the dynamic process of choosing sentences to generate a summary. In this paper, we propose a novel approach that integrates all these requirements skillfully by treating them as sentence features, making that the finally generated summary could fully reflect the combinational effect of these properties. Experimental results on the DUC2005 and DUC2006 datasets demonstrate the effectiveness of our approach.</p>
<p>【Keywords】:
lda; query-biased sentence feature; query-focused summarization; topical vector space model</p>
<h3 id="188. Time-aware topic recommendation based on micro-blogs.">188. Time-aware topic recommendation based on micro-blogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398492">Paper Link</a>】    【Pages】:1657-1661</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liang:Huizhi">Huizhi Liang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Yue">Yue Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tjondronegoro:Dian">Dian Tjondronegoro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Christen:Peter">Peter Christen</a></p>
<p>【Abstract】:
Topic recommendation can help users deal with the information overload issue in micro-blogging communities. This paper proposes to use the implicit information network formed by the multiple relationships among users, topics and micro-blogs, and the temporal information of micro-blogs to find semantically and temporally relevant topics of each topic, and to profile users' time-drifting topic interests. The Content based, Nearest Neighborhood based and Matrix Factorization models are used to make personalized recommendations. The effectiveness of the proposed approaches is demonstrated in the experiments conducted on a real world dataset that collected from Twitter.com.</p>
<p>【Keywords】:
collaborative filtering; micro-blogs; personalization; temporal dynamics; topic recommendation; web 2.0</p>
<h3 id="189. Topic-sensitive probabilistic model for expert finding in question answer communities.">189. Topic-sensitive probabilistic model for expert finding in question answer communities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398493">Paper Link</a>】    【Pages】:1662-1666</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Guangyou">Guangyou Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lai:Siwei">Siwei Lai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Kang">Kang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>【Abstract】:
In this paper, we address the problem of expert finding in community question answering (CQA). Most of the existing approaches attempt to find experts in CQA by means of link analysis techniques. However, these traditional techniques only consider the link structure while ignore the topical similarity among users (askers and answerers) and user expertise and user reputation. In this study, we propose a topic-sensitive probabilistic model, which is an extension of PageRank algorithm to find experts in CQA. Compared to the traditional link analysis techniques, our proposed method is more effective because it finds the experts by taking into account both the link structure and the topical similarity among users. We conduct experiments on real world data set from Yahoo! Answers. Experimental results show that our proposed method significantly outperforms the traditional link analysis techniques and achieves the state-of-the-art performance for expert finding in CQA.</p>
<p>【Keywords】:
expert finding; pagerank; yahoo! answers</p>
<h3 id="190. iSampling: framework for developing sampling methods considering user's interest.">190. iSampling: framework for developing sampling methods considering user's interest.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398494">Paper Link</a>】    【Pages】:1667-1671</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Oh:Jinoh">Jinoh Oh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Hwanjo">Hwanjo Yu</a></p>
<p>【Abstract】:
Sampling is one of fundamental techniques for data preprocessing and mining. It helps to reduce computational costs and improve the mining quality. A sampling method is typically developed independently for a specific problem and for a specific user's interest, because it is hard to develop a method that is generalized across various user's interests. An absence of general framework for sampling makes it inefficient to develop or revise a sampling method as user's interest changes. This paper proposes a general framework, isampling, which facilitates a user developing sampling methods and easily modifying the user's sampling interest in the method. In the framework, a user explicitly describes her sampling interest into a graph model called interest model. Then, isampling automatically selects a sample set according to the model, which satisfies the user's interest. In order to demonstrate the effectiveness of our framework, we develop new trajectory sampling methods using our framework; trajectory sampling has been a challenging problem due to its high complexity of data and various user's interests. We demonstrate the flexibility of our framework by showing how easily trajectory samples of different interests can be generated within our framework.</p>
<p>【Keywords】:
model-based sampling; sampling framework for various interest; trajectory sampling</p>
<h3 id="191. WiSeNet: building a wikipedia-based semantic network with ontologized relations.">191. WiSeNet: building a wikipedia-based semantic network with ontologized relations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398495">Paper Link</a>】    【Pages】:1672-1676</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moro_0001:Andrea">Andrea Moro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Navigli:Roberto">Roberto Navigli</a></p>
<p>【Abstract】:
In this paper we present an approach for building a Wikipedia-based semantic network by integrating Open Information Extraction with Knowledge Acquisition techniques. Our algorithm extracts relation instances from Wikipedia page bodies and ontologizes them by, first, creating sets of synonymous relational phrases, called relation synsets, second, assigning semantic classes to the arguments of these relation synsets and, third, disambiguating the initial relation instances with relation synsets. As a result we obtain WiSeNet, a Wikipedia-based Semantic Network with Wikipedia pages as concepts and labeled, ontologized relations between them.</p>
<p>【Keywords】:
information extraction; knowledge acquisition; relation ontologization; semantic network</p>
<h3 id="192. Shaping communities out of triangles.">192. Shaping communities out of triangles.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398496">Paper Link</a>】    【Pages】:1677-1681</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Prat=P=eacute=rez:Arnau">Arnau Prat-Pérez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dominguez=Sal:David">David Dominguez-Sal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brunat:Josep_M=">Josep M. Brunat</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Larriba=Pey:Josep=Lluis">Josep-Lluis Larriba-Pey</a></p>
<p>【Abstract】:
Community detection has arisen as one of the most relevant topics in the field of graph data mining due to its importance in many fields such as biology, social networks or network traffic analysis. The metrics proposed to shape communities are too lax and do not consider the internal layout of the edges in the community, which lead to undesirable results. We define a new community metric called WCC. The proposed metric meets a minimum set of basic properties that guarantees communities with structure and cohesion. We experimentally show that WCC correctly quantifies the quality of communities and community partitions using real and synthetic datasets, and compare some of the most used community detection algorithms in the state of the art.</p>
<p>【Keywords】:
community detection; conductance; modularity; social networks</p>
<h3 id="193. The early-adopter graph and its application to web-page recommendation.">193. The early-adopter graph and its application to web-page recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398497">Paper Link</a>】    【Pages】:1682-1686</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mele:Ida">Ida Mele</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bonchi:Francesco">Francesco Bonchi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gionis:Aristides">Aristides Gionis</a></p>
<p>【Abstract】:
In this paper we present a novel graph-based data abstraction for modeling the browsing behavior of web users. The objective is to identify users who discover interesting pages before others. We call these users early adopters. By tracking the browsing activity of early adopters we can identify new interesting pages early, and recommend these pages to similar users. We focus on news and blog pages, which are more dynamic in nature and more appropriate for recommendation. Our proposed model is called early-adopter graph. In this graph, nodes represent users and a directed arc between users u and v expresses the fact that u and v visit similar pages and, in particular, that user u tends to visit those pages before user v. The weight of the edge is the degree to which the temporal rule "v visits a page before v" holds. Based on the early-adopter graph, we build a recommendation system for news and blog pages, which outperforms other out-of-the-shelf recommendation systems based on collaborative filtering.</p>
<p>【Keywords】:
early-adopter graph; log mining; user-browsing analysis; web-page recommendation</p>
<h3 id="194. Relational co-clustering via manifold ensemble learning.">194. Relational co-clustering via manifold ensemble learning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398498">Paper Link</a>】    【Pages】:1687-1691</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Ping">Ping Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bu:Jiajun">Jiajun Bu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Chun">Chun Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Zhanying">Zhanying He</a></p>
<p>【Abstract】:
Co-clustering targets on grouping the samples and features simultaneously. It takes advantage of the duality between the samples and features. In many real-world applications, the data points or features usually reside on a submanifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifolds in a principled way. In this study, we focus on improving the co-clustering performance via manifold ensemble learning, which aims to maximally approximate the intrinsic manifolds of both the sample and feature spaces. To achieve this, we develop a novel co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) based on symmetric nonnegative matrix tri-factorization, which decomposes the relational data matrix into three matrices. This method considers the inter-type relationship revealed by the relational data matrix and the intra-type information reflected by the affinity matrices. Specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of a group of pre-defined candidate manifolds. We hope to learn an appropriate convex combination of them to approach the desired intrinsic manifold. To optimize the objective, the multiplicative rules are utilized to update the factorized matrices and the entropic mirror descent algorithm is exploited to automatically learn the manifold coefficients. Experimental results demonstrate the superiority of the proposed algorithm.</p>
<p>【Keywords】:
co-clustering; entropic mirror descent algorithm; manifold ensemble learning; nonnegative matrix tri-factorization</p>
<h3 id="195. SemaFor: semantic document indexing using semantic forests.">195. SemaFor: semantic document indexing using semantic forests.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398499">Paper Link</a>】    【Pages】:1692-1696</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tsatsaronis:George">George Tsatsaronis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Varlamis:Iraklis">Iraklis Varlamis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=oslash=rv=aring=g:Kjetil">Kjetil Nørvåg</a></p>
<p>【Abstract】:
Traditional document indexing techniques store documents using easily accessible representations, such as inverted indices, which can efficiently scale for large document sets. These structures offer scalable and efficient solutions in text document management tasks, though, they omit the cornerstone of the documents' purpose: meaning. They also neglect semantic relations that bind terms into coherent fragments of text that convey messages. When semantic representations are employed, the documents are mapped to the space of concepts and the similarity measures are adapted appropriately to better fit the retrieval tasks. However, these methods can be slow both at indexing and retrieval time. In this paper we propose SemaFor, an indexing algorithm for text documents, which uses semantic spanning forests constructed from lexical resources, like Wikipedia, and WordNet, and spectral graph theory in order to represent documents for further processing.</p>
<p>【Keywords】:
document indexing; semantic graphs; text representation</p>
<h3 id="196. Measuring website similarity using an entity-aware click graph.">196. Measuring website similarity using an entity-aware click graph.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398500">Paper Link</a>】    【Pages】:1697-1701</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mendes:Pablo_N=">Pablo N. Mendes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mika:Peter">Peter Mika</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zaragoza:Hugo">Hugo Zaragoza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a></p>
<p>【Abstract】:
Query logs record the actual usage of search systems and their analysis has proven critical to improving search engine functionality. Yet, despite the deluge of information, query log analysis often suffers from the sparsity of the query space. Based on the observation that most queries pivot around a single entity that represents the main focus of the user's need, we propose a new model for query log data called the entity-aware click graph. In this representation, we decompose queries into entities and modifiers, and measure their association with clicked pages. We demonstrate the benefits of this approach on the crucial task of understanding which websites fulfill similar user needs, showing that using this representation we can achieve a higher precision than other query log-based approaches.</p>
<p>【Keywords】:
click graph; query logs; website similarity</p>
<h3 id="197. Community-based classification of noun phrases in twitter.">197. Community-based classification of noun phrases in twitter.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398501">Paper Link</a>】    【Pages】:1702-1706</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Freddy_Chong_Tat">Freddy Chong Tat Chua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Betteridge:Justin">Justin Betteridge</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lim:Ee=Peng">Ee-Peng Lim</a></p>
<p>【Abstract】:
Many event monitoring systems rely on counting known keywords in streaming text data to detect sudden spikes in frequency. But the dynamic and conversational nature of Twitter makes it hard to select known keywords for monitoring. Here we consider a method of automatically finding noun phrases (NPs) as keywords for event monitoring in Twitter. Finding NPs has two aspects, identifying the boundaries for the subsequence of words which represent the NP, and classifying the NP to a specific broad category such as politics, sports, etc. To classify an NP, we define the feature vector for the NP using not just the words but also the author's behavior and social activities. Our results show that we can classify many NPs by using a sample of training data from a knowledge-base.</p>
<p>【Keywords】:
named entities; noun phrases; social media; twitter</p>
<h3 id="198. Real-time bid optimization for group-buying ads.">198. Real-time bid optimization for group-buying ads.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398502">Paper Link</a>】    【Pages】:1707-1711</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Balakrishnan:Raju">Raju Balakrishnan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhatt:Rushi_P=">Rushi P. Bhatt</a></p>
<p>【Abstract】:
Group-buying ads seeking a minimum number of customers before the deal expiry are increasingly used by the daily-deal providers. Unlike the traditional web ads, the advertiser's profits for group-buying ads depends on the time to expiry and additional customers needed to satisfy the minimum group size. Since both these quantities are time-dependent, optimal bid amounts to maximize profits change with every impression. Consequently, traditional static bidding strategies are far from optimal. Instead, bid values need to be optimized in real-time to maximize expected bidder profits. This online optimization of deal profits is made possible by the advent of ad exchanges offering real-time (spot) bidding. To this end, we propose a real-time bidding strategy for group-buying deals based on the online optimization of the bid values. We derive the expected bidder profit of deals as a function of the bid amounts, and dynamically vary bids to maximize profits. Further, to satisfy time constraints of the online bidding, we present methods of minimizing computation timings. We evaluate the proposed bidding on a multi-million click stream of 935 ads. The method shows significant profit improvement over the existing strategies.</p>
<p>【Keywords】:
daily deals; display ads; group-buying; real-time bidding</p>
<h3 id="199. Degree relations of triangles in real-world networks and graph models.">199. Degree relations of triangles in real-world networks and graph models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398503">Paper Link</a>】    【Pages】:1712-1716</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Durak:Nurcan">Nurcan Durak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pinar:Ali">Ali Pinar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kolda:Tamara_G=">Tamara G. Kolda</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seshadhri:C=">C. Seshadhri</a></p>
<p>【Abstract】:
Triangles are an important building block and distinguishing feature of real-world networks, but their structure is still poorly understood. Despite numerous reports on the abundance of triangles, there is very little information on what these triangles look like. We initiate the study of degree-labeled triangles, - specifically, degree homogeneity versus heterogeneity in triangles. This yields new insight into the structure of real-world graphs. We observe that networks coming from social and collaborative situations are dominated by homogeneous triangles, i.e., degrees of vertices in a triangle are quite similar to each other. On the other hand, information networks (e.g., web graphs) are dominated by heterogeneous triangles, i.e., the degrees in triangles are quite disparate. Surprisingly, nodes within the top 1% of degrees participate in the vast majority of triangles in heterogeneous graphs. We investigate whether current graph models reproduce the types of triangles that are observed in real data and observe that most models fail to accurately capture these salient features.</p>
<p>【Keywords】:
graph models; social networks; triangles in graphs</p>
<h3 id="200. A probabilistic approach to mining geospatial knowledge from social annotations.">200. A probabilistic approach to mining geospatial knowledge from social annotations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398504">Paper Link</a>】    【Pages】:1717-1721</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Intagorn:Suradej">Suradej Intagorn</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lerman:Kristina">Kristina Lerman</a></p>
<p>【Abstract】:
User-generated content, such as photos and videos, is often annotated by users with free-text labels, called tags. Increasingly, such content is also georeferenced, i.e., it is associated with geographic coordinates. The implicit relationships between tags and their locations can tell us much about how people conceptualize places and relations between them. However, extracting such knowledge from social annotations presents many challenges, since annotations are often ambiguous, noisy, uncertain and spatially inhomogeneous. We introduce a probabilistic framework for modeling georeferenced annotations and a method for learning model parameters from data. The framework is flexible and general, and can be used in a variety of applications that mine geospatial knowledge from user-generated content. Specifically, we study three problems: extracting place semantics, predicting locations of photos and learning part-of relations between places. We show our method performs well compared to state-of-the-art approaches developed for the first two problems, and offers a novel solution to the problem of learning relations between places.</p>
<p>【Keywords】:
data mining; geo-spatial; information extraction; social network</p>
<h3 id="201. Providing grades and feedback for student summaries by ontology-based information extraction.">201. Providing grades and feedback for student summaries by ontology-based information extraction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398505">Paper Link</a>】    【Pages】:1722-1726</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gutierrez:Fernando">Fernando Gutierrez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dou:Dejing">Dejing Dou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fickas:Stephen">Stephen Fickas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Griffiths:Gina">Gina Griffiths</a></p>
<p>【Abstract】:
Automatic grading systems for summaries and essays have been studied for years. Most commercial and research implementations are based in statistical methods, such as Latent Semantic Analysis (LSA), which can provide high accuracy on similarity between the essay and the graded or standard essays, but they can offer very limited feedback. In the present work, we propose a novel method to provide both grades and meaningful feedback for student summaries by Ontology-based Information Extraction (OBIE). We use ontological concepts and relationships to create extraction rules to identify correct statements. Based on ontology constraints (e.g., disjointness between concepts), we define patterns that are logically inconsistent with the ontology to create rules to extract incorrect statements. Experiments show that the grades given to 18 student summaries on Ecosystems by OBIE are correlated to human gradings. OBIE also provide meaningful feedback on the errors those students made in their summaries.</p>
<p>【Keywords】:
automatic grading; information extraction; ontology</p>
<h3 id="202. Joint bilingual name tagging for parallel corpora.">202. Joint bilingual name tagging for parallel corpora.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398506">Paper Link</a>】    【Pages】:1727-1731</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Qi">Qi Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li_0004:Haibo">Haibo Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Wen">Wen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Jing">Jing Zheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Fei">Fei Huang</a></p>
<p>【Abstract】:
Traditional isolated monolingual name taggers tend to yield inconsistent results across two languages. In this paper, we propose two novel approaches to jointly and consistently extract names from parallel corpora. The first approach uses standard linear-chain Conditional Random Fields (CRFs) as the learning framework, incorporating cross-lingual features propagated between two languages. The second approach is based on a joint CRFs model to jointly decode sentence pairs, incorporating bilingual factors based on word alignment. Experiments on Chinese-English parallel corpora demonstrated that the proposed methods significantly outperformed monolingual name taggers, were robust to automatic alignment noise and achieved state-of-the-art performance. With only 20%of the training data, our proposed methods can already achieve better performance compared to the baseline learned from the whole training set.1</p>
<p>【Keywords】:
bilingual; joint crfs; name tagging</p>
<h3 id="203. Using program synthesis for social recommendations.">203. Using program synthesis for social recommendations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398507">Paper Link</a>】    【Pages】:1732-1736</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheung:Alvin">Alvin Cheung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Solar=Lezama:Armando">Armando Solar-Lezama</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Madden:Samuel">Samuel Madden</a></p>
<p>【Abstract】:
This paper presents a new approach to select events of interest to users in a social media setting where events are generated from mobile devices. We argue that the problem is best solved by inductive learning, where the goal is to first generalize from the users' expressed "likes" and "dislikes" of specific events, then to produce a program that can be used to collect only data of interest. The key contribution of this paper is a new algorithm that combines machine learning techniques with program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application.1</p>
<p>【Keywords】:
program synthesis; recommender systems; social networking applications</p>
<h3 id="204. Web-scale multi-task feature selection for behavioral targeting.">204. Web-scale multi-task feature selection for behavioral targeting.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398508">Paper Link</a>】    【Pages】:1737-1741</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ahmed:Amr">Amr Ahmed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aly:Mohamed">Mohamed Aly</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Das:Abhimanyu">Abhimanyu Das</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Anastasakos:Tasos">Tasos Anastasakos</a></p>
<p>【Abstract】:
A typical behavioral targeting system optimizing purchase activities, called conversions, faces two main challenges: the web-scale amounts of user histories to process on a daily basis, and the relative sparsity of conversions. In this paper, we try to address these challenges through feature selection. We formulate a multi-task (or group) feature-selection problem among a set of related tasks (sharing a common set of features), namely advertising campaigns. We apply a group-sparse penalty consisting of a combination of an l1 and l2 penalty and an associated fast optimization algorithm for distributed parameter estimation. Our algorithm relies on a variant of the well known Fast Iterative Thresholding Algorithm (FISTA), a closed-form solution for mixed norm programming and a distributed subgradient oracle. To efficiently handle web-scale user histories, we present a distributed inference algorithm for the problem that scales to billions of instances and millions of attributes. We show the superiority of our algorithm in terms of both sparsity and ROC performance over baseline feature selection methods (both single-task -regularization and multi-task mutual-information gain).</p>
<p>【Keywords】:
behavioral targeting; feature selection; large-scale learning; sparsity</p>
<h3 id="205. Balanced coverage of aspects for text summarization.">205. Balanced coverage of aspects for text summarization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398509">Paper Link</a>】    【Pages】:1742-1746</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Makino:Takuya">Takuya Makino</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Takamura:Hiroya">Hiroya Takamura</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Okumura:Manabu">Manabu Okumura</a></p>
<p>【Abstract】:
We propose a new model for the guided text summarization task. In this task, it is required that a generated summary covers all the aspects, which are predefined for the topic of the given document cluster; for example, aspects for the topic "Accidents and Natural Disasters" include WHAT, WHEN, WHERE, WHY, WHO AFFECTED, DAMAGES and COUNTERMEASURES. We use as a scorer for an aspect, the maximum entropy classifier that predicts whether each sentence reflects the aspect or not. We formalize the coverage of the aspects as a max-min problem, which enables a summary to cover aspects in a well-balanced manner. In the max-min problem, the minimum of the aspect scores is going to be maximized so that the summary contains all the aspects as much as possible. Furthermore, we integrate the model based on the max-min problem with the maximum coverage summarization model, which generates a summary containing as many conceptual units as possible. Through the experiments on benchmark datasets for the guided summarization, we show that our model outperforms other approaches in terms of ROUGE-2.</p>
<p>【Keywords】:
guided summarization; multi-document summarization; optimization problem</p>
<h3 id="206. Dynamic effects of ad impressions on commercial actions in display advertising.">206. Dynamic effects of ad impressions on commercial actions in display advertising.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398510">Paper Link</a>】    【Pages】:1747-1751</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Barajas:Joel">Joel Barajas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Akella:Ram">Ram Akella</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Holtan:Marius">Marius Holtan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kwon:Jaimie">Jaimie Kwon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Flores:Aaron">Aaron Flores</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Andrei:Victor">Victor Andrei</a></p>
<p>【Abstract】:
In this paper, we develop a time series approach, based on Dynamic Linear Models (DLM), to estimate the impact of ad impressions on the daily number of commercial actions when no user tracking is possible. The proposed method uses aggregate data, and hence it is simple to implement without expensive infrastructure. Specifically, we model the impact of daily number of ad impressions in daily number of commercial actions. We incorporate persistence of campaign effects on actions assuming a decay factor. We relax the assumption of a linear impact of ads on actions using the log-transformation. We also account for outliers with long-tailed distributions fitted and estimated automatically without a pre-defined threshold. This is applied to observational data post-campaign and does not require an experimental set-up. We apply the method to data from one commercial ad network on 2,885 campaigns for 1,251 products during six months, to calibrate and perform model selection. We set up a randomized experiment for two campaigns where user tracking is feasible. We find that the output of the proposed method is consistent with the results of A/B testing with similar confidence intervals.</p>
<p>【Keywords】:
attribution; dlm; marketing; online display advertising</p>
<h3 id="207. A hybrid approach for efficient provenance storage.">207. A hybrid approach for efficient provenance storage.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398511">Paper Link</a>】    【Pages】:1752-1756</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Yulai">Yulai Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Dan">Dan Feng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Zhipeng">Zhipeng Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Lei">Lei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Muniswamy=Reddy:Kiran=Kumar">Kiran-Kumar Muniswamy-Reddy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li_0006:Yan">Yan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Long:Darrell_D=_E=">Darrell D. E. Long</a></p>
<p>【Abstract】:
Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the provenance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compression time and query performance when compared to other compression schemes.</p>
<p>【Keywords】:
compression; provenance graphs; storage</p>
<h2 id="Information retrieval short paper session    61">Information retrieval short paper session    61</h2>
<h3 id="208. Content-based relevance estimation on the web using inter-document similarities.">208. Content-based relevance estimation on the web using inter-document similarities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398514">Paper Link</a>】    【Pages】:1769-1773</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Raiber:Fiana">Fiana Raiber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tennenholtz:Moshe">Moshe Tennenholtz</a></p>
<p>【Abstract】:
In adversarial and noisy search settings as the Web, the document-query surface level similarity can be a highly misleading relevance signal. Thus, devising content-based relevance estimation (ranking) approaches becomes highly challenging. We address this challenge using two methods that utilize inter-document similarities in an initially retrieved list. The first removes documents from the list that exhibit high query similarity, but for which there is insufficient additional support for relevance that is based on inter-document similarities. The method is based on a probabilistic model that decouples document-query similarities from relevance estimation. The second method re-ranks the list by "rewarding" documents that exhibit high similarity both to the query and to other documents in the list. Both methods incorporate, in addition, at the model level, query-independent document quality estimates. Extensive empirical evaluation demonstrates the merits of our methods.</p>
<p>【Keywords】:
inter-document similarities; web search</p>
<h3 id="209. Trust prediction via aggregating heterogeneous social networks.">209. Trust prediction via aggregating heterogeneous social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398515">Paper Link</a>】    【Pages】:1774-1778</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jin">Jin Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nie:Feiping">Feiping Nie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Heng">Heng Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tu:Yi=Cheng">Yi-Cheng Tu</a></p>
<p>【Abstract】:
Along with the increasing popularity of social web sites, users rely more on the trustworthiness information for many online activities among users. However, such social network data often suffers from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches explore the topology of trust graph. Previous research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behavior and tastes. Such ancillary information, is often accessible and therefore could potentially help the trust prediction. In this paper, we address the link prediction problem by aggregating heterogeneous social networks and propose a novel joint manifold factorization (JMF) method. Our new joint learning model explores the user group level similarity between correlated graphs and simultaneously learns the individual graph structure, therefore the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph, but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the objective function, we break down the proposed objective function into several manageable sub-problems, then further establish the theoretical convergence with the aid of auxiliary function. Extensive experiments were conducted on real world data sets and all empirical results demonstrated the effectiveness of our method.</p>
<p>【Keywords】:
nonnegative matrix factorization; social network; transfer learning; trust prediction</p>
<h3 id="210. Estimating interleaved comparison outcomes from historical click data.">210. Estimating interleaved comparison outcomes from historical click data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398516">Paper Link</a>】    【Pages】:1779-1783</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hofmann:Katja">Katja Hofmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Whiteson:Shimon">Shimon Whiteson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
Interleaved comparison methods, which compare rankers using click data, are a promising alternative to traditional information retrieval evaluation methods that require expensive explicit judgments. A major limitation of these methods is that they assume access to live data, meaning that new data must be collected for every pair of rankers compared. We investigate the use of previously collected click data (i.e., historical data) for interleaved comparisons. We start by analyzing to what degree existing interleaved comparison methods can be applied and find that a recent probabilistic method allows such data reuse, even though it is biased when applied to historical data. We then propose an interleaved comparison method that is based on the probabilistic approach but uses importance sampling to compensate for bias. We experimentally confirm that probabilistic methods make the use of historical data for interleaved comparisons possible and effective.</p>
<p>【Keywords】:
a/b testing; evaluation; implicit feedback; information retrieval; interleaved comparisons; reusability</p>
<h3 id="211. Automatic image annotation using tag-related random search over visual neighbors.">211. Automatic image annotation using tag-related random search over visual neighbors.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398517">Paper Link</a>】    【Pages】:1784-1788</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Zijia">Zijia Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Guiguang">Guiguang Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Mingqing">Mingqing Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Jianmin">Jianmin Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Jiaguang">Jiaguang Sun</a></p>
<p>【Abstract】:
In this paper, we propose a novel image auto-annotation model using tag-related random search over range-constrained visual neighbors of the to-be-annotated image. The proposed model, termed as TagSearcher, observes that the annotating performances of many previous visual-neighbor-based models are generally sensitive to the quantity setting of visual neighbors, and the probabilities for visual neighbors to be selected is better to be tag-dependent, meaning that each candidate tag can have its own trustworthy part of visual neighbors for score prediction. And thus TagSearcher uses a constrained range rather than an identical and fixed number of visual neighbors for auto-annotation. By performing a novel tag-related random search process over the graphical model made up of range-constrained visual neighbors, TagSearcher can find the trustworthy part for each candidate tag, and further utilize both visual similarities and tag correlations for score prediction. With the range constraint for visual neighbors and the tag-related random search process, TagSearcher can not only achieve satisfactory annotating performances, but also reduce the performance sensitivity. Experiments conducted on benchmark Corel5k well demonstrate its rationality and effectiveness.</p>
<p>【Keywords】:
image annotation; random search; tagsearcher</p>
<h3 id="212. Diversionary comments under political blog posts.">212. Diversionary comments under political blog posts.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398518">Paper Link</a>】    【Pages】:1789-1793</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jing">Jing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Clement_T=">Clement T. Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Philip_S=">Philip S. Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu_0001:Bing">Bing Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meng:Weiyi">Weiyi Meng</a></p>
<p>【Abstract】:
An important issue that has been neglected so far is the identification of diversionary comments. Diversionary comments under political blog posts are defined as comments that deliberately twist the bloggers' intention and divert the topic to another one. The purpose is to distract readers from the original topic and draw attention to a new topic. Given that political blogs have significant impact on the society, we believe it is imperative to identify such comments. We then categorize diversionary comments into 5 types, and propose an effective technique to rank comments in descending order of being diversionary. To the best of our knowledge, the problem of detecting diversionary comments has not been studied so far. Our evaluation on 2,109 comments under 20 different blog posts from Digg.com shows that the proposed method achieves the high mean average precision (MAP) of 92.6%. Sensitivity analysis indicates that the effectiveness of the method is stable under different parameter settings.</p>
<p>【Keywords】:
coreference resolution; diversionary comments; extraction from wikipedia; lda; spam; topic model</p>
<h3 id="213. Discover breaking events with popular hashtags in twitter.">213. Discover breaking events with popular hashtags in twitter.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398519">Paper Link</a>】    【Pages】:1794-1798</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Anqi">Anqi Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0006:Min">Min Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yiqun">Yiqun Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Shaoping">Shaoping Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Kuo">Kuo Zhang</a></p>
<p>【Abstract】:
In this paper, we utilize tags in Twitter (the hashtags) as an indicator of events. We first study the properties of hashtags for event detection. Based on several observations, we proposed three attributes of hashtags, including (1) instability for temporal analysis, (2) Twitter meme possibility to distinguish social events from virtual topics or memes, and (3) authorship entropy for mining the most contributed authors. Based on these attributes, breaking events are discovered with hashtags, which cover a wide range of social events among different languages in the real world.</p>
<p>【Keywords】:
burst detection; hashtag; social media; twitter</p>
<h3 id="214. Query likelihood with negative query generation.">214. Query likelihood with negative query generation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398520">Paper Link</a>】    【Pages】:1799-1803</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Yuanhua">Yuanhua Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
The query likelihood retrieval function has proven to be empirically effective for many retrieval tasks. From theoretical perspective, however, the justification of the standard query likelihood retrieval function requires an unrealistic assumption that ignores the generation of a "negative query" from a document. This suggests that it is a potentially non-optimal retrieval function. In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation. We propose an effective approach to estimate the probabilities of negative query generation based on the principle of maximum entropy, and derive a more complete query likelihood retrieval function that also contains the negative query generation component. The proposed approach not only bridges the theoretical gap in the existing query likelihood retrieval function, but also improves retrieval effectiveness significantly with no additional computational cost.</p>
<p>【Keywords】:
language model; negative query generation; principle of maximum entropy; probability ranking principle; query likelihood</p>
<h3 id="215. On the connections between explicit semantic analysis and latent semantic analysis.">215. On the connections between explicit semantic analysis and latent semantic analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398521">Paper Link</a>】    【Pages】:1804-1808</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Chao">Chao Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yi=Min">Yi-Min Wang</a></p>
<p>【Abstract】:
Semantic analysis tries to solve problems arising from polysemy and synonymy that are abundant in natural languages. Recently, Gabrilovich and Markovitch propose the Explicit Semantic Analysis (ESA) technique, which complements the well-known Latent Semantic Analysis (LSA) technique. In this paper, we show that the two techniques are not as distinct as their names suggest; instead, we find that ESA is equivalent to a LSA variant, and this equivalence generalizes to all kernel methods using kernels arising from the canonical dot product. Effectively, this result guarantees that ESA would not outperform the peak efficacy of LSA for any applications using the above kernel methods. In short, this paper for the first time establishes the connections between ESA and LSA, quantifies their relative efficacy, and generalizes the result to a big category of kernel methods.</p>
<p>【Keywords】:
explicit semantic analysis; kernel methods; latent semantic analysis</p>
<h3 id="216. Variance maximization via noise injection for active sampling in learning to rank.">216. Variance maximization via noise injection for active sampling in learning to rank.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398522">Paper Link</a>】    【Pages】:1809-1813</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Wenbin">Wenbin Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ya">Ya Zhang</a></p>
<p>【Abstract】:
Active learning for ranking, which is to selectively label the most informative examples, has been widely studied in recent years. In this paper, we propose a general active learning for ranking strategy called Variance Maximization (VM). The algorithm relies on noise injection to perturb the original unlabeled examples and generate the rank distribution of each example. Using a DCG-like gain function to measure each ranked list sampled from the rank distribution, Variance Maximization selects the unlabeled example with the largest variance in the gain. The VM strategy is applied at both the query level and the document level, and a two-stage active learning algorithm is further derived. Experimental results on both the LETOR 4.0 dataset and a real-world Web search ranking dataset have demonstrated the effectiveness of the proposed active learning approach.</p>
<p>【Keywords】:
active learning; learning to rank; noise injection; variance maximization</p>
<h3 id="217. More than relevance: high utility query recommendation by mining users' search behaviors.">217. More than relevance: high utility query recommendation by mining users' search behaviors.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398523">Paper Link</a>】    【Pages】:1814-1818</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaofei">Xiaofei Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lan:Yanyan">Yanyan Lan</a></p>
<p>【Abstract】:
Query recommendation plays a critical role in helping users' search. Most existing approaches on query recommendation aim to recommend relevant queries. However, the ultimate goal of query recommendation is to assist users to reformulate queries so that they can accomplish their search task successfully and quickly. Only considering relevance in query recommendation is apparently not directly toward this goal. In this paper, we argue that it is more important to directly recommend queries with high utility, i.e., queries that can better satisfy users' information needs. For this purpose, we propose a novel generative model, referred to as Query Utility Model (QUM), to capture query utility by simultaneously modeling users' reformulation and click behaviors. The experimental results on a publicly released query log show that, our approach is more effective in helping users find relevant search results and thus satisfying their information needs.</p>
<p>【Keywords】:
generative model; query logs; query recommendation; utility</p>
<h3 id="218. Finding nuggets in IP portfolios: core patent mining through textual temporal analysis.">218. Finding nuggets in IP portfolios: core patent mining through textual temporal analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398524">Paper Link</a>】    【Pages】:1819-1823</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Po">Po Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Minlie">Minlie Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Peng">Peng Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Weichang">Weichang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Usadi:Adam_K=">Adam K. Usadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaoyan">Xiaoyan Zhu</a></p>
<p>【Abstract】:
Patents are critical for a company to protect its core technologies. Effective patent mining in massive patent databases can provide companies with valuable insights to develop strategies for IP management and marketing. In this paper, we study a novel patent mining problem of automatically discovering core patents (i.e., patents with high novelty and influence in a domain). We address the unique patent vocabulary usage problem, which is not considered in traditional word-based statistical methods, and propose a topic-based temporal mining approach to quantify a patent's novelty and influence. Comprehensive experimental results on real-world patent portfolios show the effectiveness of our method.</p>
<p>【Keywords】:
core patent mining; patent influence; patent novelty; textual temporal analysis</p>
<h3 id="219. Interest-matching information propagation in multiple online social networks.">219. Interest-matching information propagation in multiple online social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398525">Paper Link</a>】    【Pages】:1824-1828</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Yilin">Yilin Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dinh:Thang_N=">Thang N. Dinh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Huiyuan">Huiyuan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thai:My_T=">My T. Thai</a></p>
<p>【Abstract】:
Online social networks have become an imperative channel for extremely fast information propagation and influence. Thus, the problem of finding a minimum number of seed users who can eventually influence as many users in the network as possible has become one of the central research topics recently. Unfortunately, most of related works have only focused on the network topologies and largely ignored many other important factors such as the users' engagements and the negative or positive impacts between users. More challengingly, the behavior of information propagation across multiple networks simultaneously remains an untrodden area and becomes an urgent need. Our work is the first attempt to tackle the above problem in multiple networks, considering these lacking important factors. In order to capture the users' engagement, we propose to targeting the set of interest-matching users whose interests are similar to what we try to propagate. Then, we develop our Iterative Semi-Supervising Learning based approach to identify the minimum seed users. We validate the effectiveness of our solution by using real-world Twitter-Foursquare networks and academic collaboration multiple networks.</p>
<p>【Keywords】:
information propagation; interest prediction; iterative semi-supervised learning; multiple online social networks</p>
<h3 id="220. Customizing search results for non-native speakers.">220. Customizing search results for non-native speakers.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398526">Paper Link</a>】    【Pages】:1829-1833</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lappas:Theodoros">Theodoros Lappas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vlachos:Michail">Michail Vlachos</a></p>
<p>【Abstract】:
Blog posts, news articles and other webpages are present on the web in multiple languages. Standard search engines evaluate the relevance of the candidate documents to the given query. However, when considering documents with overlapping content, many of them written in a foreign language other than the user's own native tongue, it is beneficial to promote documents that are easy enough for the user to read. Here, we show how to rank a collection of foreign documents based on both: a) relevance to the query, and b) the comprehension difficulty of the document. We design effective ranking operators that evaluate the difficulty of a foreign document with respect to the user's native language. We show that existing search engines can easily augment their scoring function by incorporating the proposed comprehensibility metrics. Finally, we provide extensive experimental evidence that the comprehensibility-aware ranking model significantly improves the standard relevance-based ranking paradigm.</p>
<p>【Keywords】:
document comprehensibility; multilingual document search</p>
<h3 id="221. Quality models for microblog retrieval.">221. Quality models for microblog retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398527">Paper Link</a>】    【Pages】:1834-1838</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Choi:Jaeho">Jaeho Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Jinyoung">Jinyoung Kim</a></p>
<p>【Abstract】:
Microblog services typically contain very short documents (e.g., tweets) containing comments about the latest news and events. Many of these documents are not informative or have very little content due to their personal and ephemeral nature. Providing effective retrieval in a microblog service will require addressing the challenge of distinguishing the high-quality, informative documents from the others. Recent work has focused on finding features that indicate the quality of microblog documents, but the impact these quality features on retrieval is not clear. In this paper, we suggest a low-cost quality model using surrogate judgments based on user behavior (i.e., retweets) that can be collected automatically. We analyze the relationship between document informativeness and relevance judgments for microblog retrieval. Then we demonstrate that our behavior-based quality metric has a high correlation with manual judgments. Also, we perform experiments to study the impact of the quality model on microblog retrieval. The results based on the TREC Microblog track show that the proposed quality model, combined with a variety of retrieval models, can improve retrieval performance and is competitive with a model trained using manual relevance judgments.</p>
<p>【Keywords】:
microblogs; quality model; quality-biased ranking</p>
<h3 id="222. Do ads compete or collaborate?: designing click models with full relationship incorporated.">222. Do ads compete or collaborate?: designing click models with full relationship incorporated.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398528">Paper Link</a>】    【Pages】:1839-1843</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xin:Xin">Xin Xin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/King:Irwin">Irwin King</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agrawal:Ritesh">Ritesh Agrawal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lyu:Michael_R=">Michael R. Lyu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Heyan">Heyan Huang</a></p>
<p>【Abstract】:
Traditionally click models predict click-through rate (CTR) of an advertisement (ad) independent of other ads. Recent researches however indicate that the CTR of an ad is dependent on the quality of the ad itself but also of the neighboring ads. Using historical click-through data of a commercially available ad server, we identify two types (competing and collaborating) of influences among sponsored ads and further propose a novel click-model, Full Relation Model (FRM), which explicitly models dependencies between ads. On a test data, FRM shows significant improvement in CTR prediction as compared to earlier click models.</p>
<p>【Keywords】:
click models; collaborating and competing influence; sponsored search</p>
<h3 id="223. Exploiting concept hierarchy for result diversification.">223. Exploiting concept hierarchy for result diversification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398529">Paper Link</a>】    【Pages】:1844-1848</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Wei">Wei Zheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fang_0001:Hui">Hui Fang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yao:Conglei">Conglei Yao</a></p>
<p>【Abstract】:
The goal of result diversification is to maximize the coverage of query subtopics while minimizing the redundancy in the search results. Intuitively, it is more desirable for a diversification system to cover independent subtopics since it would retrieve sets of non-overlapped relevant documents, which leads to less redundancy in the search results. Unfortunately, existing diversification methods assume that query subtopics are independent and ignore their relations in the diversification process. To overcome this limitation, we propose to exploit concept hierarchies to extract query subtopics and infer their relations. We then apply axiomatic approaches to derive a structural diversification method that can leverage the subtopic relations in result diversification. Experimental results over an enterprise collection show that the relations among query subtopics are useful to improve the diversification performance.</p>
<p>【Keywords】:
axiomatic approaches; concept hierarchy; enterprise search; structural diversification</p>
<h3 id="224. Ranking news events by influence decay and information fusion for media and users.">224. Ranking news events by influence decay and information fusion for media and users.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398530">Paper Link</a>】    【Pages】:1849-1853</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kong:Liang">Liang Kong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Shan">Shan Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Rui">Rui Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Shize">Shize Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Yan">Yan Zhang</a></p>
<p>【Abstract】:
In many cases, people would like to read the news with great importance on the Internet. However, what users can grasp covers a very small part compared with the huge amount of news which never stops increasing. In this paper, we try to find what users are most likely to be interested in. We notice that media focus plays an essential role in distinguishing news topics and user attention is also an important factor. Therefore, we first propose five strategies which only exploit media focus to decide news influence impact. Then we provide three strategies to combine user attention with media focus. Meanwhile, we also take four types of interaction between user attention and media focus into consideration. To the best of our knowledge, this is the first work to establish different models for computing influence decay of news topics. Experiments show that better influence scores will be achieved by a decay algorithm based on Ebbinghaus forgetting curve and information fusion by considering interactions between user attention and media focus.</p>
<p>【Keywords】:
influence decay; media-user interaction; news ranking</p>
<h3 id="225. Leveraging tagging for neighborhood-aware probabilistic matrix factorization.">225. Leveraging tagging for neighborhood-aware probabilistic matrix factorization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398531">Paper Link</a>】    【Pages】:1854-1858</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Le">Le Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Enhong">Enhong Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Qi">Qi Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Linli">Linli Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bao:Tengfei">Tengfei Bao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Lei">Lei Zhang</a></p>
<p>【Abstract】:
Collaborative Filtering(CF) is a popular way to build recommender systems and has been successfully employed in many applications. Generally, two kinds of approaches to CF, the local neighborhood methods and the global matrix factorization models, have been widely studied. Though some previous researches target on combining the complementary advantages of both approaches, the performance is still limited due to the extreme sparsity of the rating data. Therefore, it is necessary to consider more information for better reflecting user preference and item content. To that end, in this paper, by leveraging the extra tagging data, we propose a novel unified two-stage recommendation framework, named Neighborhood-aware Probabilistic Matrix Factorization(NHPMF). Specifically, we first use the tagging data to select neighbors of each user and each item, then add unique Gaussian distributions on each user's(item's) latent feature vector in the matrix factorization to ensure similar users(items) will have similar latent features}. Since the proposed method can effectively explores the external data source(i.e., tagging data) in a unified probabilistic model, it leads to more accurate recommendations. Extensive experimental results on two real world datasets demonstrate that our NHPMF model outperforms the state-of-the-art methods.</p>
<p>【Keywords】:
collaborative filtering; matrix factorization; neighborhood method</p>
<h3 id="226. Semantic context learning with large-scale weakly-labeled image set.">226. Semantic context learning with large-scale weakly-labeled image set.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398532">Paper Link</a>】    【Pages】:1859-1863</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Yao">Yao Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Wei">Wei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ke">Ke Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Xiangyang">Xiangyang Xue</a></p>
<p>【Abstract】:
There are a large number of images available on the web; meanwhile, only a subset of web images can be labeled by professionals because manual annotation is time-consuming and labor-intensive. Although we can now use the collaborative image tagging system, e.g., Flickr, to get a lot of tagged images provided by Internet users, these labels may be incorrect or incomplete. Furthermore, semantics richness requires more than one label to describe one image in real applications, and multiple labels usually interact with each other in semantic space. It is of significance to learn semantic context with large-scale weakly-labeled image set in the task of multi-label annotation. In this paper, we develop a novel method to learn semantic context and predict the labels of web images in a semi-supervised framework. To address the scalability issue, a small number of exemplar images are first obtained to cover the whole data cloud; then the label vector of each image is estimated as a local combination of the exemplar label vectors. Visual context, semantic context, and neighborhood consistency in both visual and semantic spaces are sufficiently leveraged in the proposed framework. Finally, the semantic context and the label confidence vectors for exemplar images are both learned in an iterative way. Experimental results on the real-world image dataset demonstrate the effectiveness of our method.</p>
<p>【Keywords】:
image annotation; large scale; semantic context; weakly labeled</p>
<h3 id="227. Sketch-based indexing of n-words.">227. Sketch-based indexing of n-words.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398533">Paper Link</a>】    【Pages】:1864-1868</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huston:Samuel">Samuel Huston</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Culpepper:J=_Shane">J. Shane Culpepper</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in information retrieval. However, accessing these types of statistics using standard inverted indexes requires unreasonable processing time or incurs a substantial space overhead. Establishing a balance between these competing space and time trade-offs can dramatically improve system performance. In this paper, we present and analyze a new index structure designed to improve query efficiency in term dependency retrieval models, with bounded space requirements. By adapting a class of (ε,δ)-approximation algorithms originally proposed for sketch summarization in networking applications, we show how to accurately estimate various statistics important in term dependency models with low, probabilistically bounded error rates. The space requirements of the sketch index structure is largely independent of this size and the number of phrase term dependencies. Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index of all n-grams consisting of between 1 and 5 words extracted from the Clueweb-Part-B collection to less than 0.2% of the requirements of an equivalent full index. We show that n-gram queries of 5 words can be processed more efficiently than in current alternatives, such as next-word indexes. We show retrieval using the sketch index to be up to 400 times faster than with positional indexes, and 15 times faster than next-word indexes.</p>
<p>【Keywords】:
indexing; scalability; sketching; term dependency models</p>
<h3 id="228. Interactive and context-aware tag spell check and correction.">228. Interactive and context-aware tag spell check and correction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398534">Paper Link</a>】    【Pages】:1869-1873</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bonchi:Francesco">Francesco Bonchi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Frieder:Ophir">Ophir Frieder</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nardini:Franco_Maria">Franco Maria Nardini</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silvestri:Fabrizio">Fabrizio Silvestri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vahabi:Hossein">Hossein Vahabi</a></p>
<p>【Abstract】:
Collaborative content creation and annotation creates vast repositories of all sorts of media, and user-defined tags play a central role as they are a simple yet powerful tool for organizing, searching and exploring the available resources. We observe that when a user annotates a resource with a set of tags, those tags are introduced one at a time. Therefore, when the fourth tag is introduced, a knowledge represented by the previous three tags, i.e., the context in which the fourth tag is produced, is available and exploitable for generating potential correction of the current tag. This context, together with the "wisdom of the crowd" represented by the co-occurrences of tags in all the resources of the repository, can be exploited to provide interactive tag spell check and correction. We develop this idea in a framework, based on a weighted tag co-occurrence graph and on nodes relatedness measures defined on weighted neighborhoods. We test our proposal on a dataset coming from YouTube. The results show that our framework is effective as it outperforms two important baselines. We also show that it is efficient, thus enabling its use in modern tagging services.</p>
<p>【Keywords】:
tag co-occurrence graph; tag spell checking and correction</p>
<h3 id="229. Federated search in the wild: the combined power of over a hundred search engines.">229. Federated search in the wild: the combined power of over a hundred search engines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398535">Paper Link</a>】    【Pages】:1874-1878</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Dong">Dong Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Demeester:Thomas">Thomas Demeester</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Trieschnigg:Dolf">Dolf Trieschnigg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hiemstra:Djoerd">Djoerd Hiemstra</a></p>
<p>【Abstract】:
Federated search has the potential of improving web search: the user becomes less dependent on a single search provider and parts of the deep web become available through a unified interface, leading to a wider variety in the retrieved search results. However, a publicly available dataset for federated search reflecting an actual web environment has been absent. As a result, it has been difficult to assess whether proposed systems are suitable for the web setting. We introduce a new test collection containing the results from more than a hundred actual search engines, ranging from large general web search engines such as Google and Bing to small domain-specific engines. We discuss the design and analyze the effect of several sampling methods. For a set of test queries, we collected relevance judgements for the top 10 results of each search engine. The dataset is publicly available and is useful for researchers interested in resource selection for web search collections, result merging and size estimation of uncooperative resources.</p>
<p>【Keywords】:
dataset; distributed information retrieval; evaluation; federated search; test collection; web search</p>
<h3 id="230. From sBoW to dCoT marginalized encoders for text representation.">230. From sBoW to dCoT marginalized encoders for text representation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398536">Paper Link</a>】    【Pages】:1879-1884</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Zhixiang_Eddie">Zhixiang Eddie Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Minmin">Minmin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sha:Fei">Fei Sha</a></p>
<p>【Abstract】:
In text mining, information retrieval, and machine learning, text documents are commonly represented through variants of sparse Bag of Words (sBoW) vectors (e.g. TF-IDF [1]). Although simple and intuitive, sBoW style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy. Especially when labeled data is limited (e.g. in document classification), or the text documents are short (e.g. emails or abstracts), many features are rarely observed within the training corpus. This leads to overfitting and reduced generalization accuracy. In this paper we propose Dense Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW document features. dCoT explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled corpus. With this approach, dCoT learns to reconstruct frequent words from co-occurring infrequent words and maps the high dimensional sparse sBoW vectors into a low-dimensional dense representation. We show that the feature removal can be marginalized out and that the reconstruction can be solved for in closed-form. We demonstrate empirically, on several benchmark datasets, that dCoT features significantly improve the classification accuracy across several document classification tasks.</p>
<p>【Keywords】:
denoising autoencoder; marginalized; stacked; text features</p>
<h3 id="231. Task tours: helping users tackle complex search tasks.">231. Task tours: helping users tackle complex search tasks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398537">Paper Link</a>】    【Pages】:1885-1889</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Awadallah:Ahmed_Hassan">Ahmed Hassan Awadallah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a></p>
<p>【Abstract】:
Complex search tasks such as planning a vacation often comprise multiple queries and may span a number of search sessions. When engaged in such tasks, users may require holistic support in determining the required task activities. Unfortunately, current search engines do not offer such support to their users. In this paper, we propose methods to automatically generate task tours comprising a starting task and a set of relevant related tasks, some or all of which may be necessary to satisfy a user's information needs. Applications of the tours include helping users understand the required steps to complete a task, finding URLs related to the active task, and alerting users to activities they may have missed. We demonstrate through experimentation with human judges and large-scale search logs that our tours are of good quality and can benefit a significant fraction of search engine users.</p>
<p>【Keywords】:
search task support; task graph; task tours</p>
<h3 id="232. Structured query reformulations in commerce search.">232. Structured query reformulations in commerce search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398538">Paper Link</a>】    【Pages】:1890-1894</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gollapudi:Sreenivas">Sreenivas Gollapudi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ieong:Samuel">Samuel Ieong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kannan:Anitha">Anitha Kannan</a></p>
<p>【Abstract】:
Recent work in commerce search has shown that understanding the semantics in user queries enables more effective query analysis and retrieval of relevant products. However, due to lack of sufficient domain knowledge, user queries often include terms that cannot be mapped directly to any product attribute. For example, a user looking for designer handbags might start with such a query because she is not familiar with the manufacturers, the price ranges, and/or the material that gives a handbag designer appeal. Current commerce search engines treat terms such as designer as keywords and attempt to match them to contents such as product reviews and product descriptions, often resulting in poor user experience. In this study, we propose to address this problem by reformulating queries involving terms such as designer, which we call modifiers, to queries that specify precise product attributes. We learn to rewrite the modifiers to attribute values by analyzing user behavior and leveraging structured data sources such as the product catalog that serves the queries. We first produce a probabilistic mapping between the modifiers and attribute values based on user behavioral data. These initial associations are then used to retrieve products from the catalog, over which we infer sets of attribute values that best describe the semantics of the modifiers. We evaluate the effectiveness of our approach based on a comprehensive Mechanical Turk study. We find that users agree with the attribute values selected by our approach in about 95% of the cases and they prefer the results surfaced for our reformulated queries to ones for the original queries in 87% of the time.</p>
<p>【Keywords】:
structured search</p>
<h3 id="233. Towards jointly extracting aspects and aspect-specific sentiment knowledge.">233. Towards jointly extracting aspects and aspect-specific sentiment knowledge.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398539">Paper Link</a>】    【Pages】:1895-1899</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Xueke">Xueke Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Songbo">Songbo Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yue">Yue Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Zheng">Zheng Lin</a></p>
<p>【Abstract】:
In this paper, we aim to jointly extract aspects and aspect-specific sentiment knowledge from online reviews, where the sentiment knowledge refers to the aspect-specific opinion words along with their aspect-aware sentiment polarities. To this end, we propose a Joint Aspect/Sentiment model (JAS). JAS detects aspect-specific opinion words by integrating opinion word lexicon knowledge to explicitly separate opinion words from factual words. More importantly, JAS exploits sentiment prior and aspect-contextual sentence-level co-occurrences of opinion words in reviews to further identify aspect-aware sentiment polarities for the opinion words. We apply the learned aspect-specific sentiment knowledge to practical aspect-level sentiment analysis tasks. Experimental results show the effectiveness of JAS in learning aspect-specific sentiment knowledge and the practical value of this knowledge when applied to aspect-level sentiment classification.</p>
<p>【Keywords】:
aspect-level sentiment analysis; aspect-specific sentiment knowledge; joint aspect/setniment model; online reviews</p>
<h3 id="234. Collaborative ranking: improving the relevance for tail queries.">234. Collaborative ranking: improving the relevance for tail queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398540">Paper Link</a>】    【Pages】:1900-1904</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ke">Ke Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xin">Xin Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Hongyuan">Hongyuan Zha</a></p>
<p>【Abstract】:
It is well known that tail queries contribute to a substantial fraction of distinct queries submitted to search engines and thus become a major battle field for search engines. Unfortunately, compared with popular queries, it is much more difficult to obtain good search results for tail queries due to the lack of important relevance signals, such as user clicks, phrase matches and so on. In this paper, we propose to utilize the similarities between different queries to overcome the data sparsity problem for tail queries. Specifically, we propose to jointly learn query similarities and the ranking function from data so that the relevance signals of different but related queries can be collaboratively pooled to enhance the ranking of tail queries. We emphasize that the joint optimization is critical so that the learned query similarity function can adapt to the problem of learning ranking functions. Our proposed method is evaluated on two data sets and the results show that our method improves the relevance of tail queries over several baseline alternatives.</p>
<p>【Keywords】:
collaborative ranking; gradient boosting; learning to rank; relevance; tail query</p>
<h3 id="235. BiasTrust: teaching biased users about controversial topics.">235. BiasTrust: teaching biased users about controversial topics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398541">Paper Link</a>】    【Pages】:1905-1909</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vydiswaran:V=_G=_Vinod">V. G. Vinod Vydiswaran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pirolli:Peter">Peter Pirolli</a></p>
<p>【Abstract】:
Deciding whether a claim is true or false often requires understanding the evidence supporting and contradicting the claim. However, when learning about a controversial claim, human biases and viewpoints may affect which evidence documents are considered "trustworthy" or credible. It is important to overcome this bias and know both viewpoints to get a balanced perspective. In this paper, we study various factors that affect learning about the truthfulness of controversial claims. We designed a user study to understand the impact of these factors. Specifically, we studied the impact of presenting evidence with contrasting viewpoints and source expertise rating on how users accessed the evidence documents. This would help us optimize how to teach users about controversial topics in the most effective way, and to design better claim verification systems. We find that users do not seek contrasting viewpoints by themselves, but explicitly presenting contrasting evidence helps them get a well-rounded understanding of the topic. Furthermore, explicit knowledge of the source credibility and the context not only affects what users read, but also how credible they perceive the document to be.</p>
<p>【Keywords】:
claim verification; information credibility; user study</p>
<h3 id="236. Recommending citations: translating papers into references.">236. Recommending citations: translating papers into references.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398542">Paper Link</a>】    【Pages】:1910-1914</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Wenyi">Wenyi Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kataria:Saurabh">Saurabh Kataria</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caragea:Cornelia">Cornelia Caragea</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Prasenjit">Prasenjit Mitra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Giles:C=_Lee">C. Lee Giles</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rokach:Lior">Lior Rokach</a></p>
<p>【Abstract】:
When we write or prepare to write a research paper, we always have appropriate references in mind. However, there are most likely references we have missed and should have been read and cited. As such a good citation recommendation system would not only improve our paper but, overall, the efficiency and quality of literature search. Usually, a citation's context contains explicit words explaining the citation. Using this, we propose a method that "translates" research papers into references. By considering the citations and their contexts from existing papers as parallel data written in two different "languages", we adopt the translation model to create a relationship between these two "vocabularies". Experiments on both CiteSeer and CiteULike dataset show that our approach outperforms other baseline methods and increase the precision, recall and f-measure by at least 5% to 10%, respectively. In addition, our approach runs much faster in the both training and recommending stage, which proves the effectiveness and the scalability of our work.</p>
<p>【Keywords】:
citation recommendation; machine translation</p>
<h3 id="237. Query-biased learning to rank for real-time twitter search.">237. Query-biased learning to rank for real-time twitter search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398543">Paper Link</a>】    【Pages】:1915-1919</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xin">Xin Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Tiejian">Tiejian Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Baobin">Baobin Li</a></p>
<p>【Abstract】:
By incorporating diverse sources of evidence of relevance, learning to rank has been widely applied to real-time Twitter search, where users are interested in fresh relevant messages. Such approaches usually rely on a set of training queries to learn a general ranking model, which we believe that the benefits brought by learning to rank may not have been fully exploited as the characteristics and aspects unique to the given target queries are ignored. In this paper, we propose to further improve the retrieval performance of learning to rank for real-time Twitter search, by taking the difference between queries into consideration. In particular, we learn a query-biased ranking model with a semi-supervised transductive learning algorithm so that the query-specific features, e.g. the unique expansion terms, are utilized to capture the characteristics of the target query. This query-biased ranking model is combined with the general ranking model to produce the final ranked list of tweets in response to the given target query. Extensive experiments on the standard TREC Tweets11 collection show that our proposed query-biased learning to rank approach outperforms strong baseline, namely the conventional application of the state-of-the-art learning to rank algorithms.</p>
<p>【Keywords】:
query-biased learning to rank; real-time twitter search; semi-supervised learning</p>
<h3 id="238. Discovering logical knowledge for deep question answering.">238. Discovering logical knowledge for deep question answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398544">Paper Link</a>】    【Pages】:1920-1924</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Zhao">Zhao Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qiu:Xipeng">Xipeng Qiu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Ling">Ling Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
Most open-domain question answering systems achieve better performances with large corpora, such as Web, by taking advantage of information redundancy. However, explicit answers are not always mentioned in the corpus, many answers are implicitly contained and can only be deducted by inference. In this paper, we propose an approach to discover logical knowledge for deep question answering, which automatically extracts knowledge in an unsupervised, domain-independent manner from background texts and reasons out implicit answers for the questions. Firstly, we use semantic role labeling to transform natural language expressions to predicates in first-order logic. Then we use association analysis to uncover the implicit relations among these predicates and build propositions for inference. Since our knowledge is drawn from different sources, we use Markov logic to merge multiple knowledge bases without resolving their inconsistencies. Our experiments show that these propositions can improve the performance of question answering significantly.</p>
<p>【Keywords】:
markov logic; question answering; semantic role labeling</p>
<h3 id="239. Mining noisy tagging from multi-label space.">239. Mining noisy tagging from multi-label space.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398545">Paper Link</a>】    【Pages】:1925-1929</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qi:Zhongang">Zhongang Qi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Ming">Ming Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhongfei_=Mark=">Zhongfei (Mark) Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhengyou">Zhengyou Zhang</a></p>
<p>【Abstract】:
In this paper we study the problem of mining noisy tagging. Most of the existing discriminative classification methods to this problem only consider one tag at a time as the classification target, and completely ignore the rest of the given tags at the same time. In this paper we argue that all the given multiple tags can be utilized simultaneously as an additional feature and the information contained in the multi-label space can be taken advantage of to improve the performance of the classification. We first propose a novel distance measure to compute the distance between instances in the multi-label space. Then we propose several novel methods to incorporate the information of the multi-label space into the discriminative classification methods in one view learning or in two views learning to solve a general multi-label classification problem and to mitigate the influence of the noise in the classification. We apply the proposed solutions to the problem with a more specific context - noisy image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods in the existing literature on solving the problem of mining noisy tagging.</p>
<p>【Keywords】:
image annotation prediction; multi-label space; noisy tagging</p>
<h3 id="240. Learning from mistakes: towards a correctable learning algorithm.">240. Learning from mistakes: towards a correctable learning algorithm.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398546">Paper Link</a>】    【Pages】:1930-1934</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Raman:Karthik">Karthik Raman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Svore:Krysta_Marie">Krysta Marie Svore</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gilad=Bachrach:Ran">Ran Gilad-Bachrach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Burges:Christopher_J=_C=">Christopher J. C. Burges</a></p>
<p>【Abstract】:
Many learning algorithms generate complex models that are difficult for a human to interpret, debug, and extend. In this paper, we address this challenge by proposing a new learning paradigm called correctable learning, where the learning algorithm receives external feedback about which data examples are incorrectly learned. We define a set of metrics which measure the correctability of a learning algorithm. We then propose a simple and efficient correctable learning algorithm which learns local models for different regions of the data space. Given an incorrect example, our method samples data in the neighborhood of that example and learns a new, more correct local model over that region. Experiments over multiple classification and ranking datasets show that our correctable learning algorithm offers significant improvements over the state-of-the-art techniques.</p>
<p>【Keywords】:
classification; correctable learning; regression</p>
<h3 id="241. CONSENTO: a new framework for opinion based entity search and summarization.">241. CONSENTO: a new framework for opinion based entity search and summarization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398547">Paper Link</a>】    【Pages】:1935-1939</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Choi:Jaehoon">Jaehoon Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Donghyeon">Donghyeon Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Seongsoon">Seongsoon Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Junkyu">Junkyu Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lim:Sangrak">Sangrak Lim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Sunwon">Sunwon Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Jaewoo">Jaewoo Kang</a></p>
<p>【Abstract】:
Search engines have become an important decision making tool today. Decision making queries are often subjective, such as "a good birthday present for my girlfriend", "best action movies in 2010", to name a few. Unfortunately, such queries may not be answered properly by conventional search systems. In order to address this problem, we introduce Consento, a consensus search engine designed to answer subjective queries. Consento performs segment indexing, as opposed to document indexing, to capture semantics from user opinions more precisely. In particular, we define a new indexing unit, Maximal Coherent Semantic Unit (MCSU). An MCSU represents a segment of a document, which captures a single coherent semantic. We also introduce a new ranking method, called ConsensusRank that counts online comments referring to an entity as a weighted vote. In order to validate the efficacy of the proposed framework, we compare Consento with standard retrieval models and their recent extensions for opinion based entity ranking. Experiments using movie and hotel data show the effectiveness of our framework.</p>
<p>【Keywords】:
consensus rank; consensus search; entity search; maximal coherent semantic unit; sentiment analysis</p>
<h3 id="242. Search result presentation based on faceted clustering.">242. Search result presentation based on faceted clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398548">Paper Link</a>】    【Pages】:1940-1944</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Stein:Benno">Benno Stein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gollub:Tim">Tim Gollub</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hoppe:Dennis">Dennis Hoppe</a></p>
<p>【Abstract】:
We propose a competence partitioning strategy for Web search result presentation: the unmodified head of a ranked result list is combined with a clustering of documents from the result list tail. We identify two principles to which such a clustering must adhere to improve the user's search experience: (1) Avoid the unwanted effect of query aspect repetition, which is called shadowing here. (2) Avoid extreme clusterings, i.e., neither the number of cluster labels nor the number of documents per cluster should exceed the size of the result list head. We present measures to quantify the shadowing effect, and with Faceted Clustering we introduce an algorithm that optimizes the identified principles. The key idea of Faceted Clustering is a dynamic, user-controlled reorganization of a clustering, similar to a faceted navigation system. We report on evaluations using the AMBIENT corpus and demonstrate the potential of our approach by a comparison with two well-known clustering search engines.</p>
<p>【Keywords】:
cluster labeling; search result clustering</p>
<h3 id="243. PolariCQ: polarity classification of political quotations.">243. PolariCQ: polarity classification of political quotations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398549">Paper Link</a>】    【Pages】:1945-1949</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Awadallah:Rawia">Rawia Awadallah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ramanath:Maya">Maya Ramanath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
We consider the problem of automatically classifying quotations about political debates into both topic and polarity. These quotations typically appear in news media and online forums. Our approach maps quotations onto one or more topics in a category system of political debates, containing more than a thousand fine-grained topics. To overcome the difficulty that pro/con classification faces due to the brevity of quotations and sparseness of features, we have devised a model of quotation expansion that harnesses antonyms from thesauri like WordNet. We developed a suite of statistical language models, judiciously customized to our settings, and use these to define similarity measures for unsupervised or supervised classifications. Experiments show the effectiveness of our method.</p>
<p>【Keywords】:
political opinion mining; web information extraction</p>
<h3 id="244. A comprehensive analysis of parameter settings for novelty-biased cumulative gain.">244. A comprehensive analysis of parameter settings for novelty-biased cumulative gain.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398550">Paper Link</a>】    【Pages】:1950-1954</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Leelanupab:Teerapong">Teerapong Leelanupab</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zuccon:Guido">Guido Zuccon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
In the TREC Web Diversity track, novelty-biased cumulative gain (α-NDCG) is one of the official measures to assess retrieval performance of IR systems. The measure is characterised by a parameter, α, the effect of which has not been thoroughly investigated. We find that common settings of α, i.e. α=0.5, may prevent the measure from behaving as desired when evaluating result diversification. This is because it excessively penalises systems that cover many intents while it rewards those that redundantly cover only few intents. This issue is crucial since it highly influences systems at top ranks. We revisit our previously proposed threshold, suggesting α be set on a query-basis. The intuitiveness of the measure is then studied by examining actual rankings from TREC 09-10 Web track submissions. By varying α according to our query-based threshold, the discriminative power of α-NDCG is not harmed and in fact, our approach improves α-NDCG's robustness. Experimental results show that the threshold for α can turn the measure to be more intuitive than using its common settings.</p>
<p>【Keywords】:
diversity; evaluation measure</p>
<h3 id="245. Entity centric query expansion for enterprise search.">245. Entity centric query expansion for enterprise search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398551">Paper Link</a>】    【Pages】:1955-1959</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xitong">Xitong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fang:Hui">Hui Fang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Fei">Fei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Min">Min Wang</a></p>
<p>【Abstract】:
Enterprise search is important, and the search quality has a direct impact on the productivity of an enterprise. Many information needs of enterprise search center around entities. Intuitively, information related to the entities mentioned in the query, such as related entities, would be useful to reformulate the query and improve the retrieval performance. However, most existing studies on query expansion are term-centric. In this paper, we propose a novel entity-centric query expansion framework for enterprise search. Specifically, given a query containing entities, we first utilize both unstructured and structured information to find entities that are related to the ones in the query. We then discuss how to adapt existing feedback methods to use the related entities to improve search quality. Experiment results show that the proposed entity-centric query expansion strategy is more effective to improve the search performance than the state-of-the-art pseudo feedback methods on longer, natural language-like queries with entities.</p>
<p>【Keywords】:
combining structured and unstructured data; enterprise search; entity centric; query expansion; retrieval</p>
<h3 id="246. Location-sensitive resources recommendation in social tagging systems.">246. Location-sensitive resources recommendation in social tagging systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398552">Paper Link</a>】    【Pages】:1960-1964</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wan:Chang">Chang Wan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kao:Ben">Ben Kao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheung:David_W=">David W. Cheung</a></p>
<p>【Abstract】:
In social tagging systems, resources such as images and videos are annotated with descriptive words called tags. It has been shown that tag-based resource searching and retrieval is much more effective than content-based retrieval. With the advances in mobile technology, many resources are also geo-tagged with location information. We observe that a traditional tag (word) can carry different semantics at different locations. We study how location information can be used to help distinguish the different semantics of a resource's tags and thus to improve retrieval accuracy. Given a search query, we propose a location-partitioning method that partitions all locations into regions such that the user query carries distinguishing semantics in each region. Based on the identified regions, we utilize location information in estimating the ranking scores of resources for the given query. These ranking scores are learned using the Bayesian Personalized Ranking (BPR) framework. Two algorithms, namely, LTD and LPITF, which apply Tucker Decomposition and Pairwise Interaction Tensor Factorization, respectively for modeling the ranking score tensor are proposed. Through experiments on real datasets, we show that LTD and LPITF outperform other tag-based resource retrieval methods.</p>
<p>【Keywords】:
location-sensitive; ranking; resources recommendation</p>
<h3 id="247. Differences in effectiveness across sub-collections.">247. Differences in effectiveness across sub-collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398553">Paper Link</a>】    【Pages】:1965-1969</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sanderson:Mark">Mark Sanderson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Turpin:Andrew">Andrew Turpin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Ying">Ying Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scholer:Falk">Falk Scholer</a></p>
<p>【Abstract】:
The relative performance of retrieval systems when evaluated on one part of a test collection may bear little or no similarity to the relative performance measured on a different part of the collection. In this paper we report the results of a detailed study of the impact that different sub-collections have on retrieval effectiveness, analyzing the effect over many collections, and with different approaches to sub-dividing the collections. The effect is shown to be substantial, impacting on comparisons between retrieval runs that are statistically significant. Some possible causes for the effect are investigated, and the implications of this work are examined for test collection design and for the strength of conclusions one can draw from experimental results.</p>
<p>【Keywords】:
information retrieval evaluation; search engines; sub-collections</p>
<h3 id="248. Map to humans and reduce error: crowdsourcing for deduplication applied to digital libraries.">248. Map to humans and reduce error: crowdsourcing for deduplication applied to digital libraries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398554">Paper Link</a>】    【Pages】:1970-1974</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Georgescu:Mihai">Mihai Georgescu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pham:Dang_Duc">Dang Duc Pham</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Firan:Claudiu_S=">Claudiu S. Firan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nejdl:Wolfgang">Wolfgang Nejdl</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gaugaz:Julien">Julien Gaugaz</a></p>
<p>【Abstract】:
Detecting duplicate entities, usually by examining metadata, has been the focus of much recent work. Several methods try to identify duplicate entities, while focusing either on accuracy or on efficiency and speed - with still no perfect solution. We propose a combined layered approach for duplicate detection with the main advantage of using Crowdsourcing as a training and feedback mechanism. By using Active Learning techniques on human provided examples, we fine tune our algorithm toward better duplicate detection accuracy. We keep the training cost low by gathering training data on demand for borderline cases or for inconclusive assessments. We apply our simple and powerful methods to an online publication search system: First, we perform a coarse duplicate detection relying on publication signatures in real time. Then, a second automatic step compares duplicate candidates and increases accuracy while adjusting based on both feedback from our online users and from Crowdsourcing platforms. Our approach shows an improvement of 14% over the untrained setting and is at only 4% difference to the human assessors in accuracy.</p>
<p>【Keywords】:
active learning; crowdsourcing; duplicate detection; machine learning; optimization</p>
<h3 id="249. Full-text citation analysis: enhancing bibliometric and scientific publication ranking.">249. Full-text citation analysis: enhancing bibliometric and scientific publication ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398555">Paper Link</a>】    【Pages】:1975-1979</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xiaozhong">Xiaozhong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Jinsong">Jinsong Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Chun">Chun Guo</a></p>
<p>【Abstract】:
The goal of this paper is to use innovative text and graph mining algorithms along with full-text citation analysis and topic modeling to enhance classical bibliometric analysis and publication ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is calculated by PageRank with edge and vertex prior distributions. Based on 104 topics (labeled with keywords) and their review papers, the cited publications of each review paper are assumed as "important publications" for ranking evaluation. The result shows that full text citation and publication content prior topic distribution along with the PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance for academic IR system.</p>
<p>【Keywords】:
bibliometrics; citation analysis; pagerank; prior knowledge; publication ranking; topic modeling</p>
<h3 id="250. Detecting offensive tweets via topical feature discovery over a large scale twitter corpus.">250. Detecting offensive tweets via topical feature discovery over a large scale twitter corpus.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398556">Paper Link</a>】    【Pages】:1980-1984</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xiang:Guang">Guang Xiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Bin">Bin Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Ling">Ling Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hong:Jason_I=">Jason I. Hong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ros=eacute=:Carolyn_Penstein">Carolyn Penstein Rosé</a></p>
<p>【Abstract】:
In this paper, we propose a novel semi-supervised approach for detecting profanity-related offensive content in Twitter. Our approach exploits linguistic regularities in profane language via statistical topic modeling on a huge Twitter corpus, and detects offensive tweets using automatically these generated features. Our approach performs competitively with a variety of machine learning (ML) algorithms. For instance, our approach achieves a true positive rate (TP) of 75.1% over 4029 testing tweets using Logistic Regression, significantly outperforming the popular keyword matching baseline, which has a TP of 69.7%, while keeping the false positive rate (FP) at the same level as the baseline at about 3.77%. Our approach provides an alternative to large scale hand annotation efforts required by fully supervised learning approaches.</p>
<p>【Keywords】:
hadoop; machine learning; topic modeling; twitter</p>
<h3 id="251. Automatic query expansion based on tag recommendation.">251. Automatic query expansion based on tag recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398557">Paper Link</a>】    【Pages】:1985-1989</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Oliveira:Vitor_Campos_de">Vitor Campos de Oliveira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gomes:Guilherme_de_Castro_Mendes">Guilherme de Castro Mendes Gomes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bel=eacute=m:Fabiano">Fabiano Belém</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brand=atilde=o:Wladmir_C=">Wladmir C. Brandão</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Almeida:Jussara_M=">Jussara M. Almeida</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Ziviani:Nivio">Nivio Ziviani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gon=ccedil=alves:Marcos_Andr=eacute=">Marcos André Gonçalves</a></p>
<p>【Abstract】:
We here propose a new method for expanding entity related queries that automatically filters, weights and ranks candidate expasion terms extracted from Wikipedia articles related to the original query. Our method is based on state-of-the-art tag recommendation methods that exploit heuristic metrics to estimate the descriptive capacity of a given term. Originally proposed for the context of tags, we here apply these recommendation methods to weight and rank terms extracted from multiple fields of Wikipedia articles according to their relevance for the article. We evaluate our method comparing it against three state-of-the-art baselines in three collections. Our results indicate that our method outperforms all baselines in all collections, with relative gains in MAP of up to 14% against the best ones.</p>
<p>【Keywords】:
query expansion; tag recommendation; wikipedia</p>
<h3 id="252. The downside of markup: examining the harmful effects of CSS and javascript on indexing today's web.">252. The downside of markup: examining the harmful effects of CSS and javascript on indexing today's web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398558">Paper Link</a>】    【Pages】:1990-1994</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gyllstrom:Karl">Karl Gyllstrom</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eickhoff:Carsten">Carsten Eickhoff</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vries:Arjen_P=_de">Arjen P. de Vries</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moens:Marie=Francine">Marie-Francine Moens</a></p>
<p>【Abstract】:
The continued development and maturation of advanced HTML features such as Cascading style sheets (CSS), Javascript, and AJAX, as well as their widespread adoption by browsers, has enabled web pages to flourish with sophistication and interactivity. Unfortunately, this presents challenges to the web search community, as a web page's representation in the browser (i.e., what users see) can diverge dramatically from its raw HTML content (i.e., what search engines index and retrieve). For example, interactive pages may contain content in regions that are not visible before a user action, such as focusing a tab, but which are nonetheless still contained within the raw HTML. We study this divergence by comparing raw HTML to its fully rendered form across a number of metrics spanning presentation, geometry, and content, using a large, representative sample of popular web pages. We find that a large divergence currently exists, and we show via a historical analysis that this divergence has grown more pronounced over the last decade. The general finding of our study is that continuing to index the web via simple HTML parsing will diminish the effectiveness of retrieval on the modern web, and that the IR community should work toward more sophisticated web page processing in indexing technology.</p>
<p>【Keywords】:
html; indexing; rendering; web</p>
<h3 id="253. You should read this! let me explain you why: explaining news recommendations to users.">253. You should read this! let me explain you why: explaining news recommendations to users.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398559">Paper Link</a>】    【Pages】:1995-1999</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Ceccarelli:Diego">Diego Ceccarelli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lucchese:Claudio">Claudio Lucchese</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perego:Raffaele">Raffaele Perego</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silvestri:Fabrizio">Fabrizio Silvestri</a></p>
<p>【Abstract】:
Recommender systems have become ubiquitous in content-based web applications, from news to shopping sites. Nonetheless, an aspect that has been largely overlooked so far in the recommender system literature is that of automatically building explanations for a particular recommendation. This paper focuses on the news domain, and proposes to enhance effectiveness of news recommender systems by adding, to each recommendation, an explanatory statement to help the user to better understand if, and why, the item can be her interest. We consider the news recommender system as a black-box, and generate different types of explanations employing pieces of information associated with the news. In particular, we engineer text-based, entity-based, and usage-based explanations, and make use of a Markov Logic Networks to rank the explanations on the basis of their effectiveness. The assessment of the model is conducted via a user study on a dataset of news read consecutively by actual users. Experiments show that news recommender systems can greatly benefit from our explanation module as it allows users to discriminate between interesting and not interesting news in the majority of the cases.</p>
<p>【Keywords】:
markov logic networks; news recommendation; query log analysis; recommendation snippets</p>
<h3 id="254. Characterizing web search queries that match very few or no results.">254. Characterizing web search queries that match very few or no results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398560">Paper Link</a>】    【Pages】:2000-2004</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Alting=ouml=vde:Ismail_Seng=ouml=r">Ismail Sengör Altingövde</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ozcan:Rifat">Rifat Ozcan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarigil:Erdem">Erdem Sarigil</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ulusoy:=Ouml=zg=uuml=r">Özgür Ulusoy</a></p>
<p>【Abstract】:
Despite the continuous efforts to improve the web search quality, a non-negligible fraction of user queries end up with very few or even no matching results in leading web search engines. In this work, we provide a detailed characterization of such queries based on an analysis of a real-life query log. Our experimental setup allows us to characterize the queries with few/no results and compare the mechanisms employed by the major search engines in handling them.</p>
<p>【Keywords】:
query difficulty; search result quality; web search engines</p>
<h3 id="255. A unified optimization framework for auction and guaranteed delivery in online advertising.">255. A unified optimization framework for auction and guaranteed delivery in online advertising.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398561">Paper Link</a>】    【Pages】:2005-2009</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Salomatin:Konstantin">Konstantin Salomatin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Yiming">Yiming Yang</a></p>
<p>【Abstract】:
This paper proposes a new unified optimization framework combining pay-per-click auctions and guaranteed delivery in sponsored search. Advertisers usually have different (and sometimes mixed) marketing goals: brand awareness and direct response. Different mechanisms are good at addressing different goals, e.g., guaranteed delivery was often used to build brand awareness and pay-per-click auctions was widely used for direct marketing. Our new method accommodates both in a unified framework, with the search engine revenue as an optimization objective. In this way, we can target a guaranteed number of ad clicks (or impressions) per campaign for advertisers willing to pay a premium and enable keyword auctions for all others. Specifically, we formulate this joint optimization problem using linear programming and a column generation strategy for efficiency. To select the best column (a ranked list of ads) given a query, we propose a novel dynamic programming algorithm that takes the special structure of the ad allocation and pricing mechanisms into account. We have tested the proposed framework and the algorithms on real ad data obtained from a commercial search engine. The results demonstrate that our proposed approach can outperform several baselines in guaranteeing the number of clicks for the given advertisers, and in increasing the total revenue for the search engine.</p>
<p>【Keywords】:
linear programming; online advertising; optimization; sponsored search</p>
<h3 id="256. Query recommendation for children.">256. Query recommendation for children.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398562">Paper Link</a>】    【Pages】:2010-2014</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Torres:Sergio_Duarte">Sergio Duarte Torres</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hiemstra:Djoerd">Djoerd Hiemstra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Ingmar">Ingmar Weber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a></p>
<p>【Abstract】:
One of the biggest problems that children experience while searching the web occurs during the query formulation process. Children have been found to struggle formulating queries based on keywords given their limited vocabulary and their difficulty to choose the right keywords. In this work we propose a method that utilizes tags from social media to suggest queries related to children topics. Concretely we propose a simple yet effective approach to bias a random walk defined on a bipartite graph of web resources and tags through keywords that are more commonly used to describe resources for children. We evaluate our method using a large query log sample of queries aimed at retrieving information for children. We show that our method outperforms query suggestions of state-of-the-art search engines and state-of-the art query suggestions based on random walks.</p>
<p>【Keywords】:
children; query formulation; social media</p>
<h3 id="257. Modeling browsing behavior for click analysis in sponsored search.">257. Modeling browsing behavior for click analysis in sponsored search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398563">Paper Link</a>】    【Pages】:2015-2019</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ashkan:Azin">Azin Ashkan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clarke:Charles_L=_A=">Charles L. A. Clarke</a></p>
<p>【Abstract】:
Clickthrough rate provides a fundamental measure of advertising quality, which is widely used in ad selection strategies. However, ads placed in contexts where they are rarely viewed, or where users are unlikely to be interested in commercial results, may receive few clicks regardless of their quality. In this paper, we gain insight into user browsing and click behavior for the purpose of click analysis in sponsored search domain. The list of ads displayed on a page, the user's initial motivation to browse this list, and the persistence of the user are among the contextual factors considered in this paper. We propose a probabilistic model for user's browsing and click behavior using these contextual factors. To evaluate the performance of the model, we compare it with state-of-the-art methods. The experimental results confirm that these contextual factors can better reflect user browsing and click behavior in sponsored search.</p>
<p>【Keywords】:
bayesian inference; click model; query log; sponsored search</p>
<h3 id="258. Sentiment-focused web crawling.">258. Sentiment-focused web crawling.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398564">Paper Link</a>】    【Pages】:2020-2024</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vural:A=_Gural">A. Gural Vural</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Senkul:Pinar">Pinar Senkul</a></p>
<p>【Abstract】:
The sentiments and opinions that are expressed in web pages towards objects, entities, and products constitute an important portion of the textual content available in the Web. Despite the vast interest in sentiment analysis and opinion mining, somewhat surprisingly, the discovery of the sentimental or opinionated web content is mostly ignored. This work aims to fill this gap and address the problem of quickly discovering and fetching the sentimental content present in the Web. To this end, we design a sentiment-focused web crawling framework for faster discovery and retrieval of such content. In particular, we propose different sentiment-focused web crawling strategies that prioritize discovered URLs based on their predicted sentiment scores. Through simulations, these strategies are shown to achieve considerable performance improvement over general-purpose web crawling strategies in discovering sentimental content.</p>
<p>【Keywords】:
focused web crawling; sentiment analysis</p>
<h3 id="259. User guided entity similarity search using meta-path selection in heterogeneous information networks.">259. User guided entity similarity search using meta-path selection in heterogeneous information networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398565">Paper Link</a>】    【Pages】:2025-2029</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Xiao">Xiao Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Yizhou">Yizhou Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Norick:Brandon">Brandon Norick</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Tiancheng">Tiancheng Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
With the emergence of web-based social and information applications, entity similarity search in information networks, aiming to find entities with high similarity to a given query entity, has gained wide attention. However, due to the diverse semantic meanings in heterogeneous information networks, which contain multi-typed entities and relationships, similarity measurement can be ambiguous without context. In this paper, we investigate entity similarity search and the resulting ambiguity problems in heterogeneous information networks. We propose to use a meta-path-based ranking model ensemble to represent semantic meanings for similarity queries, exploit the possibility of using using user-guidance to understand users query. Experiments on real-world datasets show that our framework significantly outperforms competitor methods.</p>
<p>【Keywords】:
entity similarity search; heterogeneous information network; user guided</p>
<h3 id="260. User activity profiling with multi-layer analysis.">260. User activity profiling with multi-layer analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398566">Paper Link</a>】    【Pages】:2030-2034</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Hongxia">Hongxia Jin</a></p>
<p>【Abstract】:
In this paper, we are interested in discovering semantically meaningful communities from a single user's perspective. We define a multi-layer analysis problem to derive a user's activity profile. Such an activity profile would include what activity areas a user is involved with, how important each activity is to the user, and who else is involved with the user on each activity as well as each participant's participation level. We believe a semantically meaningful community (corresponding to an activity area) must also consider the topics of the social messages rather than only the social links. While it is possible to use a hybrid approach based on traditional topic modeling, in this paper we propose a unified user modeling approach based on direct clustering over the social messages taking into considerations of both social connections and topics of social messages. Our clustering algorithm can be performed in a unified way in a unsupervised fashion as well as semi-supervised fashion when the user wants to give our algorithm some seeding inputs on his viewpoints. Moreover, when the new data comes, our algorithm can perform incremental updates on the new data without re-clustering the old data. Our experiments on social media datasets available from both within an enterprise and public social network demonstrate the effectiveness of our approach.</p>
<p>【Keywords】:
community discovery; social network; user profiling</p>
<h3 id="261. GTE: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets.">261. GTE: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398567">Paper Link</a>】    【Pages】:2035-2039</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Campos_0001:Ricardo">Ricardo Campos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dias:Ga=euml=l">Gaël Dias</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jorge:Al=iacute=pio">Alípio Jorge</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nunes:Celia">Celia Nunes</a></p>
<p>【Abstract】:
In this paper, we present an approach to identify top relevant dates in Web snippets with respect to a given implicit temporal query. Our approach is two-fold. First, we propose a generic temporal similarity measure called GTE, which evaluates the temporal similarity between a query and a date. Second, we propose a classification model to accurately relate relevant dates to their corresponding query terms and withdraw irrelevant ones. We suggest two different solutions: a threshold-based classification strategy and a supervised classifier based on a combination of multiple similarity measures. We evaluate both strategies over a set of real-world text queries and compare the performance of our Web snippet approach with a query log approach over the same set of queries. Experiments show that determining the most relevant dates of any given implicit temporal query can be improved with GTE combined with the second order similarity measure InfoSimba, the Dice coefficient and the threshold-based strategy compared to (1) first-order similarity measures and (2) the query log based approach.</p>
<p>【Keywords】:
implicit temporal queries; query log analysis; temporal information Retrieval; temporal query understanding</p>
<h3 id="262. Stochastic simulation of time-biased gain.">262. Stochastic simulation of time-biased gain.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398568">Paper Link</a>】    【Pages】:2040-2044</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Smucker:Mark_D=">Mark D. Smucker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clarke:Charles_L=_A=">Charles L. A. Clarke</a></p>
<p>【Abstract】:
Time-biased gain provides a unifying framework for information retrieval evaluation, generalizing many traditional effectiveness measures while accommodating aspects of user behavior not captured by these measures. By using time as a basis for calibration against actual user data, time-biased gain can reflect aspects of the search process that directly impact user experience, including document length, near-duplicate documents, and summaries. Unlike traditional measures, which must be arbitrarily normalized for averaging purposes, time-biased gain is reported in meaningful units, such as the total number of relevant documents seen by the user. In prior work, we proposed and validated a closed-form equation for estimating time-biased gain, explored its properties, and compared it to standard approaches. In this paper, we use stochastic simulation to numerically approximate time-biased gain. Stochastic simulation provides greater flexibility that will allow us, in future work, to easily accommodate different types of user behavior and increase the realism of the effectiveness measure.</p>
<p>【Keywords】:
information retrieval; search evaluation</p>
<h3 id="263. SonetRank: leveraging social networks to personalize search.">263. SonetRank: leveraging social networks to personalize search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398569">Paper Link</a>】    【Pages】:2045-2049</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kashyap:Abhijith">Abhijith Kashyap</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amini:Reza">Reza Amini</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hristidis:Vagelis">Vagelis Hristidis</a></p>
<p>【Abstract】:
Earlier works on personalized Web search focused on the click-through graphs, while recent works leverage social annotations, which are often unavailable. On the other hand, many users are members of the social networks and subscribe to social groups. Intuitively, users in the same group may have similar relevance judgments for queries related to these groups. SonetRank utilizes this observation to personalize the Web search results based on the aggregate relevance feedback of the users in similar groups. SonetRank builds and maintains a rich graph-based model, termed Social Aware Search Graph, consisting of groups, users, queries and results click-through information. SonetRank's personalization scheme learns in a principled way to leverage the following three signals, of decreasing strength: the personal document preferences of the user, of the users of her social groups relevant to the query, and of the other users in the network. SonetRank also uses a novel approach to measure the amount of personalization with respect to a user and a query, based on the query-specific richness of the user's social profile. We evaluate SonetRank with users on Amazon Mechanical Turk and show a significant improvement in ranking compared to state-of-the-art techniques.</p>
<p>【Keywords】:
results re-ranking; search personalization; social search</p>
<h3 id="264. Predicting web search success with fine-grained interaction data.">264. Predicting web search success with fine-grained interaction data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398570">Paper Link</a>】    【Pages】:2050-2054</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Qi">Qi Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lagun:Dmitry">Dmitry Lagun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a></p>
<p>【Abstract】:
Detecting and predicting searcher success is essential for automatically evaluating and improving Web search engine performance. In the past, Web searcher behavior data, such as result clickthrough, dwell time, and query reformulation sequences, have been successfully used for a variety of tasks, including prediction of success in a search session. However, the effectiveness of the previous approaches has been limited, as they tend to ignore how searchers actually view and interact with the visited pages. We show that fine-grained interactions, such as mouse cursor movements and scrolling, provide additional clues for better predicting success of a search session as a whole. To this end, we identify patterns of examination and interaction behavior that correspond to search success, and design a new Fine-grained Session Behavior (FSB) model to capture these patterns. Our experimental results show that FSB is significantly more effective than the state-of-the-art approaches that do not use these additional interaction data.</p>
<p>【Keywords】:
mouse cursor analysis; search session; success prediction</p>
<h3 id="265. Multi-session re-search: in pursuit of repetition and diversification.">265. Multi-session re-search: in pursuit of repetition and diversification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398571">Paper Link</a>】    【Pages】:2055-2059</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tyler:Sarah_K=">Sarah K. Tyler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Yi">Yi Zhang</a></p>
<p>【Abstract】:
Search engine users regularly re-issue queries that are the same or similar to ones they have previously issued. In this paper we study this act of query re-issuing, called re-search, focusing on multi session re-searching from an information seeking perspective. By focusing on the series of repeat or similar queries where the user shows a continued interest, new patterns of behavior not previously seen arise. We find that the well-studied re-finding behavior is only a piece of the re-search puzzle, and that even amidst repeated re-findings users exhibit diversification and novelty seeking behaviours for many re-search queries. This suggests diversity and re-finding behaviors should be jointly modelled and captured in evaluation measures, instead of being studied as two separate problems as is seen in many previous approaches.</p>
<p>【Keywords】:
query log analysis; re-search; web search</p>
<h3 id="266. Mining sentiment terminology through time.">266. Mining sentiment terminology through time.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398572">Paper Link</a>】    【Pages】:2060-2064</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Amiri:Hadi">Hadi Amiri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a></p>
<p>【Abstract】:
The correspondence between sentiment terminology and the active language used for expressing opinions is a crucial prerequisite for effective sentiment analysis. Mining sentiment terminology includes the detection of new opinion words as well as inferring their polarities. In this paper, we first propose a novel approach based on the interchangeability characteristic of words to detect new opinion words through time. We then show that the current non-time-based polarity inference approaches may assign opposite polarity to the same opinion word at different times. To tackle this issue, we consider the polarity scores computed at different times as polarity evidences (with the possibility of flawed evidences) and combine them to compute a globally correct polarity score for each opinion word. The experiments show that our approach is effective both in terms of the quality of the discovered new opinion words as well as its ability in inferring their polarities through time. Furthermore, we show the application of mining sentiment terminology through time in the sentiment classification (SC) task. The experiments show that mining more recent new opinion words leads to greater improvement in the performance of SC. To the best of our knowledge, this is the first work that investigates "time" as an important factor in mining sentiment terminology.</p>
<p>【Keywords】:
opinion word mining; sentiment orientation; temporal opinion lexicon; word polarity</p>
<h3 id="267. Theme chronicle model: chronicle consists of timestamp and topical words over each theme.">267. Theme chronicle model: chronicle consists of timestamp and topical words over each theme.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398573">Paper Link</a>】    【Pages】:2065-2069</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kawamae:Noriaki">Noriaki Kawamae</a></p>
<p>【Abstract】:
This paper presents a topic model that discovers the correlation patterns in a given time-stamped document collection and how these patterns evolve over time. Our proposal, the theme chronicle model (TCM) divides traditional topics into temporal and stable topics to detect the change of each theme over time; previous topic models ignore these differences and characterize trends as merely bursts of topics. TCM introduces a theme topic (stable topic), a trend topic (temporal topic), timestamps, and a latent switch variable in each token to realize these differences. Its topic layers allow TCM to capture not only word co-occurrence patterns in each theme, but also word co-occurrence patterns at any given time in each theme as trends. Experiments on various data sets show that the proposed model is useful as a generative model to discover fine-grained tightly coherent topics, takes advantage of previous models, and then assigns values for new documents.</p>
<p>【Keywords】:
bayesian hierarchical model; graphical models; text analysis; topic model; trend analysis</p>
<h3 id="268. Fast top-k similarity queries via matrix compression.">268. Fast top-k similarity queries via matrix compression.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398574">Paper Link</a>】    【Pages】:2070-2074</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Low:Yucheng">Yucheng Low</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Alice_X=">Alice X. Zheng</a></p>
<p>【Abstract】:
In this paper, we propose a novel method to efficiently compute the top-K most similar items given a query item, where similarity is defined by the set of items that have the highest vector inner products with the query. The task is related to the classical k-Nearest-Neighbor problem, and is widely applicable in a number of domains such as information retrieval, online advertising and collaborative filtering. Our method assumes an in-memory representation of the dataset and is designed to scale to query lengths of 100,000s of terms. Our algorithm uses a generalized Holder's inequality to upper bound the inner product with the norms of the constituent vectors. We also propose a novel compression scheme that computes bounds for groups of candidate items, thereby speeding up computation and minimizing memory requirements per query. We conduct extensive experiments on the publicly available Wikipedia dataset, and demonstrate that, with a memory overhead of 21%, our method can provide 1-3 orders of magnitude improvement in query run-time compared to naive methods and state of the art competing methods. Our median top-10 word query time is 25 us on 7.5 million words and 2.3 million documents.</p>
<p>【Keywords】:
inner product; nearest neighbor; top k</p>
<h2 id="Databases short paper session    33">Databases short paper session    33</h2>
<h3 id="269. Top-k retrieval using conditional preference networks.">269. Top-k retrieval using conditional preference networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398576">Paper Link</a>】    【Pages】:2075-2079</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Hongbing">Hongbing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Xuan">Xuan Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Wujin">Wujin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Peisheng">Peisheng Ma</a></p>
<p>【Abstract】:
This paper considers top-k retrieval using Conditional Preference Network (CP-Net). As a model for expressing user preferences on multiple mutually correlated attributes, CP-Net is of great interest for decision support systems. However, little work has addressed how to conduct efficient data retrieval using CP-Nets. This paper presents an approach to efficiently retrieve the most preferred data items based on a user's CP-Net. The proposed approach consists of a top-k algorithm and an indexing scheme. We conducted extensive experiments to compare our approach against a baseline top-k method - sequential scan. The results show that our approach outperform sequential scan in several circumstances.</p>
<p>【Keywords】:
cp-net; database; preference; top-k</p>
<h3 id="270. Sort-based query-adaptive loading of R-trees.">270. Sort-based query-adaptive loading of R-trees.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398577">Paper Link</a>】    【Pages】:2080-2084</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Achakeev:Daniar">Daniar Achakeev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seeger:Bernhard">Bernhard Seeger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Widmayer:Peter">Peter Widmayer</a></p>
<p>【Abstract】:
Bulk-loading of R-trees has been an important problem in academia and industry for more than twenty years. Current algorithms create R-trees without any information about the expected query profile. However, query profiles are extremely useful for the design of efficient indexes. In this paper, we address this deficiency and present query-adaptive algorithms for building R-trees optimally designed for a given query profile. Since optimal R-tree loading is NP-hard (even without tuning the structure to a query profile), we provide efficient, easy to implement heuristics. Our sort-based algorithms for query-adaptive loading consist of two steps: First, sorting orders are identified resulting in better R-trees than those obtained from standard space-filling curves. Second, for a given sorting order, we propose a dynamic programming algorithm for generating R-trees in linear runtime. Our experimental results confirm that our algorithms generally create significantly better R-trees than the ones obtained from standard sort-based loading algorithms, even when the query profile is unknown.</p>
<p>【Keywords】:
bulk-loading; dynamic-programming; r-tree; z-curve</p>
<h3 id="271. Efficient logging for enterprise workloads on column-oriented in-memory databases.">271. Efficient logging for enterprise workloads on column-oriented in-memory databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398578">Paper Link</a>】    【Pages】:2085-2089</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wust:Johannes">Johannes Wust</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boese:Joos=Hendrik">Joos-Hendrik Boese</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Renkes:Frank">Frank Renkes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blessing:Sebastian">Sebastian Blessing</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kr=uuml=ger_0003:Jens">Jens Krüger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Plattner:Hasso">Hasso Plattner</a></p>
<p>【Abstract】:
The introduction of a 64 bit address space in commodity operating systems and the constant drop in hardware prices made large capacities of main memory in the order of terabytes technically feasible and economically viable. Especially column-oriented in-memory databases are a promising platform to improve data management for enterprise applications. As in-memory databases hold the primary persistence in volatile memory, some form of recovery mechanism is required to prevent potential data loss in case of failures. Two desirable characteristics of any recovery mechanism are (1) that it has a minimal impact on the running system, and (2) that the system recovers quickly and without any data loss after a failure. This paper introduces an efficient logging mechanism for dictionary-compressed column structures that addresses these two characteristics by (1) reducing the overall log size by writing dictionary-compressed values and (2) allowing for parallel writing and reading of log files. We demonstrate the efficiency of our logging approach by comparing the resulting log-file size with traditional logical logging on a workload produced by a productive enterprise system.</p>
<p>【Keywords】:
column store; databases; in-memory; logging</p>
<h3 id="272. Schema-free structured querying of DBpedia data.">272. Schema-free structured querying of DBpedia data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398579">Paper Link</a>】    【Pages】:2090-2093</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Han:Lushan">Lushan Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Finin:Tim">Tim Finin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Joshi:Anupam">Anupam Joshi</a></p>
<p>【Abstract】:
We need better ways to query large linked data collections such as DBpedia. Using the SPARQL query language requires not only mastering its syntax but also understanding the RDF data model, large ontology vocabularies and URIs for denoting entities. Natural language interface systems address the problem, but are still subjects of research. We describe a compromise in which non-experts specify a graphical query "skeleton" and annotate it with freely chosen words, phrases and entity names. The combination reduces ambiguity and allows the generation of an interpretation that can be translated into SPARQL. Key research contributions are the robust methods that combine statistical association and semantic similarity to map user terms to the most appropriate classes and properties in the underlying ontology.</p>
<p>【Keywords】:
ontology mapping; question answering; schema-free query</p>
<h3 id="273. Discovering conditional inclusion dependencies.">273. Discovering conditional inclusion dependencies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398580">Paper Link</a>】    【Pages】:2094-2098</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bauckmann:Jana">Jana Bauckmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Abedjan:Ziawasch">Ziawasch Abedjan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leser:Ulf">Ulf Leser</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/M=uuml=ller:Heiko">Heiko Müller</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naumann:Felix">Felix Naumann</a></p>
<p>【Abstract】:
Data dependencies are used to improve the quality of a database schema, to optimize queries, and to ensure consistency in a database. Conditional dependencies have been introduced to analyze and improve data quality. A conditional dependency is a dependency with a limited scope defined by conditions over one or more attributes. Only the matching part of the instance must adhere to the dependency. In this paper we focus on conditional inclusion dependencies (CINDs).We generalize the definition of CINDs, distinguishing covering and completeness conditions. We present a new use case for such CINDs showing their value for solving complex data quality tasks. Further, we propose efficient algorithms that identify covering and completeness conditions conforming to given quality thresholds. Our algorithms choose not only the condition values but also the condition attributes automatically. Finally, we show that our approach efficiently provides meaningful and helpful results for our use case.</p>
<p>【Keywords】:
association rule mining; cind; link discovery</p>
<h3 id="274. Diversifying query results on semi-structured data.">274. Diversifying query results on semi-structured data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398581">Paper Link</a>】    【Pages】:2099-2103</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hasan:Mahbub">Mahbub Hasan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mueen:Abdullah">Abdullah Mueen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsotras:Vassilis_J=">Vassilis J. Tsotras</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keogh:Eamonn_J=">Eamonn J. Keogh</a></p>
<p>【Abstract】:
Queries on the web can easily result in a large number of results. Result Diversification, a process by which the query provides the k most diverse set of matches, enables the user to better understand/explore such large results. Computing the diverse subset from a large set of results needs a massive number of pair-wise distance computations as well as finding the subset that maximizes the total pair-wise distance, which is NP-hard and requires efficient approximate algorithm. The problem becomes more difficult when querying semi-structured data, since diversity can occur not only in the document content but also (and more importantly) in the document structure; thus one needs to efficiently measure the structural differences between results. The tree edit distance is the standard choice but, is too expensive for large result sets. Moreover, the generalized tree edit distance ignores the context of the query and also the content of the documents resulting in poor diversification. We present a novel algorithm for meaningful diversification that considers both the structural context of the query and the content of the matched results while computing pair-wise distances. Our algorithm is an order of magnitude faster than the tree edit distance with an elegant worst case guarantee. We also present a novel algorithm that finds the top-k diverse subset of matches in time linear on the size of the result-set. We experimentally demonstrate the utility of our algorithms as a plugin for standard query processors without introducing large error and latency to the output.</p>
<p>【Keywords】:
diversity; semi-structured data; xml</p>
<h3 id="275. LINDA: distributed web-of-data-scale entity matching.">275. LINDA: distributed web-of-data-scale entity matching.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398582">Paper Link</a>】    【Pages】:2104-2108</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/B=ouml=hm_0001:Christoph">Christoph Böhm</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Melo:Gerard_de">Gerard de Melo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naumann:Felix">Felix Naumann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
Linked Data has emerged as a powerful way of interconnecting structured data on the Web. However, the cross-linkage between Linked Data sources is not as extensive as one would hope for. In this paper, we formalize the task of automatically creating "sameAs" links across data sources in a globally consistent manner. Our algorithm, presented in a multi-core as well as a distributed version, achieves this link generation by accounting for joint evidence of a match. Experiments confirm that our system scales beyond 100 million entities and delivers highly accurate results despite the vast heterogeneity and daunting scale.</p>
<p>【Keywords】:
distributed entity matching; entity matching; linked data; mapreduce</p>
<h3 id="276. SliceSort: efficient sorting of hierarchical data.">276. SliceSort: efficient sorting of hierarchical data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398583">Paper Link</a>】    【Pages】:2109-2113</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tran:Quoc_Trung">Quoc Trung Tran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chan:Chee=Yong">Chee-Yong Chan</a></p>
<p>【Abstract】:
Sorting is a fundamental operation in data processing. While the problem of sorting flat data records has been extensively studied, there is very little work on sorting hierarchical data such as XML documents. Existing hierarchy-aware sorting approaches for hierarchical data are based on creating sorted subtrees as initial sorted runs and merging sorted subtrees to create the sorted output using either explicit pointers or absolute node key comparisons for merging subtrees. In this paper, we propose SliceSort, a novel, level-wise sorting technique for hierarchical data that avoids the drawbacks of subtree-based sorting techniques. Our experimental performance evaluation shows that SliceSort outperforms the state-of-art approach, HErMeS, by up to a factor of 27%.</p>
<p>【Keywords】:
hierarchical data; slicesort; sorting</p>
<h3 id="277. Efficient buffer management for piecewise linear representation of multiple data streams.">277. Efficient buffer management for piecewise linear representation of multiple data streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398584">Paper Link</a>】    【Pages】:2114-2118</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Qing">Qing Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jia">Jia Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sharaf:Mohamed_A=">Mohamed A. Sharaf</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Xiaofang">Xiaofang Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pang:Chaoyi">Chaoyi Pang</a></p>
<p>【Abstract】:
Piecewise Linear Representation (PLR) has been a widely used method for approximating data streams in the form of compact line segments. The buffer-based approach to PLR enables a semi-global approximation which relies on the aggregated processing of batches of streamed data so that to adjust and improve the approximation results. However, one challenge towards applying the buffer-based approach is allocating the necessary memory resources for stream buffering. This challenge is further complicated in a multi-stream environment where multiple data streams are competing for the available memory resources, especially in resource-constrained systems such as sensors and mobile devices. In this paper, we address precisely those challenges mentioned above and propose efficient buffer management techniques for the PLR of multiple data streams. In particular, we propose a new dynamic approach called Dynamic Buffer Management with Error Monitoring (DBMEM), which leverages the relationship between the buffer demands of each data stream and its exhibited pattern of data values towards estimating its sufficient buffer size. This enables DBMEM to provide a global buffer allocation strategy that maximizes the overall PLR approximation quality for multiple data streams as shown by our experimental results.</p>
<p>【Keywords】:
data streams; dynamic buffer allocation; plr</p>
<h3 id="278. On skyline groups.">278. On skyline groups.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398585">Paper Link</a>】    【Pages】:2119-2123</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chengkai">Chengkai Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Nan">Nan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hassan:Naeemul">Naeemul Hassan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rajasekaran:Sundaresan">Sundaresan Rajasekaran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Das:Gautam">Gautam Das</a></p>
<p>【Abstract】:
We formulate and investigate the novel problem of finding the skyline k-tuple groups from an n-tuple dataset - i.e., groups of k tuples which are not dominated by any other group of equal size, based on aggregate-based group dominance relationship. The major technical challenge is to identify effective anti-monotonic properties for pruning the search space of skyline groups. To this end, we show that the anti-monotonic property in the well-known Apriori algorithm does not hold for skyline group pruning. We then identify order-specific property which applies to SUM, MIN, and MAX and weak candidate-generation property which applies to MIN and MAX only. Experimental results on both real and synthetic datasets verify that the proposed algorithms achieve orders of magnitude performance gain over a baseline method.</p>
<p>【Keywords】:
anti-monotonic properties; group recommendation; skyline queries</p>
<h3 id="279. Finding the optimal path over multi-cost graphs.">279. Finding the optimal path over multi-cost graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398586">Paper Link</a>】    【Pages】:2124-2128</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Yajun">Yajun Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Jeffrey_Xu">Jeffrey Xu Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Hong">Hong Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jianzhong">Jianzhong Li</a></p>
<p>【Abstract】:
Shortest path query is an important problem in graphs and has been well-studied. However, most approaches for shortest path query are based on single-cost (weight) graphs. In this paper, we introduce the definition of multi-cost graph and study a novel query: the optimal path query over multi-cost graphs. We propose a best-first branch and bound search algorithm with two optimizing strategies. Furthermore, we propose a novel index named k-cluster index to make our method more space and time efficient for large graphs. We discuss how to construct and utilize k-cluster index. We confirm the effectiveness and efficiency of our algorithms using real-life datasets in experiments.</p>
<p>【Keywords】:
multi-cost graphs; non-linear functions; optimal path</p>
<h3 id="280. An efficient index for massive IOT data in cloud environment.">280. An efficient index for massive IOT data in cloud environment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398587">Paper Link</a>】    【Pages】:2129-2133</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Youzhong">Youzhong Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rao:Jia">Jia Rao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Weisong">Weisong Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meng:Xiaofeng">Xiaofeng Meng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Xu">Xu Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yu">Yu Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chai:Yunpeng">Yunpeng Chai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Chunqiu">Chunqiu Liu</a></p>
<p>【Abstract】:
The Internet of Things (IOT) has been widely applied in many fields, while the IOT data are always large volume, update frequently and inherently multi-dimensional, these characteristics bring big challenges to the traditional DBMSs. The traditional DBMSs have rich functionality and can deal with multi-attributes access efficiently, they can not scale good enough to deal with large volume data and can not support high insert throughput. The cloud-based database systems have good scalability, but they don't support multi-dimensional access natively.In order to deal with the large volume of IOT data, we propose an update and query efficient index framework (UQE-Index) based on key-value store that can support high insert throughput and provide efficient multi-dimensional query simultaneously. We implemented a prototype based on HBase and did comprehensive experiments to test our solution's scalability and efficiency.</p>
<p>【Keywords】:
cloud; index; internet of things</p>
<h3 id="281. Clustering Wikipedia infoboxes to discover their types.">281. Clustering Wikipedia infoboxes to discover their types.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398588">Paper Link</a>】    【Pages】:2134-2138</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Thanh_Hoang">Thanh Hoang Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Huong_Dieu">Huong Dieu Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moreira:Viviane">Viviane Moreira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Freire:Juliana">Juliana Freire</a></p>
<p>【Abstract】:
Wikipedia has emerged as an important source of structured information on the Web. But while the success of Wikipedia can be attributed in part to the simplicity of adding and modifying content, this has also created challenges when it comes to using, querying, and integrating the information. Even though authors are encouraged to select appropriate categories and provide infoboxes that follow pre-defined templates, many do not follow the guidelines or follow them loosely. This leads to undesirable effects, such as template duplication, heterogeneity, and schema drift. As a step towards addressing this problem, we propose a new unsupervised approach for clustering Wikipedia infoboxes. Instead of relying on manually assigned categories and template labels, we use the structured information available in infoboxes to group them and infer their entity types. Experiments using over 48,000 infoboxes indicate that our clustering approach is effective and produces high quality clusters.</p>
<p>【Keywords】:
clustering; wikipedia infobox</p>
<h3 id="282. CloST: a hadoop-based storage system for big spatio-temporal data analytics.">282. CloST: a hadoop-based storage system for big spatio-temporal data analytics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398589">Paper Link</a>】    【Pages】:2139-2143</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Haoyu">Haoyu Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Wuman">Wuman Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ni:Lionel_M=">Lionel M. Ni</a></p>
<p>【Abstract】:
During the past decade, various GPS-equipped devices have generated a tremendous amount of data with time and location information, which we refer to as big spatio-temporal data. In this paper, we present the design and implementation of CloST, a scalable big spatio-temporal data storage system to support data analytics using Hadoop. The main objective of CloST is to avoid scan the whole dataset when a spatio-temporal range is given. To this end, we propose a novel data model which has special treatments on three core attributes including an object id, a location and a time. Based on this data model, CloST hierarchically partitions data using all core attributes which enables efficient parallel processing of spatio-temporal range scans. According to the data characteristics, we devise a compact storage structure which reduces the storage size by an order of magnitude. In addition, we proposes scalable bulk loading algorithms capable of incrementally adding new data into the system. We conduct our experiments using a very large GPS log dataset and the results show that CloST has fast data loading speed, desirable scalability in query processing, as well as high data compression ratio.</p>
<p>【Keywords】:
big data; spatio-temporal data; storage system</p>
<h3 id="283. Keyword-based k-nearest neighbor search in spatial databases.">283. Keyword-based k-nearest neighbor search in spatial databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398590">Paper Link</a>】    【Pages】:2144-2148</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Jing">Jing Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Jianhua">Jianhua Feng</a></p>
<p>【Abstract】:
With the ever-increasing number of spatio-textual objects, many applications require to find objects close to a given query point in spatial databases. In this paper, we study the problem of keyword-based k-nearest neighbor search in spatial databases, which, given a query point and a set of keywords, finds k-nearest neighbors of the query point that contain all query keywords. To efficiently answer such queries, we propose a new indexing framework by integrating a spatial component and a textual component, which can efficiently prune search space in terms of both spatial information and textual descriptions. We develop effective index structures and pruning techniques to improve query performance. Experimental results show that our approach significantly outperforms state-of-the-art methods.</p>
<p>【Keywords】:
k-nearest neighbors; space pruning; spatio-textual objects</p>
<h3 id="284. Credibility-based product ranking for C2C transactions.">284. Credibility-based product ranking for C2C transactions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398591">Paper Link</a>】    【Pages】:2149-2153</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Rong">Rong Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sha:Chaofeng">Chaofeng Sha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Minqi">Minqi Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Aoying">Aoying Zhou</a></p>
<p>【Abstract】:
A fundamental issue for C2C transactions is how to rank the products based on the reviews written by the previous customers. In this paper, we present an approach to improve products ranking by tackling the noisy ratings that exist in the practical systems. The first problem is the credibility of the customers. We design an iterative algorithm to measure the customer credibility. In the algorithm, we use a feedback strategy to increase or decrease the customer credibility. We increase the credibility for a customer if the customer gives a high (low) score to a good (bad) product and decrease the value if the customer gives a low (high) score to a good (bad) product. The second problem is the inconsistency between the review comments and scores. To deal with it, we train a classifier on a training data that is constructed automatically. The trained classifier is used to predict the scores of the comments. Finally, we calculate the scores of products by considering the customer credibility and the predicted scores. The experimental results show that our proposed approach provides better products ranking than the baseline systems.</p>
<p>【Keywords】:
clustering; credibility; e-commerce</p>
<h3 id="285. Location selection for utility maximization with capacity constraints.">285. Location selection for utility maximization with capacity constraints.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398592">Paper Link</a>】    【Pages】:2154-2158</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Yu">Yu Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jin">Jin Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yueguo">Yueguo Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Rui">Rui Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Du:Xiaoyong">Xiaoyong Du</a></p>
<p>【Abstract】:
Given a set of client locations, a set of facility locations where each facility has a service capacity, and the assumptions that: (i) a client seeks service from its nearest facility; (ii) a facility provides service to clients in the order of their proximity, we study the problem of selecting all possible locations such that setting up a new facility with a given capacity at these locations will maximize the number of served clients. This problem has wide applications in practice, such as setting up new distribution centers for online sales business and building additional base stations for mobile subscribers. We formulate the problem as location selection query for utility maximization. After applying three pruning rules to a baseline solution,we obtain an efficient algorithm to answer the query. Extensive experiments confirm the efficiency of our proposed algorithm.</p>
<p>【Keywords】:
capacity constraints; location selection</p>
<h3 id="286. Efficient estimation of dynamic density functions with an application to outlier detection.">286. Efficient estimation of dynamic density functions with an application to outlier detection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398593">Paper Link</a>】    【Pages】:2159-2163</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qahtan:Abdulhakim_Ali">Abdulhakim Ali Qahtan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiangliang">Xiangliang Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Suojin">Suojin Wang</a></p>
<p>【Abstract】:
In this paper, we propose a new method to estimate the dynamic density over data streams, named KDE-Track as it is based on a conventional and widely used Kernel Density Estimation (KDE) method. KDE-Track can efficiently estimate the density with linear complexity by using interpolation on a kernel model, which is incrementally updated upon the arrival of streaming data. Both theoretical analysis and experimental validation show that KDE-Track outperforms traditional KDE and a baseline method Cluster-Kernels on estimation accuracy of the complex density structures in data streams, computing time and memory usage. KDE-Track is also demonstrated on timely catching the dynamic density of synthetic and real-world data. In addition, KDE-Track is used to accurately detect outliers in sensor data and compared with two existing methods developed for detecting outliers and cleaning sensor data.</p>
<p>【Keywords】:
data streams; density estimation; interpolation; outlier detection</p>
<h3 id="287. A positional access method for relational databases.">287. A positional access method for relational databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398594">Paper Link</a>】    【Pages】:2164-2168</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Dongzhe">Dongzhe Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Jianhua">Jianhua Feng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a></p>
<p>【Abstract】:
Most commercial database management systems sort tuples of a relation by their primary keys for the purpose of supporting efficient insertions, deletions, and updates. However, primary keys are usually auto-generated integers, which bear little useful information about user data. Secondary indexes have to be created sometimes to help retrieve tuples by columns other than the primary key. Evidently, a better solution is to sort the data by columns that appear frequently in retrieval conditions. Unfortunately, this method does not work, at least not immediately, when the relation is vertically partitioned, which is a popular technique to reduce I/O overhead, since it is difficult to keep tuples of two partitions in exactly the same order unless the sorting columns are replicated, which again wastes storage space and disk bandwidth unnecessarily. In this paper, we introduce a positional access method that allows a partition to be sorted by another one but incurs little storage overhead and provide details about how to improve its performance.</p>
<p>【Keywords】:
positional access method; vertical partitioning</p>
<h3 id="288. Real-time aggregate monitoring with differential privacy.">288. Real-time aggregate monitoring with differential privacy.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398595">Paper Link</a>】    【Pages】:2169-2173</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fan:Liyue">Liyue Fan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiong:Li">Li Xiong</a></p>
<p>【Abstract】:
Sharing real-time aggregate statistics of private data has given much benefit to the public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, an adaptive system to release real-time aggregate statistics under differential privacy with improved utility. To minimize overall privacy cost, FAST adaptively samples long time-series according to detected data dynamics. To improve the accuracy of data release per time stamp, filtering is used to predict data values at non-sampling points and to estimate true values from noisy observations at sampling points. Our experiments with three real data sets confirm that FAST improves the accuracy of time-series release and has excellent performance even under very small privacy cost.</p>
<p>【Keywords】:
differential privacy; estimation; sampling; time series</p>
<h3 id="289. Efficient distributed locality sensitive hashing.">289. Efficient distributed locality sensitive hashing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398596">Paper Link</a>】    【Pages】:2174-2178</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bahmani:Bahman">Bahman Bahmani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goel:Ashish">Ashish Goel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shinde:Rajendra">Rajendra Shinde</a></p>
<p>【Abstract】:
Distributed frameworks are gaining increasingly widespread use in applications that process large amounts of data. One important example application is large scale similarity search, for which Locality Sensitive Hashing (LSH) has emerged as the method of choice, specially when the data is high-dimensional. To guarantee high search quality, the LSH scheme needs a rather large number of hash tables. This entails a large space requirement, and in the distributed setting, with each query requiring a network call per hash bucket look up, also a big network load. Panigrahy's Entropy LSH scheme significantly reduces the space requirement but does not help with (and in fact worsens) the search network efficiency. In this paper, focusing on the Euclidian space under ι2 norm and building up on Entropy LSH, we propose the distributed Layered LSH scheme, and prove that it exponentially decreases the network cost, while maintaining a good load balance between different machines. Our experiments also verify that our theoretical results.</p>
<p>【Keywords】:
distributed systems; locality sensitive hashing; mapreduce; similarity search</p>
<h3 id="290. Author-conference topic-connection model for academic network search.">290. Author-conference topic-connection model for academic network search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398597">Paper Link</a>】    【Pages】:2179-2183</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jianwen">Jianwen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xiaohua">Xiaohua Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tu:Xinhui">Xinhui Tu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Tingting">Tingting He</a></p>
<p>【Abstract】:
This paper proposes a novel topic model, Author-Conference Topic-Connection (ACTC) Model for academic network search. The ACTC Model extends the author-conference-topic (ACT) model by adding subject of the conference and the latent mapping information between subjects and topics. It simultaneously models topical aspects of papers, authors and conferences with two latent topic layers: a subject layer corresponding to conference topic, and a topic layer corresponding to the word topic. Each author would be associated with a multinomial distribution over subjects of conference (eg., KM, DB, IR for CIKM 2012), the conference(CIKM 2012), and the topics are respectively generated from a sampled subject. Then the words are generated from the sampled topics. We conduct experiments on a data set with 8,523 authors, 22,487 papers and 1,243 conferences from the well-known Arnetminer website, and train the model with different number of subjects and topics. For a qualitative evaluation, we compare ACTC with three others models LDA, Author-Topic (AT) and ACT in academic search services. Experiments show that ACTC can effectively capture the semantic connection between different types of information in academic network and perform well in expert searching and conference searching.</p>
<p>【Keywords】:
academic network search; gibbs sampling; topic model</p>
<h3 id="291. Impact neighborhood indexing (INI) in diffusion graphs.">291. Impact neighborhood indexing (INI) in diffusion graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398598">Paper Link</a>】    【Pages】:2184-2188</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Jung_Hyun">Jung Hyun Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Candan:K=_Sel=ccedil=uk">K. Selçuk Candan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sapino:Maria_Luisa">Maria Luisa Sapino</a></p>
<p>【Abstract】:
A graph neighborhood consists of a set of nodes that are nearby or otherwise related to each other. While existing definitions consider the structure (or topology) of the graph, we note that they fail to take into account the information propagation and diffusion characteristics, such as decay and reinforcement, common in many networks. In this paper, we first define the propagation efficiency of nodes and edges. We use this to introduce the novel concept of zero-erasure (or impact) neighborhood (ZEN) of a given node, n, consisting of the set of nodes that receive information from (or are impacted by) n without any decay. Based on this, we present an impact neighborhood indexing (INI) algorithm that creates data structures to help quickly identify impact neighborhood of any given node. Experiment results confirm the efficiency and effectiveness of the proposed INI algorithms.</p>
<p>【Keywords】:
graph neighborhood; impact propagation; indexing</p>
<h3 id="292. Loyalty-based selection: retrieving objects that persistently satisfy criteria.">292. Loyalty-based selection: retrieving objects that persistently satisfy criteria.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398599">Paper Link</a>】    【Pages】:2189-2193</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Zhitao">Zhitao Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheema:Muhammad_Aamir">Muhammad Aamir Cheema</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Xuemin">Xuemin Lin</a></p>
<p>【Abstract】:
A traditional query returns a set of objects that satisfy user defined criteria at the time query was issued. The results are based on the values of objects at query time and may be affected by outliers. Intuitively, an object better meets the user's needs if it persistently satisfies the criteria, i.e., it satisfies the criteria for majority of the time in the past T time units. In this paper, we propose a measure named loyalty that reflects how persistently an object satisfies the criteria. Formally, the loyalty of an object is the total time (in past T time units) it satisfies the query criteria. In this paper, we study top-k loyalty queries over sliding windows that continuously report k objects with the highest loyalties. Each object issues an update when it starts satisfying the criteria or when it stops satisfying the criteria. We show that the lower bound cost of updating the results of a top-k loyalty query is O(logN), for each object update, where N is the number of updates issued in last T time units. We conduct a detailed complexity analysis and show that our proposed algorithm is optimal. Moreover, effective pruning techniques are proposed to improve the efficiency. We experimentally verify the effectiveness of the proposed approach by comparing it with a classic sweep line algorithm.</p>
<p>【Keywords】:
data streams; loyalty queries; temporal data</p>
<h3 id="293. Star-Join: spatio-textual similarity join.">293. Star-Join: spatio-textual similarity join.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398600">Paper Link</a>】    【Pages】:2194-2198</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Sitong">Sitong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Jianhua">Jianhua Feng</a></p>
<p>【Abstract】:
Location-based services have attracted significant attention due to modern mobile phones equipped with GPS devices. These services generate large amounts of spatio-textual data which contain both spatial location and textual descriptions. Since a spatio-textual object may have different representations, possibly because of deviations of GPS or different user descriptions, it calls for efficient methods to integrate spatio-textual data from different sources. In this paper we study a new research problem called spatio-textual similarity join: given two sets of spatio-textual objects, we find the similar object pairs. To the best of our knowledge, we are the first to study this problem. We make the following contributions: (1) We develop a filter-and-refine framework and devise several efficient algorithms. We first generate spatial and textual signatures for the objects and build inverted index on top of these signatures. Then we generate candidate pairs using the inverted lists of signatures. Finally we refine the candidates and generate the final result. (2) We study how to generate high-quality signatures for spatial information. We develop an MBR-prefix based signature to prune large numbers of dissimilar object pairs. (3) Experimental results on real and synthetic datasets show that our algorithms achieve high performance and scale well.</p>
<p>【Keywords】:
mbr-prefix; similarity join; spatio-textual</p>
<h3 id="294. Adapt: adaptive database schema design for multi-tenant applications.">294. Adapt: adaptive database schema design for multi-tenant applications.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398601">Paper Link</a>】    【Pages】:2199-2203</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Ni:Jiacai">Jiacai Ni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guoliang">Guoliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Jun">Jun Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feng:Jianhua">Jianhua Feng</a></p>
<p>【Abstract】:
Multi-tenant data management is a major application of software as a Service (SaaS). Many companies outsource their data to a third party which hosts a multi-tenant database system to provide data management service. The system should have high performance, low space and excellent scalability. One big challenge is to devise a high-quality database schema. Independent Tables Shared Instances and Shared Tables Shared Instances are two state-of-the-art methods. However, the former has poor scalability, while the latter achieves good scalability at the expense of poor performance and high space overhead. In this paper, we trade-off between the two methods and propose an adaptive database schema design approach to achieve good scalability and high performance with low space. To this end, we identify the important attributes and use them to generate a base table. For other attributes, we construct supplementary tables. We propose a cost-based model to adaptively generate the tables above. Our method has the following advantages. First, our method achieves high scalability. Second, our method can trade-off performance and space requirement. Third, our method can be easily applied to existing databases (e.g., MySQL) with minor revisions. Fourth, our method can adapt to any schemas and query workloads. Experimental results show our method achieves high performance and good scalability with low space and outperforms state-of-the-art method.</p>
<p>【Keywords】:
adaptive schema; multi-tenant database</p>
<h3 id="295. Optimizing data migration for cloud-based key-value stores.">295. Optimizing data migration for cloud-based key-value stores.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398602">Paper Link</a>】    【Pages】:2204-2208</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qin:Xiulei">Xiulei Qin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Wenbo">Wenbo Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0049:Wei">Wei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Jun">Jun Wei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Xin">Xin Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Tao">Tao Huang</a></p>
<p>【Abstract】:
As one database offloading strategy, elastic key-value stores are often introduced to speed up the application performance with dynamic scalability. Since the workload is varied, efficient data migration with minimal impact in service is critical for the issue of elasticity and scalability. However, due to the new virtualization technology, real-time and low-latency requirements, data migration within cloud-based key-value stores has to face new challenges: effects of VM interference, and the need to trade off between the two ingredients of migration cost, namely migration time and performance impact. To fulfill these challenges, in this paper we explore a new approach to optimize the data migration. Explicitly, we build two interference-aware models to predict the migration time and performance impact for each migration action using statistical machine learning, and then create a cost model to strike a balance between the two ingredients. Using the load rebalancing scenario as a case study, we have designed one cost-aware migration algorithm that utilizes the cost model to guide the choice of possible migration actions. Finally, we demonstrate the effectiveness of the approach using Yahoo! Cloud Serving Benchmark (YCSB).</p>
<p>【Keywords】:
data migration; key-value store; migration cost; vm interference</p>
<h3 id="296. Applying weighted queries on probabilistic databases.">296. Applying weighted queries on probabilistic databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398603">Paper Link</a>】    【Pages】:2209-2213</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lehrack:Sebastian">Sebastian Lehrack</a></p>
<p>【Abstract】:
Relational queries applied on probabilistic databases have been established as a powerful tool for accessing huge data sets of uncertain data. Often various parts of such queries have different significances for a specific user. Thus, a query language should allow us to give subqueries different weights to quantify the individual user preferences. In this work we introduce a theoretical foundation for weighted algebra operators on probabilistic databases within a SQL-like query language.</p>
<p>【Keywords】:
probabilistic database; proqua; weighted query; weighting</p>
<h3 id="297. A new tool for multi-level partitioning in teradata.">297. A new tool for multi-level partitioning in teradata.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398604">Paper Link</a>】    【Pages】:2214-2218</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Suh:Young=Kyoon">Young-Kyoon Suh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghazal:Ahmad">Ahmad Ghazal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crolotte:Alain">Alain Crolotte</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kostamaa:Pekka">Pekka Kostamaa</a></p>
<p>【Abstract】:
This paper introduces a new tool that recommends an optimized partitioning solution called Multi-Level Partitioned Primary Index (MLPPI) for a fact table based on the queries in the workload. The tool implements a new technique using a greedy algorithm for search space enumeration. The space is driven by predicates in the queries. This technique fits very well the Teradata MLPPI scheme, as it is based on a general framework using general expressions, ranges and case expressions for partition definitions. The cost model implemented in the tool is based on the Teradata optimizer, and it is used to prune the search space for reaching a final solution. The tool resides completely on the client, and interfaces the database through APIs as opposed to previous work that requires optimizer code extension. The APIs are used to simplify the workload queries, and to capture fact table predicates and costs necessary to make the recommendation. The predicate-driven method implemented by the tool is general, and it can be applied to any clustering or partitioning scheme based on simple field expressions or complex SQL predicates. Experimental results given a particular workload will show that the recommendation from the tool outperforms a human expert. The experiments also show that the solution is scalable both with the workload complexity and the size of the fact table.</p>
<p>【Keywords】:
fact table; multi-level partitioning; star schema</p>
<h3 id="298. Fast PCA computation in a DBMS with aggregate UDFs and LAPACK.">298. Fast PCA computation in a DBMS with aggregate UDFs and LAPACK.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398605">Paper Link</a>】    【Pages】:2219-2223</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Ordonez:Carlos">Carlos Ordonez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mohanam:Naveen">Naveen Mohanam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Garcia=Alvarado:Carlos">Carlos Garcia-Alvarado</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tosic:Predrag_T=">Predrag T. Tosic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Martinez:Edgar">Edgar Martinez</a></p>
<p>【Abstract】:
Efficient and scalable execution of numerical methods inside a DBMS is difficult as its architecture is not suited for intense numerical computations. We study computing Principal Component Analysis (PCA) on large data sets via Singular Value Decomposition (SVD). Given the difficulty to program and optimize numerical methods on an existing DBMS, we explore an alternative reusability approach: calling the well-known numerical library LAPACK. Thus we study several alternatives to summarize the data set with aggregate User-Defined Functions (UDFs) and how to efficiently call SVD numerical methods available in LAPACK via Stored Procedures (SPs). We propose algorithmic and system optimizations to enhance scalability and to push processing into RAM. We show it is feasible to efficiently solve PCA by first summarizing the data set with arrays incrementally updated with aggregate UDFs and then pushing heavy matrix processing in SVD to RAM calling LAPACK via SPs. We benchmark our solution on a modern DBMS. Our solution requires only one pass on the data set and it exhibits linear scalability.</p>
<p>【Keywords】:
big data; lapack; linear algebra; numerical methods; sql</p>
<h3 id="299. Scaling multiple-source entity resolution using statistically efficient transfer learning.">299. Scaling multiple-source entity resolution using statistically efficient transfer learning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398606">Paper Link</a>】    【Pages】:2224-2228</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Negahban:Sahand">Sahand Negahban</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rubinstein:Benjamin_I=_P=">Benjamin I. P. Rubinstein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gemmell:Jim">Jim Gemmell</a></p>
<p>【Abstract】:
We consider a serious, previously-unexplored challenge facing almost all approaches to scaling up entity resolution (ER) to multiple data sources: the prohibitive cost of labeling training data for supervised learning of similarity scores for each pair of sources. While there exists a rich literature describing almost all aspects of pairwise ER, this new challenge is arising now due to the unprecedented ability to acquire and store data from online sources, interest in features driven by ER such as enriched search verticals, and the uniqueness of noisy and missing data characteristics for each source. We show on real-world and synthetic data that for state-of-the-art techniques, the reality of heterogeneous sources means that the number of labeled training data must scale quadratically in the number of sources, just to maintain constant precision/recall. We address this challenge with a brand new transfer learning algorithm which requires far less training data (or equivalently, achieves superior accuracy with the same data) and is trained using fast convex optimization. The intuition behind our approach is to adaptively share structure learned about one scoring problem with all other scoring problems sharing a data source in common. We demonstrate that our theoretically-motivated approach improves upon existing techniques for multi-source ER.</p>
<p>【Keywords】:
entity resolution; multi-task learning; transfer learning</p>
<h3 id="300. A probabilistic approach to correlation queries in uncertain time series data.">300. A probabilistic approach to correlation queries in uncertain time series data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398607">Paper Link</a>】    【Pages】:2229-2233</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Orang:Mahsa">Mahsa Orang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shiri:Nematollaah">Nematollaah Shiri</a></p>
<p>【Abstract】:
Numerous real-life applications, such as wireless sensor networks and location-based services, generate large amount of uncertain time series, where the exact value at each timestamp is unavailable or unknown. In this paper, we formalize the notion of correlation for uncertain time series data and consider a family of probabilistic, threshold-based correlation queries over such data. The proposed formulation extends the notion of correlation developed for standard, certain time series. We show that uncertain correlation is a random variable approaching normal distribution. We also formalize the notion of uncertain time series normalization which is at the core of our correlation query processing approach, while it proves to be an important pre-processing technique in particular for pattern discovery tasks. The results of our numerous experiments indicate that, unlike in the standard time series, there is a trade-off between false alarms and hit ratios, which can be controlled by the probability threshold provided by users. Our results also offer users a guideline for choosing proper threshold values.</p>
<p>【Keywords】:
correlation; probabilistic queries; uncertain time series</p>
<h3 id="301. On bundle configuration for viral marketing in social networks.">301. On bundle configuration for viral marketing in social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398608">Paper Link</a>】    【Pages】:2234-2238</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:De=Nian">De-Nian Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chia:Nai=Hui">Nai-Hui Chia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Mao">Mao Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hung:Hui=Ju">Hui-Ju Hung</a></p>
<p>【Abstract】:
Prior research on viral marketing mostly focuses on promoting one single product item. In this work, we explore the idea of bundling multiple items for viral marketing and formulate a new research problem, called Bundle Configuration for SpreAd Maximization (BCSAM). Efficiently obtaining an optimal product bundle under the setting of BCSAM is very challenging. Aiming to strike a balance between the quality of solution and the computational overhead, we systematically explore various heuristics to develop a suite of algorithms, including κ-Bundle Configuration and Aggregated Bundle Configuration. Moreover, we integrate all the proposed ideas into one efficient algorithm, called Aggregated Bundle Configuration (ABC). Finally, we conduct an extensive performance evaluation on our proposals. Experimental results show that ABC significantly outperforms its counterpart and two baseline approaches in terms of both computational overhead and bundle quality.</p>
<p>【Keywords】:
personal preference; product bundling; viral marketing</p>
<h2 id="Knowledge management poster session    40">Knowledge management poster session    40</h2>
<h3 id="302. Learning to rank for hybrid recommendation.">302. Learning to rank for hybrid recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398610">Paper Link</a>】    【Pages】:2239-2242</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Jiankai">Jiankai Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Jun">Jun Ma</a></p>
<p>【Abstract】:
Most existing recommender systems can be classified into two categories: collaborative filtering and content-based filtering. Hybrid recommender systems combine the advantages of the two for improved recommendation performance. Traditional recommender systems are rating-based. However, predicting ratings is an intermediate step towards their ultimate goal of generating rankings or recommendation lists. Learning to rank is an established means of predicting rankings and has recently demonstrated high promise in improving quality of recommendations. In this paper, we propose LRHR, the first attempt that adapts learning to rank to hybrid recommender systems. LRHR first defines novel representations for both users and items so that they can be content-comparable. Then, LRHR identifies a set of novel meta-level features for learning purposes. Finally, LRHR adopts RankSVM, a pairwise learning to rank algorithm, to generate recommendation lists of items for users. Extensive experiments on benchmarks in comparison with the state-of-the-art algorithms demonstrate the performance gain of our approach.</p>
<p>【Keywords】:
collaborative filtering; features; learning to rank; recommender systems</p>
<h3 id="303. Importance weighted passive learning.">303. Importance weighted passive learning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398611">Paper Link</a>】    【Pages】:2243-2246</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xi:Xiaoming">Xiaoming Xi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Yilong">Yilong Yin</a></p>
<p>【Abstract】:
Importance weighted active learning (IWAL) introduces a weighting scheme to measure the importance of each instance for correcting the sampling bias of the probability distributions between training and test datasets. However, the weighting scheme of IWAL involves the distribution of the test data, which can be straightforwardly estimated in active learning by interactively querying users for labels of selected test instances, but difficult for conventional learning where there are no interactions with users, referred as passive learning. In this paper, we investigate the insufficient sampling bias problem, i.e., bias occurs only because of insufficient samples, but the sampling process is unbiased. In doing this, we present two assumptions on the sampling bias, based on which we propose a practical weighting scheme for the empirical loss function in conventional passive learning, and present IWPL, an importance weighted passive learning framework. Furthermore, we provide IWSVM, an importance weighted SVM for validation. Extensive experiments demonstrate significant advantages of IWSVM on benchmarks and synthetic datasets.</p>
<p>【Keywords】:
classification; discounted confidence; learning with confidence</p>
<h3 id="304. A tag-centric discriminative model for web objects classification.">304. A tag-centric discriminative model for web objects classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398612">Paper Link</a>】    【Pages】:2247-2250</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yao:Lina">Lina Yao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sheng:Quan_Z=">Quan Z. Sheng</a></p>
<p>【Abstract】:
This paper studies web object classification problem with the novel exploration of social tags. More and more web objects are increasingly annotated with human interpretable labels (i.e., tags), which can be considered as an auxiliary attribute to assist the object classification. Automatically classifying web objects into manageable semantic categories has long been a fundamental pre-process for indexing, browsing, searching, and mining heterogeneous web objects. However, such heterogeneous web objects often suffer from a lack of easy-extractable and uniform descriptive features. In this paper, we propose a discriminative tag-centric model for web object classification by jointly modeling the objects category labels and their corresponding social tags and un-coding the relevance among social tags. Our approach is based on recent techniques for learning large-scale discriminative models. We conduct experiments to validate our approach using real-life data. The results show the feasibility and good performance of our approach.</p>
<p>【Keywords】:
optimization; semantic annotation; social tagging; web objects classification</p>
<h3 id="305. Outlier detection using centrality and center-proximity.">305. Outlier detection using centrality and center-proximity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398613">Paper Link</a>】    【Pages】:2251-2254</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bae:Duck=Ho">Duck-Ho Bae</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jeong:Seo">Seo Jeong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Sang=Wook">Sang-Wook Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Minsoo">Minsoo Lee</a></p>
<p>【Abstract】:
An outlier is an object that is considerably dissimilar with the remainder of the dataset. In this paper, we first propose the notion of centrality and center-proximity as novel outlierness measures which can be considered to represent the characteristics of all of the objects in the dataset. We then propose a graph-based outlier detection method which can solve the problems of local density, micro-cluster, and fringe objects. Finally, through extensive experiments, we show the effectiveness of the proposed method.</p>
<p>【Keywords】:
center-proximity; centrality; graph-based outlier detection</p>
<h3 id="306. An effective category classification method based on a language model for question category recommendation on a cQA service.">306. An effective category classification method based on a language model for question category recommendation on a cQA service.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398614">Paper Link</a>】    【Pages】:2255-2258</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bae:Kyoungman">Kyoungman Bae</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Ko:Youngjoong">Youngjoong Ko</a></p>
<p>【Abstract】:
Classiying user's question into several topics helps respondents answering the question in a cQA service. The word weighting method must estimate the appropriate weight of a word to improve the category (or topic) classification. In this paper, we propose a novel effective word weighting method based on a language model for automatic category classification in the cQA service. We first calculate the occurrence probability of a word in each category by using a language model and then the final weight of each word is estimated by ratio of the occurrence probability of the word on a category to the occurrence probability of the word on the other categories. As a result, the proposed method significantly improves the performance of the category classification.</p>
<p>【Keywords】:
cQA service; category (or topic) classification; category (or topic) recommendation; language model; word weighting</p>
<h3 id="307. Clustering short text using Ncut-weighted non-negative matrix factorization.">307. Clustering short text using Ncut-weighted non-negative matrix factorization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398615">Paper Link</a>】    【Pages】:2259-2262</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Xiaohui">Xiaohui Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Shenghua">Shenghua Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yanfeng">Yanfeng Wang</a></p>
<p>【Abstract】:
Non-negative matrix factorization (NMF) has been successfully applied in document clustering. However, experiments on short texts, such as microblogs, Q&amp;A documents and news titles, suggest unsatisfactory performance of NMF. An major reason is that the traditional term weighting schemes, like binary weight and tfidf, cannot well capture the terms' discriminative power and importance in short texts, due to the sparsity of data. To tackle this problem, we proposed a novel term weighting scheme for NMF, derived from the Normalized Cut (Ncut) problem on the term affinity graph. Different from idf, which emphasizes discriminability on document level, the Ncut weighting measures terms' discriminability on term level. Experiments on two data sets show our weighting scheme significantly boosts NMF's performance on short text clustering.</p>
<p>【Keywords】:
NMF; clustering; normalized cut; short text</p>
<h3 id="308. Polygene-based evolution: a novel framework for evolutionary algorithms.">308. Polygene-based evolution: a novel framework for evolutionary algorithms.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398616">Paper Link</a>】    【Pages】:2263-2266</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuangling">Shuangling Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Guibao">Guibao Cao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Yilong">Yilong Yin</a></p>
<p>【Abstract】:
In this paper, we introduce polygene-based evolution, a novel framework for evolutionary algorithms (EAs) that features distinctive operations in the evolution process. In traditional EAs, the primitive evolution unit is gene, where genes are independent components during evolution. In polygene-based evolutionary algorithms (PGEAs), the evolution unit is polygene, i.e., a set of co-regulated genes. Discovering and maintaining quality polygenes can play an effective role in evolving quality individuals. Polygenes generalize genes, and PGEAs generalize EAs. Implementing the PGEA framework involves three phases: polygene discovery, polygene planting, and polygene-compatible evolution. Extensive experiments on function optimization benchmarks in comparison with the conventional and state-of-the-art EAs demonstrate the potential of the approach in accuracy and efficiency improvement.</p>
<p>【Keywords】:
data mining; evolutionary algorithms; optimization; polygene</p>
<h3 id="309. A tensor encoding model for semantic processing.">309. A tensor encoding model for semantic processing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398617">Paper Link</a>】    【Pages】:2267-2270</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Symonds:Michael">Michael Symonds</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bruza:Peter_D=">Peter D. Bruza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sitbon:Laurianne">Laurianne Sitbon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Turner:Ian">Ian Turner</a></p>
<p>【Abstract】:
This paper develops and evaluates an enhanced corpus based approach for semantic processing. Corpus based models that build representations of words directly from text do not require pre-existing linguistic knowledge, and have demonstrated psychologically relevant performance on a number of cognitive tasks. However, they have been criticised in the past for not incorporating sufficient structural information. Using ideas underpinning recent attempts to overcome this weakness, we develop an enhanced tensor encoding model to build representations of word meaning for semantic processing. Our enhanced model demonstrates superior performance when compared to a robust baseline model on a number of semantic processing tasks.</p>
<p>【Keywords】:
semantics; tensor encoding</p>
<h3 id="310. Accelerating locality preserving nonnegative matrix factorization.">310. Accelerating locality preserving nonnegative matrix factorization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398618">Paper Link</a>】    【Pages】:2271-2274</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yao:Guanhong">Guanhong Yao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Deng">Deng Cai</a></p>
<p>【Abstract】:
Matrix factorization techniques have been frequently applied in information retrieval, computer vision and pattern recognition. Among them, Non-negative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in the human brain. Locality Preserving Non-negative Matrix Factorization (LPNMF) is a recently proposed graph-based NMF extension which tries to preserves the intrinsic geometric structure of the data. Compared with the original NMF, LPNMF has more discriminating power on data representa- tion thanks to its geometrical interpretation and outstanding ability to discover the hidden topics. However, the computa- tional complexity of LPNMF is O(n3), where n is the number of samples. In this paper, we propose a novel approach called Accelerated LPNMF (A-LPNMF) to solve the com- putational issue of LPNMF. Specifically, A-LPNMF selects p (p j n) landmark points from the data and represents all the samples as the sparse linear combination of these landmarks. The non-negative factors which incorporates the geometric structure can then be efficiently computed. Experimental results on the real data sets demonstrate the effectiveness and efficiency of our proposed method.</p>
<p>【Keywords】:
non-negative matrix factorization; speedup</p>
<h3 id="311. The twitaholic next door.: scalable friend recommender system using a concept-sensitive hash function.">311. The twitaholic next door.: scalable friend recommender system using a concept-sensitive hash function.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398619">Paper Link</a>】    【Pages】:2275-2278</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bamba:Patrick">Patrick Bamba</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Subercaze:Julien">Julien Subercaze</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gravier:Christophe">Christophe Gravier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Benmira:Nabil">Nabil Benmira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fontaine:Jimi">Jimi Fontaine</a></p>
<p>【Abstract】:
In this paper we present a Friend Recommender System for micro-blogging. Traditional batch processing of massive amounts of data makes it difficult to provide a near-real time friend recommender system or even a system that can properly scale to millions of users. In order to overcome these issues, we have designed a solution that represents user-generated micro posts as a set of pseudo-cliques. These graphs are assigned a hash value using an original Concept-Sensitive Hash function, a new sub-kind of Locally-Sensitive Hash functions. Finally, since the user profiles are represented as a binary footprint, the pairwise comparison of footprints using the Hamming distance provides scalability to the recommender system. The paper goes with an online application relying on a large Twitter dataset, so that the reader can freely experiment the system.</p>
<p>【Keywords】:
friends; graph; locally-sensitive hash; pseudo-clique; recommender system; social networks; twitter</p>
<h3 id="312. Information propagation in social rating networks.">312. Information propagation in social rating networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398620">Paper Link</a>】    【Pages】:2279-2282</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Garg:Priyanka">Priyanka Garg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/King:Irwin">Irwin King</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lyu:Michael_R=">Michael R. Lyu</a></p>
<p>【Abstract】:
The polarity of opinion is a crucial part of information and ignoring the asymmetry between them, can potentially result in an inaccurate estimation of the number of product adoptions and incorrect recommendations. We analyze the propagation patterns of the negative and positive opinions on two real world datasets, Flixster and Epinions, and observe that the presence of negative opinions significantly reduces the number of expressed opinions. To account for the asymmetry between the two kind of opinions, we propose extensions of the two most popular information propagation models, Independent Cascade and Linear Threshold models. The proposed extensions give a tractable influence problem and improves the prediction accuracy of future opinions, by more than 3% on Flixster and 5% on Epinions datasets.</p>
<p>【Keywords】:
information flow; negative opinions; social networks</p>
<h3 id="313. Maximizing revenue from strategic recommendations under decaying trust.">313. Maximizing revenue from strategic recommendations under decaying trust.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398621">Paper Link</a>】    【Pages】:2283-2286</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/D=uuml=tting:Paul">Paul Dütting</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Henzinger:Monika">Monika Henzinger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Ingmar">Ingmar Weber</a></p>
<p>【Abstract】:
Suppose your sole interest in recommending a product to me is to maximize the amount paid to you by the seller for a sequence of recommendations. How should you recommend optimally if I become more inclined to ignore you with each irrelevant recommendation you make? Finding an answer to this question is a key challenge in all forms of marketing that rely on and explore social ties; ranging from personal recommendations to viral marketing. We prove that even if the recommendee regains her initial trust on each successful recommendation, the expected revenue the recommender can make over an infinite period due to payments by the seller is bounded. This can only be overcome when the recommendee also incrementally regains trust during periods without any recommendation. Here, we see a connection to "banner blindness," suggesting that showing fewer ads can lead to a higher long-term revenue.</p>
<p>【Keywords】:
banner blindness; recommendations; trust loss in advertising</p>
<h3 id="314. Weighted linear kernel with tree transformed features for malware detection.">314. Weighted linear kernel with tree transformed features for malware detection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398622">Paper Link</a>】    【Pages】:2287-2290</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Comar:Prakash_Mandayam">Prakash Mandayam Comar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Lei">Lei Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saha:Sabyasachi">Sabyasachi Saha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nucci:Antonio">Antonio Nucci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Pang=Ning">Pang-Ning Tan</a></p>
<p>【Abstract】:
Malware detection from network traffic flows is a challenging problem due to data irregularity issues such as imbalanced class distribution, noise, missing values, and heterogeneous types of features. To address these challenges, this paper presents a two-stage classification approach for malware detection. The framework initially employs random forest as a macro-level classifier to separate the malicious from non-malicious network flows, followed by a collection of one-class support vector machine classifiers to identify the specific type of malware. A novel tree-based feature construction approach is proposed to deal with data imperfection issues. As the performance of the support vector machine classifier often depends on the kernel function used to compute the similarity between every pair of data points, designing an appropriate kernel is essential for accurate identification of malware classes. We present a simple algorithm to construct a weighted linear kernel on the tree transformed features and demonstrate its effectiveness in detecting malware from real network traffic data.</p>
<p>【Keywords】:
feature construction; kernels; malware detection</p>
<h3 id="315. Learning to predict the cost-per-click for your ad words.">315. Learning to predict the cost-per-click for your ad words.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398623">Paper Link</a>】    【Pages】:2291-2294</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Chieh=Jen">Chieh-Jen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Hsin=Hsi">Hsin-Hsi Chen</a></p>
<p>【Abstract】:
In Internet ad campaign, ranking of an ad on search result pages depends on a cost-per-click (CPC) of ad words offered by an advertiser and a quality score estimated by a search engine. Bidding for ad words with a higher CPC is more competitive than bidding for the same ad words with a lower CPC in the ad ranking competition. However, offering a higher CPC will increase a burden on advertisers. In contrast, offering a lower CPC may decrease the exposure rate of their ads. Thus, how to select an appropriate CPC for ad words is indispensable for advertisers. In this paper, we extract different semantic levels of features, such as named entities, topic terminologies, and individual words from a large-scale real-world ad words corpus, and explore various learning based prediction algorithms. The thorough experimental results show that the CPC prediction models considering more ad words semantics achieve better prediction performance, and the prediction model using the support vector regression (SVR) and features from all semantic levels performs the best.</p>
<p>【Keywords】:
CPC prediction; ad ranking; search engine optimization</p>
<h3 id="316. Dual word and document seed selection for semi-supervised sentiment classification.">316. Dual word and document seed selection for semi-supervised sentiment classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398624">Paper Link</a>】    【Pages】:2295-2298</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Ju:Shengfeng">Shengfeng Ju</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Shoushan">Shoushan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Yan">Yan Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaojun">Xiaojun Li</a></p>
<p>【Abstract】:
Semi-supervised sentiment classification aims to train a classifier with a small number of labeled data (called seed data) and a large amount of unlabeled data. a big advantage of this approach is its saving of annotation effort by using the unlabeled data which is usually freely available. In this paper, we propose an approach to further minimize the annotation effort of semi-supervised sentiment classification by actively selecting the seed data. Specifically, a novel selection strategy is proposed to simultaneously select good words and documents for manual annotation by considering both of their annotation costs and informativeness. Experimental results demonstrate the effectiveness of our approach.</p>
<p>【Keywords】:
opinion mining; seed selection; semi-supervised; sentiment classification</p>
<h3 id="317. On empirical tradeoffs in large scale hierarchical classification.">317. On empirical tradeoffs in large scale hierarchical classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398625">Paper Link</a>】    【Pages】:2299-2302</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Babbar:Rohit">Rohit Babbar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Partalas:Ioannis">Ioannis Partalas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gaussier:=Eacute=ric">Éric Gaussier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amblard:C=eacute=cile">Cécile Amblard</a></p>
<p>【Abstract】:
While multi-class categorization of documents has been of research interest for over a decade, relatively fewer approaches have been proposed for large scale taxonomies in which the number of classes range from hundreds of thousand as in Directory Mozilla to over a million in Wikipedia. As a result of ever increasing number of text documents and images from various sources, there is an immense need for automatic classification of documents in such large hierarchies. In this paper, we analyze the tradeoffs between the important characteristics of different classifiers employed in the top down fashion. The properties for relative comparison of these classifiers include, (i) accuracy on test instance, (ii) training time (iii) size of the model and (iv) test time required for prediction. Our analysis is motivated by the well known error bounds from learning theory, which is also further reinforced by the empirical observations on the publicly available data from the Large Scale Hierarchical Text Classification Challenge. We show that by exploiting the data heterogenity across the large scale hierarchies, one can build an overall classification system which is approximately 4 times faster for prediction, 3 times faster to train, while sacrificing only 1% point in accuracy.</p>
<p>【Keywords】:
empirical tradeoffs; hierarchical classification</p>
<h3 id="318. An interaction framework of service-oriented ontology learning.">318. An interaction framework of service-oriented ontology learning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398626">Paper Link</a>】    【Pages】:2303-2306</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Jingsong">Jingsong Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yinglin">Yinglin Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Hao">Hao Wei</a></p>
<p>【Abstract】:
Ontology plays a very important role in supporting knowledge-based applications. In cloud computing, ontology learning technology is facing new challenges in dealing with heterogeneous data sources from different domains and researchers, which may contain various particular concepts and relations. Traditional ontology learning frameworks usually focus only on the extraction of concepts and taxonomic relations from the multi-structured corpus. However, former researches rarely studied the interactions during ontology learning process among different researchers. Lack of interactions among people who build ontology in different domains may cause inconsistent ontology. Besides, lack of incentive during the ontology building process will also result in low efficiency. To address these challenges, this paper specifies a novel solution to perform ontology learning. The solution includes a service-oriented ontology interaction framework, a service-oriented ontology learning strategy. It shows that it advances ontology learning to a higher level of performance and portability with a number of experiments in demo system.</p>
<p>【Keywords】:
cloud computing; ontology; ontology interaction; ontology learning; service-oriented framework</p>
<h3 id="319. Infobox suggestion for Wikipedia entities.">319. Infobox suggestion for Wikipedia entities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398627">Paper Link</a>】    【Pages】:2307-2310</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sultana:Afroza">Afroza Sultana</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hasan:Quazi_Mainul">Quazi Mainul Hasan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Biswas:Ashis_Kumer">Ashis Kumer Biswas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Das:Soumyava">Soumyava Das</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rahman:Habibur">Habibur Rahman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Chris_H=_Q=">Chris H. Q. Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chengkai">Chengkai Li</a></p>
<p>【Abstract】:
Given the sheer amount of work and expertise required in authoring Wikipedia articles, automatic tools that help Wikipedia contributors in generating and improving content are valuable. This paper presents our initial step towards building a full-fledged author assistant, particularly for suggesting infobox templates for articles. We build SVM classifiers to suggest infobox template types, among a large number of possible types, to Wikipedia articles without infoboxes. Different from prior works on Wikipedia article classification which deal with only a few label classes for named entity recognition, the much larger 337-class setup in our study is geared towards realistic deployment of infobox suggestion tool. We also emphasize testing on articles without infoboxes, due to that labeled and unlabeled data exhibit different distributions of features, which departs from the typical assumption that they are drawn from the same underlying population.</p>
<p>【Keywords】:
text classification; wikipedia</p>
<h3 id="320. Time feature selection for identifying active household members.">320. Time feature selection for identifying active household members.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398628">Paper Link</a>】    【Pages】:2311-2314</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Campos:Pedro_G=">Pedro G. Campos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bellog=iacute=n:Alejandro">Alejandro Bellogín</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/D=iacute=ez:Fernando">Fernando Díez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cantador:Iv=aacute=n">Iván Cantador</a></p>
<p>【Abstract】:
Popular online rental services such as Netflix and MoviePilot often manage household accounts. A household account is usually shared by various users who live in the same house, but in general does not provide a mechanism by which current active users are identified, and thus leads to considerable difficulties for making effective personalized recommendations. The identification of the active household members, defined as the discrimination of the users from a given household who are interacting with a system (e.g. an on-demand video service), is thus an interesting challenge for the recommender systems research community. In this paper, we formulate the above task as a classification problem, and address it by means of global and local feature selection methods and classifiers that only exploit time features from past item consumption records. The results obtained from a series of experiments on a real dataset show that some of the proposed methods are able to select relevant time features, which allow simple classifiers to accurately identify active members of household accounts.</p>
<p>【Keywords】:
feature selection; household member identification; recommender systems; time features</p>
<h3 id="321. Text classification with relatively small positive documents and unlabeled data.">321. Text classification with relatively small positive documents and unlabeled data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398629">Paper Link</a>】    【Pages】:2315-2318</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fukumoto:Fumiyo">Fumiyo Fukumoto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yamamoto:Takeshi">Takeshi Yamamoto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Matsuyoshi:Suguru">Suguru Matsuyoshi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suzuki:Yoshimi">Yoshimi Suzuki</a></p>
<p>【Abstract】:
This paper addresses the problem of dealing with a collection of negative training documents which is suitable for relatively small number of positive documents, and presents a method for eliminating the need for manually collecting negative training documents based on supervised machine learning techniques. We applied an error correction technique to the results of negative training data obtained by the Positive Example Based Learning (PEBL). Moreover, we used a boosting technique to learn a set of negative data to train classifiers. The results using Japanese newspaper documents showed that the method contributes for reducing the cost of manual collection of negative training documents.</p>
<p>【Keywords】:
small positive documents and unlabeled data; text classification</p>
<h3 id="322. On compressing weighted time-evolving graphs.">322. On compressing weighted time-evolving graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398630">Paper Link</a>】    【Pages】:2319-2322</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Wei">Wei Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kan:Andrey">Andrey Kan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chan:Jeffrey">Jeffrey Chan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bailey:James">James Bailey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leckie:Christopher">Christopher Leckie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pei:Jian">Jian Pei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ramamohanarao:Kotagiri">Kotagiri Ramamohanarao</a></p>
<p>【Abstract】:
Existing graph compression techniquesmostly focus on static graphs. However for many practical graphs such as social networks the edge weights frequently change over time. This phenomenon raises the question of how to compress dynamic graphs while maintaining most of their intrinsic structural patterns at each time snapshot. In this paper we show that the encoding cost of a dynamic graph is proportional to the heterogeneity of a three dimensional tensor that represents the dynamic graph. We propose an effective algorithm that compresses a dynamic graph by reducing the heterogeneity of its tensor representation, and at the same time also maintains a maximum lossy compression error at any time stamp of the dynamic graph. The bounded compression error benefits compressed graphs in that they retain good approximations of the original edge weights, and hence properties of the original graph (such as shortest paths) are well preserved. To the best of our knowledge, this is the first work that compresses weighted dynamic graphs with bounded lossy compression error at any time snapshot of the graph.</p>
<p>【Keywords】:
dynamic graphs; graph compression; graph mining.</p>
<h3 id="323. Graph-based collective classification for tweets.">323. Graph-based collective classification for tweets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398631">Paper Link</a>】    【Pages】:2323-2326</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Duan:Yajuan">Yajuan Duan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shum:Heung=Yeung">Heung-Yeung Shum</a></p>
<p>【Abstract】:
In this paper, we address the problem of classifying tweets into topical categories. Because of the short, noisy and ambiguous nature of tweets, we propose to collectively conduct the classification by exploiting the context information (i.e. related tweets) other than individually as in conventional text classification methods. In particular, we augment the content-based representation of text with tweets sharing same #hashtag or URL, which results in a tweet graph. We then formulate the tweet classification task under a graph optimization framework. We investigate three popular approaches, namely, Loopy Belief Propagation (LBP), Relaxation Labeling (RL), and Iterative Classification Algorithm (ICA). Extensive experiment results show that the graph-based tweet classification approach remarkably improves the performance, while the ICA model with relationship of sharing the same #hashtag gives the best result on separate tweet graph.</p>
<p>【Keywords】:
graph-based classification; tweet classification</p>
<h3 id="324. A word-order based graph representation for relevance identification.">324. A word-order based graph representation for relevance identification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398632">Paper Link</a>】    【Pages】:2327-2330</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ramachandran:Lakshmi">Lakshmi Ramachandran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gehringer:Edward_F=">Edward F. Gehringer</a></p>
<p>【Abstract】:
In this paper we propose a new word-order based graph representation for text. In our graph representation vertices represent words or phrases and edges represent relations between contiguous words or phrases. The graph representation also includes dependency information. Our text representation is suitable for applications involving the identification of relevance or paraphrases across texts, where word-order information would be useful. We show that this word-order based graph representation performs better than a dependency tree representation while identifying the relevance of one piece of text to another.</p>
<p>【Keywords】:
text relevance; text representation; word-order graph</p>
<h3 id="325. Tracing clusters in evolving graphs with node attributes.">325. Tracing clusters in evolving graphs with node attributes.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398633">Paper Link</a>】    【Pages】:2331-2334</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Boden:Brigitte">Brigitte Boden</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/G=uuml=nnemann:Stephan">Stephan Günnemann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seidl_0001:Thomas">Thomas Seidl</a></p>
<p>【Abstract】:
Data sources representing social networks with additional attribute information about the nodes are widely available in today's applications. Recently, combined clustering methods were introduced that consider graph information and attribute information simultaneously to detect meaningful clusters in such networks. In many cases, such attributed graphs also evolve over time. Therefore, there is a need for clustering methods that are able to trace clusters over different time steps and analyze their evolution over time. In this paper, we extend our combined clustering method DB-CSC to the analysis of evolving combined clusters.</p>
<p>【Keywords】:
community detection; evolution; graph clustering; networks</p>
<h3 id="326. Prediction of retweet cascade size over time.">326. Prediction of retweet cascade size over time.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398634">Paper Link</a>】    【Pages】:2335-2338</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kupavskii:Andrey">Andrey Kupavskii</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ostroumova:Liudmila">Liudmila Ostroumova</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Umnov:Alexey">Alexey Umnov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Usachev:Svyatoslav">Svyatoslav Usachev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gusev:Gleb">Gleb Gusev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kustarev:Andrey">Andrey Kustarev</a></p>
<p>【Abstract】:
Retweet cascades play an essential role in information diffusion in Twitter. Popular tweets reflect the current trends in Twitter, while Twitter itself is one of the most important online media. Thus, understanding the reasons why a tweet becomes popular is of great interest for sociologists, marketers and social media researches. What is even more important is the possibility to make a prognosis of a tweet's future popularity. Besides the scientific significance of such possibility, this sort of prediction has lots of practical applications such as breaking news detection, viral marketing etc. In this paper we try to forecast how many retweets a given tweet will gain during a fixed time period. We train an algorithm that predicts the number of retweets during time T since the initial moment. In addition to a standard set of features we utilize several new ones. One of the most important features is the flow of the cascade. Another one is PageRank on the retweet graph, which can be considered as the measure of influence of users.</p>
<p>【Keywords】:
influence; information diffusion; retweet cascade</p>
<h3 id="327. An efficient and simple under-sampling technique for imbalanced time series classification.">327. An efficient and simple under-sampling technique for imbalanced time series classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398635">Paper Link</a>】    【Pages】:2339-2342</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liang:Guohua">Guohua Liang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Chengqi">Chengqi Zhang</a></p>
<p>【Abstract】:
Imbalanced time series classification (TSC) involving many real-world applications has increasingly captured attention of researchers. Previous work has proposed an intelligent-structure preserving over-sampling method (SPO), which the authors claimed achieved better performance than other existing over-sampling and state-of-the-art methods in TSC. The main disadvantage of over-sampling methods is that they significantly increase the computational cost of training a classification model due to the addition of new minority class instances to balance data-sets with high dimensional features. These challenging issues have motivated us to find a simple and efficient solution for imbalanced TSC. Statistical tests are applied to validate our conclusions. The experimental results demonstrate that this proposed simple random under-sampling technique with SVM is efficient and can achieve results that compare favorably with the existing complicated SPO method for imbalanced TSC.</p>
<p>【Keywords】:
SVM; imbalanced time series classification; under-sampling</p>
<h3 id="328. Top-N recommendation through belief propagation.">328. Top-N recommendation through belief propagation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398636">Paper Link</a>】    【Pages】:2343-2346</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Ha:Jiwoon">Jiwoon Ha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kwon:Soon=Hyoung">Soon-Hyoung Kwon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Sang=Wook">Sang-Wook Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faloutsos:Christos">Christos Faloutsos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Sunju">Sunju Park</a></p>
<p>【Abstract】:
The top-n recommendation focuses on finding the top-n items that the target user is likely to purchase rather than predicting his/her ratings on individual items. In this paper, we propose a novel method that provides top-n recommendation by probabilistically determining the target user's preference on items. This method models the purchasing relationships between users and items as a bipartite graph and employs Belief Propagation to compute the preference of the target user on items. We analyze the proposed method in detail by examining the changes in recommendation accuracy under different parameter settings. We also show that the proposed method is up to 40% more accurate than an existing method by comparing it with an RWR-based method via extensive experiments.</p>
<p>【Keywords】:
belief propagation; data mining; top-n recommendation</p>
<h3 id="329. Mining advices from weblogs.">329. Mining advices from weblogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398637">Paper Link</a>】    【Pages】:2347-2350</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wicaksono:Alfan_Farizki">Alfan Farizki Wicaksono</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Myaeng:Sung=Hyon">Sung-Hyon Myaeng</a></p>
<p>【Abstract】:
Weblog, one of the fastest growing user generated contents, often contains key learnings gleaned from people's past experiences which are really worthy to be well presented to other people. One of the key learnings contained in weblogs is often vented in the form of advice. In this paper, we aim to provide a methodology to extract sentences that reveal advices on weblogs. We observed our data to discover the characteristics of advices contained in weblogs. Based on this observation, we define our task as a classification problem using various linguistic features. We show that our proposed method significantly outperforms the baseline. The presence or absence of imperative mood expression appears to be the most important feature in this task. It is also worth noting that the work presented in this paper is the first attempt on mining advices from English data.</p>
<p>【Keywords】:
advice mining; text mining</p>
<h3 id="330. Parallel proximal support vector machine for high-dimensional pattern classification.">330. Parallel proximal support vector machine for high-dimensional pattern classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398638">Paper Link</a>】    【Pages】:2351-2354</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Zhenfeng">Zhenfeng Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xingquan">Xingquan Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Yangdong">Yangdong Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Yue=Fei">Yue-Fei Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Xiangyang">Xiangyang Xue</a></p>
<p>【Abstract】:
Proximal support vector machine (PSVM) is a simple but effective classifier, especially for solving large-scale data classification problems. An inherent deficiency of PSVM lies on its inefficiency for dealing with high-dimensional data. In this paper, we propose a parallel version of PSVM (PPSVM). Based on random dimensionality partitioning, PPSVM can obtain partitioned local model parameters in parallel, with combined parameters to form the final global solution. In fact, PPSVM enjoys two properties: 1) It can calculate model parameters in parallel and is therefore a fast learning method with theoretically proved convergence; and 2) It can avoid the inversion of large matrix, which makes it suitable for high-dimensional data. In the paper, we also propose a random PPSVM with randomly partitioned data in each iteration to improve the performance of PSVM. Experimental results on real-world data demonstrate that the proposed methods can obtain similar or even better prediction accuracy than PSVM with much better runtime efficiency.</p>
<p>【Keywords】:
high dimensionality; parallel; proximal support vector machine</p>
<h3 id="331. On using category experts for improving the performance and accuracy in recommender systems.">331. On using category experts for improving the performance and accuracy in recommender systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398639">Paper Link</a>】    【Pages】:2355-2358</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hwang:Won=Seok">Won-Seok Hwang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Ho=Jong">Ho-Jong Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Sang=Wook">Sang-Wook Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Minsoo">Minsoo Lee</a></p>
<p>【Abstract】:
A variety of recommendation methods have been proposed to satisfy the performance and accuracy; however, it is fairly difficult to satisfy both of them because there is a trade-off between them. In this paper, we introduce the notion of category experts and propose the recommendation method by exploiting the ratings of category experts instead of those of the users similar to a target user. We also extend the method that uses both the category preference of a target user and his/her similarity to category experts. We show that our method significantly outperforms the existing methods in terms of performance and accuracy through extensive experiments with real-world data.</p>
<p>【Keywords】:
collaborative filtering; expert; performance evaluation; recommender system</p>
<h3 id="332. Finding influential products on social domination game.">332. Finding influential products on social domination game.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398640">Paper Link</a>】    【Pages】:2359-2362</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yeo:Jinyoung">Jinyoung Yeo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Jin=Woo">Jin-Woo Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hwang:Seung=won">Seung-won Hwang</a></p>
<p>【Abstract】:
In this paper, we propose a new type of market model called the social domination game model. Given a set C of customers and a set P of products, this model simulates market competition among P and estimates market shares, considering both the dominance relation between C and P and the influence relation among the members of C. With this model, we propose a greedy product positioning algorithm for designing a new product that approximately maximizes market share. Our experimental results show that the proposed algorithm creates a new product gaining up to 97.5% market share of the best product's market share obtained by the exact method, while significantly outperforming the exact method in terms of running time, i.e., by up to two orders of magnitude.</p>
<p>【Keywords】:
domination game; influence propagation</p>
<h3 id="333. Entity resolution using search engine results.">333. Entity resolution using search engine results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398641">Paper Link</a>】    【Pages】:2363-2366</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Khabsa:Madian">Madian Khabsa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Treeratpituk:Pucktada">Pucktada Treeratpituk</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Giles:C=_Lee">C. Lee Giles</a></p>
<p>【Abstract】:
Given a set of automatically extracted entities E of size n, we would like to cluster all the various names referring to the same canonical entity together. The variations of each entity include acronyms, full name, and informal naming conventions. We propose using search engine results to cluster variations of each entity based on the URLs appearing in those results. We create a cluster C for each top search result returned by querying for the entity e ∈ E assigning e to the cluster C. Our experiments on a manually created dataset shows that our approach achieves higher precision and recall than string matching algorithm and hierarchical clustering based disambiguation methods.</p>
<p>【Keywords】:
disambiguation; entity resolution; search engines</p>
<h3 id="334. Tweet classification based on their lifetime duration.">334. Tweet classification based on their lifetime duration.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398642">Paper Link</a>】    【Pages】:2367-2370</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Takemura:Hikaru">Hikaru Takemura</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tajima:Keishi">Keishi Tajima</a></p>
<p>【Abstract】:
Many microblog messages remain useful only within a short time, and users often find such a message after its informational value has vanished. Users also sometimes miss old but still useful messages buried among outdated ones. To solve these problems, we develop a method of classifying messages into the following three categories: (1) messages that users should read now because their value will diminish soon, (2) messages that users may read later because their value will not largely change soon, and (3) messages that are not useful anymore because their value has vanished. Our method uses an error correcting output code consisting of binary classifiers each of which determines whether a given message has value at specific time point. Our experiments on Twitter data confirmed that it outperforms naive methods.</p>
<p>【Keywords】:
filtering; microblog; real-time; time-dependency; twitter</p>
<h3 id="335. Scalable collaborative filtering using incremental update and local link prediction.">335. Scalable collaborative filtering using incremental update and local link prediction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398643">Paper Link</a>】    【Pages】:2371-2374</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Xiao">Xiao Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhaoxin">Zhaoxin Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Ke">Ke Wang</a></p>
<p>【Abstract】:
The traditional collaborative filtering approaches have been shown to suffer from two fundamental problems: data sparsity and difficulty in scalability. To address these problems, we present a novel scalable item-based collaborative filtering method by using incremental update and local link prediction. By subdividing the computations and analyzing the factors in different cases of item-to-item similarity, we design the incremental update strategies in item-based CF, which can make the recommender system more efficient and scalable. Based on the transitive structure of item similarity graph, we use the local link prediction method to find implicit candidates to alleviate the lack of neighbors in predictions and recommendations caused by the sparsity of data. The experiment results validate that our algorithm can improve the performance of traditional CF, and can increase the efficiency in recommendations.</p>
<p>【Keywords】:
collaborative filtering; incremental update; link prediction; scalability; similarity graph</p>
<h3 id="336. Composing activity groups in social networks.">336. Composing activity groups in social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398644">Paper Link</a>】    【Pages】:2375-2378</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Cheng=Te">Cheng-Te Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shan:Man=Kwan">Man-Kwan Shan</a></p>
<p>【Abstract】:
One important function of current social networking services is allowing users to initialize different kinds of activity groups (e.g. study group, cocktail party, and group buying) and invite friends to attend in either manual or collaborative manners. However, such process of group formation is tedious, and could either include inappropriate group members or miss relevant ones. This work proposes to automatically compose the activity groups in a social network according to user-specified activity information. Given the activity host, a set of labels representing the activity's subjects, the desired group size, and a set of must-inclusive persons, we aim to find a set of individuals as the activity group, in which members are required to not only be familiar with the host but also have great communications with each other. We devise an approximation algorithm to greedily solve the group composing problem. Experiments on a real social network show the promising effectiveness of the proposed approach as well as the satisfactory human subjective study.</p>
<p>【Keywords】:
activity group; group formation; social network</p>
<h3 id="337. A co-training based method for chinese patent semantic annotation.">337. A co-training based method for chinese patent semantic annotation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398645">Paper Link</a>】    【Pages】:2379-2382</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Xu">Xu Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Peng:Zhiyong">Zhiyong Peng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zeng:Cheng">Cheng Zeng</a></p>
<p>【Abstract】:
Patents are public and scientific literatures protected by the law, and their abstracts highly contained valuable information. Patent's semantic annotation can effectively protect intellectual property rights and promote corporations' scientific research innovation. Currently, automatic patent annotation mainly used supervised machine learning algorithms, which required abundant expensive labeled patent data. Due to lack of enough labeled Chinese patent data, this paper adopted a semi-supervised machine learning method named co-training, which started from a little labeled data. This method combined keyword extraction with list extraction, and incrementally annotated functional clauses in patent abstract. Experiment results indicated this method can gradually improve the recall without sacrificing the precision.</p>
<p>【Keywords】:
co-training; information extraction; patent mining; semantic annotation</p>
<h3 id="338. Automatic labeling hierarchical topics.">338. Automatic labeling hierarchical topics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398646">Paper Link</a>】    【Pages】:2383-2386</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Xianling">Xianling Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ming:Zhaoyan">Zhaoyan Ming</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Zheng=Jun">Zheng-Jun Zha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Hongfei">Hongfei Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a></p>
<p>【Abstract】:
Recently, statistical topic modeling has been widely applied in text mining and knowledge management due to its powerful ability. A topic, as a probability distribution over words, is usually difficult to be understood. A common, major challenge in applying such topic models to other knowledge management problem is to accurately interpret the meaning of each topic. Topic labeling, as a major interpreting method, has attracted significant attention recently. However, previous works simply treat topics individually without considering the hierarchical relation among topics, and less attention has been paid to creating a good hierarchical topic descriptors for a hierarchy of topics. In this paper, we propose two effective algorithms that automatically assign concise labels to each topic in a hierarchy by exploiting sibling and parent-child relations among topics. The experimental results show that the inter-topic relation is effective in boosting topic labeling accuracy and the proposed algorithms can generate meaningful topic labels that are useful for interpreting the hierarchical topics.</p>
<p>【Keywords】:
statistical topic models; topic model labeling</p>
<h3 id="339. An unsupervised method for author extraction from web pages containing user-generated content.">339. An unsupervised method for author extraction from web pages containing user-generated content.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398647">Paper Link</a>】    【Pages】:2387-2390</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Jing">Jing Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Xinying">Xinying Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Jingtian">Jingtian Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a></p>
<p>【Abstract】:
In this paper, we address the problem of author extraction (AE) from user generated content (UGC) pages. Most existing solutions for web information extraction, including AE, adopt supervised approaches, which require expensive manual annotation. We propose a novel unsupervised approach for automatically collecting and labeling training data based on two key observations of author names: (1) people tend to use a single name across sites if their preferred names are available; (2) people tend to create unique usernames to easily distinguish themselves from others, e.g. travelbug61. Our AE solution only requires features extracted from a single UGC page instead of relying on clues from multiple UGC pages. We conducted extensive experiments. (1) The evaluation of automatically labeled author field data shows 95.0% precision. (2) Our method achieves an F1 score of 96.1%, which significantly outperforms a state-of-the-art supervised approach with single page features (F1 score: 68.4%) and has a comparable performance to its multiple page solution (F1 score: 95.4%). (3) We also examine the robustness of our approach on various UGC pages from forums and review sites, and achieve promising results as well.</p>
<p>【Keywords】:
author extraction; unsupervised approach</p>
<h3 id="340. Hierarchical target type identification for entity-oriented queries.">340. Hierarchical target type identification for entity-oriented queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398648">Paper Link</a>】    【Pages】:2391-2394</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Balog:Krisztian">Krisztian Balog</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Neumayer:Robert">Robert Neumayer</a></p>
<p>【Abstract】:
A significant portion of information needs in web search target entities. These may come in different forms or flavours, ranging from short keyword queries to more verbose requests, expressed in natural language. We address the task of automatically annotating queries with target types from an ontology. The identified types can subsequently be used, e.g., for creating semantically more informed query and retrieval models, filtering results, or directing the requests to specific verticals. Our study makes the following contributions. First, we formalise the task of hierarchical target type identification, argue that it is best viewed as a ranking problem, and propose multiple evaluation metrics. Second, we develop a purpose-built test collection by hand-annotating over 300 queries, from various recent entity search benchmarking campaigns, with target types from the DBpedia ontology. Finally, we introduce and examine two baseline models, inspired by federated search techniques. We show that these methods perform surprisingly well when target types are limited to a flat list of top level categories; finding the right level of granularity in the hierarchy, however, is particularly challenging and requires further investigation.</p>
<p>【Keywords】:
entity retrieval; query classification; semantic search</p>
<h3 id="341. Dictionary based sparse representation for domain adaptation.">341. Dictionary based sparse representation for domain adaptation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398649">Paper Link</a>】    【Pages】:2395-2398</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mehrotra:Rishabh">Rishabh Mehrotra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agrawal:Rushabh">Rushabh Agrawal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Haider:Syed_Aqueel">Syed Aqueel Haider</a></p>
<p>【Abstract】:
Machine Learning algorithms are often as good as the data they can learn from. Enormous amount of unlabeled data is readily available and the ability to efficiently use such amount of unlabeled data holds a significant promise in terms of increasing the performance of various learning tasks. We consider the task of supervised Domain Adaptation and present a Self-Taught learning based framework which makes use of the K-SVD algorithm for learning sparse representation of data in an unsupervised manner. To the best of our knowledge this is the first work that integrates K-SVD algorithm into the self-taught learning framework. The K-SVD algorithm iteratively alternates between sparse coding of the instances based on the current dictionary and a process of updating/adapting the dictionary to better fit the data so as to achieve a sparse representation under strict sparsity constraints. Using the learnt dictionary, a rich feature representation of the few labeled instances is obtained which is fed to a classifier along with class labels to build the model. We evaluate our framework on the task of domain adaptation for sentiment classification. Both self-domain (requiring very few domain-specific training instances) and cross-domain classification (requiring 0 labeled instances of target domain and very few labeled instances of source domain) are performed. Empirical comparisons of self-domain and cross-domain results establish the efficacy of the proposed framework.</p>
<p>【Keywords】:
domain adaptation; sparse representation; transfer learning</p>
<h2 id="Information retrieval poster session    60">Information retrieval poster session    60</h2>
<h3 id="342. Selecting expansion terms as a set via integer linear programming.">342. Selecting expansion terms as a set via integer linear programming.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398651">Paper Link</a>】    【Pages】:2399-2402</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Qi">Qi Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Yan">Yan Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
Pseudo-relevance feedback via query expansion has been widely studied from various perspectives in the past decades. Its effectiveness in improving retrieval effectiveness has been shown in many tasks. A variety of criteria were proposed to select additional terms for the original queries. However, most of the existing methods weight and select terms individually and do not consider the impact of term-to-term relationship. In this paper, we first examine the influence of combinations of terms through data analysis, which demonstrate the significant effect of term-to-term relationship on retrieval effectiveness. Then, to address this problem, we formalize the query expansion task as an integer linear programming (ILP) problem. The model combines the weights learned from a supervised method for individual terms, and integrates constraints to capture relations between terms. Finally, three standard TREC collections are used to evaluate the proposed method. Experimental results demonstrate that the proposed method can significantly improve the effectiveness of retrieval.</p>
<p>【Keywords】:
integer linear programming; relevance feedback</p>
<h3 id="343. An evaluation and enhancement of densitometric fragmentation for content slicing reuse.">343. An evaluation and enhancement of densitometric fragmentation for content slicing reuse.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398652">Paper Link</a>】    【Pages】:2403-2406</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Levacher:Killian">Killian Levacher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lawless:S=eacute=amus">Séamus Lawless</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wade:Vincent">Vincent Wade</a></p>
<p>【Abstract】:
Content slicing addresses the need of adaptive systems to reuse open corpus material by converting it into re-composable information objects. However this conversion is highly dependent upon the ability to correctly fragment pages into structurally sound atomic pieces. A recently suggested approach to fragmentation, which relies on densitometric page representation, claims to achieve high accuracy and time performance. Although it has been well received within the research community, a full evaluation of this approach and identification of strengths and weaknesses across a range of characteristics hasn't been performed. This paper proposes an independent evaluation of the approach with respect to granularity control, accuracy, time performance, content diversity and linguistic dependency. Moreover, this paper also provides a significant contribution to address important weaknesses discovered during the analysis, in order to improve the suitability and impact of the original algorithm within the context of content slicing.</p>
<p>【Keywords】:
analysis; densitometric; fragmentation; open-corpus</p>
<h3 id="344. Mathematical equation retrieval using plain words as a query.">344. Mathematical equation retrieval using plain words as a query.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398653">Paper Link</a>】    【Pages】:2407-2410</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Shinil">Shinil Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Seon">Seon Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Ko:Youngjoong">Youngjoong Ko</a></p>
<p>【Abstract】:
This paper proposes how to effectively retrieve the mathematical equations when the plain words are given as a query. The proposed system requires no complicated mathematical symbols, no particular input tool and no constraint of query. Users can enter a query with plain words like the traditional Information Retrieval. For this, we extract features from the plain texts that are converted from the real math equations. Experimental results show an outstanding performance, a MRR of 0.6585.</p>
<p>【Keywords】:
identifier &amp; number; mathML; mathematical equation retrievel; operator &amp; structure</p>
<h3 id="345. Serial position effects of clicking behavior on result pages returned by search engines.">345. Serial position effects of clicking behavior on result pages returned by search engines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398654">Paper Link</a>】    【Pages】:2411-2414</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Mingda">Mingda Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Shan">Shan Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Yan">Yan Zhang</a></p>
<p>【Abstract】:
Under the joint influence of the presentation of search results and users' browsing and clicking habits, the click probability distribution does not merely obey a monotonic decreasing Zipf function. In this paper, we present evidence that the click behavior on the entries of search engines' result pages is influenced by Serial Position Effect, which is independent of how these entries are ordered, and introduce a new function to characterize the click probability distribution.</p>
<p>【Keywords】:
click behavior; principle of least effort; serial position effect; zipf's law</p>
<h3 id="346. Towards measuring the visualness of a concept.">346. Towards measuring the visualness of a concept.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398655">Paper Link</a>】    【Pages】:2415-2418</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jeong:Jin=Woo">Jin-Woo Jeong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xin=Jing">Xin-Jing Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Dong=Ho">Dong-Ho Lee</a></p>
<p>【Abstract】:
In this paper, we propose a new method to measure the visualness of a concept. The visualness of a concept is generally defined as what extent a concept has visual characteristics. Even though the visualness of a concept is important and useful for various image search tasks, it has not received much spotlight yet. In this work, we especially focus on how to measure the visualness of a complex concept such as "round table", "dry bed" rather than a simple concept like "ball", "apple". To measure the visualness, we first collect sample images of a complex concept using web image search engines, and then group the images based on the visual features. Finally, we compute visual purity and weighted entropy of the clusters, which will act as a visualness score for the concept. Through various experiments, we show and discuss interesting results about the visualness of a concept.</p>
<p>【Keywords】:
concept visualness; image clustering; visual purity</p>
<h3 id="347. Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters.">347. Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398656">Paper Link</a>】    【Pages】:2419-2422</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Asadi:Nima">Nima Asadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy">Jimmy Lin</a></p>
<p>【Abstract】:
Most modern web search engines employ a two-phase ranking strategy: a candidate list of documents is generated using a "cheap" but low-quality scoring function, which is then reranked by an "expensive" but high-quality method (usually machine-learned). This paper focuses on the problem of candidate generation for conjunctive query processing in this context. We describe and evaluate a fast, approximate postings list intersection algorithms based on Bloom filters. Due to the power of modern learning-to-rank techniques and emphasis on early precision, significant speedups can be achieved without loss of end-to-end retrieval effectiveness. Explorations reveal a rich design space where effectiveness and efficiency can be balanced in response to specific hardware configurations and application scenarios.</p>
<p>【Keywords】:
learning to rank; postings lists intersection; scalability and efficiency</p>
<h3 id="348. Semantically coherent image annotation with a learning-based keyword propagation strategy.">348. Semantically coherent image annotation with a learning-based keyword propagation strategy.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398657">Paper Link</a>】    【Pages】:2423-2426</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Chaoran">Chaoran Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Jun">Jun Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Shuai">Shuai Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lian:Tao">Tao Lian</a></p>
<p>【Abstract】:
Automatic image annotation plays an important role in modern keyword-based image retrieval systems. Recently, many neighbor-based methods have been proposed and achieved good performance for image annotation. However, existing work mainly focused on exploring a distance metric learning algorithm to determine the neighbors of an image, and neglected the subsequent keyword propagation process. They usually used some simple heuristic propagation rules, and propagated each keyword independently without considering the inherent semantic coherence among keywords. In this paper, we propose a novel learning-based keyword propagation strategy and incorporate it into the neighbor-based method framework. In particular, we employ the structural SVM to learn a scoring function which can evaluate different candidate keyword sets for a test image. Moreover, we explicitly enforce the semantic coherence constraint for the propagated keywords in our approach. The annotation of the test image is propagated as a whole rather than separate keywords. Experiments on two benchmark data sets demonstrate the effectiveness of our approach for image annotation and ranked retrieval.</p>
<p>【Keywords】:
image annotation; semantic coherence; structural learning</p>
<h3 id="349. Language processing for arabic microblog retrieval.">349. Language processing for arabic microblog retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398658">Paper Link</a>】    【Pages】:2427-2430</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Darwish:Kareem">Kareem Darwish</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Magdy:Walid">Walid Magdy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mourad:Ahmed">Ahmed Mourad</a></p>
<p>【Abstract】:
The use of social media has profoundly affected social and political dynamics in the Arab world. In this paper, we explore the Arabic microblogs retrieval. We illustrate some of the challenges associated with Arabic microblog retrieval, which mainly stem from the use of different Arabic dialects that vary in lexical selection, morphology, and phonetics and lack orthographic and spelling conventions. We present some of the required processing for effective retrieval such as improved letter normalization, elongated word handling, stopword removal, and stemming</p>
<p>【Keywords】:
arabic retrieval; arabic twitter; dialect arabic normalization; microblog search</p>
<h3 id="350. Hierarchical image annotation using semantic hierarchies.">350. Hierarchical image annotation using semantic hierarchies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398659">Paper Link</a>】    【Pages】:2431-2434</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bannour:Hichem">Hichem Bannour</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hudelot:C=eacute=line">Céline Hudelot</a></p>
<p>【Abstract】:
Semantic hierarchies have been introduced recently to improve image annotation. They was used as a framework for hierarchical image classification, and thus to improve classifiers accuracy and reduce the complexity of managing large scale data. In this paper, we investigate the contribution of semantic hierarchies for hierarchical image classification. We propose first a new method based on the hierarchy structure to train efficiently hierarchical classifiers. Our method, named One-Versus-Opposite-Nodes, allows decomposing the problem in several independent tasks and therefore scales well with large database. We also propose two methods for computing a hierarchical decision function that serves to annotate new image samples. The former is performed by a top-down classifiers voting, while the second is based on a bottom-up score fusion. The experiments on Pascal VOC'2010 dataset showed that our methods improve well the image annotation results.</p>
<p>【Keywords】:
hierarchical image classification; image annotation; machine learning; semantic hierarchies</p>
<h3 id="351. On the inference of average precision from score distributions.">351. On the inference of average precision from score distributions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398660">Paper Link</a>】    【Pages】:2435-2438</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cummins:Ronan">Ronan Cummins</a></p>
<p>【Abstract】:
Modelling the document scores returned from an IR system for a given query using parameterised score distributions is an area of research that has become more popular in recent years. Score distribution (SD) models are useful for a number of IR tasks. These include data fusion, query performance prediction, determining thresholds in filtering applications, and tasks in the area of distributed retrieval. The inference of performance metrics, such as average precision, from these SD models is an important consideration. In this paper, we study the accuracy of a number of methods of inferring average precision from an SD model.</p>
<p>【Keywords】:
inference; information retrieval; score distributions</p>
<h3 id="352. An evaluation of corpus-driven measures of medical concept similarity for information retrieval.">352. An evaluation of corpus-driven measures of medical concept similarity for information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398661">Paper Link</a>】    【Pages】:2439-2442</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Koopman:Bevan">Bevan Koopman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zuccon:Guido">Guido Zuccon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bruza:Peter">Peter Bruza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sitbon:Laurianne">Laurianne Sitbon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lawley:Michael">Michael Lawley</a></p>
<p>【Abstract】:
Measures of semantic similarity between medical concepts are central to a number of techniques in medical informatics, including query expansion in medical information retrieval. Previous work has mainly considered thesaurus-based path measures of semantic similarity and has not compared different corpus-driven approaches in depth. We evaluate the effectiveness of eight common corpus-driven measures in capturing semantic relatedness and compare these against human judged concept pairs assessed by medical professionals. Our results show that certain corpus-driven measures correlate strongly (approx 0.8) with human judgements. An important finding is that performance was significantly affected by the choice of corpus used in priming the measure, i.e., used as evidence from which corpus-driven similarities are drawn. This paper provides guidelines for the implementation of semantic similarity measures for medical informatics and concludes with implications for medical information retrieval.</p>
<p>【Keywords】:
medical information retrieval; semantic similarity</p>
<h3 id="353. A constraint to automatically regulate document-length normalisation.">353. A constraint to automatically regulate document-length normalisation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398662">Paper Link</a>】    【Pages】:2443-2446</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cummins:Ronan">Ronan Cummins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/O=Riordan:Colm">Colm O'Riordan</a></p>
<p>【Abstract】:
Retrieval functions in information retrieval (IR) are fundamental to the effectiveness of search systems. However, considerable parameter tuning is often needed to increase the effectiveness of the retrieval. Document length normalisation is one such aspect that requires tuning on a per-query and per-collection basis for many retrieval functions. In this paper, we develop an approach that regularises the level of normalisation to apply on a per-query basis. We formally describe the interaction between query-terms and document length normalisation using a constraint. We then develop a general pre-retrieval approach to adapt a number of state-of-the-art ranking functions so that they adhere to the constraint. Finally, we empirically demonstrate that the adapted retrieval functions outperform default versions of the original retrieval functions, and perform at least comparably to tuned versions of the original functions, on a number of datasets. Essentially this regulates the normalisation parameter in a number of retrieval functions on a per-query basis in a principled manner.</p>
<p>【Keywords】:
constraints; retrieval functions</p>
<h3 id="354. Bridging offline and online social graph dynamics.">354. Bridging offline and online social graph dynamics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398663">Paper Link</a>】    【Pages】:2447-2450</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gomez=Rodriguez:Manuel">Manuel Gomez-Rodriguez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rogati:Monica">Monica Rogati</a></p>
<p>【Abstract】:
The online and offline worlds are converging. Location-based services, ubiquitous mobile devices and on-the-go social network accessibility are blurring the distinction between in-person activities and their virtual counterpart. An important effect of this convergence is the rapid and powerful impact of offline events (meetings, conferences) on the evolution and temporal dynamics of the online connectivity between members of social and professional networks. However, these effects have been largely unexplored. We study these effects by using data from LinkedIn, a popular professional social networking site. We find that offline events may induce connectivity changes in the online network -- there is a dramatic increase in the number of connections between event attendees shortly after the date of the event. Building on these insights, we describe a non-supervised method that exploits connectivity changes temporally correlated to real world events to successfully infer more than 40% of specific event attendees. Finally, we revisit the link prediction problem by including user contributed information about off-line events to achieve higher link prediction performance.</p>
<p>【Keywords】:
link prediction; real world events; social networks; temporal dynamics</p>
<h3 id="355. Predicting the performance of passage retrieval for question answering.">355. Predicting the performance of passage retrieval for question answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398664">Paper Link</a>】    【Pages】:2451-2454</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Krikon:Eyal">Eyal Krikon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carmel:David">David Carmel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a></p>
<p>【Abstract】:
We present a novel approach to predicting the performance of passage retrieval for question answering. That is, estimating the effectiveness, for answer extraction, of a list of passages retrieved in response to a question when relevance judgments are not available. Our prediction model integrates two types of estimates. The first estimates the probability that the information need expressed by the question is satisfied by the passages. This estimate is devised by adapting query-performance predictors developed for the document retrieval task. The second type estimates the probability that the passages contain the answers. This estimate relies on the occurrences of named entities that are likely to answer the question. Empirical evaluation demonstrates the merits of our prediction approach. For example, the prediction quality is much better than that of the only previous prediction method devised for the task at hand.</p>
<p>【Keywords】:
passage retrieval; query performance prediction; question answering</p>
<h3 id="356. Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context.">356. Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398665">Paper Link</a>】    【Pages】:2455-2458</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu_0007:Jun">Jun Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Ruifeng">Ruifeng Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Qin">Qin Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xiaolong">Xiaolong Wang</a></p>
<p>【Abstract】:
This paper proposes a novel approach using a coarse-to-fine analysis strategy for sentence-level emotion classification which takes into consideration of similarities to sentences in training set as well as adjacent sentences in the context. First, we use intra-sentence based features to determine the emotion label set of a target sentence coarsely through the statistical information gained from the label sets of the k most similar sentences in the training data. Then, we use the emotion transfer probabilities between neighboring sentences to refine the emotion labels of the target sentences. Such iterative refinements terminate when the emotion classification converges. The proposed algorithm is evaluated on Ren-CECps, a Chinese blog emotion corpus. Experimental results show that the coarse-to-fine emotion classification algorithm improves the sentence-level emotion classification by 19.11% on the average precision metric, which outperforms the baseline methods.</p>
<p>【Keywords】:
emotion classification; machine learning; multi-label classification</p>
<h3 id="357. Query-performance prediction and cluster ranking: two sides of the same coin.">357. Query-performance prediction and cluster ranking: two sides of the same coin.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398666">Paper Link</a>】    【Pages】:2459-2462</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raiber:Fiana">Fiana Raiber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shtok:Anna">Anna Shtok</a></p>
<p>【Abstract】:
We show that two tasks which were independently addressed in the information retrieval literature actually amount to the exact same task. The first is query performance prediction; i.e., estimating the effectiveness of a search performed in response to a query in the absence of relevance judgments. The second task is cluster ranking, that is, ranking clusters of similar documents by their presumed effectiveness (i.e., relevance) with respect to the query. Furthermore, we show that several state-of-the-art methods that were independently devised for each of the two tasks are based on the same principles. Finally, we empirically demonstrate that using insights gained in work on query-performance prediction can help, in many cases, to improve the performance of a previously proposed cluster ranking method.</p>
<p>【Keywords】:
cluster ranking; query-performance prediction</p>
<h3 id="358. Learning to rank search results for time-sensitive queries.">358. Learning to rank search results for time-sensitive queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398667">Paper Link</a>】    【Pages】:2463-2466</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=oslash=rv=aring=g:Kjetil">Kjetil Nørvåg</a></p>
<p>【Abstract】:
Retrieval effectiveness of temporal queries can be improved by taking into account the time dimension. Existing temporal ranking models follow one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, and 2) a probabilistic model generating a query from the textual and temporal part of document independently. In this paper, we propose a novel time-aware ranking model based on learning-to-rank techniques. We employ two classes of features for learning a ranking model, entity-based and temporal features, which are derived from annotation data. Entity-based features are aimed at capturing the semantic similarity between a query and a document, whereas temporal features measure the temporal similarity. Through extensive experiments we show that our ranking model significantly improves the retrieval effectiveness over existing time-aware ranking models.</p>
<p>【Keywords】:
temporal queries; time-aware ranking models</p>
<h3 id="359. On active learning in hierarchical classification.">359. On active learning in hierarchical classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398668">Paper Link</a>】    【Pages】:2467-2470</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Yu">Yu Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Kunpeng">Kunpeng Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Yusheng">Yusheng Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agrawal:Ankit">Ankit Agrawal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Choudhary:Alok_N=">Alok N. Choudhary</a></p>
<p>【Abstract】:
Most of the existing active learning algorithms assume all the category labels as independent or consider them in a "flat" structure. However, in reality, there are many applications in which the set of possible labels are often organized in a hierarchical structure. In this paper, we consider the problem of active learning when the categories are represented as a tree. Our goal is to exploit the structure information of the label tree in active learning to select the most informative samples to be labeled. We propose an algorithm that estimates the semantic space, embedding the category hierarchy. In this space, each category label is represented as a prototype and the uncertainty is measured using a variance-based fashion. We also demonstrate notable performance improvement with the proposed approach on synthetic and real datasets.</p>
<p>【Keywords】:
active learning; hierarchical classification; label tree embedding</p>
<h3 id="360. Question-answer topic model for question retrieval in community question answering.">360. Question-answer topic model for question retrieval in community question answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398669">Paper Link</a>】    【Pages】:2471-2474</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Ji:Zongcheng">Zongcheng Ji</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Fei">Fei Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Bin">Bin Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a></p>
<p>【Abstract】:
The major challenge for Question Retrieval (QR) in Community Question Answering (CQA) is the lexical gap between the queried question and the historical questions. This paper proposes a novel Question-Answer Topic Model (QATM) to learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answer share the same topic distribution. Experiments conducted on a real world CQA dataset from Yahoo! Answers show that combining both parts properly can get more knowledge than each part or both parts in a simple mixing way and combining our QATM with the state-of-the-art translation-based language model, where the topic and translation information is learned from the question-answer pairs at two different grained semantic levels respectively, can significantly improve the QR performance.</p>
<p>【Keywords】:
community question answering; question retrieval; question-answer topic model; topic model; translation model</p>
<h3 id="361. How do humans distinguish different people with identical names on the web?">361. How do humans distinguish different people with identical names on the web?</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398670">Paper Link</a>】    【Pages】:2475-2478</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Murakami:Harumi">Harumi Murakami</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Miyake:Yuki">Yuki Miyake</a></p>
<p>【Abstract】:
This research investigates how humans distinguish different people with identical names on the web to improve web people search. We asked subjects to classify 20 pages of web people-search results for each of 20 person names and analyzed their decision processes through questionnaire, protocol analysis, and interview. We found that keywords, vocations, works (for a real person, works are those made by the individual and, for a fictional person, works are those in which the individual appears), facial images, and the names of related people are important for distinguishing individuals. We proposed a model for distinguishing individuals and a knowledge-structure model based on the experiment's results.</p>
<p>【Keywords】:
distinguishing individual model; knowledge-structure model; person name disambiguation; web people search</p>
<h3 id="362. Enhancing product search by best-selling prediction in e-commerce.">362. Enhancing product search by best-selling prediction in e-commerce.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398671">Paper Link</a>】    【Pages】:2479-2482</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Long:Bo">Bo Long</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bian:Jiang">Jiang Bian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dong:Anlei">Anlei Dong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a></p>
<p>【Abstract】:
With the rapid growth of E-Commerce on the Internet, online product search service has emerged as a popular and effective paradigm for customers to find desired products and select transactions. Most product search engines today are based on adaptations of relevance models devised for information retrieval. However, there is still a big gap between the mechanism of finding products that customers really desire to purchase and that of retrieving products of high relevance to customers' query. In this paper, we address this problem by proposing a new ranking framework for enhancing product search based on dynamic best-selling prediction in E-Commerce. Specifically, we first develop an effective algorithm to predict the dynamic best-selling, i.e. the volume of sales, for each product item based on its transaction history. By incorporating such best-selling prediction with relevance, we propose a new ranking model for product search, in which we rank higher the product items that are not only relevant to the customer's need but with higher probability to be purchased by the customer. Results of a large scale evaluation, conducted over the dataset from a commercial product search engine, demonstrate that our new ranking method is more effective for locating those product items that customers really desire to buy at higher rank positions without hurting the search relevance.</p>
<p>【Keywords】:
best selling prediction; e-commerce; product search; transaction history</p>
<h3 id="363. Survival analysis for freshness in microblogging search.">363. Survival analysis for freshness in microblogging search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398672">Paper Link</a>】    【Pages】:2483-2486</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Amati:Gianni">Gianni Amati</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amodeo:Giuseppe">Giuseppe Amodeo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gaibisso:Carlo">Carlo Gaibisso</a></p>
<p>【Abstract】:
Freshness of information in real-time search is central in social networks, news, blogs and micro-blogs. Nevertheless, there is not a clear experimental evidence that shows what principled approach effectively combines time and content. We introduce a novel approach to model freshness using a survival analysis of relevance over time. In such models, freshness is measured by the tail probability of relevance over time. We also assume that the probability distributions for freshness are heavy-tailed. The heavy-tailed models of freshness are shown to be highly effective on the micro-blogging test collection of TREC 2011. The improvements over the state-of-the-art time-based models are statistically significant or moderately significant.</p>
<p>【Keywords】:
blog search; combining searches</p>
<h3 id="364. Information preservation in static index pruning.">364. Information preservation in static index pruning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398673">Paper Link</a>】    【Pages】:2487-2490</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ruey=Cheng">Ruey-Cheng Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Chia=Jung">Chia-Jung Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsai:Chiung=Min">Chiung-Min Tsai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsiang:Jieh">Jieh Hsiang</a></p>
<p>【Abstract】:
We develop a new static index pruning criterion based on the notion of information preservation. This idea is motivated by the fact that model degeneration, as does static index pruning, inevitably reduces the predictive power of the resulting model. We model this loss in predictive power using conditional entropy and show that the decision in static index pruning can therefore be optimized to preserve information as much as possible. We evaluated the proposed approach on three different test corpora, and the result shows that our approach is comparable in retrieval performance to state-of-the-art methods. When efficiency is of concern, our method has some advantages over the reference methods and is therefore suggested in Web retrieval settings.</p>
<p>【Keywords】:
conditional entropy; index pruning; information retrieval</p>
<h3 id="365. Temporal models for microblogs.">365. Temporal models for microblogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398674">Paper Link</a>】    【Pages】:2491-2494</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Choi:Jaeho">Jaeho Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
Time information impacts relevance in retrieval for the queries that are sensitive to trends and events. Microblog services particularly focused on recent news and events so dealing with the temporal aspects of microblogs is essential for providing effective retrieval. Recent work on time-based retrieval has shown that selecting the relevant time period for query expansion is promising. In this paper, we suggest a method for selecting the time period for query expansion based on a user behavior (i.e., retweets) that can be collected easily. We then use these time periods for query expansion in a pseudo-relevance feedback setting. More specifically, we use the difference in the temporal distribution between the top retrieved documents and retweets. The experimental results based on the TREC Microblog collection show that our method for selecting periods for query expansion improves retrieval performance compared to another approach.</p>
<p>【Keywords】:
microblogs; query expansion; time-based model</p>
<h3 id="366. I want what i need!: analyzing subjectivity of online forum threads.">366. I want what i need!: analyzing subjectivity of online forum threads.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398675">Paper Link</a>】    【Pages】:2495-2498</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Biyani:Prakhar">Prakhar Biyani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caragea:Cornelia">Cornelia Caragea</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Amit">Amit Singh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Prasenjit">Prasenjit Mitra</a></p>
<p>【Abstract】:
Online forums have become a popular source of information due to the unique nature of information they contain. Internet users use these forums to get opinions of other people on issues and to find factual answers to specific questions. Topics discussed in online forum threads can be subjective seeking personal opinions or non-subjective seeking factual information. Hence, knowing subjectivity orientation of threads would help forum search engines to satisfy user's information needs more effectively by matching the subjectivities of user's query and topics discussed in the threads in addition to lexical match between the two. We study methods to analyze the subjectivity of online forum threads. Experimental results on a popular online forum demonstrate the effectiveness of our methods.</p>
<p>【Keywords】:
online forums; subjectivity</p>
<h3 id="367. Improving the performance of the reinforcement learning model for answering complex questions.">367. Improving the performance of the reinforcement learning model for answering complex questions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398676">Paper Link</a>】    【Pages】:2499-2502</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chali:Yllias">Yllias Chali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hasan:Sadid_A=">Sadid A. Hasan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Imam:Kaisar">Kaisar Imam</a></p>
<p>【Abstract】:
This paper addresses the task of answering complex questions using a multi-document summarization approach within a reinforcement learning setting. Given a set of complex questions, a list of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to unseen complex questions. Previous works on this task have utilized a fully automatic reinforcement learning framework that selects the document sentences as the potential candidate (i.e. machine-generated) summary sentences by exploiting a relatedness measure with the available human-written summaries. In this paper, we propose an extension to this model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experimental results reveal the effectiveness of the user interaction component in the reinforcement learning framework.</p>
<p>【Keywords】:
complex question answering; multi-document summarization; reinforcement learning; user interaction</p>
<h3 id="368. Relation regularized subspace recommending for related scientific articles.">368. Relation regularized subspace recommending for related scientific articles.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398677">Paper Link</a>】    【Pages】:2503-2506</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Qing">Qing Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jianwu">Jianwu Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Zhiping">Zhiping Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Li">Li Wang</a></p>
<p>【Abstract】:
Recommending related scientific articles for a researcher is very important and useful in practice but also is full of challenges due to the latent complex semantic relations among scientific literatures. To deal with these challenges, this paper proposes a novel framework with link-missing data adaption, which casts the recommendation task to subspace embedding and similarity ranking problems. The relation regularized subspace in this framework is constructed via Relation Regularized Matrix Factorization (RRMF) for well modeling both content and link structure simultaneously. However, the link structure for an article is not always available in practical recommending. To solve this problem, we further propose two alternative approaches based on Latent Dirichlet Allocation (LDA) for link-missing articles recommendation as an extension of RRMF. Experiments on CiteSeer dataset demonstrate our method is more effective in comparison with some state-of-the-art approaches and is able to handle the link-missing case which the link-based methods never can fit.</p>
<p>【Keywords】:
latent dirichlet allocation; link-missing data; recommendation; regularized matrix factorization; related scientific articles</p>
<h3 id="369. Exploring the cluster hypothesis, and cluster-based retrieval, over the web.">369. Exploring the cluster hypothesis, and cluster-based retrieval, over the web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398678">Paper Link</a>】    【Pages】:2507-2510</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Raiber:Fiana">Fiana Raiber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a></p>
<p>【Abstract】:
We present a study of the cluster hypothesis, and of the performance of cluster-based retrieval methods, performed over large scale Web collections. Among the findings we present are (i) the cluster hypothesis can hold, as determined by a specific test, for large scale Web corpora to the same extent it does for newswire corpora; (ii) while spam documents do not affect the extent to which the cluster hypothesis holds, they considerably affect the performance of cluster based, as well as that of document-based, retrieval methods; and, (iii) as is the case for newswire corpora, cluster-based methods can yield better performance than document-based methods for Web corpora.</p>
<p>【Keywords】:
cluster hypothesis; cluster-based retrieval</p>
<h3 id="370. A picture paints a thousand words: a method of generating image-text timelines.">370. A picture paints a thousand words: a method of generating image-text timelines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398679">Paper Link</a>】    【Pages】:2511-2514</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Shize">Shize Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kong:Liang">Liang Kong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Yan">Yan Zhang</a></p>
<p>【Abstract】:
Manual timelines have greatly helped us to keep pace with the big world. In this paper, we introduce a novel solution which generates image-text timelines for news events based on Evolutionary Image-Text Summarization, which is an important and challenging problem. We first extract image's semantic information under translation model, and then fuse the high quality images with text timeline under an image assignment algorithm which can optimize the global coordination of the final timeline. The experimental results show that news readers can receive more satisfaction from the image-text timelines we generate.</p>
<p>【Keywords】:
cross-modality; image-text; summarization; timeline</p>
<h3 id="371. Short-text domain specific key terms/phrases extraction using an n-gram model with wikipedia.">371. Short-text domain specific key terms/phrases extraction using an n-gram model with wikipedia.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398680">Paper Link</a>】    【Pages】:2515-2518</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qureshi_0001:Muhammad_Atif">Muhammad Atif Qureshi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/O=Riordan:Colm">Colm O'Riordan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pasi:Gabriella">Gabriella Pasi</a></p>
<p>【Abstract】:
Finding domain specific key terms/phrases from a given set of documents is a challenging task. A domain may be defined as an area of interest over a collection of documents which may not be explicitly defined but implicitly observable in those documents. When considering a collection of documents related to academic research, examples of key terms/phrases may be Information Retrieval", "Marine Biology", etc. In this paper a technique for extracting important key terms/phrases in a considered topical domain is proposed using external evidence from the titles of Wikipedia articles and the Wikipedia category graph. We performed some experiments over the document collection of Web sites of different post-graduate schools. Our preliminary evaluations show promising results for the detection of domain specific key terms/phrases from the given set of domain focused Web pages.</p>
<p>【Keywords】:
community detection; n-gram model; open-domain knowledge; wikipedia</p>
<h3 id="372. A new probabilistic model for top-k ranking problem.">372. A new probabilistic model for top-k ranking problem.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398681">Paper Link</a>】    【Pages】:2519-2522</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Niu:Shuzi">Shuzi Niu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lan:Yanyan">Yanyan Lan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
This paper is concerned with top-k ranking problem, which reflects the fact that people pay more attention to the top ranked objects in real ranking application like information retrieval. A popular approach to top-k ranking problem is based on probabilistic models, such as Luce model and Mallows model. However, whether the sequential generative process described in these models is a suitable way for top-k ranking remains a question. According to the riffled independence factorization proposed in recent literature, which is a natural structural assumption on top-k ranking, we propose a new generative process of top-k ranking data. Our approach decomposes distributions over the top-k ranking into two layers: the first layer describes the relative ordering between the top k objects and the rest n-k objects, and the second layer describes the full ordering on the top k objects. On this basis, we propose a new probabilistic model for top-k ranking problem, called hierarchical ordering model. Specifically, we use three different probabilistic models to describe different generative processes of the first layer, and Luce model to describe the sequential generative process of the second layer, thus we obtain three different specific hierarchical ordering models. We also conduct extensive experiments on benchmark datasets to show that our proposed models can outperform previous models significantly.</p>
<p>【Keywords】:
learning to rank; ranking model; top-k</p>
<h3 id="373. Large scale analysis of changes in english vocabulary over recent time.">373. Large scale analysis of changes in english vocabulary over recent time.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398682">Paper Link</a>】    【Pages】:2523-2526</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jatowt:Adam">Adam Jatowt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tanaka:Katsumi">Katsumi Tanaka</a></p>
<p>【Abstract】:
Recently many historical texts have become digitized and made accessible for search and browsing. As human language is subject to constant evolution, these texts pose varying challenges to current users. In this paper we report the results of large-scale studies on the usage of words and the evolution of English language vocabulary over the last two centuries to help with understanding its impact on readability and retrieval of historical documents. We perform analysis of several lexical factors which may influence accessibility and readability of historical texts based on two large scale lexical corpora: the Corpus of Historical American English and Google Books 1-gram.</p>
<p>【Keywords】:
historical texts; information retrieval; language evolution; readability</p>
<h3 id="374. Climbing the app wall: enabling mobile app discovery through context-aware recommendations.">374. Climbing the app wall: enabling mobile app discovery through context-aware recommendations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398683">Paper Link</a>】    【Pages】:2527-2530</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Karatzoglou:Alexandros">Alexandros Karatzoglou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Baltrunas:Linas">Linas Baltrunas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Church:Karen">Karen Church</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/B=ouml=hmer_0001:Matthias">Matthias Böhmer</a></p>
<p>【Abstract】:
The explosive growth of the mobile application (app) market has made it difficult for users to find the most interesting and relevant apps from the hundreds of thousands that exist today. Context is key in the mobile space and so too are proactive services that ease user input and facilitate effective interaction. We believe that to enable truly novel mobile app recommendation and discovery, we need to support real context-aware recommendation that utilizes the diverse range of implicit mobile data available in a fast and scalable manner. In this paper we introduce the Djinn model, a novel context-aware collaborative filtering algorithm for implicit feedback data that is based on tensor factorization. We evaluate our approach using a dataset from an Android mobile app recommendation service called appazaar. Our results show that our approach compares favorably with state-of-the-art collaborative filtering methods.</p>
<p>【Keywords】:
collaborative filtering; implicit feedback; mobile apps; mobile recommendation; tensor factorization; context</p>
<h3 id="375. TwiSent: a multistage system for analyzing sentiment in twitter.">375. TwiSent: a multistage system for analyzing sentiment in twitter.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398684">Paper Link</a>】    【Pages】:2531-2534</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mukherjee:Subhabrata">Subhabrata Mukherjee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Malu:Akshat">Akshat Malu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/R=:Balamurali_A=">Balamurali A. R.</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a></p>
<p>【Abstract】:
In this paper, we present TwiSent, a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes them into the different polarity classes positive, negative and objective. However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent, we address the problems of 1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc., 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automatically annotated tweet set based on hashtags. It is a common practise to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser classification accurcy when tested on generic twitter dataset. We also show that our system performs much better than an existing system.</p>
<p>【Keywords】:
entity specific twitter sentiment; micro blogs; sentiment analysis; spam; twitter</p>
<h3 id="376. Twitter hyperlink recommendation with user-tweet-hyperlink three-way clustering.">376. Twitter hyperlink recommendation with user-tweet-hyperlink three-way clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398685">Paper Link</a>】    【Pages】:2535-2538</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Dehong">Dehong Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Renxian">Renxian Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hou:Yuexian">Yuexian Hou</a></p>
<p>【Abstract】:
Twitter, the most famous micro-blogging service and online social network, collects millions of tweets every day. Due to the length limitation, users usually need to explore other ways to enrich the content of their tweets. Some studies have provided findings to suggest that users can benefit from added hyperlinks in tweets. In this paper, we focus on the hyperlinks in Twitter and propose a new application, called hyperlink recommendation in Twitter. We expect that the recommended hyperlinks can be used to enrich the information of user tweets. A three-way tensor is used to model the user-tweet-hyperlink collaborative relations. Two tensor-based clustering approaches, tensor decomposition-based clustering (TDC) and tensor approximation-based clustering (TAC) are developed to group the users, tweets and hyperlinks with similar interests, or similar contexts. Recommendation is then made based on the reconstructed tensor using cluster information. The evaluation results in terms of Mean Absolute Error (MAE) shows the advantages of both the TDC and TAC approaches over a baseline recommendation approach, i.e., memory-based collaborative filtering. Comparatively, the TAC approach achieves better performance than the TDC approach.</p>
<p>【Keywords】:
three-way clustering; twitter hyperlink recommendation</p>
<h3 id="377. Concavity in IR models.">377. Concavity in IR models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398686">Paper Link</a>】    【Pages】:2539-2542</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Clinchant:St=eacute=phane">Stéphane Clinchant</a></p>
<p>【Abstract】:
We study the impact of concavity in IR models and propose to use a generalized logarithm function, the n-logarithm to weight words in documents. We extend the family of information based Information Retrieval (IR) models with this function. We show that that concavity is indeed an important property of IR models. Experiments conducted for IR tasks, Latent Semantic Indexing and Text Categorization show improvements.</p>
<p>【Keywords】:
IR models; concavity; information models</p>
<h3 id="378. Extracting interesting association rules from toolbar data.">378. Extracting interesting association rules from toolbar data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398687">Paper Link</a>】    【Pages】:2543-2546</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bordino:Ilaria">Ilaria Bordino</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Donato:Debora">Debora Donato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Poblete:Barbara">Barbara Poblete</a></p>
<p>【Abstract】:
Toolbar navigation logs provide rich data for enhancing information discovery on the Web. The value of this data resides in its scope, which goes beyond that of traditional query-mining data sources, such as search-engine logs. In this paper we present a methodology for extracting relevant association rules for queries, based on historic user navigational data. In addition, we propose a graph-based approach for extracting related queries and URLs for a given query.</p>
<p>【Keywords】:
association rules; toolbar data; web data mining</p>
<h3 id="379. Predicting CTR of new ads via click prediction.">379. Predicting CTR of new ads via click prediction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398688">Paper Link</a>】    【Pages】:2547-2550</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kolesnikov_0002:Alexander">Alexander Kolesnikov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Logachev:Yury">Yury Logachev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Topinskiy:Valeriy">Valeriy Topinskiy</a></p>
<p>【Abstract】:
Predicting CTR of ads on the search result page is an urgent topic. The reason for this is that choosing the right advertisement greatly affects revenue of the search engine and advertisers and user's satisfaction. For ads with the large click history it is quite clear how to predict CTR by utilizing statistical data. But for new ads with a poor click history such approach is not robust and reliable. We suggest a model for predicting CTR of such new ads. Contrary to the previous models of predicting CTR of new ads, our model uses events - clicks and skips1 instead of the observed CTR. In addition we have implemented several novel features, that resulted into the increase of the performance of our model. Offline and online experiments on the real search engine system demonstrated that our model outperforms the baseline and the approaches suggested in previous papers.</p>
<p>【Keywords】:
CPC; CTR; click-through rate; paid search; sponsored search; web advertising</p>
<h3 id="380. An examination of content farms in web search using crowdsourcing.">380. An examination of content farms in web search using crowdsourcing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398689">Paper Link</a>】    【Pages】:2551-2554</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/McCreadie:Richard">Richard McCreadie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Giles:Jim">Jim Giles</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jabr:Ferris">Ferris Jabr</a></p>
<p>【Abstract】:
On the Web, content farms produce articles engineered such that search engines rank them highly, in order to turn a profit from online advertising. Recently, content farms have increasingly been the target of demotion strategies by Web search engines, since content farm articles are often considered to be of suspect quality. In this paper, we study the prevalence of content farms in the results returned by three major Web search engines over time. In particular, we develop a crowdsourced approach to identify content farm articles from the results returned by these search engines. Our results show that between the period of March and August 2011, the number of content farm articles observed on a number of indicative queries was reduced by up to 55% in the top ranks.</p>
<p>【Keywords】:
content farms; crowdsourcing; web search</p>
<h3 id="381. Demographic context in web search re-ranking.">381. Demographic context in web search re-ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398690">Paper Link</a>】    【Pages】:2555-2558</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kharitonov:Eugene">Eugene Kharitonov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a></p>
<p>【Abstract】:
In this paper we study usefulness of user's demographical context for improving ranking of ambiguous queries. Context-aware relevance model is learnt from implicit user behaviour by using a simple yet general modification of a state-of-art click model which is capable to catch dependences from the search context. After that the machine learned click model is used in an offline re-ranking experiment and it is demonstrated that the demographical context ranking features provide improvements in ranking quality. Further, we perform a study to investigate the impact of different facets of demographical features (gender, age, and income) on search ranking performance and manually analyse queries which exhibit strong context dependences to get an additional understanding of the model behaviour.</p>
<p>【Keywords】:
click models; context-aware ranking; personalization; web search</p>
<h3 id="382. On the usefulness of query features for learning to rank.">382. On the usefulness of query features for learning to rank.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398691">Paper Link</a>】    【Pages】:2559-2562</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Santos:Rodrygo_L=_T=">Rodrygo L. T. Santos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
Learning to rank studies have mostly focused on query-dependent and query-independent document features, which enable the learning of ranking models of increased effectiveness. Modern learning to rank techniques based on regression trees can support query features, which are document-independent, and hence have the same values for all documents being ranked for a query. In doing so, such techniques are able to learn sub-trees that are specific to certain types of query. However, it is unclear which classes of features are useful for learning to rank, as previous studies leveraged anonymised features. In this work, we examine the usefulness of four classes of query features, based on topic classification, the history of the query in a query log, the predicted performance of the query, and the presence of concepts such as persons and organisations in the query. Through experiments on the ClueWeb09 collection, our results using a state-of-the-art learning to rank technique based on regression trees show that all four classes of query features can significantly improve upon an effective learned model that does not use any query feature.</p>
<p>【Keywords】:
learning to rank; query features</p>
<h3 id="383. Session-based query performance prediction.">383. Session-based query performance prediction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398692">Paper Link</a>】    【Pages】:2563-2566</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kustarev:Andrey">Andrey Kustarev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ustinovsky:Yury">Yury Ustinovsky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mazur:Anna">Anna Mazur</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a></p>
<p>【Abstract】:
Search sessions are known to be a rich source of diverse valuable information for individual query analysis. In this paper, we address the problem of query performance prediction by utilizing the entire logical search sessions containing the given query. Guided by the intuitions based on the observations made after the analysis of the search sessions' properties and performance of the queries they contain, we propose a number of features that significantly advance the existing query performance prediction models. Some of them specifically allow to focus on tail queries with sparse click-through statistics.</p>
<p>【Keywords】:
query flow graph; query performance prediction; user sessions</p>
<h3 id="384. A latent pairwise preference learning approach for recommendation from implicit feedback.">384. A latent pairwise preference learning approach for recommendation from implicit feedback.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398693">Paper Link</a>】    【Pages】:2567-2570</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fang:Yi">Yi Fang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Most of the current recommender systems heavily rely on explicit user feedback such as ratings on items to model users' interests. However, in many applications, it is very hard to collect the explicit feedback, while implicit feedback such as user clicks may be more available. Furthermore, it is often more suitable for many recommender systems to address a ranking problem than a rating predicting problem. This paper proposes a latent pairwise preference learning (LPPL) approach for recommendation with implicit feedback. LPPL directly models user preferences with respect to a set of items rather than the rating scores on individual items, which are modeled with a set of features by analyzing clickthrough data available in many real-world recommender systems. The LPPL approach models both the latent variables of group structure of users and the pairwise preferences simultaneously. We conduct experiments on the testbed from a real-world recommender system and demonstrate that the proposed approach can effectively improve the recommendation performance against several baseline algorithms.</p>
<p>【Keywords】:
implicit feedback; pairwise preferences; recommender systems</p>
<h3 id="385. Topic based pose relevance learning in dance archives.">385. Topic based pose relevance learning in dance archives.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398694">Paper Link</a>】    【Pages】:2571-2574</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ren:Reede">Reede Ren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Collomosse:John_P=">John P. Collomosse</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
This paper improves spatial pyramid kernel (SPK) and proposes a relevance learning approach to compare performer's poses in a large dance archive, the NRCD collection1. Domain knowledge of Choreutics is exploited to define pose topics and a selection operator is developed for pose topic matching. The visual structure descriptor of self similarity (SSF) is extended to hierarchical self similarity (HSSF) to keep shape context. The framework of Bag-of-Visual Words (BOVW) is applied to encode as well as to speed up the matching on pose topics/topic combinations. This alleviates the complexity in limb allocation which is infeasible in our data. Extensive experiments show that the new approach outperforms the original SPK in both precision and robustness.</p>
<p>【Keywords】:
dance retrieval; pose relevance learning; spatial pyramid kernel</p>
<h3 id="386. PhotoFall: discovering weblog stories through photographs.">386. PhotoFall: discovering weblog stories through photographs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398695">Paper Link</a>】    【Pages】:2575-2578</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wienberg:Christopher">Christopher Wienberg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gordon:Andrew_S=">Andrew S. Gordon</a></p>
<p>【Abstract】:
An effective means of retrieving relevant photographs from the web is to search for terms that would likely appear in the surrounding text in multimedia documents. In this paper, we investigate the complementary search strategy, where relevant multimedia documents are retrieved using the photographs they contain. We concentrate our efforts on the retrieval of large numbers of personal stories posted to Internet weblogs that are relevant to a particular search topic. Photographs are often included in posts of this sort, typically taken by the author during the course of the narrated events of the story. We describe a new story search tool, PhotoFall, which allows users to quickly find stories related to their topic of interest by judging the relevance of the photographs extracted from top search results. We evaluate the accuracy of relevance judgments made using this interface, and discuss the implications of the results for improving topic-based searches of multimedia content.</p>
<p>【Keywords】:
photographs; storytelling; weblogs</p>
<h3 id="387. RESQ: rank-energy selective query forwarding for distributed search systems.">387. RESQ: rank-energy selective query forwarding for distributed search systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398696">Paper Link</a>】    【Pages】:2579-2582</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Teymorian:Amin_Y=">Amin Y. Teymorian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Qin:Xiao">Xiao Qin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Frieder:Ophir">Ophir Frieder</a></p>
<p>【Abstract】:
Selective query forwarding is a promising technique to help scale high-quality and cost-efficient query evaluation in distributed search systems. The basic idea is simple. After a local site receives a query, it determines non-local sites to forward the query to and returns an aggregation of local and non-local results. We introduce "RESQ", a hybrid rank-energy selective query forwarding model. The novel contribution of RESQ is to simultaneously consider both ranking quality and energy costs when making forwarding decisions. Using a large-scale query log and publicly-available energy price time series, we demonstrate the ability of RESQ forwarding to achieve favorable tradeoffs between the possibility of returning high ranking query results and savings in temporally- and spatially-varying energy prices.</p>
<p>【Keywords】:
distributed IR; energy; linear program; query forwarding</p>
<h3 id="388. The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy.">388. The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398697">Paper Link</a>】    【Pages】:2583-2586</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kamps:Jaap">Jaap Kamps</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Milic=Frayling:Natasa">Natasa Milic-Frayling</a></p>
<p>【Abstract】:
Information retrieval systems require human contributed relevance labels for their training and evaluation. Increasingly such labels are collected under the anonymous, uncontrolled conditions of crowdsourcing, leading to varied output quality. While a range of quality assurance and control techniques have now been developed to reduce noise during or after task completion, little is known about the workers themselves and possible relationships between workers' characteristics and the quality of their work. In this paper, we ask how do the relatively well or poorly-performing crowds, working under specific task conditions, actually look like in terms of worker characteristics, such as demographics or personality traits. Our findings show that the face of a crowd is in fact indicative of the quality of their work.</p>
<p>【Keywords】:
crowdsourcing; demographics; personality traits; worker accuracy</p>
<h3 id="389. Data filtering in humor generation: comparative analysis of hit rate and co-occurrence rankings as a method to choose usable pun candidates.">389. Data filtering in humor generation: comparative analysis of hit rate and co-occurrence rankings as a method to choose usable pun candidates.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398698">Paper Link</a>】    【Pages】:2587-2590</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dybala:Pawel">Pawel Dybala</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rzepka:Rafal">Rafal Rzepka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Araki:Kenji">Kenji Araki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sayama:Kohichi">Kohichi Sayama</a></p>
<p>【Abstract】:
In this paper we propose a method of filtering excessive amount of textual data acquired from the Internet. In our research on pun generation in Japanese we experienced problems with extensively long data processing time, caused by the amount of phonetic candidates generated (i.e. phrases that can be used to generate actual puns) by our system. Simple, naive approach in which we take into considerations only phrases with the highest occurrence in the Internet, can effect in deletion of those candidates that are actually usable. Thus, we propose a data filtering method in which we compare two Internet-based rankings: a co-occurrence ranking and a hit rate ranking, and select only candidates which occupy the same or similar positions in these rankings. In this work we analyze the effects of such data reduction, considering 1 cases: when the candidates are on exactly the same positions in both rankings, and when their positions differ by 1, 2, 3 and 4. The analysis is conducted on data acquired by comparing pun candidates generated by the system (and filtered with our method) with phrases that were actually used in puns created by humans. The results show that the proposed method can be used to filter excessive amounts of textual data acquired from the Internet.</p>
<p>【Keywords】:
AI; HCI; NLP; humor processing; web-based data extraction</p>
<h3 id="390. Predicting primary categories of business listings for local search.">390. Predicting primary categories of business listings for local search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398699">Paper Link</a>】    【Pages】:2591-2594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Changsung">Changsung Kang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Jeehaeng">Jeehaeng Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a></p>
<p>【Abstract】:
We consider the problem of identifying primary categories of a business listing among the categories provided by the owner of the business. The category information submitted by business owners cannot be trusted with absolute certainty since they may purposefully add some secondary or irrelevant categories to increase recall in local search results, which makes category search very challenging for local search engines. Thus, identifying primary categories of a business is a crucial problem in local search. This problem can be cast as a multi-label classification problem with a large number of categories. However, the large scale of the problem makes it infeasible to use conventional supervised-learning-based text categorization approaches. We propose a large-scale classification framework that leverages multiple types of classification labels to produce a highly accurate classifier with fast training time. We effectively combine the complementary label sources to refine prediction. The experimental results indicate that our framework achieves very high precision and recall and outperforms a Centroid-based method.</p>
<p>【Keywords】:
primary category; text categorization; vertical search</p>
<h3 id="391. Where do the query terms come from?: an analysis of query reformulation in collaborative web search.">391. Where do the query terms come from?: an analysis of query reformulation in collaborative web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398700">Paper Link</a>】    【Pages】:2595-2598</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yue:Zhen">Zhen Yue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Jiepu">Jiepu Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Shuguang">Shuguang Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Daqing">Daqing He</a></p>
<p>【Abstract】:
This paper presents a user study aiming to investigate the query reformulation in collaborative Web search. 7 pairs of participants were recruited and each pair worked as a team on two collaborative exploratory Web search tasks. Through the log analysis, we compared possible sources for participants to draw query terms from. The results show that both search and collaborative actions are possible resources for new query terms. Traditional resources for query expansion such as previous search histories and relevant documents are still important resources for new query terms. The content in chat and workspace generated by participants themselves seems more likely to be the resource for new query terms than that of their partners. Task types also affect the influences on query reformulations. For the academic task, previously saved relevance documents are the most important resources for new query terms while chat histories are the most important resources for the leisure task.</p>
<p>【Keywords】:
collaborative web search; query reformulation</p>
<h3 id="392. Learning to recommend with social relation ensemble.">392. Learning to recommend with social relation ensemble.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398701">Paper Link</a>】    【Pages】:2599-2602</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Lei">Lei Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Jun">Jun Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhumin">Zhumin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Haoran">Haoran Jiang</a></p>
<p>【Abstract】:
Recommender systems with social networks (RSSN) have been well studied in recent works. However, these methods ignore the relationships among items, which may affect the quality of recommendations. Motivated by the observation that related items often have similar ratings, we propose a framework integrating items' relations, users' social graph and user-item rating matrix for recommendation. Experimental results show that our approach performs better than the state-of-art algorithm and the method with only users' social graph ensemble in terms of MAP and RMSE.</p>
<p>【Keywords】:
item relation; matrix factorization; recommender systems; social network; social recommendation</p>
<h3 id="393. A scalable approach for performing proximal search for verbose patent search queries.">393. A scalable approach for performing proximal search for verbose patent search queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398702">Paper Link</a>】    【Pages】:2603-2606</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bhatia:Sumit">Sumit Bhatia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Bin">Bin He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Qi">Qi He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Spangler:W=_Scott">W. Scott Spangler</a></p>
<p>【Abstract】:
Even though queries received by traditional information retrieval systems are quite short, there are many application scenarios where long natural language queries are more effective. Further, incorporating term position information can help improve results of long queries. However, the techniques for incorporating term position information have been developed for terse queries and hence, can not be directly applied to long queries. Though there exist some methods for performing proximal search for long queries, they are not scalable due to long query response times. We describe an intuitive and simple, yet effective technique that implicitly incorporates term position information for long queries in a scalable manner. Our proposed approach achieves more than 700% faster query response times while maintaining the quality of retrieved results when compared with a state-of-the-art method for performing proximal search for very long queries.</p>
<p>【Keywords】:
long queries; patent search; prior art search; proximal search; term proximity; verbose queries</p>
<h3 id="394. Is wikipedia too difficult?: comparative analysis of readability of wikipedia, simple wikipedia and britannica.">394. Is wikipedia too difficult?: comparative analysis of readability of wikipedia, simple wikipedia and britannica.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398703">Paper Link</a>】    【Pages】:2607-2610</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jatowt:Adam">Adam Jatowt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tanaka:Katsumi">Katsumi Tanaka</a></p>
<p>【Abstract】:
Readability is one of key factors determining document quality and reader's satisfaction. In this paper we analyze readability of Wikipedia, which is a popular source of information for searchers about unknown topics. Although Wikipedia articles are frequently listed by search engines on top ranks, they are often too difficult for average readers searching information about difficult queries. We examine the average readability of content in Wikipedia and compare it to the one in Simple Wikipedia and Britannica. Next, we investigate readability of selected categories in Wikipedia. Apart from standard readability measures we use some new metrics based on words' popularity and their distributions across different document genres and topics.</p>
<p>【Keywords】:
readability; web content analysis; web search</p>
<h3 id="395. Finding food entity relationships using user-generated data in recipe service.">395. Finding food entity relationships using user-generated data in recipe service.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398704">Paper Link</a>】    【Pages】:2611-2614</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chung:Young=joo">Young-joo Chung</a></p>
<p>【Abstract】:
Rakuten recipe is a recipe site where users can submit their recipes and share with the others. Since recipe contents are generated by users, they usually contain many misspellings, abbreviations, synonyms, hypernyms and hyponyms. Identifying and normalizing these words is essential to retrieve relevant recipes to user's request. In this paper, we introduce a new approach to finding related words in a recipe domain using the data structure. Based on the observation that people usually write the main ingredient in the first position of ingredient lists of each recipe and such a ingredient is strongly related to the categories where recipes belong, we calculate relation scores of word pairs using real service data, which contains 790 categories and 405,519 recipes. The experimental result showed that we successfully found semantically related word pairs with f-score of 0.93.</p>
<p>【Keywords】:
food entity relation; recipe; search effectiveness</p>
<h3 id="396. SRGSIS: a novel framework based on social relationship graph for social image search.">396. SRGSIS: a novel framework based on social relationship graph for social image search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398705">Paper Link</a>】    【Pages】:2615-2618</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Bo">Bo Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Ye">Ye Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Guoren">Guoren Wang</a></p>
<p>【Abstract】:
Tag-based social image search predominately focus on using user-annotated tags to find out the results of user query. However, the performance of tag-based social image search is usually unable to satisfy the needs of users. In this paper, we propose a novel framework based on Social Relationship Graph for Social Image Search (SRGSIS), which involves two stages. In the first stage, we use heterogeneous data from multiple modalities to build a social relationship graph. Then, for the given query keywords, we execute an efficient keyword search algorithm over the social relationship graph and obtain top-k candidate results based on relevance score. We model these results as the answer trees connecting keyword nodes that match keywords in the query. In the second stage, for refining the candidate results, each image in social relationship graph is represented as a region adjacency graph by using the visual content of image. We further model these region adjacency graphs as a closure tree and compute approximate graph similarity between the candidate results and the closure tree to obtain more desirable results. Extensive experimental results demonstrate the effectiveness of the proposed approach.</p>
<p>【Keywords】:
closure-tree; keyword search; multimodality; social relationship graph</p>
<h3 id="397. Exploring simultaneous keyword and key sentence extraction: improve graph-based ranking using wikipedia.">397. Exploring simultaneous keyword and key sentence extraction: improve graph-based ranking using wikipedia.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398706">Paper Link</a>】    【Pages】:2619-2622</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xun">Xun Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Lei">Lei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jiwei">Jiwei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a></p>
<p>【Abstract】:
Summarization and Keyword Selection are two important tasks in NLP community. Although both aim to summarize the source articles, they are usually treated separately by using sentences or words. In this paper, we propose a two-level graph based ranking algorithm to generate summarization and extract keywords at the same time. Previous works have reached a consensus that important sentence is composed by important keywords. In this paper, we further study the mutual impact between them through context analysis. We use Wikipedia to build a two-level concept-based graph, instead of traditional term-based graph, to express their homogenous relationship and heterogeneous relationship. We run PageRank and HITS rank on the graph to adjust both homogenous and heterogeneous relationships. A more reasonable relatedness value will be got for key sentence selection and keyword selection. We evaluate our algorithm on TAC 2011 data set. Traditional term-based approach achieves a score of 0.255 in ROUGE-1 and a score of 0.037 and ROUGE-2 and our approach can improve them to 0.323 and 0.048 separately.</p>
<p>【Keywords】:
graph; keyword; markov chain; summarization</p>
<h3 id="398. Estimating query difficulty for news prediction retrieval.">398. Estimating query difficulty for news prediction retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398707">Paper Link</a>】    【Pages】:2623-2626</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=oslash=rv=aring=g:Kjetil">Kjetil Nørvåg</a></p>
<p>【Abstract】:
News prediction retrieval has recently emerged as the task of retrieving predictions related to a given news story (or a query). Predictions are defined as sentences containing time references to future events. Such future-related information is crucially important for understanding the temporal development of news stories, as well as strategies planning and risk management. The aforementioned work has been shown to retrieve a significant number of relevant predictions. However, only a certain news topics achieve good retrieval effectiveness. In this paper, we study how to determine the difficulty in retrieving predictions for a given news story. More precisely, we address the query difficulty estimation problem for news prediction retrieval. We propose different entity-based predictors used for classifying queries into two classes, namely, Easy and Difficult. Our prediction model is based on a machine learning approach. Through experiments on real-world data, we show that our proposed approach can predict query difficulty with high accuracy.</p>
<p>【Keywords】:
future events; news predictions; query difficulty estimation; relevance ranking</p>
<h3 id="399. Recency-sensitive model of web page authority.">399. Recency-sensitive model of web page authority.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398708">Paper Link</a>】    【Pages】:2627-2630</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhukovskiy:Maxim">Maxim Zhukovskiy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vinogradov:Dmitry">Dmitry Vinogradov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gusev:Gleb">Gleb Gusev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raigorodskii:Andrei_M=">Andrei M. Raigorodskii</a></p>
<p>【Abstract】:
Traditional link-based web ranking algorithms run on a single web snapshot without concern of the dynamics of web pages and links. In particular, the correlation of web pages freshness and their classic PageRank is negative (see [11]). For this reason, in recent years a number of authors introduce some algorithms of PageRank actualization. We introduce our new algorithm called Actual PageRank, which generalizes some previous approaches and therefore provides better capability for capturing the dynamics of the Web. To the best of our knowledge we are the first to conduct ranking evaluations of a fresh-aware variation of PageRank on a large data set. The results demonstrate that our method achieves more relevant and fresh results than both classic PageRank and its "fresh" modifications.</p>
<p>【Keywords】:
freshness; link-based ranking; pagerank; web search</p>
<h3 id="400. Evaluating reward and risk for vertical selection.">400. Evaluating reward and risk for vertical selection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398709">Paper Link</a>】    【Pages】:2631-2634</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ke">Ke Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cummins:Ronan">Ronan Cummins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lalmas:Mounia">Mounia Lalmas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
The aggregation of search results from heterogeneous verticals (news, videos, blogs, etc) has become an important consideration in search. When aiming to select suitable verticals, from which items are selected to be shown along with the standard "ten blue links", there exists the potential to both help (selecting relevant verticals) and harm (selecting irrelevant verticals) the existing result set. In this paper, we present an approach that considers both reward and risk within the task of vertical selection (VS). We propose a novel risk-aware VS evaluation metric that incorporates users' risk-levels and users' individual preference of verticals. Using the proposed metric, we present a detailed analysis of both reward and risk of current resource selection approaches within a multi-label classification framework. The results bring insights into the effectiveness and robustness of current vertical selection approaches.</p>
<p>【Keywords】:
aggregated search; evaluation; vertical selection</p>
<h3 id="401. Contextual evaluation of query reformulations in a search session by user simulation.">401. Contextual evaluation of query reformulations in a search session by user simulation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398710">Paper Link</a>】    【Pages】:2635-2638</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Jiepu">Jiepu Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Daqing">Daqing He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Shuguang">Shuguang Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yue:Zhen">Zhen Yue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ni:Chaoqun">Chaoqun Ni</a></p>
<p>【Abstract】:
We propose a method to dynamically estimate the utility of documents in a search session by modeling the users' browsing behaviors and novelty. The method can be applied to evaluate query reformulations in a search session.</p>
<p>【Keywords】:
evaluation; interactive search; query reformulation; query sugges-tion; search session</p>
<h2 id="Databases poster session    8">Databases poster session    8</h2>
<h3 id="402. Information-complete and redundancy-free keyword search over large data graphs.">402. Information-complete and redundancy-free keyword search over large data graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398712">Paper Link</a>】    【Pages】:2639-2642</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhumin">Zhumin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Qi">Qi Kang</a></p>
<p>【Abstract】:
Keyword search over graphs has a wide array of applications in querying structured, semi-structured and unstructured data. Existing models typically use minimal trees or bounded subgraphs as query answers. While such models emphasize relevancy, they would suffer from incompleteness of information and redundancy among answers, making it difficult for users to effectively explore query answers. To overcome these drawbacks, we propose a novel cluster-based model, where query answers are relevancy-connected clusters. A cluster is a subgraph induced from a maximal set of relevancy-connected nodes. Such clusters are coherent and relevant, yet complete and redundancy free. They can be of arbitrary shape in contrast to the sphere-shaped bounded subgraphs in existing models. We also propose an efficient search algorithm and a corresponding graph index for large, disk-resident data graphs.</p>
<p>【Keywords】:
indexing; information complete; keyword search over graphs; redundancy free</p>
<h3 id="403. Spatial-aware interest group queries in location-based social networks.">403. Spatial-aware interest group queries in location-based social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398713">Paper Link</a>】    【Pages】:2643-2646</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yafei">Yafei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Dingming">Dingming Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Jianliang">Jianliang Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Choi:Byron">Byron Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Weifeng">Weifeng Su</a></p>
<p>【Abstract】:
Location-based social networks, such as Foursquare and Facebook Places, are bridging the gap between the physical world and online social networking services through acquired user locations. Some social networks released check-in services that allow users to share their visiting locations with their friends. In this paper, users' interests are modeled by check-in actions. We propose a new spatial-aware interest group (SIG) query that retrieves a user group of size k where every user is highly interested in the query keyword and also spatially close to each other. An efficient algorithm AIR based on the IR-tree is proposed for the processing of SIG queries. Furthermore, an optimization is developed and achieves a much better performance than the baseline algorithm.</p>
<p>【Keywords】:
query processing; social networks; spatial databases</p>
<h3 id="404. Probabilistic ranking in fuzzy object databases.">404. Probabilistic ranking in fuzzy object databases.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398714">Paper Link</a>】    【Pages】:2647-2650</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bernecker:Thomas">Thomas Bernecker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Emrich:Tobias">Tobias Emrich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kriegel:Hans=Peter">Hans-Peter Kriegel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Renz:Matthias">Matthias Renz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Z=uuml=fle:Andreas">Andreas Züfle</a></p>
<p>【Abstract】:
Ranking queries have been investigated extensively in the past due to their broad range of applications. In this paper, we study this problem in the context of fuzzy objects that have indeterministic boundaries. Fuzzy objects play an important role in many areas, such as biomedical image databases and GIS. To the best of our knowledge, we present the first efficient approach for similarity ranking in fuzzy object databases. The main challenge of ranking fuzzy objects is that these objects consist of multiple instances, each associated with a probability. We propose a framework to transform fuzzy objects into probabilistic objects which can then be ranked using existing algorithms for probabilistic objects.</p>
<p>【Keywords】:
fuzzy data; probabilistic data; probabilistic ranking</p>
<h3 id="405. Enabling ontology based semantic queries in biomedical database systems.">405. Enabling ontology based semantic queries in biomedical database systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398715">Paper Link</a>】    【Pages】:2651-2654</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zheng_0003:Shuai">Shuai Zheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Fusheng">Fusheng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lu:James_J=">James J. Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saltz:Joel_H=">Joel H. Saltz</a></p>
<p>【Abstract】:
While current biomedical ontology repositories offer primitive query capabilities, it is difficult or cumbersome to support ontology based semantic queries directly in semantically annotated biomedical databases. The problem may be largely attributed to the mismatch between the models of the ontologies and the databases, and the mismatch between the query interfaces of the two systems. To fully realize semantic query capabilities based on ontologies, we develop a system DBOntoLink to provide unified semantic query interfaces by extending database query languages. With DBOntoLink, semantic queries can be directly and naturally specified as extended functions of the database query languages without any programming needed. DBOntoLink is adaptable to different ontologies through customizations and supports major biomedical ontologies hosted at the NCBO BioPortal. We demonstrate the use of DBOntoLink in a real world biomedical database with semantically annotated medical image annotations.</p>
<p>【Keywords】:
ontology; query languages</p>
<h3 id="406. Similarity search in 3D object-based video data.">406. Similarity search in 3D object-based video data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398716">Paper Link</a>】    【Pages】:2655-2658</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lokoc:Jakub">Jakub Lokoc</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/W=uuml=nschmann:J=uuml=rgen">Jürgen Wünschmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Skopal:Tom=aacute=s">Tomás Skopal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rothermel:Albrecht">Albrecht Rothermel</a></p>
<p>【Abstract】:
In this paper, we present the vision of the usage of an object-based video data storage format for similarity search. The efficient (fast) and effective (accurate) search in video streams is an ongoing and still unsolved problem. Using an object-based format of multimedia data, all the information that is needed to answer queries is already available in a machine accessible format. This way, the process of creating (video) descriptors as well as the similarity search becomes easier, because the data is already organized in a manner that allows fast access to specific information. To demonstrate the concept of similarity search process using the object-based 3D video format, we present experiments conducted on generated clouds of points (an abstraction of 3D video data).</p>
<p>【Keywords】:
object-based video coding; similarity search; video retrieval</p>
<h3 id="407. Continuous top-k query for graph streams.">407. Continuous top-k query for graph streams.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398717">Paper Link</a>】    【Pages】:2659-2662</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pan:Shirui">Shirui Pan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xingquan">Xingquan Zhu</a></p>
<p>【Abstract】:
In this paper, we propose to query correlated graphs in a data stream scenario, where an algorithm is required to retrieve the top k graphs which are mostly correlated to a query graph q. Due to the dynamic changing nature of the stream data and the inherent complexity of the graph query process, treating graph streams as static datasets is computationally infeasible or ineffective. In the paper, we propose a novel algorithm, Hoe-PGPL, to identify top-k correlated graphs from data stream, by using a sliding window which covers a number of consecutive batches of stream data records. Our theme is to employ Hoeffding bound to discover some potential candidates and use two level candidate checking (one corresponding to the whole sliding window level and one corresponding to the local data batch level) to accurately estimate the correlation of the emerging candidate patterns, without rechecking the historical stream data. Experimental results demonstrate that the proposed algorithm not only achieves good performance in terms of query precision and recall, but also is several times, or even an order of magnitude, more efficient than the straightforward algorithm with respect to the time and the memory consumption. Our method represents the first research endeavor for data stream based top-k correlated graph query.</p>
<p>【Keywords】:
correlated graph query; graph stream; pearson's correlation coefficient</p>
<h3 id="408. Latent topics in graph-structured data.">408. Latent topics in graph-structured data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398718">Paper Link</a>】    【Pages】:2663-2666</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/B=ouml=hm_0001:Christoph">Christoph Böhm</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kasneci:Gjergji">Gjergji Kasneci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naumann:Felix">Felix Naumann</a></p>
<p>【Abstract】:
Large amounts of graph-structured data are emerging from various avenues, ranging from natural and life sciences to social and semantic web communities. We address the problem of discovering subgraphs of entities that reflect latent topics in graph-structured data. These topics are structured meta-information providing further insights into the data. The presented approach effectively detects such topics by exploiting only the structure of the underlying graph, thus avoiding the dependency on textual labels, which are a scarce asset in prevalent graph datasets. The viability of our approach is demonstrated in experiments on real-world datasets.</p>
<p>【Keywords】:
conceptual patterns; latent topics; subgraph mining</p>
<h3 id="409. Fast and accurate incremental entity resolution relative to an entity knowledge base.">409. Fast and accurate incremental entity resolution relative to an entity knowledge base.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398719">Paper Link</a>】    【Pages】:2667-2670</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Welch:Michael_J=">Michael J. Welch</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sane:Aamod">Aamod Sane</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Drome:Chris">Chris Drome</a></p>
<p>【Abstract】:
User facing topical web applications such as events or shopping sites rely on large collections of data records about real world entities that are updated at varying latencies ranging from days to seconds. For example, event venue details are changed relatively infrequently whereas ticket pricing and availability for an event is often updated in near-realtime. Users regard these sites as high quality if they seldom show duplicates, the URLs are stable, and their content is fresh, so it is important to resolve duplicate entity records with high quality and low latencies. High quality entity resolution typically evaluates the entire record corpus for similar record clusters at the cost of latency, while low latency resolution examines the least possible entities to keep time to a minimum, even at the cost of quality. In this paper we show how to keep low latency while achieving high quality, combining the best of both approaches: given an entity to be resolved, our incremental Fastpath system, in a matter of milliseconds, makes approximately the same decisions that the underlying batch system would have made. Our experiments show that the Fastpath system makes matching decisions for previously unseen entities with 90% precision and 98% recall relative to batch decisions, with latencies under 20ms on commodity hardware.</p>
<p>【Keywords】:
deduplication; entity resolution; knowledge base</p>
<h2 id="Knowledge management demonstration session    8">Knowledge management demonstration session    8</h2>
<h3 id="410. LUKe and MIKe: learning from user knowledge and managing interactive knowledge extraction.">410. LUKe and MIKe: learning from user knowledge and managing interactive knowledge extraction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398721">Paper Link</a>】    【Pages】:2671-2673</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Metzger:Steffen">Steffen Metzger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Stoll:Michael">Michael Stoll</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hose:Katja">Katja Hose</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schenkel:Ralf">Ralf Schenkel</a></p>
<p>【Abstract】:
Semantic recognition and annotation of unqiue enities and their relations is a key in understanding the essence contained in large text corpora. It typically requires a combination of efficient automatic methods and manual verification. Usually, both parts are seen as consecutive steps. In this demo we present MIKE, a user interface enabling the integration of user feedback into an iterative extraction process. We show how an extraction system can directly learn from such integrated user supervision. In general, this setup allows for stepwise training of the extraction system to a particular domain, while using user feedback early in the iterative extraction process improves extraction quality and reduces the overall human effort needed.</p>
<p>【Keywords】:
gui; information extraction; knowledge acquisition; learning; user feedback; web service</p>
<h3 id="411. PRAVDA-live: interactive knowledge harvesting.">411. PRAVDA-live: interactive knowledge harvesting.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398722">Paper Link</a>】    【Pages】:2674-2676</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yafang">Yafang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dylla:Maximilian">Maximilian Dylla</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ren:Zhaochun">Zhaochun Ren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Spaniol:Marc">Marc Spaniol</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
Acquiring high-quality (temporal) facts for knowledge bases is a labor-intensive process. Although there has been recent progress in the area of semi-supervised fact extraction, these approaches still have limitations, including a restricted corpus, a fixed set of relations to be extracted or a lack of assessment capabilities. In this paper we introduce PRAVDA-live, a framework that overcomes these limitations and supports the entire pipeline of interactive knowledge harvesting. To this end, our demo exhibits fact extraction from ad-hoc corpus creation, via relation specification, labeling and assessment all the way to ready-to-use RDF exports.</p>
<p>【Keywords】:
interactive knowledge harvesting; label propagation</p>
<h3 id="412. 4Is of social bully filtering: identity, inference, influence, and intervention.">412. 4Is of social bully filtering: identity, inference, influence, and intervention.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398723">Paper Link</a>】    【Pages】:2677-2679</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yunfei">Yunfei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Lanbo">Lanbo Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Michelony:Aaron">Aaron Michelony</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yi">Yi Zhang</a></p>
<p>【Abstract】:
As the increasing of popularity of social web, cyber bullying has become a more and more serious issue among children. Bullying causes huge negative effects on children, even suicide. SocialFilter is a realtime system that helps parents and educators track children's messages on Twitter, especially in order to detect whether they have been bullied or bullying others. The aim of the system is 4 I's, identity of bullies, inference of bullying message, influence of bully behavior, and intervention. We solve this problem by using machine learning technique. The current system is tracking tens of thousands of active children users on Twitter and automatically detect bullying messages at real time.</p>
<p>【Keywords】:
bully; detecting; twitter</p>
<h3 id="413. Lonomics Atlas: a tool to explore interconnected ionomic, genomic and environmental data.">413. Lonomics Atlas: a tool to explore interconnected ionomic, genomic and environmental data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398724">Paper Link</a>】    【Pages】:2680-2682</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dragut:Eduard_C=">Eduard C. Dragut</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ouzzani:Mourad">Mourad Ouzzani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Madkour:Amgad">Amgad Madkour</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mohamed:Nabeel">Nabeel Mohamed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Baker:Peter">Peter Baker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Salt:David_E=">David E. Salt</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
genomic data; ionomic data; visualization</p>
<h3 id="414. CarbonDB: a semantic life cycle inventory database.">414. CarbonDB: a semantic life cycle inventory database.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398725">Paper Link</a>】    【Pages】:2683-2685</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bertin:Benjamin">Benjamin Bertin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scuturici:Vasile=Marian">Vasile-Marian Scuturici</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pinon:Jean=Marie">Jean-Marie Pinon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Risler:Emmanuel">Emmanuel Risler</a></p>
<p>【Abstract】:
We demonstrate CarbonDB, a web application for Life Cycle Inventory data management. Life Cycle Assessment provides a well-accepted methodology for modelling environmental impacts of human activities. This methodology relies on the decomposition of a studied system into interdependent processes in a phase called Life Cycle Inventory. Several organisations provide processes databases containing thousands of processes with their interdependency links. The usual workflow to manage those databases is based on the manipulation of individual processes, which turns out to be a very harnessing work even if there are strong semantic similarities between the involved processes. In previous publications, we proposed a new workflow for LCA inventory databases maintenance based on the addition of semantic information to the processes they contained. This method considerably eases the modeling process and offers a synthetic view of the dependencies links. We created a web application based on this approach composed of a back-end for data management and a front-end for searching processes and visualize the dependencies links in a graph.</p>
<p>【Keywords】:
clustering; environmental database; life cycle assessment; onotology; semantic annotation</p>
<h3 id="415. Supporting temporal analytics for health-related events in microblogs.">415. Supporting temporal analytics for health-related events in microblogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398726">Paper Link</a>】    【Pages】:2686-2688</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Romano:Sara">Sara Romano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Stewart:Avar=eacute=">Avaré Stewart</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nejdl:Wolfgang">Wolfgang Nejdl</a></p>
<p>【Abstract】:
Microblogging services, such as Twitter, are gaining interests as a means of sharing information in social networks. Numerous works have shown the potential of using Twitter posts (or tweets) in order to infer the existence and magnitude of real-world events. In the medical domain, there has been a surge in detecting public health related tweets for early warning so that a rapid response from health authorities can take place. In this paper, we present a temporal analytics tool for supporting a comparative, temporal analysis of disease outbreaks between Twitter and official sources, such as, World Health Organization (WHO) and ProMED-mail. We automatically extract and aggregate outbreak events from official outbreak reports, producing time series data. Our tool can support a correlation analysis and an understanding of the temporal developments of outbreak mentions in Twitter, based on comparisons with official sources.</p>
<p>【Keywords】:
Twitter; disease outbreaks; event detection; time series analysis</p>
<h3 id="416. InCaToMi: integrative causal topic miner between textual and non-textual time series data.">416. InCaToMi: integrative causal topic miner between textual and non-textual time series data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398727">Paper Link</a>】    【Pages】:2689-2691</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Hyun_Duk">Hyun Duk Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rietz:Thomas_A=">Thomas A. Rietz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Diermeier:Daniel">Daniel Diermeier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Meichun">Meichun Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Castellanos:Mal=uacute=">Malú Castellanos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Limon:Carlos_Ceja">Carlos Ceja Limon</a></p>
<p>【Abstract】:
Topic modeling is popular for text mining tasks. Recently, topic modeling has been combined with time lines when textual data is related to external non-textual time series data such as stock prices. However, no previous work has used the external non-textual time series data in the process of topic modeling. In this paper, we describe a novel text mining system, Integrative Causal Topic Miner (InCaToMi) that integrates textual and non-textual time series data. InCaToMi automatically finds causal relationships and topics using text data and external non-textual time series data using Granger Testing. Moreover, InCaToMi considers the non-textual time series data in the topic modeling process, using the time series data to iteratively improve modeling results through interactions between it and the textual data at both topic and word levels.</p>
<p>【Keywords】:
causal topic mining; integrative topic mining; time series</p>
<h3 id="417. A tool for automated evaluation of algorithms.">417. A tool for automated evaluation of algorithms.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398728">Paper Link</a>】    【Pages】:2692-2694</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kranen:Philipp">Philipp Kranen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wels:Stephan">Stephan Wels</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rohlfs:Tim">Tim Rohlfs</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raubach:Sebastian">Sebastian Raubach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seidl_0001:Thomas">Thomas Seidl</a></p>
<p>【Abstract】:
Testing algorithms and systems involves trying different sets of parameter values on different domains or data sets. Even for a moderate number of parameters and domains the number of possible experiments can get very large due to the combinatorial explosion. Evaluating the outcome of these experiments requires comparing the results, which is often done by writing a script or inspecting the result files manually. For a new algorithm or version, the work has to be done over again. With hundreds, thousands, or even more possible experiments, both the preparation and the evaluation can become complex and tedious. In this demonstrator we present a software tool, called ET, for evaluating the parameters of an algorithm or system, either automatically or controlled by the user. It allows to launch large numbers of experiments in just a few clicks, visually explore the results and analyze the performance of the algorithm.</p>
<p>【Keywords】:
evaluation; framework; structured testing</p>
<h2 id="Information retrieval demonstration session    10">Information retrieval demonstration session    10</h2>
<h3 id="418. A summarization tool for time-sensitive social media.">418. A summarization tool for time-sensitive social media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398730">Paper Link</a>】    【Pages】:2695-2697</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Magdy:Walid">Walid Magdy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Ali:Ahmed_M=">Ahmed M. Ali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Darwish:Kareem">Kareem Darwish</a></p>
<p>【Abstract】:
Searching social content in general and microblogs (aka tweets) in particular has been basic and limited, especially for time-sensitive topics. The currently implemented microblog search on sites such as Twitter is based on simple word matching and retrieves the most recent microblogs that match a given query. Furthermore, a user may obtain hundreds or perhaps thousands of microblogs in response to a given query, leading to information overload. We present a new multidimensional microblog search tool that generates a comprehensive report from microblogs instead of a flat list of recent/relevant microblogs for a given query. Reports may include tag-clouds, topic time series, and most popular and funny microblogs, etc. The tool can be configured for monitoring time-sensitive topics using a set of predefined queries. We demonstrate our system on Arabic and English microblog collections. Additionally, we show a special configuration of the system for monitoring the 2012 Egyptian presidential elections.</p>
<p>【Keywords】:
elections; microblog search; retrieval results summarization; twitter</p>
<h3 id="419. CrowdTiles: presenting crowd-based information for event-driven information needs.">419. CrowdTiles: presenting crowd-based information for event-driven information needs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398731">Paper Link</a>】    【Pages】:2698-2700</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Whiting:Stewart">Stewart Whiting</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ke">Ke Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Alonso:Omar">Omar Alonso</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leelanupab:Teerapong">Teerapong Leelanupab</a></p>
<p>【Abstract】:
Time plays a central role in many web search information needs relating to recent events. For recency queries where fresh information is most desirable, there is likely to be a great deal of highly-relevant information created very recently by crowds of people across the world, particularly on platforms such as Wikipedia and Twitter. With so many users, mainstream events are often very quickly reflected in these sources. The English Wikipedia encyclopedia consists of a vast collection of user-edited articles covering a range of topics. During events, users collaboratively create and edit existing articles in near real-time. Simultaneously, users on Twitter disseminate and discuss event details, with a small number of users becoming influential for the topic. In this demo, we propose a novel approach to presenting a summary of new information and users related to recent or ongoing events associated with the user's search topic, therefore aiding most recent information discovery. We outline methods to detect search topics which are driven by events, identify and extract changing Wikipedia article passages and find influential Twitter users. Using these, we provide a system which displays familiar tiles in search results to present recent changes in the event-related Wikipedia articles, as well as Twitter users who have tweeted recent relevant information about the event topics.</p>
<p>【Keywords】:
Twitter; Wikipedia; events; time</p>
<h3 id="420. ESA: emergency situation awareness via microbloggers.">420. ESA: emergency situation awareness via microbloggers.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398732">Paper Link</a>】    【Pages】:2701-2703</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Jie">Jie Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karimi:Sarvnaz">Sarvnaz Karimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Robinson:Bella">Bella Robinson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cameron:Mark_A=">Mark A. Cameron</a></p>
<p>【Abstract】:
During a disastrous event, such as an earthquake or river flooding, information on what happened, who was affected and how, where help is needed, and how to aid people who were affected, is crucial. While communication is important in such times of crisis, damage to infrastructure such as telephone lines makes it difficult for authorities and victims to communicate. Microblogging has played a critical role as an important communication platform during crises when other media has failed. We demonstrate our ESA (Emergency Situation Awareness) system that mines microblogs in real-time to extract and visualise useful information about incidents and their impact on the community in order to equip the right authorities and the general public with situational awareness.</p>
<p>【Keywords】:
filtering; social media monitoring; social web mining</p>
<h3 id="421. Cager: a framework for cross-page search.">421. Cager: a framework for cross-page search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398733">Paper Link</a>】    【Pages】:2704-2706</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhumin">Zhumin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Qi">Qi Kang</a></p>
<p>【Abstract】:
Existing search engines have page as the unit of information of retrieval. They typically return a ranked list of pages, each being a search result containing the query keywords. This within-one-page constraint disallows utilization of relationship information that is often available and greatly beneficial. To utilize relationship information and improve search precision, we explore cross-page search, where each answer is a logical page consisting of multiple closely related pages that collectively contain the query keywords. We have implemented a prototype Cager, providing cross-page search and visualization over real dataset.</p>
<p>【Keywords】:
cross-page search; keyword search over graphs</p>
<h3 id="422. Mixed-initiative conversational system using question-answer pairs mined from the web.">422. Mixed-initiative conversational system using question-answer pairs mined from the web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398734">Paper Link</a>】    【Pages】:2707-2709</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wong:Wilson">Wilson Wong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cavedon:Lawrence">Lawrence Cavedon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thangarajah:John">John Thangarajah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Padgham:Lin">Lin Padgham</a></p>
<p>【Abstract】:
One of the biggest bottlenecks for conversational systems is large-scale provision of suitable content. Our approach readily provides this without the need for custom-crafting. In this demonstration, we present the use of question-answer (QA) pairs mined from online question-and-answer websites to construct system utterances for a conversational agent. Our system uses QA pairs to formulate utterances that drive a conversation in addition to the answering of user questions as has been done in previous work. We use a collection of strategies that specify how and when the different parts of our question-answer pairs can be used and augmented with a small number of generic hand-crafted text snippets to generate natural and coherent system utterances.</p>
<p>【Keywords】:
conversational system; question-answer pairs</p>
<h3 id="423. PicAlert!: a system for privacy-aware image classification and retrieval.">423. PicAlert!: a system for privacy-aware image classification and retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398735">Paper Link</a>】    【Pages】:2710-2712</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zerr:Sergej">Sergej Zerr</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Siersdorfer:Stefan">Stefan Siersdorfer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hare:Jonathon_S=">Jonathon S. Hare</a></p>
<p>【Abstract】:
Photo publishing in Social Networks and other Web2.0 applications has become very popular due to the pervasive availability of cheap digital cameras, powerful batch upload tools and a huge amount of storage space. A portion of uploaded images are of a highly sensitive nature, disclosing many details of the users' private life. We have developed a web service which can detect private images within a user's photo stream and provide support in making privacy decisions in the sharing context. In addition, we present a privacy-oriented image search application which automatically identifies potentially sensitive images in the result set and separates them from the remaining pictures.</p>
<p>【Keywords】:
classification; diversification; image analysis; privacy</p>
<h3 id="424. TASE: a time-aware search engine.">424. TASE: a time-aware search engine.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398736">Paper Link</a>】    【Pages】:2713-2715</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Sheng">Sheng Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Peiquan">Peiquan Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Xujian">Xujian Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yue:Lihua">Lihua Yue</a></p>
<p>【Abstract】:
Most Web pages contain temporal information, which can be utilized by search engines to improve searching performance for users. However, traditional search engines have little support in processing temporal-textual Web queries. Aiming at solving this problem, in this paper we present and implement a prototype system for time-sensitive queries, which is called TASE (Time-Aware Search Engine). TASE extracts both the explicit and implicit temporal expressions for each Web page, and calculates the relevant score between the Web page and each temporal expression, and then re-rank search results based on the temporal-textual relevance between Web pages and the queries. It is demonstrated that TASE can improve the effectiveness of temporal-textual Web queries.</p>
<p>【Keywords】:
re-ranking; time; web search</p>
<h3 id="425. Gumshoe quality toolkit: administering programmable search.">425. Gumshoe quality toolkit: administering programmable search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398737">Paper Link</a>】    【Pages】:2716-2718</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bao:Zhuowei">Zhuowei Bao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kimelfeld:Benny">Benny Kimelfeld</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yunyao">Yunyao Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raghavan:Sriram">Sriram Raghavan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Huahai">Huahai Yang</a></p>
<p>【Abstract】:
Enterprise search is challenging due to various reasons, notably the dynamic terminology and domain structure that are specific to the enterprise, combined with the fact that search deployments are typically managed by domain experts who are not necessarily search experts. To address that, it has been proposed to design search architectures that feature two principles: comprehensibility of the ranking mechanism and customizability of the search engine by means of intuitive runtime rules. The proposed demonstration operates on top of an engine implementation based on this search philosophy, and provides an administrator toolkit to realize the two principles. In particular, the toolkit provides a complete visualization of the provenance (hence ranking) of search results, embeds an editor for programming runtime rules, facilitates the investigation of (the cause of) missing or low-ranked desired results, and provides suggestions of rewrite rules to handle such results.</p>
<p>【Keywords】:
enterprise search; rule suggestion; search administration toolkit; search provenance visualization</p>
<h3 id="426. Simultaneous realization of page-centric communication and search.">426. Simultaneous realization of page-centric communication and search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398738">Paper Link</a>】    【Pages】:2719-2721</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shiraishi:Yuhki">Yuhki Shiraishi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Jianwei">Jianwei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kawai:Yukiko">Yukiko Kawai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Akiyama:Toyokazu">Toyokazu Akiyama</a></p>
<p>【Abstract】:
We present a novel system that combines the advantages of social communication and Web search by simultaneously discovering important pages and users. First, the system provides a communication interface attached to pages, which allows users to talk with each other in real time while browsing the same page, i.e., page-centric communication. Then, the system can efficiently provide two ranking lists of pages and users by analyzing a hybrid structure of hyperlinks (page-page relationship) and social links (page-user relationship and user-user relationship). Thus, users can efficiently search for important pages as well as important users related to their queries through the ranking function, and immediately obtain useful information or knowledge from not only pages themselves but also from other users.</p>
<p>【Keywords】:
communication; ranking; search; social networking</p>
<h3 id="427. MOUNA: mining opinions to unveil neglected arguments.">427. MOUNA: mining opinions to unveil neglected arguments.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398739">Paper Link</a>】    【Pages】:2722-2724</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kacimi:Mouna">Mouna Kacimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gamper:Johann">Johann Gamper</a></p>
<p>【Abstract】:
A query topic can be subjective involving a variety of opinions, judgments, arguments, and many other debatable aspects. Typically, search engines process queries independently from the nature of their topics using a relevance-based retrieval strategy. Hence, search results about subjective topics are often biased towards a specific view point or version. In this demo, we shall present MOUNA, a novel approach for opinion diversification. Given a query on a subjective topic, MOUNA ranks search results based on three scores: (1) relevance of documents, (2) semantic diversity to avoid redundancy and capture the different arguments used to discuss the query topic, and (3) sentiment diversity to cover a balanced set of documents having positive, negative, and neutral sentiments about the query topic. Moreover, MOUNA enhances the representation of search results with a summary of the different arguments and sentiments related to the query topic. Thus, the user can navigate through the results and explore the links between them. We provide an example scenario in this demonstration to illustrate the inadequacy of relevance-based techniques for searching subjective topics and highlight the innovative aspects of MOUNA. A video showing the demo can be found in <a href="http://www.youtube.com/user/mounakacimi/videos">http://www.youtube.com/user/mounakacimi/videos</a> .</p>
<p>【Keywords】:
diversification; ranking; sentiment analysis</p>
<h2 id="Databases demonstration session    9">Databases demonstration session    9</h2>
<h3 id="428. MAGIK: managing completeness of data.">428. MAGIK: managing completeness of data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398741">Paper Link</a>】    【Pages】:2725-2727</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Savkovic:Ognjen">Ognjen Savkovic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mirza:Paramita">Paramita Mirza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Paramonov:Sergey">Sergey Paramonov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nutt:Werner">Werner Nutt</a></p>
<p>【Abstract】:
MAGIK demonstrates how to use meta-information about the completeness of a database to assess the quality of the answers returned by a query. The system holds so-called table-completeness (TC) statements, by which one can express that a table is partially complete, that is, it contains all facts about some aspect of the domain. Given a query, MAGIK determines from such meta-information whether the database contains sufficient data for the query answer to be complete. If, according to the TC statements, the database content is not sufficient for a complete answer, MAGIK explains which further TC statements are needed to guarantee completeness. MAGIK extends and complements theoretical work on modeling and reasoning about data completeness by providing the first implementation of a reasoner. The reasoner operates by translating completeness reasoning tasks into logic programs, which are executed by an answer set engine.</p>
<p>【Keywords】:
answer set programming; data completeness; data quality</p>
<h3 id="429. Exploration of monte-carlo based probabilistic query processing in uncertain graphs.">429. Exploration of monte-carlo based probabilistic query processing in uncertain graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398742">Paper Link</a>】    【Pages】:2728-2730</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Emrich:Tobias">Tobias Emrich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kriegel:Hans=Peter">Hans-Peter Kriegel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Niedermayer:Johannes">Johannes Niedermayer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Renz:Matthias">Matthias Renz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suhartha:Andr=eacute=">André Suhartha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Z=uuml=fle:Andreas">Andreas Züfle</a></p>
<p>【Abstract】:
This demo presents a framework for running probabilistic graph queries on uncertain graphs and visualizing their results. The framework supports the most common uncertainty model for uncertain graphs, i.e. existential uncertainty for the edges of the graph. A large variety of meaningful graph queries are supported, such as shortest path, range, kN, reverse kN, reachability and various aggregation queries. Since the problem of exact probability computation according to possible world semantics is in #P-Time for many combinations of model and query, and since ignoring uncertainty (e.g. by using expectations only) will yield counterintuitive and hard to interpret results, our framework uses an optimized version of Monte-Carlo sampling to estimate the results which allows us not only to perform queries that conform to possible world semantics but also to sample only parts of a graph relevant for a given query. The main strength of this framework is the visualization combined with statistic hypothesis tests, which gives the user not only the estimated result of a query, but also an indication of how significant and reliable these results are. The aim of this demonstration is to give an intuition that a sampling based approach to probabilistic graphs is viable, and that the estimated results quickly converge even for very large graphs. A video demonstrating our framework can be downloaded at <a href="http://www.dbs.ifi.lmu.de/Publikationen/videos/PGraph.html">http://www.dbs.ifi.lmu.de/Publikationen/videos/PGraph.html</a></p>
<p>【Keywords】:
monte-carlo; probabilistic graph; sampling; visualization</p>
<h3 id="430. The nautilus analyzer: understanding and debugging data transformations.">430. The nautilus analyzer: understanding and debugging data transformations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398743">Paper Link</a>】    【Pages】:2731-2733</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Herschel:Melanie">Melanie Herschel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eichelberger:Hanno">Hanno Eichelberger</a></p>
<p>【Abstract】:
When developing data transformations - a task omnipresent in applications like data integration, data migration, data cleaning, or scientific data processing -developers quickly face the need to verify the semantic correctness of the transformation. Declarative specifications of data transformations, e.g., SQL or ETL tools, increase developer productivity but usually provide limited or no means for inspection or debugging. In this situation, developers today have no choice but to manually analyze the transformation and, in case of an error, to (repeatedly) fix and test the transformation. The goal of the Nautilus project is to semi-automatically support this analysis-fix-test cycle. This demonstration focuses on one main component of Nautilus, namely the Nautilus Analyzer that helps developers in understanding and debugging their data transformations. The demonstration will show the capabilities of this component for data transformations specified in SQL on scenarios from different domains that are based on real-world data. We provide an overview the Nautilus Analyzer, discuss components and implementation techniques, and outline our demonstration plan. The Nautilus website (<a href="http://nautilus-system.org">http://nautilus-system.org</a>) features a video, screenshots, and further details.</p>
<p>【Keywords】:
data provenance; query analysis</p>
<h3 id="431. Demonstrating ProApproX 2.0: a predictive query engine for probabilistic XML.">431. Demonstrating ProApproX 2.0: a predictive query engine for probabilistic XML.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398744">Paper Link</a>】    【Pages】:2734-2736</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Souihli:Asma">Asma Souihli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Senellart:Pierre">Pierre Senellart</a></p>
<p>【Abstract】:
ProApproX 2.0 allows users to query uncertain tree-structured data in the form of probabilistic XML documents. The demonstrated version integrates a fully redesigned query engine that, first, produces a propositional formula that represents the probabilistic lineage of a given answer over the probabilistic XML document, and, second, searches for an optimal strategy to approximate the probability of the lineage. This latter part relies on a query-optimizer-like approach: exploring different evaluation plans for different parts of the formula and predicting the cost of each plan, using a cost model for the various evaluation algorithms. The demonstration presents the graphical user interface of ProApproX 2.0, that allows a user to input an XPath query and approximation parameters, and lists query results with their probabilities; the interface also gives insight into the way the computation is performed, by displaying the compilation of the query lineage as a tree annotated with evaluation operators.</p>
<p>【Keywords】:
approximation algorithm; cost model; probabilistic data; query processing; xml</p>
<h3 id="432. HadoopXML: a suite for parallel processing of massive XML data with multiple twig pattern queries.">432. HadoopXML: a suite for parallel processing of massive XML data with multiple twig pattern queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398745">Paper Link</a>】    【Pages】:2737-2739</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Choi:Hyebong">Hyebong Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Kyong=Ha">Kyong-Ha Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Soo=Hyong">Soo-Hyong Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Yoon=Joon">Yoon-Joon Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moon:Bongki">Bongki Moon</a></p>
<p>【Abstract】:
The volume of XML data is tremendous in many areas, but especially in data logging and scientific areas. XML data in the areas are accumulated over time as new data are continuously collected. It is a challenge to process massive XML data with multiple twig pattern queries given by multiple users in a timely manner. We showcase HadoopXML, a system that simultaneously processes many twig pattern queries for a massive volume of XML data with Hadoop. Specifically, HadoopXML provides an efficient way to process a single large XML file in parallel. It processes multiple twig pattern queries simultaneously with a shared input scan. Users do not need to iterate M/R jobs for each query. HadoopXML also reduces many I/Os by enabling twig pattern queries to share their path solutions each other. Moreover, HadoopXML provides a sophisticated runtime load balancing scheme for fairly assigning multiple twig pattern joins across nodes. With synthetic and real world XML dataset, we demonstrate how efficiently HadoopXML processes many twig pattern queries in a shared and balanced way.</p>
<p>【Keywords】:
mapreduce; parallel processing; query optimization; xml</p>
<h3 id="433. MADden: query-driven statistical text analytics.">433. MADden: query-driven statistical text analytics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398746">Paper Link</a>】    【Pages】:2740-2742</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Grant:Christan_Earl">Christan Earl Grant</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gumbs:Joir=dan">Joir-dan Gumbs</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Kun">Kun Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Daisy_Zhe">Daisy Zhe Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chitouras:George">George Chitouras</a></p>
<p>【Abstract】:
In many domains, structured data and unstructured text are both important natural resources to fuel data analysis. Statistical text analysis needs to be performed over text data to extract structured information for further query processing. Typically, developers will need to connect multiple tools to build off-line batch processes to perform text analytic tasks. MADden is an integrated system developed for relational database systems such as PostgreSQL and Greenplum for real-time ad hoc query processing over structured and unstructured data. MADden implements four important text analytic functions that we have contributed to the MADlib open source library for textual analytics. In this demonstration, we will show the capability of the MADden text analytic library using computational journalism as the driving application. We show real-time declarative query processing over multiple data sources with both structured and text information.</p>
<p>【Keywords】:
databases; query-driven; text analytics</p>
<h3 id="434. STFMap: query- and feature-driven visualization of large time series data sets.">434. STFMap: query- and feature-driven visualization of large time series data sets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398747">Paper Link</a>】    【Pages】:2743-2745</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Candan:K=_Sel=ccedil=uk">K. Selçuk Candan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rossini:Rosaria">Rosaria Rossini</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sapino:Maria_Luisa">Maria Luisa Sapino</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xiaolan">Xiaolan Wang</a></p>
<p>【Abstract】:
Since many applications rely on time-based data, visualizing temporal data and helping experts explore large time series data sets are critical in many application domains. In this interactive system preview, we argue that time series often carry structural features that can, if efficiently identified and effectively visualized, help reduce visual overload and help the user quickly focus on the relevant portions of the data sets. Relying on this observation, we introduce a novel STFMap system, which includes four innovative query- and feature-driven time series data set visualization techniques: (a) segment-maps, (b) warp-maps, (c) stretch-maps, and (d) feature-maps. These rely on the salient temporal features of the time series and their alignments with respect to the given user query to help users explore the data set in a query-driven fashion.</p>
<p>【Keywords】:
data exploration; time series data sets</p>
<h3 id="435. Primates: a privacy management system for social networks.">435. Primates: a privacy management system for social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398748">Paper Link</a>】    【Pages】:2746-2748</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dhia:Imen_Ben">Imen Ben Dhia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Abdessalem:Talel">Talel Abdessalem</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sozio:Mauro">Mauro Sozio</a></p>
<p>【Abstract】:
While online social networks (OSN) present unprecedented opportunities for sharing information and multimedia content among users, they raise major privacy issues as users could often access personal or confidential data of other users. Most social networks provide some basic access control policies, which however seem to be very limited given the diversity of user relationships in the current social networks (e.g. friend, acquaintance, son) as well as the needs of social network users who might want to express sophisticated access control policies (e.g. invite all children of my colleagues to my child's birthday party). In this demonstration proposal, we present Primates a privacy management system for social networks. Primates allows users to specify access control rules for their resources and enforces access control over all shared resources. The set of users who are allowed to access a given resource is defined by a set of constraints on the paths connecting the owner of a resource to its requester in the social graph. We demonstrate the accuracy of our access control model and the scalability of our system.</p>
<p>【Keywords】:
access control; online social networks</p>
<h3 id="436. AMADA: web data repositories in the amazon cloud.">436. AMADA: web data repositories in the amazon cloud.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398749">Paper Link</a>】    【Pages】:2749-2751</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Aranda=And=uacute=jar:Andr=eacute=s">Andrés Aranda-Andújar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bugiotti:Francesca">Francesca Bugiotti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Camacho=Rodr=iacute=guez:Jes=uacute=s">Jesús Camacho-Rodríguez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Colazzo:Dario">Dario Colazzo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goasdou=eacute=:Fran=ccedil=ois">François Goasdoué</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kaoudi:Zoi">Zoi Kaoudi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Manolescu:Ioana">Ioana Manolescu</a></p>
<p>【Abstract】:
We present AMADA, a platform for storing Web data (in particular, XML documents and RDF graphs) based on the Amazon Web Services (AWS) cloud infrastructure. AMADA operates in a Software as a Service (SaaS) approach, allowing users to upload, index, store, and query large volumes of Web data. The demonstration shows (i) the step-by-step procedure for building and exploiting the warehouse (storing, indexing, querying) and (ii) the monitoring tools enabling one to control the expenses (monetary costs) charged by AWS for the operations involved while running AMADA.</p>
<p>【Keywords】:
aws; cloud computing; monetary cost; query processing; web data management</p>
<h2 id="Workshop summaries    15">Workshop summaries    15</h2>
<h3 id="437. DUBMMSM'12: international workshop on data-driven user behavioral modeling and mining from social media.">437. DUBMMSM'12: international workshop on data-driven user behavioral modeling and mining from social media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398751">Paper Link</a>】    【Pages】:2752-2753</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mahmud:Jalal">Jalal Mahmud</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caverlee:James">James Caverlee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nichols:Jeffrey">Jeffrey Nichols</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/O=Donovan:John">John O'Donovan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Michelle_X=">Michelle X. Zhou</a></p>
<p>【Abstract】:
Massive amounts of data are being generated on social media sites, such as Twitter and Facebook. This data can be used to better understand people, such as their personality traits, perceptions, and preferences, and predict their behavior. This deeper understanding of users and their behaviors can benefit a wide range of intelligent applications, such as advertising, social recommender systems, and personalized knowledge management. These applications will also benefit individual users themselves by optimizing their experiences across a wide variety of domains, such as retail, healthcare, and education. Since mining and understanding user behavior from social media often requires interdisciplinary effort, including machine learning, text mining, human-computer interaction, and social science, our workshop aims to bring together researchers and practitioners from multiple fields to discuss the creation of deeper models of individual users by mining the content that they publish and the social networking behavior that they exhibit.</p>
<p>【Keywords】:
data-driven; social media analysis; user modeling</p>
<h3 id="438. CloudDB 2012: fourth international workshop on cloud data management.">438. CloudDB 2012: fourth international workshop on cloud data management.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398752">Paper Link</a>】    【Pages】:2754-2755</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Meng:Xiaofeng">Xiaofeng Meng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silberstein:Adam">Adam Silberstein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Fusheng">Fusheng Wang</a></p>
<p>【Abstract】:
The fourth ACM international workshop on cloud data management is held in Maui, Hawaii, USA on October 29, 2012 and co-located with the ACM 21th Conference on Information and Knowledge Management (CIKM). The main objective of the workshop is to address the challenges of large scale data management based on the cloud computing infrastructure. The workshop brings together researchers and practitioners from cloud computing, distributed storage, query processing, parallel algorithms, data mining, and system analysis, all attendees share common research interests in maximizing performance, reducing cost of cloud data management and enlarging the scale of their endeavors. We have constructed an exciting program of seven refereed papers and four invited keynote talks that will give participants a full dose of emerging research.</p>
<p>【Keywords】:
cloud computing; data management</p>
<h3 id="439. CDMW 2012 - city data management workshop: workshop summary.">439. CDMW 2012 - city data management workshop: workshop summary.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398753">Paper Link</a>】    【Pages】:2756-2757</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bicer:Veli">Veli Bicer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tran:Thanh">Thanh Tran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ozcan:Fatma">Fatma Ozcan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Etzion:Opher">Opher Etzion</a></p>
<p>【Abstract】:
Cities today have become highly dense, dynamic living areas for the majority of planet's population and also focal points of innovation, commerce, and growth in a highly modernized world. Due to its intensifying importance, cities need to transform into sustainable, smarter and credible places to enable a tenantable and comfortable life for their citizens. City data, which is the source of our digitized knowledge about the cities, is a highly important element to achive this goal as it is the main input to build complex city ecosystems and to solve particular problems that are encountered in the cities today. In this respect, this workshop will provide a major forum to identify the challenges and opportunities in terms of better managing city data and to reveal its discriminating importance in various applications in a city ecosystem. As city data becomes more widespread and prevailing, it poses novel research problems which importantly are open to the investigation of a broad community of researchers in various fields.</p>
<p>【Keywords】:
data analytics;; data management; data monitoring; information retrieval; smart cities</p>
<h3 id="440. Managing interoperability and compleXity in health systems - MIXHS'12.">440. Managing interoperability and compleXity in health systems - MIXHS'12.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398754">Paper Link</a>】    【Pages】:2758-2759</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tao:Cui">Cui Tao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bouamrane:Matt=Mouley">Matt-Mouley Bouamrane</a></p>
<p>【Abstract】:
Data management and knowledge engineering have long been important research fields in computer science, and rapid progress in recent years have increasingly seen these technologies successfully applied to solve complex biomedical challenges and support health services professionals in the course of their intellectually-demanding clinical duties, such as through the use of decision-support or expert systems. Yet, as the biomedical knowledge available in the modern digital world grows exponentially, there is a pressing need for a focused forum to promote technology and knowledge transfer from basic research to biomedical applications as well as allowing for implementers of healthcare systems to share their experiences with the research community. The Managing Interoperability and Complexity in Health Systems, MIXHS workshops are designed for such a purpose with the view that multi-disciplinary approaches within a holistic forum is essential to rise to the ever new challenges of biomedical knowledge complexity and interoperability of health systems and services.</p>
<p>【Keywords】:
bio-medical knowledge management; electronic health systems interoperability and integration</p>
<h3 id="441. The 2012 international workshop on web-scale knowledge representation, retrieval, and reasoning.">441. The 2012 international workshop on web-scale knowledge representation, retrieval, and reasoning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398755">Paper Link</a>】    【Pages】:2760-2761</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kotoulas:Spyros">Spyros Kotoulas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zeng:Yi">Yi Zeng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Zhisheng">Zhisheng Huang</a></p>
<p>【Abstract】:
The rapid and perpetual growth of knowledge on the Web has given rise to many grand challenges (such as scalability, inconsistency, uncertainty, distribution and dynamics) for traditional knowledge processing methods and systems. Knowledge representation, retrieval and reasoning methods need to evolve and adapt to the Web to face these challenges and make this vast, heterogenous knowledge useful and accessible. In this light, the International Workshop on Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR) is initiated. This workshop serves as the third one in this workshop series. This summary discusses the scope of Web-KR and introduces the advances in this field through the accepted papers in the Web-KR 2012 workshop, co-located with CIKM 2012.</p>
<p>【Keywords】:
knowledge representation; knowledge retrieval; scalability; semantic search; web reasoning</p>
<h3 id="442. SHB 2012: international workshop on smart health and wellbeing.">442. SHB 2012: international workshop on smart health and wellbeing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398756">Paper Link</a>】    【Pages】:2762-2763</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Christopher_C=">Christopher C. Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Hsinchun">Hsinchun Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wactlar:Howard_D=">Howard D. Wactlar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Combi:Carlo">Carlo Combi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Xuning">Xuning Tang</a></p>
<p>【Abstract】:
The Smart Health and Wellbeing workshop is organized to develop a platform for authors to discuss fundamental principles, algorithms or applications of intelligent data acquisition, processing and analysis of healthcare data. We are particularly interested in information and knowledge management papers, in which the approaches are accompanied by an in-depth experimental evaluation with real world data. This paper provides an overview of the workshop and the accepted contributions.</p>
<p>【Keywords】:
healthcare; information technology; wellness</p>
<h3 id="443. Booksonline'12: 5th workshop on online books, complementary social media and their impact.">443. Booksonline'12: 5th workshop on online books, complementary social media and their impact.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398757">Paper Link</a>】    【Pages】:2764-2765</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Landoni:Monica">Monica Landoni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eickhoff:Carsten">Carsten Eickhoff</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brusilovsky:Peter">Peter Brusilovsky</a></p>
<p>【Abstract】:
BooksOnline'12, the fifth workshop in the series, aims to offer a forum for bringing together expertise from academia, industry and libraries to facilitate the exchange of research results and technology in the field of digital libraries with specific focus on online books and complementary social media. The focus of this year's workshop is "engaging reading experiences", starting from the act of deciding what to read, through the exploration and interpretation of a book's content, to sharing the overall experience. Within this overall umbrella theme, the accepted papers naturally showed three salient themes: (1) Search and Discovery, (2) Personalization and Recommendation, and Reading Experiences beyond Text. The contributions demonstrate a range of technologies, including a collaborative tabletop visual approach to support the searching and discovery of books, co-citation methods to enhance document retrieval; exploring open issues in audio-book production to support non-text based reading and improving e-book accessibility; new approaches to recommendation that take into account writing style as well as looking specifically to young readers and their needs in order to develop recommendation tools that consider both content and reading level and match these against the readers' specific interests and reading ability. Following in the theme of the reader playing a central role in the future of our digital era, we are honored to welcome Maribeth Back from FX Palo Alto and Natasa Milic-Frayling from Microsoft Research as our keynote speakers.</p>
<p>【Keywords】:
booksonline'12 workshop papers summary</p>
<h3 id="444. DTMBIO 2012: international workshop on data and text mining in biomedical informatics.">444. DTMBIO 2012: international workshop on data and text mining in biomedical informatics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398758">Paper Link</a>】    【Pages】:2766-2767</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Song:Min">Min Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Doheon">Doheon Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Hua">Hua Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Ananiadou:Sophia">Sophia Ananiadou</a></p>
<p>【Abstract】:
The organizers of ACM Sixth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO 12) are happy announce that the sixth DTMBIO will be held in conjunction with CIKM, one of the largest data management conferences. The major interests of DTMBIO are on the state-of-the-art applications of data and text mining on biomedical research problems. DTMBIO 12 will be a forum of discussing and exchanging informatics related techniques and problems in the context of biomedical research.</p>
<p>【Keywords】:
medical information systems</p>
<h3 id="445. PLEAD 2012: politics, elections and data.">445. PLEAD 2012: politics, elections and data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398759">Paper Link</a>】    【Pages】:2768-2769</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Ingmar">Ingmar Weber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Popescu:Ana=Maria">Ana-Maria Popescu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pennacchiotti:Marco">Marco Pennacchiotti</a></p>
<p>【Abstract】:
What is the role of the internet in politics general and during campaigns in particular? And what is the role of large amounts of user data in all of this? In the 2008 U.S. presidential campaign the Democrats were far more successful than the Republicans in utilizing online media for mobilization, co-ordination and fundraising. For the first time, social media and the Internet played a fundamental role in political campaigns. However, technical research in this area has been surprisingly limited and fragmented. The goal of this workshop is to bring together, for the first time, researchers working at the intersection of social network analysis, computational social science and political science, to share and discuss their ideas in a common forum; and to inspire further developments in this growing, fascinating field. The workshop has Filippo Menczer as keynote speaker, it includes technical presentations of accepted papers and concludes with a panel discussion where scientists and media experts from different fields can interact and share views.</p>
<p>【Keywords】:
Twitter; computational political science; facebook; politics; elections; social media</p>
<h3 id="446. Workshop on multimodal crowd sensing (CrowdSens 2012).">446. Workshop on multimodal crowd sensing (CrowdSens 2012).</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398760">Paper Link</a>】    【Pages】:2770-2771</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Roitman:Haggai">Haggai Roitman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cantador:Iv=aacute=n">Iván Cantador</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fern=aacute=ndez:Miriam">Miriam Fernández</a></p>
<p>【Abstract】:
This paper provides an overview of the 1st International Workshop on Multimodal Crowd Sensing (CrowdSens 2012), held at the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012). This workshop aimed to provide an open forum for researchers from various fields such as fields such as Natural Language Processing, Information Extraction, Data Mining, Information Retrieval, User Modeling and Personalization, Stream Processing, and Sensor Networks, for addressing the challenges of effectively mining, analyzing, fusing, and exploiting information sourced from multimodal physical and social sensor data sources.</p>
<p>【Keywords】:
algorithms; experimentation; human factors; performance</p>
<h3 id="447. Fifth workshop on exploiting semantic annotations in information retrieval: ESAIR"12).">447. Fifth workshop on exploiting semantic annotations in information retrieval: ESAIR"12).</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398761">Paper Link</a>】    【Pages】:2772-2773</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kamps:Jaap">Jaap Kamps</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Karlgren:Jussi">Jussi Karlgren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mika:Peter">Peter Mika</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Murdock:Vanessa">Vanessa Murdock</a></p>
<p>【Abstract】:
There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use. To complicate matters, standard text search excels at shallow information needs expressed by short keyword queries, and here semantic annotation contributes very little, if anything. The main questions for the workshop are how to leverage the rich context currently available, especially in a mobile search scenario, giving powerful new handles to exploit semantic annotations. And how can we fruitfully combine information retrieval and semantic web approaches, and for the first time work actively toward a unified view on exploiting semantic annotations.</p>
<p>【Keywords】:
semantic annotation</p>
<h3 id="448. First international workshop on information and knowledge management for developing region.">448. First international workshop on information and knowledge management for developing region.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398762">Paper Link</a>】    【Pages】:2774-2775</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agrawal:Rakesh">Rakesh Agrawal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Oard:Douglas_W=">Douglas W. Oard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rajput:Nitendra">Nitendra Rajput</a></p>
<p>【Abstract】:
Several issues arise with management of content that is generated in developing regions. Some result from linguistic diversity (as in India and Africa), some result from content being available only in forms that are more difficult to computationally manipulate (e.g., handwriting, speech, and legacy digital text in nonstandard encodings), some result from underinvestment in language resources for the languages of these regions, and some result from increased contact between cultures that have different views regarding the proper use of information and information artifacts. Such issues warrant focused attention if we are to optimally leverage information and knowledge management to the advantage of populations in developing regions. That is the purpose of this workshop.</p>
<p>【Keywords】:
information management; international development; knowledge management</p>
<h3 id="449. PIKM 2012: 5th ACM workshop for PhD students in information and knowledge management.">449. PIKM 2012: 5th ACM workshop for PhD students in information and knowledge management.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398763">Paper Link</a>】    【Pages】:2776-2777</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Varde:Aparna_S=">Aparna S. Varde</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suchanek:Fabian_M=">Fabian M. Suchanek</a></p>
<p>【Abstract】:
The PIKM 2012 workshop is the 5th of its kind after 4 successful PhD workshops at ACM CIKM. This PhD workshop invites papers that describe the Ph.D. dissertation proposals of doctoral students in any of the CIKM areas: databases, information retrieval, data mining and knowledge management. Interdisciplinary work across these tracks is particularly encouraged. This year PIKM has received around 25 submissions from over 12 countries across the globe, among which 10 have been accepted as full papers for oral presentation while 4 have been accepted as short ones for poster presentation. The selection has been conducted based on reviews submitted by an expert team comprising 21 PC members spanning 12 countries and 6 continents with a good balance of industry and academia.</p>
<p>【Keywords】:
phd; workshop</p>
<h3 id="450. WIDM 2012: the 12th international workshop on web information and data management.">450. WIDM 2012: the 12th international workshop on web information and data management.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398764">Paper Link</a>】    【Pages】:2778-2779</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fletcher:George_H=_L=">George H. L. Fletcher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Prasenjit">Prasenjit Mitra</a></p>
<p>【Abstract】:
We give an overview of WIDM 2012, held in conjunction with CIKM 2012 in Maui, Hawaii. WIDM 2012 is the twelfth in a series of international workshops on Web Information and Data Management held in conjunction with CIKM since 1998. The objective of the workshop is to bring together researchers and industrial practitioners to present and discuss leading research into how web data and information can be extracted, stored, analyzed, and processed to provide useful knowledge to end users for advanced database and web applications.</p>
<p>【Keywords】:
web data; web exploration; web information; web mining</p>
<h3 id="451. DOLAP 2012 workshop summary.">451. DOLAP 2012 workshop summary.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2396761.2398765">Paper Link</a>】    【Pages】:2780-2781</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Golfarelli:Matteo">Matteo Golfarelli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Il=Yeol">Il-Yeol Song</a></p>
<p>【Abstract】:
The ACM DOLAP workshop presents research on data warehousing and On-Line Analytical Processing (OLAP). The DOLAP 2012 program is organized in four interesting sessions on data warehouse design and maintainability, OLAP querying and trends, warehousing of complex data, performance optimization and benchmarking.</p>
<p>【Keywords】:
data warehouse; olap</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
