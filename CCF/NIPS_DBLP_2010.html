 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>


<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#23. NIPS 2010:Vancouver, British Columbia, Canada">23. NIPS 2010:Vancouver, British Columbia, Canada</a><ul>
<li><a href="#Paper Num: 292 || Session Num: 0">Paper Num: 292 || Session Num: 0</a><ul>
<li><a href="#1. Repeated Games against Budgeted Adversaries.">1. Repeated Games against Budgeted Adversaries.</a></li>
<li><a href="#2. Towards Property-Based Classification of Clustering Paradigms.">2. Towards Property-Based Classification of Clustering Paradigms.</a></li>
<li><a href="#3. Tree-Structured Stick Breaking for Hierarchical Data.">3. Tree-Structured Stick Breaking for Hierarchical Data.</a></li>
<li><a href="#4. Sparse Instrumental Variables (SPIV">4. Sparse Instrumental Variables (SPIV) for Genome-Wide Studies.</a> for Genome-Wide Studies.)</li>
<li><a href="#5. Fast global convergence rates of gradient methods for high-dimensional statistical recovery.">5. Fast global convergence rates of gradient methods for high-dimensional statistical recovery.</a></li>
<li><a href="#6. Learning Multiple Tasks using Manifold Regularization.">6. Learning Multiple Tasks using Manifold Regularization.</a></li>
<li><a href="#7. Switched Latent Force Models for Movement Segmentation.">7. Switched Latent Force Models for Movement Segmentation.</a></li>
<li><a href="#8. A POMDP Extension with Belief-dependent Rewards.">8. A POMDP Extension with Belief-dependent Rewards.</a></li>
<li><a href="#9. Global seismic monitoring as probabilistic inference.">9. Global seismic monitoring as probabilistic inference.</a></li>
<li><a href="#10. Learning invariant features using the Transformed Indian Buffet Process.">10. Learning invariant features using the Transformed Indian Buffet Process.</a></li>
<li><a href="#11. Supervised Clustering.">11. Supervised Clustering.</a></li>
<li><a href="#12. Occlusion Detection and Motion Estimation with Convex Optimization.">12. Occlusion Detection and Motion Estimation with Convex Optimization.</a></li>
<li><a href="#13. Batch Bayesian Optimization via Simulation Matching.">13. Batch Bayesian Optimization via Simulation Matching.</a></li>
<li><a href="#14. Structured sparsity-inducing norms through submodular functions.">14. Structured sparsity-inducing norms through submodular functions.</a></li>
<li><a href="#15. A Bayesian Approach to Concept Drift.">15. A Bayesian Approach to Concept Drift.</a></li>
<li><a href="#16. Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting.">16. Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting.</a></li>
<li><a href="#17. The LASSO risk: asymptotic results and real world examples.">17. The LASSO risk: asymptotic results and real world examples.</a></li>
<li><a href="#18. Extensions of Generalized Binary Search to Group Identification and Exponential Costs.">18. Extensions of Generalized Binary Search to Group Identification and Exponential Costs.</a></li>
<li><a href="#19. Label Embedding Trees for Large Multi-Class Tasks.">19. Label Embedding Trees for Large Multi-Class Tasks.</a></li>
<li><a href="#20. Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach.">20. Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach.</a></li>
<li><a href="#21. Online Classification with Specificity Constraints.">21. Online Classification with Specificity Constraints.</a></li>
<li><a href="#22. Agnostic Active Learning Without Constraints.">22. Agnostic Active Learning Without Constraints.</a></li>
<li><a href="#23. Inference with Multivariate Heavy-Tails in Linear Models.">23. Inference with Multivariate Heavy-Tails in Linear Models.</a></li>
<li><a href="#24. CUR from a Sparse Optimization Viewpoint.">24. CUR from a Sparse Optimization Viewpoint.</a></li>
<li><a href="#25. Optimal learning rates for Kernel Conjugate Gradient regression.">25. Optimal learning rates for Kernel Conjugate Gradient regression.</a></li>
<li><a href="#26. Simultaneous Object Detection and Ranking with Weak Supervision.">26. Simultaneous Object Detection and Ranking with Weak Supervision.</a></li>
<li><a href="#27. Kernel Descriptors for Visual Recognition.">27. Kernel Descriptors for Visual Recognition.</a></li>
<li><a href="#28. Fractionally Predictive Spiking Neurons.">28. Fractionally Predictive Spiking Neurons.</a></li>
<li><a href="#29. Gaussian Process Preference Elicitation.">29. Gaussian Process Preference Elicitation.</a></li>
<li><a href="#30. Predictive State Temporal Difference Learning.">30. Predictive State Temporal Difference Learning.</a></li>
<li><a href="#31. Variational Inference over Combinatorial Spaces.">31. Variational Inference over Combinatorial Spaces.</a></li>
<li><a href="#32. Bootstrapping Apprenticeship Learning.">32. Bootstrapping Apprenticeship Learning.</a></li>
<li><a href="#33. Random Projections for $k$-means Clustering.">33. Random Projections for $k$-means Clustering.</a></li>
<li><a href="#34. Segmentation as Maximum-Weight Independent Set.">34. Segmentation as Maximum-Weight Independent Set.</a></li>
<li><a href="#35. Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.">35. Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.</a></li>
<li><a href="#36. Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition.">36. Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition.</a></li>
<li><a href="#37. Learning concept graphs from text with stick-breaking priors.">37. Learning concept graphs from text with stick-breaking priors.</a></li>
<li><a href="#38. Rates of convergence for the cluster tree.">38. Rates of convergence for the cluster tree.</a></li>
<li><a href="#39. Evidence-Specific Structures for Rich Tractable CRFs.">39. Evidence-Specific Structures for Rich Tractable CRFs.</a></li>
<li><a href="#40. Predictive Subspace Learning for Multi-view Data: a Large Margin Approach.">40. Predictive Subspace Learning for Multi-view Data: a Large Margin Approach.</a></li>
<li><a href="#41. Two-Layer Generalization Analysis for Ranking Using Rademacher Average.">41. Two-Layer Generalization Analysis for Ranking Using Rademacher Average.</a></li>
<li><a href="#42. SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system.">42. SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system.</a></li>
<li><a href="#43. Movement extraction by detecting dynamics switches and repetitions.">43. Movement extraction by detecting dynamics switches and repetitions.</a></li>
<li><a href="#44. Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors.">44. Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors.</a></li>
<li><a href="#45. Universal Kernels on Non-Standard Input Spaces.">45. Universal Kernels on Non-Standard Input Spaces.</a></li>
<li><a href="#46. Causal discovery in multiple models from different experiments.">46. Causal discovery in multiple models from different experiments.</a></li>
<li><a href="#47. Empirical Risk Minimization with Approximations of Probabilistic Grammars.">47. Empirical Risk Minimization with Approximations of Probabilistic Grammars.</a></li>
<li><a href="#48. Mixture of time-warped trajectory models for movement decoding.">48. Mixture of time-warped trajectory models for movement decoding.</a></li>
<li><a href="#49. Learning Bounds for Importance Weighting.">49. Learning Bounds for Importance Weighting.</a></li>
<li><a href="#50. Learning via Gaussian Herding.">50. Learning via Gaussian Herding.</a></li>
<li><a href="#51. Spatial and anatomical regularization of SVM for brain image analysis.">51. Spatial and anatomical regularization of SVM for brain image analysis.</a></li>
<li><a href="#52. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.">52. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.</a></li>
<li><a href="#53. Random Projection Trees Revisited.">53. Random Projection Trees Revisited.</a></li>
<li><a href="#54. Throttling Poisson Processes.">54. Throttling Poisson Processes.</a></li>
<li><a href="#55. t-logistic regression.">55. t-logistic regression.</a></li>
<li><a href="#56. Implicit Differentiation by Perturbation.">56. Implicit Differentiation by Perturbation.</a></li>
<li><a href="#57. Nonparametric Bayesian Policy Priors for Reinforcement Learning.">57. Nonparametric Bayesian Policy Priors for Reinforcement Learning.</a></li>
<li><a href="#58. Over-complete representations on recurrent neural networks can support persistent percepts.">58. Over-complete representations on recurrent neural networks can support persistent percepts.</a></li>
<li><a href="#59. Distributed Dual Averaging In Networks.">59. Distributed Dual Averaging In Networks.</a></li>
<li><a href="#60. Copula Bayesian Networks.">60. Copula Bayesian Networks.</a></li>
<li><a href="#61. Error Propagation for Approximate Policy and Value Iteration.">61. Error Propagation for Approximate Policy and Value Iteration.</a></li>
<li><a href="#62. PAC-Bayesian Model Selection for Reinforcement Learning.">62. PAC-Bayesian Model Selection for Reinforcement Learning.</a></li>
<li><a href="#63. A Computational Decision Theory for Interactive Assistants.">63. A Computational Decision Theory for Interactive Assistants.</a></li>
<li><a href="#64. Parametric Bandits: The Generalized Linear Case.">64. Parametric Bandits: The Generalized Linear Case.</a></li>
<li><a href="#65. A Novel Kernel for Learning a Neuron Model from Spike Train Data.">65. A Novel Kernel for Learning a Neuron Model from Spike Train Data.</a></li>
<li><a href="#66. Extended Bayesian Information Criteria for Gaussian Graphical Models.">66. Extended Bayesian Information Criteria for Gaussian Graphical Models.</a></li>
<li><a href="#67. Shadow Dirichlet for Restricted Probability Modeling.">67. Shadow Dirichlet for Restricted Probability Modeling.</a></li>
<li><a href="#68. Size Matters: Metric Visual Search Constraints from Monocular Metadata.">68. Size Matters: Metric Visual Search Constraints from Monocular Metadata.</a></li>
<li><a href="#69. A Bayesian Framework for Figure-Ground Interpretation.">69. A Bayesian Framework for Figure-Ground Interpretation.</a></li>
<li><a href="#70. Attractor Dynamics with Synaptic Depression.">70. Attractor Dynamics with Synaptic Depression.</a></li>
<li><a href="#71. Learning Kernels with Radiuses of Minimum Enclosing Balls.">71. Learning Kernels with Radiuses of Minimum Enclosing Balls.</a></li>
<li><a href="#72. Short-term memory in neuronal networks through dynamical compressed sensing.">72. Short-term memory in neuronal networks through dynamical compressed sensing.</a></li>
<li><a href="#73. Implicit encoding of prior probabilities in optimal neural populations.">73. Implicit encoding of prior probabilities in optimal neural populations.</a></li>
<li><a href="#74. Group Sparse Coding with a Laplacian Scale Mixture Prior.">74. Group Sparse Coding with a Laplacian Scale Mixture Prior.</a></li>
<li><a href="#75. Improvements to the Sequence Memoizer.">75. Improvements to the Sequence Memoizer.</a></li>
<li><a href="#76. On Herding and the Perceptron Cycling Theorem.">76. On Herding and the Perceptron Cycling Theorem.</a></li>
<li><a href="#77. Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models.">77. Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models.</a></li>
<li><a href="#78. The Neural Costs of Optimal Control.">78. The Neural Costs of Optimal Control.</a></li>
<li><a href="#79. LSTD with Random Projections.">79. LSTD with Random Projections.</a></li>
<li><a href="#80. Humans Learn Using Manifolds, Reluctantly.">80. Humans Learn Using Manifolds, Reluctantly.</a></li>
<li><a href="#81. Universal Consistency of Multi-Class Support Vector Classification.">81. Universal Consistency of Multi-Class Support Vector Classification.</a></li>
<li><a href="#82. Learning Efficient Markov Networks.">82. Learning Efficient Markov Networks.</a></li>
<li><a href="#83. Transduction with Matrix Completion: Three Birds with One Stone.">83. Transduction with Matrix Completion: Three Birds with One Stone.</a></li>
<li><a href="#84. Near-Optimal Bayesian Active Learning with Noisy Observations.">84. Near-Optimal Bayesian Active Learning with Noisy Observations.</a></li>
<li><a href="#85. Discriminative Clustering by Regularized Information Maximization.">85. Discriminative Clustering by Regularized Information Maximization.</a></li>
<li><a href="#86. Learning to localise sounds with spiking neural networks.">86. Learning to localise sounds with spiking neural networks.</a></li>
<li><a href="#87. Feature Set Embedding for Incomplete Data.">87. Feature Set Embedding for Incomplete Data.</a></li>
<li><a href="#88. Active Instance Sampling via Matrix Partition.">88. Active Instance Sampling via Matrix Partition.</a></li>
<li><a href="#89. Avoiding False Positive in Multi-Instance Learning.">89. Avoiding False Positive in Multi-Instance Learning.</a></li>
<li><a href="#90. Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable.">90. Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable.</a></li>
<li><a href="#91. Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.">91. Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.</a></li>
<li><a href="#92. Double Q-learning.">92. Double Q-learning.</a></li>
<li><a href="#93. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction.">93. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction.</a></li>
<li><a href="#94. An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA.">94. An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA.</a></li>
<li><a href="#95. Online Learning for Latent Dirichlet Allocation.">95. Online Learning for Latent Dirichlet Allocation.</a></li>
<li><a href="#96. Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development.">96. Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development.</a></li>
<li><a href="#97. Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.">97. Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.</a></li>
<li><a href="#98. Active Learning by Querying Informative and Representative Examples.">98. Active Learning by Querying Informative and Representative Examples.</a></li>
<li><a href="#99. Exact inference and learning for cumulative distribution functions on loopy graphs.">99. Exact inference and learning for cumulative distribution functions on loopy graphs.</a></li>
<li><a href="#100. Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks.">100. Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks.</a></li>
<li><a href="#101. Co-regularization Based Semi-supervised Domain Adaptation.">101. Co-regularization Based Semi-supervised Domain Adaptation.</a></li>
<li><a href="#102. Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication.">102. Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication.</a></li>
<li><a href="#103. Dynamic Infinite Relational Model for Time-varying Relational Data Analysis.">103. Dynamic Infinite Relational Model for Time-varying Relational Data Analysis.</a></li>
<li><a href="#104. Inductive Regularized Learning of Kernel Functions.">104. Inductive Regularized Learning of Kernel Functions.</a></li>
<li><a href="#105. Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning.">105. Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning.</a></li>
<li><a href="#106. Guaranteed Rank Minimization via Singular Value Projection.">106. Guaranteed Rank Minimization via Singular Value Projection.</a></li>
<li><a href="#107. A Dirty Model for Multi-task Learning.">107. A Dirty Model for Multi-task Learning.</a></li>
<li><a href="#108. Lifted Inference Seen from the Other Side : The Tractable Features.">108. Lifted Inference Seen from the Other Side : The Tractable Features.</a></li>
<li><a href="#109. Factorized Latent Spaces with Structured Sparsity.">109. Factorized Latent Spaces with Structured Sparsity.</a></li>
<li><a href="#110. Bayesian Action-Graph Games.">110. Bayesian Action-Graph Games.</a></li>
<li><a href="#111. On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.">111. On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.</a></li>
<li><a href="#112. Linear Complementarity for Regularized Policy Evaluation and Improvement.">112. Linear Complementarity for Regularized Policy Evaluation and Improvement.</a></li>
<li><a href="#113. Synergies in learning words and their referents.">113. Synergies in learning words and their referents.</a></li>
<li><a href="#114. Structural epitome: a way to summarize one's visual experience.">114. Structural epitome: a way to summarize one's visual experience.</a></li>
<li><a href="#115. Probabilistic Belief Revision with Structural Constraints.">115. Probabilistic Belief Revision with Structural Constraints.</a></li>
<li><a href="#116. Efficient Optimization for Discriminative Latent Class Models.">116. Efficient Optimization for Discriminative Latent Class Models.</a></li>
<li><a href="#117. Non-Stochastic Bandit Slate Problems.">117. Non-Stochastic Bandit Slate Problems.</a></li>
<li><a href="#118. Static Analysis of Binary Executables Using Structural SVMs.">118. Static Analysis of Binary Executables Using Structural SVMs.</a></li>
<li><a href="#119. Using body-anchored priors for identifying actions in single images.">119. Using body-anchored priors for identifying actions in single images.</a></li>
<li><a href="#120. Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks.">120. Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks.</a></li>
<li><a href="#121. Learning Convolutional Feature Hierarchies for Visual Recognition.">121. Learning Convolutional Feature Hierarchies for Visual Recognition.</a></li>
<li><a href="#122. Accounting for network effects in neuronal responses using L1 regularized point process models.">122. Accounting for network effects in neuronal responses using L1 regularized point process models.</a></li>
<li><a href="#123. Variational bounds for mixed-data factor analysis.">123. Variational bounds for mixed-data factor analysis.</a></li>
<li><a href="#124. Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.">124. Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.</a></li>
<li><a href="#125. Regularized estimation of image statistics by Score Matching.">125. Regularized estimation of image statistics by Score Matching.</a></li>
<li><a href="#126. Random Conic Pursuit for Semidefinite Programming.">126. Random Conic Pursuit for Semidefinite Programming.</a></li>
<li><a href="#127. Generalized roof duality and bisubmodular functions.">127. Generalized roof duality and bisubmodular functions.</a></li>
<li><a href="#128. Energy Disaggregation via Discriminative Sparse Coding.">128. Energy Disaggregation via Discriminative Sparse Coding.</a></li>
<li><a href="#129. Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories.">129. Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories.</a></li>
<li><a href="#130. Structured Determinantal Point Processes.">130. Structured Determinantal Point Processes.</a></li>
<li><a href="#131. MAP Estimation for Graphical Models by Likelihood Maximization.">131. MAP Estimation for Graphical Models by Likelihood Maximization.</a></li>
<li><a href="#132. Self-Paced Learning for Latent Variable Models.">132. Self-Paced Learning for Latent Variable Models.</a></li>
<li><a href="#133. Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions.">133. Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions.</a></li>
<li><a href="#134. Beyond Actions: Discriminative Models for Contextual Group Activities.">134. Beyond Actions: Discriminative Models for Contextual Group Activities.</a></li>
<li><a href="#135. Functional Geometry Alignment and Localization of Brain Areas.">135. Functional Geometry Alignment and Localization of Brain Areas.</a></li>
<li><a href="#136. Efficient Relational Learning with Hidden Variable Detection.">136. Efficient Relational Learning with Hidden Variable Detection.</a></li>
<li><a href="#137. Learning to combine foveal glimpses with a third-order Boltzmann machine.">137. Learning to combine foveal glimpses with a third-order Boltzmann machine.</a></li>
<li><a href="#138. Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations.">138. Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations.</a></li>
<li><a href="#139. Identifying Dendritic Processing.">139. Identifying Dendritic Processing.</a></li>
<li><a href="#140. Tiled convolutional neural networks.">140. Tiled convolutional neural networks.</a></li>
<li><a href="#141. Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings.">141. Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings.</a></li>
<li><a href="#142. Evaluation of Rarity of Fingerprints in Forensics.">142. Evaluation of Rarity of Fingerprints in Forensics.</a></li>
<li><a href="#143. Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces.">143. Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces.</a></li>
<li><a href="#144. Adaptive Multi-Task Lasso: with Application to eQTL Detection.">144. Adaptive Multi-Task Lasso: with Application to eQTL Detection.</a></li>
<li><a href="#145. Practical Large-Scale Optimization for Max-norm Regularization.">145. Practical Large-Scale Optimization for Max-norm Regularization.</a></li>
<li><a href="#146. Joint Cascade Optimization Using A Product Of Boosted Classifiers.">146. Joint Cascade Optimization Using A Product Of Boosted Classifiers.</a></li>
<li><a href="#147. Learning To Count Objects in Images.">147. Learning To Count Objects in Images.</a></li>
<li><a href="#148. Optimal Web-Scale Tiering as a Flow Problem.">148. Optimal Web-Scale Tiering as a Flow Problem.</a></li>
<li><a href="#149. Feature Construction for Inverse Reinforcement Learning.">149. Feature Construction for Inverse Reinforcement Learning.</a></li>
<li><a href="#150. Convex Multiple-Instance Learning by Estimating Likelihood Ratio.">150. Convex Multiple-Instance Learning by Estimating Likelihood Ratio.</a></li>
<li><a href="#151. Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification.">151. Object Bank: A High-Level Image Representation for Scene Classification &amp; Semantic Feature Sparsification.</a></li>
<li><a href="#152. Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles.">152. Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles.</a></li>
<li><a href="#153. Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models.">153. Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models.</a></li>
<li><a href="#154. b-Bit Minwise Hashing for Estimating Three-Way Similarities.">154. b-Bit Minwise Hashing for Estimating Three-Way Similarities.</a></li>
<li><a href="#155. Construction of Dependent Dirichlet Processes based on Poisson Processes.">155. Construction of Dependent Dirichlet Processes based on Poisson Processes.</a></li>
<li><a href="#156. Deep Coding Network.">156. Deep Coding Network.</a></li>
<li><a href="#157. Moreau-Yosida Regularization for Grouped Tree Structure Learning.">157. Moreau-Yosida Regularization for Grouped Tree Structure Learning.</a></li>
<li><a href="#158. Stability Approach to Regularization Selection (StARS">158. Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models.</a> for High Dimensional Graphical Models.)</li>
<li><a href="#159. Multivariate Dyadic Regression Trees for Sparse Learning Problems.">159. Multivariate Dyadic Regression Trees for Sparse Learning Problems.</a></li>
<li><a href="#160. Robust Clustering as Ensembles of Affinity Relations.">160. Robust Clustering as Ensembles of Affinity Relations.</a></li>
<li><a href="#161. Graph-Valued Regression.">161. Graph-Valued Regression.</a></li>
<li><a href="#162. Multi-Stage Dantzig Selector.">162. Multi-Stage Dantzig Selector.</a></li>
<li><a href="#163. Decoding Ipsilateral Finger Movements from ECoG Signals in Humans.">163. Decoding Ipsilateral Finger Movements from ECoG Signals in Humans.</a></li>
<li><a href="#164. Approximate Inference by Compilation to Arithmetic Circuits.">164. Approximate Inference by Compilation to Arithmetic Circuits.</a></li>
<li><a href="#165. Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference.">165. Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference.</a></li>
<li><a href="#166. Functional form of motion priors in human motion perception.">166. Functional form of motion priors in human motion perception.</a></li>
<li><a href="#167. Learning from Candidate Labeling Sets.">167. Learning from Candidate Labeling Sets.</a></li>
<li><a href="#168. Decomposing Isotonic Regression for Efficiently Solving Large Problems.">168. Decomposing Isotonic Regression for Efficiently Solving Large Problems.</a></li>
<li><a href="#169. Getting lost in space: Large sample analysis of the resistance distance.">169. Getting lost in space: Large sample analysis of the resistance distance.</a></li>
<li><a href="#170. Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform.">170. Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform.</a></li>
<li><a href="#171. Permutation Complexity Bound on Out-Sample Error.">171. Permutation Complexity Bound on Out-Sample Error.</a></li>
<li><a href="#172. Basis Construction from Power Series Expansions of Value Functions.">172. Basis Construction from Power Series Expansions of Value Functions.</a></li>
<li><a href="#173. Scrambled Objects for Least-Squares Regression.">173. Scrambled Objects for Least-Squares Regression.</a></li>
<li><a href="#174. Network Flow Algorithms for Structured Sparsity.">174. Network Flow Algorithms for Structured Sparsity.</a></li>
<li><a href="#175. Sphere Embedding: An Application to Part-of-Speech Induction.">175. Sphere Embedding: An Application to Part-of-Speech Induction.</a></li>
<li><a href="#176. Variable margin losses for classifier design.">176. Variable margin losses for classifier design.</a></li>
<li><a href="#177. Why are some word orders more common than others? A uniform information density account.">177. Why are some word orders more common than others? A uniform information density account.</a></li>
<li><a href="#178. Direct Loss Minimization for Structured Prediction.">178. Direct Loss Minimization for Structured Prediction.</a></li>
<li><a href="#179. Gated Softmax Classification.">179. Gated Softmax Classification.</a></li>
<li><a href="#180. A Family of Penalty Functions for Structured Sparsity.">180. A Family of Penalty Functions for Structured Sparsity.</a></li>
<li><a href="#181. Subgraph Detection Using Eigenvector L1 Norms.">181. Subgraph Detection Using Eigenvector L1 Norms.</a></li>
<li><a href="#182. A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model.">182. A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model.</a></li>
<li><a href="#183. Large-Scale Matrix Factorization with Missing Data under Additional Constraints.">183. Large-Scale Matrix Factorization with Missing Data under Additional Constraints.</a></li>
<li><a href="#184. Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks.">184. Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks.</a></li>
<li><a href="#185. An analysis on negative curvature induced by singularity in multi-layer neural-network learning.">185. An analysis on negative curvature induced by singularity in multi-layer neural-network learning.</a></li>
<li><a href="#186. Layer-wise analysis of deep networks with Gaussian kernels.">186. Layer-wise analysis of deep networks with Gaussian kernels.</a></li>
<li><a href="#187. Probabilistic latent variable models for distinguishing between cause and effect.">187. Probabilistic latent variable models for distinguishing between cause and effect.</a></li>
<li><a href="#188. A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups.">188. A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups.</a></li>
<li><a href="#189. Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures.">189. Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures.</a></li>
<li><a href="#190. Improving Human Judgments by Decontaminating Sequential Dependencies.">190. Improving Human Judgments by Decontaminating Sequential Dependencies.</a></li>
<li><a href="#191. A Theory of Multiclass Boosting.">191. A Theory of Multiclass Boosting.</a></li>
<li><a href="#192. A biologically plausible network for the computation of orientation dominance.">192. A biologically plausible network for the computation of orientation dominance.</a></li>
<li><a href="#193. Slice sampling covariance hyperparameters of latent Gaussian models.">193. Slice sampling covariance hyperparameters of latent Gaussian models.</a></li>
<li><a href="#194. On the Convexity of Latent Social Network Inference.">194. On the Convexity of Latent Social Network Inference.</a></li>
<li><a href="#195. Infinite Relational Modeling of Functional Connectivity in Resting State fMRI.">195. Infinite Relational Modeling of Functional Connectivity in Resting State fMRI.</a></li>
<li><a href="#196. Minimum Average Cost Clustering.">196. Minimum Average Cost Clustering.</a></li>
<li><a href="#197. Global Analytic Solution for Variational Bayesian Matrix Factorization.">197. Global Analytic Solution for Variational Bayesian Matrix Factorization.</a></li>
<li><a href="#198. Random Walk Approach to Regret Minimization.">198. Random Walk Approach to Regret Minimization.</a></li>
<li><a href="#199. Sample Complexity of Testing the Manifold Hypothesis.">199. Sample Complexity of Testing the Manifold Hypothesis.</a></li>
<li><a href="#200. Learning the context of a category.">200. Learning the context of a category.</a></li>
<li><a href="#201. Online Markov Decision Processes under Bandit Feedback.">201. Online Markov Decision Processes under Bandit Feedback.</a></li>
<li><a href="#202. Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization.">202. Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization.</a></li>
<li><a href="#203. Generative Local Metric Learning for Nearest Neighbor Classification.">203. Generative Local Metric Learning for Nearest Neighbor Classification.</a></li>
<li><a href="#204. Approximate inference in continuous time Gaussian-Jump processes.">204. Approximate inference in continuous time Gaussian-Jump processes.</a></li>
<li><a href="#205. New Adaptive Algorithms for Online Classification.">205. New Adaptive Algorithms for Online Classification.</a></li>
<li><a href="#206. Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs.">206. Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs.</a></li>
<li><a href="#207. Gaussian sampling by local perturbations.">207. Gaussian sampling by local perturbations.</a></li>
<li><a href="#208. Large Margin Multi-Task Metric Learning.">208. Large Margin Multi-Task Metric Learning.</a></li>
<li><a href="#209. Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers.">209. Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers.</a></li>
<li><a href="#210. (RF">210. (RF)^2 - Random Forest Random Field.</a>^2 - Random Forest Random Field.)</li>
<li><a href="#211. On the Theory of Learnining with Privileged Information.">211. On the Theory of Learnining with Privileged Information.</a></li>
<li><a href="#212. Empirical Bernstein Inequalities for U-Statistics.">212. Empirical Bernstein Inequalities for U-Statistics.</a></li>
<li><a href="#213. Learning Networks of Stochastic Differential Equations.">213. Learning Networks of Stochastic Differential Equations.</a></li>
<li><a href="#214. Word Features for Latent Dirichlet Allocation.">214. Word Features for Latent Dirichlet Allocation.</a></li>
<li><a href="#215. Reverse Multi-Label Learning.">215. Reverse Multi-Label Learning.</a></li>
<li><a href="#216. Probabilistic Deterministic Infinite Automata.">216. Probabilistic Deterministic Infinite Automata.</a></li>
<li><a href="#217. The Maximal Causes of Natural Scenes are Edge Filters.">217. The Maximal Causes of Natural Scenes are Edge Filters.</a></li>
<li><a href="#218. A New Probabilistic Model for Rank Aggregation.">218. A New Probabilistic Model for Rank Aggregation.</a></li>
<li><a href="#219. Multitask Learning without Label Correspondences.">219. Multitask Learning without Label Correspondences.</a></li>
<li><a href="#220. Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics.">220. Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics.</a></li>
<li><a href="#221. Online Learning: Random Averages, Combinatorial Parameters, and Learnability.">221. Online Learning: Random Averages, Combinatorial Parameters, and Learnability.</a></li>
<li><a href="#222. Evaluating neuronal codes for inference using Fisher information.">222. Evaluating neuronal codes for inference using Fisher information.</a></li>
<li><a href="#223. Generating more realistic images using gated MRF's.">223. Generating more realistic images using gated MRF's.</a></li>
<li><a href="#224. An Approximate Inference Approach to Temporal Optimization in Optimal Control.">224. An Approximate Inference Approach to Temporal Optimization in Optimal Control.</a></li>
<li><a href="#225. MAP estimation in Binary MRFs via Bipartite Multi-cuts.">225. MAP estimation in Binary MRFs via Bipartite Multi-cuts.</a></li>
<li><a href="#226. Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model.">226. Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model.</a></li>
<li><a href="#227. Link Discovery using Graph Feature Tracking.">227. Link Discovery using Graph Feature Tracking.</a></li>
<li><a href="#228. An Alternative to Low-level-Sychrony-Based Methods for Speech Detection.">228. An Alternative to Low-level-Sychrony-Based Methods for Speech Detection.</a></li>
<li><a href="#229. Tight Sample Complexity of Large-Margin Learning.">229. Tight Sample Complexity of Large-Margin Learning.</a></li>
<li><a href="#230. Boosting Classifier Cascades.">230. Boosting Classifier Cascades.</a></li>
<li><a href="#231. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm.">231. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm.</a></li>
<li><a href="#232. Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation.">232. Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation.</a></li>
<li><a href="#233. Deterministic Single-Pass Algorithm for LDA.">233. Deterministic Single-Pass Algorithm for LDA.</a></li>
<li><a href="#234. Active Estimation of F-Measures.">234. Active Estimation of F-Measures.</a></li>
<li><a href="#235. Trading off Mistakes and Don't-Know Predictions.">235. Trading off Mistakes and Don't-Know Predictions.</a></li>
<li><a href="#236. Sparse Inverse Covariance Selection via Alternating Linearization Methods.">236. Sparse Inverse Covariance Selection via Alternating Linearization Methods.</a></li>
<li><a href="#237. Spike timing-dependent plasticity as dynamic filter.">237. Spike timing-dependent plasticity as dynamic filter.</a></li>
<li><a href="#238. A novel family of non-parametric cumulative based divergences for point processes.">238. A novel family of non-parametric cumulative based divergences for point processes.</a></li>
<li><a href="#239. Online Learning in The Manifold of Low-Rank Matrices.">239. Online Learning in The Manifold of Low-Rank Matrices.</a></li>
<li><a href="#240. Identifying graph-structured activation patterns in networks.">240. Identifying graph-structured activation patterns in networks.</a></li>
<li><a href="#241. A rational decision making framework for inhibitory control.">241. A rational decision making framework for inhibitory control.</a></li>
<li><a href="#242. Penalized Principal Component Regression on Graphs for Analysis of Subnetworks.">242. Penalized Principal Component Regression on Graphs for Analysis of Subnetworks.</a></li>
<li><a href="#243. Monte-Carlo Planning in Large POMDPs.">243. Monte-Carlo Planning in Large POMDPs.</a></li>
<li><a href="#244. Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models.">244. Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models.</a></li>
<li><a href="#245. More data means less inference: A pseudo-max approach to structured learning.">245. More data means less inference: A pseudo-max approach to structured learning.</a></li>
<li><a href="#246. Reward Design via Online Gradient Ascent.">246. Reward Design via Online Gradient Ascent.</a></li>
<li><a href="#247. Smoothness, Low Noise and Fast Rates.">247. Smoothness, Low Noise and Fast Rates.</a></li>
<li><a href="#248. Efficient Minimization of Decomposable Submodular Functions.">248. Efficient Minimization of Decomposable Submodular Functions.</a></li>
<li><a href="#249. Learning from Logged Implicit Exploration Data.">249. Learning from Logged Implicit Exploration Data.</a></li>
<li><a href="#250. Layered image motion with explicit occlusions, temporal consistency, and depth ordering.">250. Layered image motion with explicit occlusions, temporal consistency, and depth ordering.</a></li>
<li><a href="#251. Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices.">251. Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices.</a></li>
<li><a href="#252. A Reduction from Apprenticeship Learning to Classification.">252. A Reduction from Apprenticeship Learning to Classification.</a></li>
<li><a href="#253. Semi-Supervised Learning with Adversarially Missing Label Information.">253. Semi-Supervised Learning with Adversarially Missing Label Information.</a></li>
<li><a href="#254. Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch.">254. Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch.</a></li>
<li><a href="#255. Switching state space model for simultaneously estimating state transitions and nonstationary firing rates.">255. Switching state space model for simultaneously estimating state transitions and nonstationary firing rates.</a></li>
<li><a href="#256. Pose-Sensitive Embedding by Nonlinear NCA Regression.">256. Pose-Sensitive Embedding by Nonlinear NCA Regression.</a></li>
<li><a href="#257. Fast Large-scale Mixture Modeling with Component-specific Data Partitions.">257. Fast Large-scale Mixture Modeling with Component-specific Data Partitions.</a></li>
<li><a href="#258. Policy gradients in linearly-solvable MDPs.">258. Policy gradients in linearly-solvable MDPs.</a></li>
<li><a href="#259. Phoneme Recognition with Large Hierarchical Reservoirs.">259. Phoneme Recognition with Large Hierarchical Reservoirs.</a></li>
<li><a href="#260. Exact learning curves for Gaussian process regression on large random graphs.">260. Exact learning curves for Gaussian process regression on large random graphs.</a></li>
<li><a href="#261. Brain covariance selection: better individual functional connectivity models using population prior.">261. Brain covariance selection: better individual functional connectivity models using population prior.</a></li>
<li><a href="#262. Fast detection of multiple change-points shared by many signals using group LARS.">262. Fast detection of multiple change-points shared by many signals using group LARS.</a></li>
<li><a href="#263. Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.">263. Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.</a></li>
<li><a href="#264. Worst-case bounds on the quality of max-product fixed-points.">264. Worst-case bounds on the quality of max-product fixed-points.</a></li>
<li><a href="#265. Multiple Kernel Learning and the SMO Algorithm.">265. Multiple Kernel Learning and the SMO Algorithm.</a></li>
<li><a href="#266. Spectral Regularization for Support Estimation.">266. Spectral Regularization for Support Estimation.</a></li>
<li><a href="#267. Unsupervised Kernel Dimension Reduction.">267. Unsupervised Kernel Dimension Reduction.</a></li>
<li><a href="#268. Multi-View Active Learning in the Non-Realizable Case.">268. Multi-View Active Learning in the Non-Realizable Case.</a></li>
<li><a href="#269. A Discriminative Latent Model of Image Region and Object Tag Correspondence.">269. A Discriminative Latent Model of Image Region and Object Tag Correspondence.</a></li>
<li><a href="#270. Joint Analysis of Time-Evolving Binary Matrices and Associated Documents.">270. Joint Analysis of Time-Evolving Binary Matrices and Associated Documents.</a></li>
<li><a href="#271. Heavy-Tailed Process Priors for Selective Shrinkage.">271. Heavy-Tailed Process Priors for Selective Shrinkage.</a></li>
<li><a href="#272. Sidestepping Intractable Inference with Structured Ensemble Cascades.">272. Sidestepping Intractable Inference with Structured Ensemble Cascades.</a></li>
<li><a href="#273. The Multidimensional Wisdom of Crowds.">273. The Multidimensional Wisdom of Crowds.</a></li>
<li><a href="#274. Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains.">274. Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains.</a></li>
<li><a href="#275. Active Learning Applied to Patient-Adaptive Heartbeat Classification.">275. Active Learning Applied to Patient-Adaptive Heartbeat Classification.</a></li>
<li><a href="#276. Probabilistic Inference and Differential Privacy.">276. Probabilistic Inference and Differential Privacy.</a></li>
<li><a href="#277. Copula Processes.">277. Copula Processes.</a></li>
<li><a href="#278. Linear readout from a neural population with partial correlation data.">278. Linear readout from a neural population with partial correlation data.</a></li>
<li><a href="#279. A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration.">279. A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration.</a></li>
<li><a href="#280. A unified model of short-range and long-range motion perception.">280. A unified model of short-range and long-range motion perception.</a></li>
<li><a href="#281. Distributionally Robust Markov Decision Processes.">281. Distributionally Robust Markov Decision Processes.</a></li>
<li><a href="#282. Inference and communication in the game of Password.">282. Inference and communication in the game of Password.</a></li>
<li><a href="#283. Robust PCA via Outlier Pursuit.">283. Robust PCA via Outlier Pursuit.</a></li>
<li><a href="#284. Relaxed Clipping: A Global Training Method for Robust Regression and Classification.">284. Relaxed Clipping: A Global Training Method for Robust Regression and Classification.</a></li>
<li><a href="#285. Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike.">285. Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike.</a></li>
<li><a href="#286. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty.">286. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty.</a></li>
<li><a href="#287. Lower Bounds on Rate of Convergence of Cutting Plane Methods.">287. Lower Bounds on Rate of Convergence of Cutting Plane Methods.</a></li>
<li><a href="#288. Probabilistic Multi-Task Feature Selection.">288. Probabilistic Multi-Task Feature Selection.</a></li>
<li><a href="#289. Worst-Case Linear Discriminant Analysis.">289. Worst-Case Linear Discriminant Analysis.</a></li>
<li><a href="#290. Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework.">290. Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework.</a></li>
<li><a href="#291. Large Margin Learning of Upstream Scene Understanding Models.">291. Large Margin Learning of Upstream Scene Understanding Models.</a></li>
<li><a href="#292. Parallelized Stochastic Gradient Descent.">292. Parallelized Stochastic Gradient Descent.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="23. NIPS 2010:Vancouver, British Columbia, Canada">23. NIPS 2010:Vancouver, British Columbia, Canada</h1>
<p><a href="http://papers.nips.cc/book/advances-in-neural-information-processing-systems-23-2010">Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada.</a> NIPS
【<a href="http://dblp.uni-trier.de/db/conf/nips/nips2010.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 292 || Session Num: 0">Paper Num: 292 || Session Num: 0</h2>
<h3 id="1. Repeated Games against Budgeted Adversaries.">1. Repeated Games against Budgeted Adversaries.</h3>
<p>【<a href="http://papers.nips.cc/paper/4123-repeated-games-against-budgeted-adversaries">Paper Link</a>】    【Pages】:1-9</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Abernethy:Jacob">Jacob Abernethy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Warmuth:Manfred_K=">Manfred K. Warmuth</a></p>
<p>【Abstract】:
We study repeated zero-sum games against an adversary on a budget. Given that an adversary has some constraint on the sequence of actions that he plays, we consider what ought to be the player's best mixed strategy with knowledge of this budget. We show that, for a general class of normal-form games, the minimax strategy is indeed efficiently computable and relies on a random playout" technique. We give three diverse applications of this algorithmic template: a cost-sensitive "Hedge" setting, a particular problem in Metrical Task Systems, and the design of combinatorial prediction markets."</p>
<p>【Keywords】:</p>
<h3 id="2. Towards Property-Based Classification of Clustering Paradigms.">2. Towards Property-Based Classification of Clustering Paradigms.</h3>
<p>【<a href="http://papers.nips.cc/paper/4101-towards-property-based-classification-of-clustering-paradigms">Paper Link</a>】    【Pages】:10-18</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ackerman:Margareta">Margareta Ackerman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Ben=David:Shai">Shai Ben-David</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Loker:David">David Loker</a></p>
<p>【Abstract】:
Clustering is a basic data mining task with a wide variety of applications. Not surprisingly, there exist many clustering algorithms. However, clustering is an ill defined problem - given a data set, it is not clear what a “correct” clustering for that set is. Indeed, different algorithms may yield dramatically different outputs for the same input sets. Faced with a concrete clustering task, a user needs to choose an appropriate clustering algorithm. Currently, such decisions are often made in a very ad hoc, if not completely random, manner. Given the crucial effect of the choice of a clustering algorithm on the resulting clustering, this state of affairs is truly regrettable. In this paper we address the major research challenge of developing tools for helping users make more informed decisions when they come to pick a clustering tool for their data. This is, of course, a very ambitious endeavor, and in this paper, we make some first steps towards this goal. We propose to address this problem by distilling abstract properties of the input-output behavior of different clustering paradigms.  In this paper, we demonstrate how abstract, intuitive properties of clustering functions can be used to taxonomize a set of popular clustering algorithmic paradigms. On top of addressing deterministic clustering algorithms, we also propose similar properties for randomized algorithms and use them to highlight functional differences between different common implementations of k-means clustering. We also study relationships between the properties, independent of any particular algorithm. In particular, we strengthen Kleinbergs famous impossibility result, while providing a simpler proof.</p>
<p>【Keywords】:</p>
<h3 id="3. Tree-Structured Stick Breaking for Hierarchical Data.">3. Tree-Structured Stick Breaking for Hierarchical Data.</h3>
<p>【<a href="http://papers.nips.cc/paper/4108-tree-structured-stick-breaking-for-hierarchical-data">Paper Link</a>】    【Pages】:19-27</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Adams:Ryan_Prescott">Ryan Prescott Adams</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghahramani:Zoubin">Zoubin Ghahramani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jordan:Michael_I=">Michael I. Jordan</a></p>
<p>【Abstract】:
Many data are naturally modeled by an unobserved hierarchical structure. In this paper we propose a flexible nonparametric prior over unknown data hierarchies. The approach uses nested stick-breaking processes to allow for trees of unbounded width and depth, where data can live at any node and are infinitely exchangeable. One can view our model as providing infinite mixtures where the components have a dependency structure corresponding to an evolutionary diffusion down a tree. By using a stick-breaking approach, we can apply Markov chain Monte Carlo methods based on slice sampling to perform Bayesian inference and simulate from the posterior distribution on trees. We apply our method to hierarchical clustering of images and topic modeling of text data.</p>
<p>【Keywords】:</p>
<h3 id="4. Sparse Instrumental Variables (SPIV) for Genome-Wide Studies.">4. Sparse Instrumental Variables (SPIV) for Genome-Wide Studies.</h3>
<p>【<a href="http://papers.nips.cc/paper/3976-sparse-instrumental-variables-spiv-for-genome-wide-studies">Paper Link</a>】    【Pages】:28-36</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agakov:Felix_V=">Felix V. Agakov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/McKeigue:Paul">Paul McKeigue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krohn:Jon">Jon Krohn</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Storkey:Amos_J=">Amos J. Storkey</a></p>
<p>【Abstract】:
This paper describes a probabilistic framework for studying associations between multiple genotypes, biomarkers, and phenotypic traits in the presence of noise and unobserved confounders for large genetic studies. The framework builds on sparse linear methods developed for regression and modified here for inferring causal structures of richer networks with latent variables. The method is motivated by the use of genotypes as ``instruments'' to infer causal associations between phenotypic biomarkers and outcomes, without making the common restrictive assumptions of instrumental variable methods. The method may be used for an effective screening of potentially interesting genotype phenotype and biomarker-phenotype associations in genome-wide studies, which may have important implications for validating biomarkers as possible proxy endpoints for early stage clinical trials. Where the biomarkers are gene transcripts, the method can be used for fine mapping of quantitative trait loci (QTLs) detected in genetic linkage studies. The method is applied for examining effects of gene transcript levels in the liver on plasma HDL cholesterol levels for a sample of sequenced mice from a heterogeneous stock, with $\sim 10^5$ genetic instruments and $\sim 47 \times 10^3$ gene transcripts.</p>
<p>【Keywords】:</p>
<h3 id="5. Fast global convergence rates of gradient methods for high-dimensional statistical recovery.">5. Fast global convergence rates of gradient methods for high-dimensional statistical recovery.</h3>
<p>【<a href="http://papers.nips.cc/paper/3984-fast-global-convergence-rates-of-gradient-methods-for-high-dimensional-statistical-recovery">Paper Link</a>】    【Pages】:37-45</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Alekh">Alekh Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Negahban:Sahand">Sahand Negahban</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wainwright:Martin_J=">Martin J. Wainwright</a></p>
<p>【Abstract】:
Many statistical $M$-estimators are based on convex optimization problems formed by the weighted sum of a loss function with a norm-based regularizer.  We analyze the convergence rates of first-order gradient methods for solving such problems within a high-dimensional framework that allows the data dimension $d$ to grow with (and possibly exceed) the sample size $n$.  This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie classical optimization analysis.  We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models.  Under these conditions, our theory guarantees that Nesterov's first-order method~\cite{Nesterov07} has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical Euclidean distance between the true unknown parameter $\theta^*$ and the optimal solution $\widehat{\theta}$.  This globally linear rate is substantially faster than previous analyses of global convergence for specific methods that yielded only sublinear rates.  Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\ell_1$-regularized regression), group Lasso, block sparsity, and low-rank matrix recovery using nuclear norm regularization.  Overall, this result reveals an interesting connection between statistical precision and computational efficiency in high-dimensional estimation.</p>
<p>【Keywords】:</p>
<h3 id="6. Learning Multiple Tasks using Manifold Regularization.">6. Learning Multiple Tasks using Manifold Regularization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4163-learning-multiple-tasks-using-manifold-regularization">Paper Link</a>】    【Pages】:46-54</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Arvind">Arvind Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Daum=eacute=_III:Hal">Hal Daumé III</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gerber:Samuel">Samuel Gerber</a></p>
<p>【Abstract】:
We present a novel method for multitask learning (MTL) based on {\it manifold regularization}: assume that all task parameters lie on a manifold. This is the generalization of a common assumption made in the existing literature: task parameters share a common {\it linear} subspace. One proposed method uses the projection distance from the manifold to regularize the task parameters. The manifold structure and the task parameters are learned using an alternating optimization framework. When the manifold structure is fixed, our method decomposes across tasks which can be learnt independently. An approximation of the manifold regularization scheme is presented that preserves the convexity of the single task learning problem, and makes the proposed MTL framework efficient and easy to implement. We show the efficacy of our method on several datasets.</p>
<p>【Keywords】:</p>
<h3 id="7. Switched Latent Force Models for Movement Segmentation.">7. Switched Latent Force Models for Movement Segmentation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4001-switched-latent-force-models-for-movement-segmentation">Paper Link</a>】    【Pages】:55-63</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/=/=Aacute=lvarez:Mauricio_A=">Mauricio A. Álvarez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Peters_0001:Jan">Jan Peters</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=ouml=lkopf:Bernhard">Bernhard Schölkopf</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lawrence:Neil_D=">Neil D. Lawrence</a></p>
<p>【Abstract】:
Latent force models encode the interaction between multiple related dynamical systems in the form of a kernel or covariance function. Each variable to be modeled is represented as the output of a differential equation and each differential equation is driven by a weighted sum of latent functions with uncertainty given by a Gaussian process prior. In this paper we consider employing the latent force model framework for the problem of determining robot motor primitives. To deal with discontinuities in the dynamical systems or the latent driving force we introduce an extension of the basic latent force model, that switches between different latent functions and potentially different dynamical systems. This creates a versatile representation for robot movements that can capture discrete changes and non-linearities in the dynamics. We give illustrative examples on both synthetic data and for striking movements recorded using a Barrett WAM robot as haptic input device. Our inspiration is robot motor primitives, but we expect our model to have wide application for dynamical systems including models for human motion capture data and systems biology.</p>
<p>【Keywords】:</p>
<h3 id="8. A POMDP Extension with Belief-dependent Rewards.">8. A POMDP Extension with Belief-dependent Rewards.</h3>
<p>【<a href="http://papers.nips.cc/paper/3971-a-pomdp-extension-with-belief-dependent-rewards">Paper Link</a>】    【Pages】:64-72</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Araya=L=oacute=pez:Mauricio">Mauricio Araya-López</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Buffet:Olivier">Olivier Buffet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thomas:Vincent">Vincent Thomas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Charpillet:Fran=ccedil=ois">François Charpillet</a></p>
<p>【Abstract】:
Partially Observable Markov Decision Processes (POMDPs) model sequential decision-making problems under uncertainty and partial observability. Unfortunately, some problems cannot be modeled with state-dependent reward functions, e.g., problems whose objective explicitly implies reducing the uncertainty on the state. To that end, we introduce rho-POMDPs, an extension of POMDPs where the reward function rho depends on the belief state. We show that, under the common assumption that rho is convex, the value function is also convex, what makes it possible to (1) approximate rho arbitrarily well with a piecewise linear and convex (PWLC) function, and (2) use state-of-the-art exact or approximate solving algorithms with limited changes.</p>
<p>【Keywords】:</p>
<h3 id="9. Global seismic monitoring as probabilistic inference.">9. Global seismic monitoring as probabilistic inference.</h3>
<p>【<a href="http://papers.nips.cc/paper/4100-global-seismic-monitoring-as-probabilistic-inference">Paper Link</a>】    【Pages】:73-81</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Arora:Nimar_S=">Nimar S. Arora</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Russell:Stuart_J=">Stuart J. Russell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kidwell:Paul">Paul Kidwell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sudderth:Erik_B=">Erik B. Sudderth</a></p>
<p>【Abstract】:
The International Monitoring System (IMS) is a global network of sensors whose purpose is to identify potential violations of the Comprehensive Nuclear-Test-Ban Treaty (CTBT), primarily through detection and localization of seismic events. We report on the first stage of a project to improve on the current automated software system with a Bayesian inference system that computes the most likely global event history given the record of local sensor data. The new system, VISA (Vertically Integrated Seismological Analysis), is based on empirically calibrated, generative models of event occurrence, signal propagation, and signal detection. VISA exhibits significantly improved precision and recall compared to the current operational system and is able to detect events that are missed even by the human analysts who post-process the IMS output.</p>
<p>【Keywords】:</p>
<h3 id="10. Learning invariant features using the Transformed Indian Buffet Process.">10. Learning invariant features using the Transformed Indian Buffet Process.</h3>
<p>【<a href="http://papers.nips.cc/paper/4049-learning-invariant-features-using-the-transformed-indian-buffet-process">Paper Link</a>】    【Pages】:82-90</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Austerweil:Joseph_L=">Joseph L. Austerweil</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Griffiths:Thomas_L=">Thomas L. Griffiths</a></p>
<p>【Abstract】:
Identifying the features of objects becomes a challenge when those features can change in their appearance. We introduce the Transformed Indian Buffet Process (tIBP), and use it to define a nonparametric Bayesian model that infers features that can transform across instantiations. We show that this model can identify features that are location invariant by modeling a previous experiment on human feature learning. However, allowing features to transform adds new kinds of ambiguity: Are two parts of an object the same feature with different transformations or two unique features? What transformations can features undergo? We present two new experiments in which we explore how people resolve these questions, showing that the tIBP model demonstrates a similar sensitivity to context to that shown by human learners when determining the invariant aspects of features.</p>
<p>【Keywords】:</p>
<h3 id="11. Supervised Clustering.">11. Supervised Clustering.</h3>
<p>【<a href="http://papers.nips.cc/paper/4115-supervised-clustering">Paper Link</a>】    【Pages】:91-99</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Awasthi:Pranjal">Pranjal Awasthi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zadeh:Reza_Bosagh">Reza Bosagh Zadeh</a></p>
<p>【Abstract】:
Despite the ubiquity of clustering as a tool in unsupervised learning, there is not yet a consensus on a formal theory, and the vast majority  of work in this direction has focused on unsupervised clustering. We study a recently proposed framework for supervised clustering where there is access to a teacher. We give an improved generic algorithm to cluster any concept class  in that model. Our algorithm is query-efficient in the sense that it involves only a small amount  of interaction with the teacher. We also present and study two natural generalizations of the model.  The model assumes that the teacher response to the algorithm is perfect. We eliminate  this limitation by proposing a noisy model and give an algorithm for  clustering the class of intervals in this noisy model. We also propose a dynamic model where the teacher sees  a random subset of the points. Finally, for datasets  satisfying a spectrum of weak to strong properties, we give query bounds, and show that a class  of clustering functions containing Single-Linkage will find the target clustering under the strongest  property.</p>
<p>【Keywords】:</p>
<h3 id="12. Occlusion Detection and Motion Estimation with Convex Optimization.">12. Occlusion Detection and Motion Estimation with Convex Optimization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4118-occlusion-detection-and-motion-estimation-with-convex-optimization">Paper Link</a>】    【Pages】:100-108</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ayvaci:Alper">Alper Ayvaci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raptis:Michalis">Michalis Raptis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Soatto:Stefano">Stefano Soatto</a></p>
<p>【Abstract】:
We tackle the problem of simultaneously detecting occlusions and estimating optical flow. We show that, under standard assumptions of Lambertian reflection and static illumination, the task can be posed as a convex minimization problem. Therefore, the solution, computed using efficient algorithms, is guaranteed to be globally optimal, for any number of independently moving objects, and any number of occlusion layers. We test the proposed algorithm on benchmark datasets, expanded to enable evaluation of occlusion detection performance.</p>
<p>【Keywords】:</p>
<h3 id="13. Batch Bayesian Optimization via Simulation Matching.">13. Batch Bayesian Optimization via Simulation Matching.</h3>
<p>【<a href="http://papers.nips.cc/paper/4083-batch-bayesian-optimization-via-simulation-matching">Paper Link</a>】    【Pages】:109-117</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Azimi:Javad">Javad Azimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fern:Alan">Alan Fern</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fern:Xiaoli">Xiaoli Fern</a></p>
<p>【Abstract】:
Bayesian optimization methods are often used to optimize unknown functions that are costly to evaluate. Typically, these methods sequentially select inputs to be evaluated one at a time based on a posterior over the unknown function that is updated after each evaluation. There are a number of effective sequential policies for selecting the individual inputs. In many applications, however, it is desirable to perform multiple evaluations in parallel, which requires selecting batches of multiple inputs to evaluate at once. In this paper, we propose a novel approach to batch Bayesian optimization, providing a policy for selecting batches of inputs with the goal of optimizing the function as efficiently as possible. The key idea is to exploit the availability of high-quality and efficient sequential policies, by using Monte-Carlo simulation to select input batches that closely match their expected behavior. To the best of our knowledge, this is the first batch selection policy for Bayesian optimization. Our experimental results on six benchmarks show that the proposed approach significantly outperforms two baselines and can lead to large advantages over a top sequential approach in terms of performance per unit time.</p>
<p>【Keywords】:</p>
<h3 id="14. Structured sparsity-inducing norms through submodular functions.">14. Structured sparsity-inducing norms through submodular functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/3933-structured-sparsity-inducing-norms-through-submodular-functions">Paper Link</a>】    【Pages】:118-126</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Francis_R=">Francis R. Bach</a></p>
<p>【Abstract】:
Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its Lovasz extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.</p>
<p>【Keywords】:</p>
<h3 id="15. A Bayesian Approach to Concept Drift.">15. A Bayesian Approach to Concept Drift.</h3>
<p>【<a href="http://papers.nips.cc/paper/4129-a-bayesian-approach-to-concept-drift">Paper Link</a>】    【Pages】:127-135</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Stephen_H=">Stephen H. Bach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maloof:Marcus_A=">Marcus A. Maloof</a></p>
<p>【Abstract】:
To cope with concept drift, we placed a probability distribution over the location of the most-recent drift point. We used Bayesian model comparison to update this distribution from the predictions of models trained on blocks of consecutive observations and pruned potential drift points with low probability. We compare our approach to a non-probabilistic method for drift and a probabilistic method for change-point detection. In our experiments, our approach generally yielded improved accuracy and/or speed over these other methods.</p>
<p>【Keywords】:</p>
<h3 id="16. Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting.">16. Auto-Regressive HMM Inference with Incomplete Data for Short-Horizon Wind Forecasting.</h3>
<p>【<a href="http://papers.nips.cc/paper/3892-auto-regressive-hmm-inference-with-incomplete-data-for-short-horizon-wind-forecasting">Paper Link</a>】    【Pages】:136-144</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Barber:Chris">Chris Barber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bockhorst:Joseph">Joseph Bockhorst</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roebber:Paul">Paul Roebber</a></p>
<p>【Abstract】:
Accurate short-term wind forecasts (STWFs), with time horizons from 0.5 to 6 hours, are essential for efficient integration of wind power to the electrical power grid. Physical models based on numerical weather predictions are currently not competitive, and research on machine learning approaches is ongoing. Two major challenges confronting these efforts are missing observations and weather-regime induced dependency shifts among wind variables at geographically distributed sites. In this paper we introduce approaches that address both of these challenges. We describe a new regime-aware approach to STWF that use auto-regressive hidden Markov models (AR-HMM), a subclass of conditional linear Gaussian (CLG) models. Although AR-HMMs are a natural representation for weather regimes, as with CLG models in general, exact inference is NP-hard when observations are missing (Lerner and Parr, 2001). Because of this high cost, we introduce a simple approximate inference method for AR-HMMs, which we believe has applications to other sequential and temporal problem domains that involve continuous variables. In an empirical evaluation on publicly available wind data from two geographically distinct regions, our approach makes significantly more accurate predictions than baseline models, and uncovers meteorologically relevant regimes.</p>
<p>【Keywords】:</p>
<h3 id="17. The LASSO risk: asymptotic results and real world examples.">17. The LASSO risk: asymptotic results and real world examples.</h3>
<p>【<a href="http://papers.nips.cc/paper/4096-the-lasso-risk-asymptotic-results-and-real-world-examples">Paper Link</a>】    【Pages】:145-153</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bayati:Mohsen">Mohsen Bayati</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bento:Jos=eacute=">José Bento</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Montanari:Andrea">Andrea Montanari</a></p>
<p>【Abstract】:
We consider the problem of learning a coefficient vector x0 from noisy linear observation y=Ax0+w. In many contexts (ranging from model selection to image processing) it is desirable to construct a sparse estimator. In this case, a popular approach consists in solving an l1-penalized least squares problem known as the LASSO or BPDN.  For sequences of matrices A of increasing dimensions, with iid gaussian entries, we prove that the normalized risk of the LASSO converges to a limit, and we obtain an explicit expression for this limit. Our result is the first rigorous derivation of an explicit formula for the asymptotic risk of the LASSO for random instances. The proof technique is based on the analysis of AMP, a recently developed efficient algorithm, that is inspired from graphical models ideas.   Through simulations on real data matrices (gene expression data and hospital medical records) we observe that these results can be relevant in a broad array of practical applications.</p>
<p>【Keywords】:</p>
<h3 id="18. Extensions of Generalized Binary Search to Group Identification and Exponential Costs.">18. Extensions of Generalized Binary Search to Group Identification and Exponential Costs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4103-extensions-of-generalized-binary-search-to-group-identification-and-exponential-costs">Paper Link</a>】    【Pages】:154-162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bellala:Gowtham">Gowtham Bellala</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhavnani:Suresh_K=">Suresh K. Bhavnani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scott:Clayton">Clayton Scott</a></p>
<p>【Abstract】:
Generalized Binary Search (GBS) is a well known greedy algorithm for identifying an unknown object while minimizing the number of yes" or "no" questions posed about that object, and arises in problems such as active learning and active diagnosis. Here, we provide a coding-theoretic interpretation for GBS and show that GBS can be viewed as a top-down algorithm that greedily minimizes the expected number of queries required to identify an object. This interpretation is then used to extend GBS in two ways. First, we consider the case where the objects are partitioned into groups, and the objective is to identify only the group to which the object belongs. Then, we consider the case where the cost of identifying an object grows exponentially in the number of queries. In each case, we present an exact formula for the objective function involving Shannon or Renyi entropy, and develop a greedy algorithm for minimizing it."</p>
<p>【Keywords】:</p>
<h3 id="19. Label Embedding Trees for Large Multi-Class Tasks.">19. Label Embedding Trees for Large Multi-Class Tasks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4027-label-embedding-trees-for-large-multi-class-tasks">Paper Link</a>】    【Pages】:163-171</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bengio:Samy">Samy Bengio</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weston:Jason">Jason Weston</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grangier:David">David Grangier</a></p>
<p>【Abstract】:
Multi-class classification becomes challenging at test time when the number of classes is very large and testing against every possible class can become computationally infeasible. This problem can be alleviated by imposing (or learning) a structure over the set of classes. We propose an algorithm for learning a tree-structure of classifiers which, by optimizing the overall tree loss, provides superior accuracy to existing tree labeling methods. We also propose a method that learns to embed labels in a low dimensional space that is faster than non-embedding approaches and has superior accuracy to existing embedding approaches. Finally we combine the two ideas resulting in the label embedding tree that outperforms alternative methods including One-vs-Rest while being orders of magnitude faster.</p>
<p>【Keywords】:</p>
<h3 id="20. Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach.">20. Exploiting weakly-labeled Web images to improve object classification: a domain adaptation approach.</h3>
<p>【<a href="http://papers.nips.cc/paper/4064-exploiting-weakly-labeled-web-images-to-improve-object-classification-a-domain-adaptation-approach">Paper Link</a>】    【Pages】:181-189</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bergamo:Alessandro">Alessandro Bergamo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Torresani:Lorenzo">Lorenzo Torresani</a></p>
<p>【Abstract】:
Most current image categorization methods require large collections of manually annotated training examples to learn accurate visual recognition models. The time-consuming human labeling effort effectively limits these approaches to recognition problems involving a small number of different object classes. In order to address this shortcoming, in recent years several authors have proposed to learn object classifiers from weakly-labeled Internet images, such as photos retrieved by keyword-based image search engines. While this strategy eliminates the need for human supervision, the recognition accuracies of these methods are considerably lower than those obtained with fully-supervised approaches, because of the noisy nature of the labels associated to Web data.  In this paper we investigate and compare methods that learn image classifiers by combining very few manually annotated examples (e.g., 1-10 images per class) and a large number of weakly-labeled Web photos retrieved using keyword-based image search. We cast this as a domain adaptation problem: given a few strongly-labeled examples in a target domain (the manually annotated examples) and many source domain examples (the weakly-labeled Web photos), learn classifiers yielding small generalization error on the target domain. Our experiments demonstrate that, for the same number of strongly-labeled examples, our domain adaptation approach produces significant recognition rate improvements over the best published results (e.g., 65% better when using 5 labeled training examples per class) and that our classifiers are one order of magnitude faster to learn and to evaluate than the best competing method, despite our use of large weakly-labeled data sets.</p>
<p>【Keywords】:</p>
<h3 id="21. Online Classification with Specificity Constraints.">21. Online Classification with Specificity Constraints.</h3>
<p>【<a href="http://papers.nips.cc/paper/3896-online-classification-with-specificity-constraints">Paper Link</a>】    【Pages】:190-198</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bernstein:Andrey">Andrey Bernstein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mannor:Shie">Shie Mannor</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shimkin:Nahum">Nahum Shimkin</a></p>
<p>【Abstract】:
We consider the online binary classification problem, where we are given m classifiers. At each stage, the classifiers map the input to the probability that the input belongs to the positive class. An online classification meta-algorithm is an algorithm that combines the outputs of the classifiers in order to attain a certain goal, without having prior knowledge on the form and statistics of the input, and without prior knowledge on the performance of the given classifiers. In this paper, we use sensitivity and  specificity as the performance metrics of the meta-algorithm. In particular, our goal is to design an algorithm which satisfies the following two properties (asymptotically): (i) its average false positive rate (fp-rate) is under some given threshold, and (ii) its average true positive rate (tp-rate) is not worse than the tp-rate of the best convex combination of the m given classifiers that satisfies fp-rate constraint, in hindsight. We show that this problem is in fact a special case of the regret minimization problem with constraints, and therefore the above goal is not attainable. Hence, we pose a relaxed goal and propose a corresponding practical online learning meta-algorithm that attains it. In the case of two classifiers, we show that this algorithm takes a very simple form. To our best knowledge, this is the first algorithm that addresses the problem of the average tp-rate maximization under average fp-rate constraints in the online setting.</p>
<p>【Keywords】:</p>
<h3 id="22. Agnostic Active Learning Without Constraints.">22. Agnostic Active Learning Without Constraints.</h3>
<p>【<a href="http://papers.nips.cc/paper/4014-agnostic-active-learning-without-constraints">Paper Link</a>】    【Pages】:199-207</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Beygelzimer:Alina">Alina Beygelzimer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Daniel_J=">Daniel J. Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Langford:John">John Langford</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Tong">Tong Zhang</a></p>
<p>【Abstract】:
We present and analyze an agnostic active learning algorithm that works without keeping a version space. This is unlike all previous approaches where a restricted set of candidate hypotheses is maintained throughout learning, and only hypotheses from this set are ever returned. By avoiding this version space approach, our algorithm sheds the computational burden and brittleness associated with maintaining version spaces, yet still allows for substantial improvements over supervised learning for classification.</p>
<p>【Keywords】:</p>
<h3 id="23. Inference with Multivariate Heavy-Tails in Linear Models.">23. Inference with Multivariate Heavy-Tails in Linear Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3949-inference-with-multivariate-heavy-tails-in-linear-models">Paper Link</a>】    【Pages】:208-216</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bickson:Danny">Danny Bickson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guestrin:Carlos">Carlos Guestrin</a></p>
<p>【Abstract】:
Heavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy tailed distributions.   In this work, we propose a novel simple linear graphical model for  independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, L\'evy and Gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to only stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both).   We provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise.</p>
<p>【Keywords】:</p>
<h3 id="24. CUR from a Sparse Optimization Viewpoint.">24. CUR from a Sparse Optimization Viewpoint.</h3>
<p>【<a href="http://papers.nips.cc/paper/3890-cur-from-a-sparse-optimization-viewpoint">Paper Link</a>】    【Pages】:217-225</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bien:Jacob">Jacob Bien</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Ya">Ya Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mahoney:Michael_W=">Michael W. Mahoney</a></p>
<p>【Abstract】:
The CUR decomposition provides an approximation of a matrix X that has low reconstruction error and that is sparse in the sense that the resulting approximation lies in the span of only a few columns of X. In this regard, it appears to be similar to many sparse PCA methods. However, CUR takes a randomized algorithmic approach whereas most sparse PCA methods are framed as convex optimization problems. In this paper, we try to understand CUR from a sparse optimization viewpoint. In particular, we show that CUR is implicitly optimizing a sparse regression objective and, furthermore, cannot be directly cast as a sparse PCA method. We observe that the sparsity attained by CUR possesses an interesting structure, which leads us to formulate a sparse PCA method that achieves a CUR-like sparsity.</p>
<p>【Keywords】:</p>
<h3 id="25. Optimal learning rates for Kernel Conjugate Gradient regression.">25. Optimal learning rates for Kernel Conjugate Gradient regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/4077-optimal-learning-rates-for-kernel-conjugate-gradient-regression">Paper Link</a>】    【Pages】:226-234</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blanchard:Gilles">Gilles Blanchard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kr=auml=mer:Nicole">Nicole Krämer</a></p>
<p>【Abstract】:
We prove rates of convergence in the statistical sense for kernel-based least squares regression using a conjugate gradient algorithm, where regularization against overfitting is obtained by early stopping. This method is directly related to Kernel Partial Least Squares, a regression method that combines supervised dimensionality reduction with least squares projection. The rates depend on two key quantities: first, on the regularity of the target regression function and second, on the effective dimensionality of the data mapped into the kernel space. Lower bounds on attainable rates depending on these two quantities were established in earlier literature, and we obtain upper bounds for the considered method that match these lower bounds (up to a log factor) if  the true regression function belongs to the reproducing kernel Hilbert space. If the latter assumption is not fulfilled, we obtain similar convergence rates provided additional unlabeled data are available. The order of the learning rates in these two situations match state-of-the-art results that were recently obtained for the least squares support vector machine and for linear regularization operators.</p>
<p>【Keywords】:</p>
<h3 id="26. Simultaneous Object Detection and Ranking with Weak Supervision.">26. Simultaneous Object Detection and Ranking with Weak Supervision.</h3>
<p>【<a href="http://papers.nips.cc/paper/4105-simultaneous-object-detection-and-ranking-with-weak-supervision">Paper Link</a>】    【Pages】:235-243</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blaschko:Matthew_B=">Matthew B. Blaschko</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vedaldi:Andrea">Andrea Vedaldi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zisserman:Andrew">Andrew Zisserman</a></p>
<p>【Abstract】:
A standard approach to learning object category detectors is to provide strong supervision in the form of a region of interest (ROI) specifying each instance of the object in the training images. In this work are goal is to learn from heterogeneous labels, in which some images are only weakly supervised, specifying only the presence or absence of the object or a weak indication of object location, whilst others are fully annotated. To this end we develop a discriminative learning approach and make two contributions: (i) we propose a structured output formulation for weakly annotated images where full annotations are treated as latent variables; and (ii) we propose to optimize a ranking objective function, allowing our method to more effectively use negatively labeled images to improve detection average precision performance. The method is demonstrated on the benchmark INRIA pedestrian detection dataset of Dalal and Triggs and the PASCAL VOC dataset, and it is shown that for a significant proportion of weakly supervised images the performance achieved is very similar to the fully supervised (state of the art) results.</p>
<p>【Keywords】:</p>
<h3 id="27. Kernel Descriptors for Visual Recognition.">27. Kernel Descriptors for Visual Recognition.</h3>
<p>【<a href="http://papers.nips.cc/paper/4147-kernel-descriptors-for-visual-recognition">Paper Link</a>】    【Pages】:244-252</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bo:Liefeng">Liefeng Bo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ren:Xiaofeng">Xiaofeng Ren</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fox:Dieter">Dieter Fox</a></p>
<p>【Abstract】:
The design of low-level image features is critical for computer vision algorithms. Orientation histograms, such as those in SIFT~\cite{Lowe2004Distinctive} and HOG~\cite{Dalal2005Histograms}, are the most successful and popular features for visual object and scene recognition. We highlight the kernel view of orientation histograms, and show that they are equivalent to a certain type of match kernels over image patches. This novel view allows us to design a family of kernel descriptors which provide a unified and principled framework to turn pixel attributes (gradient, color, local binary pattern, \etc) into compact patch-level features. In particular, we introduce three types of match kernels to measure similarities between image patches, and construct compact low-dimensional kernel descriptors from these match kernels using kernel principal component analysis (KPCA)~\cite{Scholkopf1998Nonlinear}. Kernel descriptors are easy to design and can turn any type of pixel attribute into patch-level features. They outperform carefully tuned and sophisticated features including SIFT and deep belief networks. We report superior performance on standard image classification benchmarks: Scene-15, Caltech-101, CIFAR10 and CIFAR10-ImageNet.</p>
<p>【Keywords】:</p>
<h3 id="28. Fractionally Predictive Spiking Neurons.">28. Fractionally Predictive Spiking Neurons.</h3>
<p>【<a href="http://papers.nips.cc/paper/3983-fractionally-predictive-spiking-neurons">Paper Link</a>】    【Pages】:253-261</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bohte:Sander_M=">Sander M. Bohte</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rombouts:Jaldert_O=">Jaldert O. Rombouts</a></p>
<p>【Abstract】:
Recent experimental work has suggested that the neural firing rate can be interpreted as a fractional derivative, at least when signal variation induces neural adaptation. Here, we show that the actual neural spike-train itself can be considered as the fractional derivative, provided that the neural signal is approximated by a sum of power-law kernels. A simple standard thresholding spiking neuron suffices to carry out such an approximation, given a suitable refractory response. Empirically, we find that the online approximation of signals with a sum of power-law kernels is beneficial for encoding signals with slowly varying components, like long-memory self-similar signals. For such signals, the online power-law kernel approximation typically required less than half the number of spikes for similar SNR as compared to sums of similar but exponentially decaying kernels. As power-law kernels can be accurately approximated using sums or cascades of weighted exponentials, we demonstrate that the corresponding decoding of spike-trains by a receiving neuron allows for natural and transparent temporal signal filtering by tuning the weights of the decoding kernel.</p>
<p>【Keywords】:</p>
<h3 id="29. Gaussian Process Preference Elicitation.">29. Gaussian Process Preference Elicitation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4141-gaussian-process-preference-elicitation">Paper Link</a>】    【Pages】:262-270</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bonilla:Edwin_V=">Edwin V. Bonilla</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Shengbo">Shengbo Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanner:Scott">Scott Sanner</a></p>
<p>【Abstract】:
Bayesian approaches to preference elicitation (PE) are particularly attractive due to their ability to explicitly model uncertainty in users' latent utility functions. However, previous approaches to Bayesian PE have ignored the important problem of generalizing from previous users to an unseen user in order to reduce the elicitation burden on new users. In this paper, we address this deficiency by introducing a Gaussian Process (GP) prior over users' latent utility functions on the joint space of user and item features. We learn the hyper-parameters of this GP on a set of preferences of previous users and use it to aid in the elicitation process for a new user. This approach provides a flexible model of a multi-user utility function, facilitates an efficient value of information (VOI) heuristic query selection strategy, and provides a principled way to incorporate the elicitations of multiple users back into the model. We show the effectiveness of our method in comparison to previous work on a real dataset of user preferences over sushi types.</p>
<p>【Keywords】:</p>
<h3 id="30. Predictive State Temporal Difference Learning.">30. Predictive State Temporal Difference Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3952-predictive-state-temporal-difference-learning">Paper Link</a>】    【Pages】:271-279</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Boots:Byron">Byron Boots</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gordon:Geoffrey_J=">Geoffrey J. Gordon</a></p>
<p>【Abstract】:
We propose a new approach to value function approximation which combines linear temporal difference reinforcement learning with subspace identification. In practical applications, reinforcement learning (RL) is complicated by the fact that state is either high-dimensional or partially observable. Therefore, RL methods are designed to work with features of state rather than state itself, and the success or failure of learning is often determined by the suitability of the selected features. By comparison, subspace identification (SSID) methods are designed to select a feature set which preserves as much information as possible about state. In this paper we connect the two approaches, looking at the problem of reinforcement learning with a large set of features, each of which may only be marginally useful for value function approximation. We introduce a new algorithm for this situation, called Predictive State Temporal Difference (PSTD) learning. As in SSID for predictive state representations, PSTD finds a linear compression operator that projects a large set of features down to a small set that preserves the maximum amount of predictive information. As in RL, PSTD then uses a Bellman recursion to estimate a value function. We discuss the connection between PSTD and prior approaches in RL and SSID. We prove that PSTD is statistically consistent, perform several experiments that illustrate its properties, and demonstrate its potential on a difficult optimal stopping problem.</p>
<p>【Keywords】:</p>
<h3 id="31. Variational Inference over Combinatorial Spaces.">31. Variational Inference over Combinatorial Spaces.</h3>
<p>【<a href="http://papers.nips.cc/paper/4036-variational-inference-over-combinatorial-spaces">Paper Link</a>】    【Pages】:280-288</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bouchard=C=ocirc=t=eacute=:Alexandre">Alexandre Bouchard-Côté</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jordan:Michael_I=">Michael I. Jordan</a></p>
<p>【Abstract】:
Since the discovery of sophisticated fully polynomial randomized algorithms for a range of #P problems (Karzanov et al., 1991; Jerrum et al., 2001; Wilson, 2004), theoretical work on approximate inference in combinatorial spaces has focused on Markov chain Monte Carlo methods.  Despite their strong theoretical guarantees, the slow running time of many of these randomized algorithms and the restrictive assumptions on the potentials have hindered the applicability of these algorithms to machine learning.  Because of this, in applications to combinatorial spaces simple exact models are often preferred to more complex models that require approximate inference (Siepel et al., 2004).   Variational inference would appear to provide an appealing alternative, given the success of variational methods for graphical models (Wainwright et al., 2008); unfortunately, however, it is not obvious how to develop variational approximations for combinatorial objects such as matchings, partial orders, plane partitions and sequence alignments.   We propose a new framework that extends variational inference to a wide range of combinatorial spaces.  Our method is based on a simple assumption: the existence of a tractable measure factorization, which we show holds in many examples. Simulations on a range of matching models show that the algorithm is more general and empirically faster than a popular fully polynomial randomized algorithm.   We also apply the framework to the problem of multiple alignment of protein sequences, obtaining state-of-the-art results on the BAliBASE dataset (Thompson et al., 1999).</p>
<p>【Keywords】:</p>
<h3 id="32. Bootstrapping Apprenticeship Learning.">32. Bootstrapping Apprenticeship Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4160-bootstrapping-apprenticeship-learning">Paper Link</a>】    【Pages】:289-297</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Boularias:Abdeslam">Abdeslam Boularias</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chaib=draa:Brahim">Brahim Chaib-draa</a></p>
<p>【Abstract】:
We consider the problem of apprenticeship learning where the examples, demonstrated by an expert, cover only a small part of a large state space. Inverse Reinforcement Learning (IRL) provides an efficient tool for generalizing the demonstration, based on the assumption that the expert is maximizing a utility function that is a linear combination of state-action features. Most IRL algorithms use a simple Monte Carlo estimation to approximate the expected feature counts under the expert's policy. In this paper, we show that the quality of the learned policies is highly sensitive to the error in estimating the feature counts. To reduce this error, we introduce a novel approach for bootstrapping the demonstration by assuming that: (i), the expert is (near-)optimal, and (ii), the dynamics of the system is known. Empirical results on gridworlds and car racing problems show that our approach is able to learn good policies from a small number of demonstrations.</p>
<p>【Keywords】:</p>
<h3 id="33. Random Projections for $k$-means Clustering.">33. Random Projections for $k$-means Clustering.</h3>
<p>【<a href="http://papers.nips.cc/paper/3901-random-projections-for-k-means-clustering">Paper Link</a>】    【Pages】:298-306</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Boutsidis:Christos">Christos Boutsidis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zouzias:Anastasios">Anastasios Zouzias</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Drineas:Petros">Petros Drineas</a></p>
<p>【Abstract】:
This paper discusses the topic of dimensionality reduction for $k$-means clustering. We prove that any set of $n$ points in $d$ dimensions (rows in a matrix $A \in \RR^{n \times d}$) can be projected into $t = \Omega(k / \eps^2)$ dimensions, for any $\eps \in (0,1/3)$, in $O(n d \lceil \eps^{-2} k/ \log(d) \rceil )$ time, such that with  constant probability the optimal $k$-partition of the point set is preserved within a factor of $2+\eps$. The projection is done by post-multiplying $A$ with a $d \times t$ random matrix $R$ having entries $+1/\sqrt{t}$ or $-1/\sqrt{t}$ with equal probability. A numerical implementation of our technique and experiments on a large face images dataset verify the speed and the accuracy of our theoretical results.</p>
<p>【Keywords】:</p>
<h3 id="34. Segmentation as Maximum-Weight Independent Set.">34. Segmentation as Maximum-Weight Independent Set.</h3>
<p>【<a href="http://papers.nips.cc/paper/3909-segmentation-as-maximum-weight-independent-set">Paper Link</a>】    【Pages】:307-315</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Brendel:William">William Brendel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Todorovic:Sinisa">Sinisa Todorovic</a></p>
<p>【Abstract】:
Given an ensemble of distinct, low-level segmentations of an image, our goal is to identify visually meaningful" segments in the ensemble. Knowledge about any specific objects and surfaces present in the image is not available. The selection of image regions occupied by objects is formalized as the maximum-weight independent set (MWIS) problem. MWIS is the heaviest subset of mutually non-adjacent nodes of an attributed graph. We construct such a graph from all segments in the ensemble. Then, MWIS selects maximally distinctive segments that together partition the image. A new MWIS algorithm is presented. The algorithm seeks a solution directly in the discrete domain, instead of relaxing MWIS to a continuous problem, as common in previous work. It iteratively finds a candidate discrete solution of the Taylor series expansion of the original MWIS objective function around the previous solution. The algorithm is shown to converge to a maximum. Our empirical evaluation on the benchmark Berkeley segmentation dataset shows that the new algorithm eliminates the need for hand-picking optimal input parameters of the state-of-the-art segmenters, and outperforms their best, manually optimized results."</p>
<p>【Keywords】:</p>
<h3 id="35. Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.">35. Computing Marginal Distributions over Continuous Markov Networks for Statistical Relational Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3942-computing-marginal-distributions-over-continuous-markov-networks-for-statistical-relational-learning">Paper Link</a>】    【Pages】:316-324</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Broecheler:Matthias">Matthias Broecheler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Getoor:Lise">Lise Getoor</a></p>
<p>【Abstract】:
Continuous Markov random fields are a general formalism to model joint probability distributions over events with continuous outcomes. We prove that marginal computation for constrained continuous MRFs is #P-hard in general and present a polynomial-time approximation scheme under mild assumptions on the structure of the random field. Moreover, we introduce a sampling algorithm to compute marginal distributions and develop novel techniques to increase its efficiency. Continuous MRFs are a general purpose probabilistic modeling tool and we demonstrate how they can be applied to statistical relational learning. On the problem of collective classification, we evaluate our algorithm and show that the standard deviation of marginals serves as a useful measure of confidence.</p>
<p>【Keywords】:</p>
<h3 id="36. Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition.">36. Multi-label Multiple Kernel Learning by Stochastic Approximation: Application to Visual Object Recognition.</h3>
<p>【<a href="http://papers.nips.cc/paper/4177-multi-label-multiple-kernel-learning-by-stochastic-approximation-application-to-visual-object-recognition">Paper Link</a>】    【Pages】:325-333</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bucak:Serhat_Selcuk">Serhat Selcuk Bucak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Rong">Rong Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jain:Anil_K=">Anil K. Jain</a></p>
<p>【Abstract】:
Recent studies have shown that multiple kernel learning is very effective for object recognition, leading to the popularity  of kernel learning in computer vision problems. In this work, we develop an efficient algorithm for multi-label multiple kernel learning (ML-MKL). We assume that all the classes under  consideration  share the same combination of kernel functions, and the objective is to find the optimal kernel combination that benefits all the classes. Although several algorithms have been developed for ML-MKL, their computational cost is linear in the number of classes, making them unscalable when the number of classes is large, a challenge  frequently  encountered in visual object recognition. We address this computational challenge by developing a framework for ML-MKL that combines the worst-case analysis with stochastic approximation. Our analysis shows that the complexity of our algorithm is $O(m^{1/3}\sqrt{ln m})$, where $m$ is the number of classes. Empirical studies with object recognition show that while achieving similar classification accuracy, the proposed method is significantly more efficient than the state-of-the-art algorithms for ML-MKL.</p>
<p>【Keywords】:</p>
<h3 id="37. Learning concept graphs from text with stick-breaking priors.">37. Learning concept graphs from text with stick-breaking priors.</h3>
<p>【<a href="http://papers.nips.cc/paper/3963-learning-concept-graphs-from-text-with-stick-breaking-priors">Paper Link</a>】    【Pages】:334-342</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chambers:America">America Chambers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smyth:Padhraic">Padhraic Smyth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Steyvers:Mark">Mark Steyvers</a></p>
<p>【Abstract】:
We present a generative probabilistic model for learning general graph structures, which we term concept graphs, from text. Concept graphs provide a visual summary of the thematic content of a collection of documents-a task that is difficult to accomplish using only keyword search. The proposed model can learn different types of concept graph structures and is capable of utilizing partial prior knowledge about graph structure as well as labeled documents. We describe a generative model that is based on a stick-breaking process for graphs, and a Markov Chain Monte Carlo inference procedure. Experiments on simulated data show that the model can recover known graph structure when learning in both unsupervised and semi-supervised modes. We also show that the proposed model is competitive in terms of empirical log likelihood with existing structure-based topic models (such as hPAM and hLDA) on real-world text data sets. Finally, we illustrate the application of the model to the problem of updating Wikipedia category graphs.</p>
<p>【Keywords】:</p>
<h3 id="38. Rates of convergence for the cluster tree.">38. Rates of convergence for the cluster tree.</h3>
<p>【<a href="http://papers.nips.cc/paper/4068-rates-of-convergence-for-the-cluster-tree">Paper Link</a>】    【Pages】:343-351</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chaudhuri:Kamalika">Kamalika Chaudhuri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dasgupta:Sanjoy">Sanjoy Dasgupta</a></p>
<p>【Abstract】:
For a density f on R^d, a high-density cluster is any connected component of {x: f(x) &gt;= c}, for some c &gt; 0. The set of all high-density clusters form a hierarchy called the cluster tree of f. We present a procedure for estimating the cluster tree given samples from f. We give finite-sample convergence rates for our algorithm, as well as lower bounds on the sample complexity of this estimation problem.</p>
<p>【Keywords】:</p>
<h3 id="39. Evidence-Specific Structures for Rich Tractable CRFs.">39. Evidence-Specific Structures for Rich Tractable CRFs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4002-evidence-specific-structures-for-rich-tractable-crfs">Paper Link</a>】    【Pages】:352-360</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chechetka:Anton">Anton Chechetka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guestrin:Carlos">Carlos Guestrin</a></p>
<p>【Abstract】:
We present a simple and effective approach to learning tractable conditional random fields with structure that depends on the evidence. Our approach retains the advantages of tractable discriminative models, namely efficient exact inference and exact parameter learning. At the same time, our algorithm does not suffer a large expressive power penalty inherent to fixed tractable structures. On real-life relational datasets, our approach matches or exceeds state of the art accuracy of the dense models, and at the same time provides an order of magnitude speedup</p>
<p>【Keywords】:</p>
<h3 id="40. Predictive Subspace Learning for Multi-view Data: a Large Margin Approach.">40. Predictive Subspace Learning for Multi-view Data: a Large Margin Approach.</h3>
<p>【<a href="http://papers.nips.cc/paper/4128-predictive-subspace-learning-for-multi-view-data-a-large-margin-approach">Paper Link</a>】    【Pages】:361-369</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ning">Ning Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jun">Jun Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
Learning from multi-view data is important in many applications, such as image classification and annotation. In this paper, we present a large-margin learning framework to discover a predictive latent subspace representation shared by multiple views. Our approach is based on an undirected latent space Markov network that fulfills a weak conditional independence assumption that multi-view observations and response variables are independent given a set of latent variables. We provide efficient inference and parameter estimation methods for the latent subspace model. Finally, we demonstrate the advantages of large-margin learning on real video and web image data for discovering predictive latent representations and improving the performance on image classification, annotation and retrieval.</p>
<p>【Keywords】:</p>
<h3 id="41. Two-Layer Generalization Analysis for Ranking Using Rademacher Average.">41. Two-Layer Generalization Analysis for Ranking Using Rademacher Average.</h3>
<p>【<a href="http://papers.nips.cc/paper/4019-two-layer-generalization-analysis-for-ranking-using-rademacher-average">Paper Link</a>】    【Pages】:370-378</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Wei">Wei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Zhiming">Zhiming Ma</a></p>
<p>【Abstract】:
This paper is concerned with the generalization analysis on learning to rank for information retrieval (IR). In IR, data are hierarchically organized, i.e., consisting of queries and documents per query. Previous generalization analysis for ranking, however, has not fully considered this structure, and cannot explain how the simultaneous change of query number and document number in the training data will affect the performance of algorithms. In this paper, we propose performing generalization analysis under the assumption of two-layer sampling, i.e., the i.i.d. sampling of queries and the conditional i.i.d sampling of documents per query. Such a sampling can better describe the generation mechanism of real data, and the corresponding generalization analysis can better explain the real behaviors of learning to rank algorithms. However, it is challenging to perform such analysis, because the documents associated with different queries are not identically distributed, and the documents associated with the same query become no longer independent if represented by features extracted from the matching between document and query. To tackle the challenge, we decompose the generalization error according to the two layers, and make use of the new concept of two-layer Rademacher average. The generalization bounds we obtained are quite intuitive and are in accordance with previous empirical studies on the performance of ranking algorithms.</p>
<p>【Keywords】:</p>
<h3 id="42. SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system.">42. SpikeAnts, a spiking neuron network modelling the emergence of organization in a complex system.</h3>
<p>【<a href="http://papers.nips.cc/paper/3900-spikeants-a-spiking-neuron-network-modelling-the-emergence-of-organization-in-a-complex-system">Paper Link</a>】    【Pages】:379-387</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chevallier:Sylvain">Sylvain Chevallier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Paugam=Moisy:H=eacute=l=egrave=ne">Hélène Paugam-Moisy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sebag:Mich=egrave=le">Michèle Sebag</a></p>
<p>【Abstract】:
Many complex systems, ranging from neural cell assemblies to insect societies, involve and rely on some division of labor. How to enforce such a division in a decentralized and distributed way, is tackled in this paper, using a spiking neuron network architecture. Specifically, a spatio-temporal model called SpikeAnts is shown to enforce the emergence of synchronized activities in an ant colony. Each ant is modelled from two spiking neurons; the ant colony is a sparsely connected spiking neuron network. Each ant makes its decision (among foraging, sleeping and self-grooming) from the competition between its two neurons, after the signals received from its neighbor ants. Interestingly, three types of temporal patterns emerge in the ant colony: asynchronous, synchronous, and synchronous periodic foraging activities - similar to the actual behavior of some living ant colonies. A phase diagram of the emergent activity patterns with respect to two control parameters, respectively accounting for ant sociability and receptivity, is presented and discussed.</p>
<p>【Keywords】:</p>
<h3 id="43. Movement extraction by detecting dynamics switches and repetitions.">43. Movement extraction by detecting dynamics switches and repetitions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4109-movement-extraction-by-detecting-dynamics-switches-and-repetitions">Paper Link</a>】    【Pages】:388-396</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chiappa:Silvia">Silvia Chiappa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Peters_0001:Jan">Jan Peters</a></p>
<p>【Abstract】:
Many time-series such as human movement data consist of a sequence of basic actions, e.g., forehands and backhands in tennis. Automatically extracting and characterizing such actions is an important problem for a variety of different applications. In this paper, we present a probabilistic segmentation approach in which an observed time-series is modeled as a concatenation of segments corresponding to different basic actions. Each segment is generated through a noisy transformation of one of a few hidden trajectories representing different types of movement, with possible time re-scaling. We analyze three different approximation methods for dealing with model intractability, and demonstrate how the proposed approach can successfully segment table tennis movements recorded using a robot arm as haptic input device.</p>
<p>【Keywords】:</p>
<h3 id="44. Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors.">44. Learning sparse dynamic linear systems using stable spline kernels and exponential hyperpriors.</h3>
<p>【<a href="http://papers.nips.cc/paper/4174-learning-sparse-dynamic-linear-systems-using-stable-spline-kernels-and-exponential-hyperpriors">Paper Link</a>】    【Pages】:397-405</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chiuso:Alessandro">Alessandro Chiuso</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pillonetto:Gianluigi">Gianluigi Pillonetto</a></p>
<p>【Abstract】:
We introduce a new Bayesian nonparametric approach to identification of sparse dynamic linear systems. The impulse responses are modeled as Gaussian processes whose autocovariances encode the BIBO stability constraint, as defined by the recently introduced “Stable Spline kernel”. Sparse solutions are obtained by placing exponential hyperpriors on the scale factors of such kernels. Numerical experiments regarding estimation of ARMAX models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization.</p>
<p>【Keywords】:</p>
<h3 id="45. Universal Kernels on Non-Standard Input Spaces.">45. Universal Kernels on Non-Standard Input Spaces.</h3>
<p>【<a href="http://papers.nips.cc/paper/4168-universal-kernels-on-non-standard-input-spaces">Paper Link</a>】    【Pages】:406-414</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Christmann:Andreas">Andreas Christmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Steinwart:Ingo">Ingo Steinwart</a></p>
<p>【Abstract】:
During the last years support vector machines (SVMs) have been successfully applied even in situations where the input space $X$ is not necessarily a subset of $R^d$. Examples include SVMs using probability measures to analyse e.g. histograms or coloured images, SVMs for text classification and web mining, and SVMs for applications from computational biology using, e.g., kernels for trees and graphs. Moreover, SVMs are known to be consistent to the Bayes risk, if either the input space is a complete separable metric space and the reproducing kernel Hilbert space (RKHS) $H\subset L_p(P_X)$ is dense, or if the SVM is based on a universal kernel $k$.  So far, however, there are no RKHSs of practical interest known that satisfy these assumptions on $\cH$ or $k$ if $X \not\subset R^d$.  We close this gap by providing a general technique based on Taylor-type kernels to explicitly construct universal kernels on compact metric spaces which are not subset of $R^d$. We apply this technique for the following special cases: universal kernels on the set of probability measures, universal kernels based on Fourier transforms, and universal kernels for signal processing.</p>
<p>【Keywords】:</p>
<h3 id="46. Causal discovery in multiple models from different experiments.">46. Causal discovery in multiple models from different experiments.</h3>
<p>【<a href="http://papers.nips.cc/paper/3961-causal-discovery-in-multiple-models-from-different-experiments">Paper Link</a>】    【Pages】:415-423</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Claassen:Tom">Tom Claassen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Heskes:Tom">Tom Heskes</a></p>
<p>【Abstract】:
A long-standing open research problem is how to use information from different experiments, including background knowledge, to infer causal relations. Recent developments have shown ways to use multiple data sets, provided they originate from identical experiments. We present the MCI-algorithm as the first method that can infer provably valid causal relations in the large sample limit from different experiments. It is fast, reliable and produces very clear and easily interpretable output. It is based on a result that shows that constraint-based causal discovery is decomposable into a candidate pair identification and subsequent elimination step that can be applied separately from different models. We test the algorithm on a variety of synthetic input model sets to assess its behavior and the quality of the output. The method shows promising signs that it can be adapted to suit causal discovery in real-world application areas as well, including large databases.</p>
<p>【Keywords】:</p>
<h3 id="47. Empirical Risk Minimization with Approximations of Probabilistic Grammars.">47. Empirical Risk Minimization with Approximations of Probabilistic Grammars.</h3>
<p>【<a href="http://papers.nips.cc/paper/3975-empirical-risk-minimization-with-approximations-of-probabilistic-grammars">Paper Link</a>】    【Pages】:424-432</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
Probabilistic grammars are generative statistical models that are  useful for compositional and sequential structures. We present a framework, reminiscent of structural risk minimization, for empirical risk minimization of the parameters of a fixed probabilistic grammar using the log-loss. We derive sample complexity bounds in this framework that apply both to the supervised setting and the unsupervised setting.</p>
<p>【Keywords】:</p>
<h3 id="48. Mixture of time-warped trajectory models for movement decoding.">48. Mixture of time-warped trajectory models for movement decoding.</h3>
<p>【<a href="http://papers.nips.cc/paper/3989-mixture-of-time-warped-trajectory-models-for-movement-decoding">Paper Link</a>】    【Pages】:433-441</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Corbett:Elaine">Elaine Corbett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perreault:Eric_J=">Eric J. Perreault</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koerding:Konrad_P=">Konrad P. Koerding</a></p>
<p>【Abstract】:
Applications of Brain-Machine-Interfaces typically estimate user intent based on biological signals that are under voluntary control. For example, we might want to estimate how a patient with a paralyzed arm wants to move based on residual muscle activity. To solve such problems it is necessary to integrate obtained information over time. To do so, state of the art approaches typically use a probabilistic model of how the state, e.g. position and velocity of the arm, evolves over time – a so-called trajectory model. We wanted to further develop this approach using two intuitive insights: (1) At any given point of time there may be a small set of likely movement targets, potentially identified by the location of objects in the workspace or by gaze information from the user. (2) The user may want to produce movements at varying speeds. We thus use a generative model with a trajectory model incorporating these insights. Approximate inference on that generative model is implemented using a mixture of extended Kalman filters. We find that the resulting algorithm allows us to decode arm movements dramatically better than when we use a trajectory model with linear dynamics.</p>
<p>【Keywords】:</p>
<h3 id="49. Learning Bounds for Importance Weighting.">49. Learning Bounds for Importance Weighting.</h3>
<p>【<a href="http://papers.nips.cc/paper/4156-learning-bounds-for-importance-weighting">Paper Link</a>】    【Pages】:442-450</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cortes:Corinna">Corinna Cortes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mansour:Yishay">Yishay Mansour</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mohri:Mehryar">Mehryar Mohri</a></p>
<p>【Abstract】:
This paper presents an analysis of importance weighting for learning from finite samples and gives a series of theoretical and algorithmic results. We point out simple cases where importance weighting can fail, which suggests the need for an analysis of the properties of this technique. We then give both upper and lower bounds for generalization with bounded importance weights and, more significantly, give learning guarantees for the more common case of unbounded importance weights under the weak assumption that the second moment is bounded, a condition related to the Renyi divergence of the training and test distributions. These results are based on a series of novel and general bounds we derive for unbounded loss functions, which are of independent interest. We use these bounds to guide the definition of an alternative reweighting algorithm and report the results of experiments demonstrating its benefits. Finally, we analyze the properties of normalized importance weights which are also commonly used.</p>
<p>【Keywords】:</p>
<h3 id="50. Learning via Gaussian Herding.">50. Learning via Gaussian Herding.</h3>
<p>【<a href="http://papers.nips.cc/paper/3893-learning-via-gaussian-herding">Paper Link</a>】    【Pages】:451-459</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Crammer:Koby">Koby Crammer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Daniel_D=">Daniel D. Lee</a></p>
<p>【Abstract】:
We introduce a new family of online learning algorithms based upon   constraining the velocity flow over a distribution of weight   vectors.  In particular, we show how to effectively  herd a   Gaussian weight vector distribution by trading off velocity   constraints with a loss function.  By uniformly bounding this loss   function, we demonstrate how to solve the resulting optimization   analytically.  We compare the resulting algorithms on a variety of    real world datasets, and demonstrate how these algorithms achieve   state-of-the-art robust performance, especially with high label   noise in the training data.</p>
<p>【Keywords】:</p>
<h3 id="51. Spatial and anatomical regularization of SVM for brain image analysis.">51. Spatial and anatomical regularization of SVM for brain image analysis.</h3>
<p>【<a href="http://papers.nips.cc/paper/3914-spatial-and-anatomical-regularization-of-svm-for-brain-image-analysis">Paper Link</a>】    【Pages】:460-468</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cuingnet:R=eacute=mi">Rémi Cuingnet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chupin:Marie">Marie Chupin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Benali:Habib">Habib Benali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Colliot:Olivier">Olivier Colliot</a></p>
<p>【Abstract】:
Support vector machines (SVM) are increasingly used in brain image analyses since they allow capturing complex multivariate relationships in the data. Moreover, when the kernel is linear, SVMs can be used to localize spatial patterns of discrimination between two groups of subjects. However, the features' spatial distribution is not taken into account. As a consequence, the optimal margin hyperplane is often scattered and lacks spatial coherence, making its anatomical interpretation difficult. This paper introduces a framework to spatially regularize SVM for brain image analysis. We show that Laplacian regularization provides a flexible framework to integrate various types of constraints and can be applied to both cortical surfaces and 3D brain images. The proposed framework is applied to the classification of MR images based on gray matter concentration maps and cortical thickness measures from 30 patients with Alzheimer's disease and 30 elderly controls. The results demonstrate that the proposed method enables natural spatial and anatomical regularization of the classifier.</p>
<p>【Keywords】:</p>
<h3 id="52. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.">52. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine.</h3>
<p>【<a href="http://papers.nips.cc/paper/4169-phone-recognition-with-the-mean-covariance-restricted-boltzmann-machine">Paper Link</a>】    【Pages】:469-477</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dahl:George_E=">George E. Dahl</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ranzato:Marc=Aurelio">Marc'Aurelio Ranzato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mohamed:Abdel=rahman">Abdel-rahman Mohamed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hinton:Geoffrey_E=">Geoffrey E. Hinton</a></p>
<p>【Abstract】:
Straightforward application of Deep Belief Nets (DBNs) to acoustic modeling produces a rich distributed representation of speech data that is useful for recognition and yields impressive results on the speaker-independent TIMIT phone recognition task. However, the first-layer Gaussian-Bernoulli Restricted Boltzmann Machine (GRBM) has an important limitation, shared with mixtures of diagonal-covariance Gaussians: GRBMs treat different components of the acoustic input vector as conditionally independent given the hidden state. The mean-covariance restricted Boltzmann machine (mcRBM), first introduced for modeling natural images, is a much more representationally efficient and powerful way of modeling the covariance structure of speech data. Every configuration of the precision units of the mcRBM specifies a different precision matrix for the conditional distribution over the acoustic space. In this work, we use the mcRBM to learn features of speech data that serve as input into a standard DBN. The mcRBM features combined with DBNs allow us to achieve a phone error rate of 20.5\%, which is superior to all published results on speaker-independent TIMIT to date.</p>
<p>【Keywords】:</p>
<h3 id="53. Random Projection Trees Revisited.">53. Random Projection Trees Revisited.</h3>
<p>【<a href="http://papers.nips.cc/paper/4039-random-projection-trees-revisited">Paper Link</a>】    【Pages】:496-504</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dhesi:Aman">Aman Dhesi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kar:Purushottam">Purushottam Kar</a></p>
<p>【Abstract】:
The Random Projection Tree (RPTree) structures proposed in [Dasgupta-Freund-STOC-08] are space partitioning data structures that automatically adapt to various notions of intrinsic dimensionality of data. We prove new results for both the RPTree-Max and the RPTree-Mean data structures. Our result for RPTree-Max gives a near-optimal bound on the number of levels required by this data structure to reduce the size of its cells by a factor s &gt;= 2. We also prove a packing lemma for this data structure. Our final result shows that low-dimensional manifolds possess bounded Local Covariance Dimension. As a consequence we show that RPTree-Mean adapts to manifold dimension as well.</p>
<p>【Keywords】:</p>
<h3 id="54. Throttling Poisson Processes.">54. Throttling Poisson Processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/4025-throttling-poisson-processes">Paper Link</a>】    【Pages】:505-513</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dick:Uwe">Uwe Dick</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Haider:Peter">Peter Haider</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vanck:Thomas">Thomas Vanck</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Br=uuml=ckner:Michael">Michael Brückner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scheffer:Tobias">Tobias Scheffer</a></p>
<p>【Abstract】:
We study a setting in which Poisson processes generate sequences of decision-making events. The optimization goal is allowed to depend on the rate of decision outcomes; the rate may depend on a potentially long backlog of events and decisions. We model the problem as a Poisson process with a throttling policy that enforces a data-dependent rate limit and reduce the learning problem to a convex optimization problem that can be solved efficiently. This problem setting matches applications in which damage caused by an attacker grows as a function of the rate of unsuppressed hostile events. We report on experiments on abuse detection for an email service.</p>
<p>【Keywords】:</p>
<h3 id="55. t-logistic regression.">55. t-logistic regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/3928-t-logistic-regression">Paper Link</a>】    【Pages】:514-522</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Nan">Nan Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:S=_V=_N=">S. V. N. Vishwanathan</a></p>
<p>【Abstract】:
We extend logistic regression by using t-exponential families which were introduced recently in statistical physics. This gives rise to a regularized risk minimization problem with a non-convex loss  function. An efficient block coordinate descent optimization  scheme can be derived for estimating the parameters. Because of the  nature of the loss function, our algorithm is tolerant to label noise. Furthermore, unlike other algorithms which employ non-convex   loss functions, our algorithm is fairly robust to the choice of  initial values. We verify both these observations empirically on a  number of synthetic and real datasets.</p>
<p>【Keywords】:</p>
<h3 id="56. Implicit Differentiation by Perturbation.">56. Implicit Differentiation by Perturbation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4107-implicit-differentiation-by-perturbation">Paper Link</a>】    【Pages】:523-531</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Domke:Justin">Justin Domke</a></p>
<p>【Abstract】:
This paper proposes a simple and efficient finite difference method for implicit differentiation of marginal inference results in discrete graphical models. Given an arbitrary loss function, defined on marginals, we show that the derivatives of this loss with respect to model parameters can be obtained by running the inference procedure twice, on slightly perturbed model parameters. This method can be used with approximate inference, with a loss function over approximate marginals. Convenient choices of loss functions make it practical to fit graphical models with hidden variables, high treewidth and/or model misspecification.</p>
<p>【Keywords】:</p>
<h3 id="57. Nonparametric Bayesian Policy Priors for Reinforcement Learning.">57. Nonparametric Bayesian Policy Priors for Reinforcement Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3992-nonparametric-bayesian-policy-priors-for-reinforcement-learning">Paper Link</a>】    【Pages】:532-540</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Doshi=Velez:Finale">Finale Doshi-Velez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wingate:David">David Wingate</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roy:Nicholas">Nicholas Roy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tenenbaum:Joshua_B=">Joshua B. Tenenbaum</a></p>
<p>【Abstract】:
We consider reinforcement learning in partially observable domains where the agent can query an expert for demonstrations. Our nonparametric Bayesian approach combines model knowledge, inferred from expert information and independent exploration, with policy knowledge inferred from expert trajectories. We introduce priors that bias the agent towards models with both simple representations and simple policies, resulting in improved policy and model learning.</p>
<p>【Keywords】:</p>
<h3 id="58. Over-complete representations on recurrent neural networks can support persistent percepts.">58. Over-complete representations on recurrent neural networks can support persistent percepts.</h3>
<p>【<a href="http://papers.nips.cc/paper/4020-over-complete-representations-on-recurrent-neural-networks-can-support-persistent-percepts">Paper Link</a>】    【Pages】:541-549</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Druckmann:Shaul">Shaul Druckmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chklovskii:Dmitri_B=">Dmitri B. Chklovskii</a></p>
<p>【Abstract】:
A striking aspect of cortical neural networks is the divergence of a relatively small number of input channels from the peripheral sensory apparatus into a large number of cortical neurons, an over-complete representation strategy. Cortical neurons are then connected by a sparse network of lateral synapses. Here we propose that such architecture may increase the persistence of the representation of an incoming stimulus, or a percept. We demonstrate that for a family of networks in which the receptive field of each neuron is re-expressed by its outgoing connections, a represented percept can remain constant despite changing activity. We term this choice of connectivity REceptive FIeld REcombination (REFIRE) networks. The sparse REFIRE network may serve as a high-dimensional integrator and a biologically plausible model of the local cortical circuit.</p>
<p>【Keywords】:</p>
<h3 id="59. Distributed Dual Averaging In Networks.">59. Distributed Dual Averaging In Networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4164-distributed-dual-averaging-in-networks">Paper Link</a>】    【Pages】:550-558</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Duchi:John_C=">John C. Duchi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Alekh">Alekh Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wainwright:Martin_J=">Martin J. Wainwright</a></p>
<p>【Abstract】:
The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and we provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.</p>
<p>【Keywords】:</p>
<h3 id="60. Copula Bayesian Networks.">60. Copula Bayesian Networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/3956-copula-bayesian-networks">Paper Link</a>】    【Pages】:559-567</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Elidan:Gal">Gal Elidan</a></p>
<p>【Abstract】:
We present the Copula Bayesian Network model for representing multivariate continuous distributions. Our approach builds on a novel copula-based parameterization of a conditional density that, joined with a graph that encodes independencies, offers great flexibility in modeling high-dimensional densities, while maintaining control over the form of the univariate marginals. We demonstrate the advantage of our framework for generalization over standard Bayesian networks as well as tree structured copula models for varied real-life domains that are of substantially higher dimension than those typically considered in the copula literature.</p>
<p>【Keywords】:</p>
<h3 id="61. Error Propagation for Approximate Policy and Value Iteration.">61. Error Propagation for Approximate Policy and Value Iteration.</h3>
<p>【<a href="http://papers.nips.cc/paper/4181-error-propagation-for-approximate-policy-and-value-iteration">Paper Link</a>】    【Pages】:568-576</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Farahmand:Amir_Massoud">Amir Massoud Farahmand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Munos:R=eacute=mi">Rémi Munos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a></p>
<p>【Abstract】:
We address the question of how the approximation error/Bellman residual at each iteration of the Approximate Policy/Value Iteration algorithms influences the quality of the resulted policy. We quantify the performance loss as the Lp norm of the approximation error/Bellman residual at each iteration. Moreover, we show that the performance loss depends on the expectation of the squared Radon-Nikodym derivative of a certain distribution rather than its supremum -- as opposed to what has been suggested by the previous results.  Also our results indicate that the contribution of the approximation/Bellman error to the performance loss is more prominent in the later iterations of API/AVI, and the effect of an error term in the earlier iterations decays exponentially fast.</p>
<p>【Keywords】:</p>
<h3 id="62. PAC-Bayesian Model Selection for Reinforcement Learning.">62. PAC-Bayesian Model Selection for Reinforcement Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4117-pac-bayesian-model-selection-for-reinforcement-learning">Paper Link</a>】    【Pages】:1624-1632</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fard:Mahdi_Milani">Mahdi Milani Fard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pineau:Joelle">Joelle Pineau</a></p>
<p>【Abstract】:
This paper introduces the first set of PAC-Bayesian bounds for the batch reinforcement learning problem in finite state spaces. These bounds hold regardless of the correctness of the prior distribution. We demonstrate how such bounds can be used for model-selection in control problems where prior information is available either on the dynamics of the environment, or on the value of actions. Our empirical results confirm that PAC-Bayesian model-selection is able to leverage prior distributions when they are informative and, unlike standard Bayesian RL approaches, ignores them when they are misleading.</p>
<p>【Keywords】:</p>
<h3 id="63. A Computational Decision Theory for Interactive Assistants.">63. A Computational Decision Theory for Interactive Assistants.</h3>
<p>【<a href="http://papers.nips.cc/paper/4052-a-computational-decision-theory-for-interactive-assistants">Paper Link</a>】    【Pages】:577-585</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fern:Alan">Alan Fern</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tadepalli:Prasad">Prasad Tadepalli</a></p>
<p>【Abstract】:
We study several classes of interactive assistants from the points of view of decision theory and computational complexity. We first introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalize the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection in finite horizon HGMDPs is PSPACE-complete even in domains with deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), where the assistant's action is accepted by the agent when it is helpful, and can be easily ignored by the agent otherwise. We show classes of HAMDPs that are complete for PSPACE and NP along with a polynomial time class. Furthermore, we show that for general HAMDPs a simple myopic policy achieves a regret, compared to an omniscient assistant, that is bounded by the entropy of the initial goal distribution. A variation of this policy is shown to achieve worst-case regret that is logarithmic in the number of goals for any goal distribution.</p>
<p>【Keywords】:</p>
<h3 id="64. Parametric Bandits: The Generalized Linear Case.">64. Parametric Bandits: The Generalized Linear Case.</h3>
<p>【<a href="http://papers.nips.cc/paper/4166-parametric-bandits-the-generalized-linear-case">Paper Link</a>】    【Pages】:586-594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Filippi:Sarah">Sarah Filippi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Capp=eacute=:Olivier">Olivier Cappé</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Garivier:Aur=eacute=lien">Aurélien Garivier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a></p>
<p>【Abstract】:
We consider structured multi-armed bandit tasks in which the agent is guided by prior structural knowledge that can be exploited to efficiently select the optimal arm(s) in situations where the number of arms is large, or even infinite. We pro- pose a new optimistic, UCB-like, algorithm for non-linearly parameterized bandit problems using the Generalized Linear Model (GLM) framework. We analyze the regret of the proposed algorithm, termed GLM-UCB, obtaining results similar to those recently proved in the literature for the linear regression case. The analysis also highlights a key difficulty of the non-linear case which is solved in GLM-UCB by focusing on the reward space rather than on the parameter space. Moreover, as the actual efficiency of current parameterized bandit algorithms is often deceiving in practice, we provide an asymptotic argument leading to significantly faster convergence. Simulation studies on real data sets illustrate the performance and the robustness of the proposed GLM-UCB approach.</p>
<p>【Keywords】:</p>
<h3 id="65. A Novel Kernel for Learning a Neuron Model from Spike Train Data.">65. A Novel Kernel for Learning a Neuron Model from Spike Train Data.</h3>
<p>【<a href="http://papers.nips.cc/paper/4155-a-novel-kernel-for-learning-a-neuron-model-from-spike-train-data">Paper Link</a>】    【Pages】:595-603</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fisher:Nicholas">Nicholas Fisher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Banerjee:Arunava">Arunava Banerjee</a></p>
<p>【Abstract】:
From a functional viewpoint, a spiking neuron is a device that transforms input spike trains on its various synapses into an output spike train on its axon. We demonstrate in this paper that the function mapping underlying the device can be tractably learned based on input and output spike train data alone. We begin by posing the problem in a classification based framework. We then derive a novel kernel for an SRM0 model that is based on PSP and AHP like functions. With the kernel we demonstrate how the learning problem can be posed as a Quadratic Program. Experimental results demonstrate the strength of our approach.</p>
<p>【Keywords】:</p>
<h3 id="66. Extended Bayesian Information Criteria for Gaussian Graphical Models.">66. Extended Bayesian Information Criteria for Gaussian Graphical Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/4087-extended-bayesian-information-criteria-for-gaussian-graphical-models">Paper Link</a>】    【Pages】:604-612</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Foygel:Rina">Rina Foygel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Drton:Mathias">Mathias Drton</a></p>
<p>【Abstract】:
Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the asymptotic consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjuction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.</p>
<p>【Keywords】:</p>
<h3 id="67. Shadow Dirichlet for Restricted Probability Modeling.">67. Shadow Dirichlet for Restricted Probability Modeling.</h3>
<p>【<a href="http://papers.nips.cc/paper/3934-shadow-dirichlet-for-restricted-probability-modeling">Paper Link</a>】    【Pages】:613-621</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Frigyik:Bela_A=">Bela A. Frigyik</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gupta:Maya_R=">Maya R. Gupta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yihua">Yihua Chen</a></p>
<p>【Abstract】:
Although the Dirichlet distribution is widely used, the independence structure of its components limits its accuracy as a model. The proposed shadow Dirichlet distribution manipulates the support in order to model probability mass functions (pmfs) with dependencies or constraints that often arise in real world problems, such as regularized pmfs, monotonic pmfs, and pmfs with bounded variation. We describe some properties of this new class of distributions, provide maximum entropy constructions, give an expectation-maximization method for estimating the mean parameter, and illustrate with real data.</p>
<p>【Keywords】:</p>
<h3 id="68. Size Matters: Metric Visual Search Constraints from Monocular Metadata.">68. Size Matters: Metric Visual Search Constraints from Monocular Metadata.</h3>
<p>【<a href="http://papers.nips.cc/paper/4104-size-matters-metric-visual-search-constraints-from-monocular-metadata">Paper Link</a>】    【Pages】:622-630</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fritz:Mario">Mario Fritz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saenko:Kate">Kate Saenko</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Darrell:Trevor">Trevor Darrell</a></p>
<p>【Abstract】:
Metric constraints are known to be highly discriminative for many objects, but if training is limited to data captured from a particular 3-D sensor the quantity of training data may be severly limited. In this paper, we show how a crucial aspect of 3-D information–object and feature absolute size–can be added to models learned from commonly available online imagery, without use of any 3-D sensing or re- construction at training time. Such models can be utilized at test time together with explicit 3-D sensing to perform robust search. Our model uses a “2.1D” local feature, which combines traditional appearance gradient statistics with an estimate of average absolute depth within the local window. We show how category size information can be obtained from online images by exploiting relatively unbiquitous metadata fields specifying camera intrinstics. We develop an efficient metric branch-and-bound algorithm for our search task, imposing 3-D size constraints as part of an optimal search for a set of features which indicate the presence of a category. Experiments on test scenes captured with a traditional stereo rig are shown, exploiting training data from from purely monocular sources with associated EXIF metadata.</p>
<p>【Keywords】:</p>
<h3 id="69. A Bayesian Framework for Figure-Ground Interpretation.">69. A Bayesian Framework for Figure-Ground Interpretation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4058-a-bayesian-framework-for-figure-ground-interpretation">Paper Link</a>】    【Pages】:631-639</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Froyen:Vicky">Vicky Froyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feldman:Jacob">Jacob Feldman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Manish">Manish Singh</a></p>
<p>【Abstract】:
Figure/ground assignment, in which the visual image is divided into nearer (figural) and farther (ground) surfaces, is an essential step in visual processing, but its underlying computational mechanisms are poorly understood. Figural assignment (often referred to as border ownership) can vary along a contour, suggesting a spatially distributed process whereby local and global cues are combined to yield local estimates of border ownership. In this paper we model figure/ground estimation in a Bayesian belief network, attempting to capture the propagation of border ownership across the image as local cues (contour curvature and T-junctions) interact with more global cues to yield a figure/ground assignment. Our network includes as a nonlocal factor skeletal (medial axis) structure, under the hypothesis that medial structure ``draws'' border ownership so that borders are owned by their interiors. We also briefly present a psychophysical experiment in which we measured local border ownership along a contour at various distances from an inducing cue (a T-junction). Both the human subjects and the network show similar patterns of performance, converging rapidly to a similar pattern of spatial variation in border ownership along contours.</p>
<p>【Keywords】:</p>
<h3 id="70. Attractor Dynamics with Synaptic Depression.">70. Attractor Dynamics with Synaptic Depression.</h3>
<p>【<a href="http://papers.nips.cc/paper/3939-attractor-dynamics-with-synaptic-depression">Paper Link</a>】    【Pages】:640-648</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fung:C=_C=_Alan">C. C. Alan Fung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wong:K=_Y=_Michael">K. Y. Michael Wong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:He">He Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Si">Si Wu</a></p>
<p>【Abstract】:
Neuronal connection weights exhibit short-term depression (STD). The present study investigates the impact of STD on the dynamics of a continuous attractor neural network (CANN) and its potential roles in neural information processing. We find that the network with STD can generate both static and traveling bumps, and STD enhances the performance of the network in tracking external inputs. In particular, we find that STD endows the network with slow-decaying plateau behaviors, namely, the network being initially stimulated to an active state will decay to silence very slowly in the time scale of STD rather than that of neural signaling. We argue that this provides a mechanism for neural systems to hold short-term memory easily and shut off persistent activities naturally.</p>
<p>【Keywords】:</p>
<h3 id="71. Learning Kernels with Radiuses of Minimum Enclosing Balls.">71. Learning Kernels with Radiuses of Minimum Enclosing Balls.</h3>
<p>【<a href="http://papers.nips.cc/paper/3954-learning-kernels-with-radiuses-of-minimum-enclosing-balls">Paper Link</a>】    【Pages】:649-657</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gai:Kun">Kun Gai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Guangyun">Guangyun Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Changshui">Changshui Zhang</a></p>
<p>【Abstract】:
In this paper, we point out that there exist scaling and initialization problems in most existing multiple kernel learning (MKL) approaches, which employ the large margin principle to jointly learn both a kernel and an SVM classifier. The reason is that the margin itself can not well describe how good a kernel is due to the negligence of the scaling. We use the ratio between the margin and the radius of the minimum enclosing ball to measure the goodness of a kernel, and present a new minimization formulation for kernel learning. This formulation is invariant to scalings of learned kernels, and when learning linear combination of basis kernels it is also invariant to scalings of basis kernels and to the types (e.g., L1 or L2) of norm constraints on combination coefficients. We establish the differentiability of our formulation, and propose a gradient projection algorithm for kernel learning. Experiments show that our method significantly outperforms both SVM with the uniform combination of basis kernels and other state-of-art MKL approaches.</p>
<p>【Keywords】:</p>
<h3 id="72. Short-term memory in neuronal networks through dynamical compressed sensing.">72. Short-term memory in neuronal networks through dynamical compressed sensing.</h3>
<p>【<a href="http://papers.nips.cc/paper/3980-short-term-memory-in-neuronal-networks-through-dynamical-compressed-sensing">Paper Link</a>】    【Pages】:667-675</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ganguli:Surya">Surya Ganguli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sompolinsky:Haim">Haim Sompolinsky</a></p>
<p>【Abstract】:
Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that {\it exceeds} the number of neurons. This enhanced capacity is achieved by a class of ``orthogonal recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance."</p>
<p>【Keywords】:</p>
<h3 id="73. Implicit encoding of prior probabilities in optimal neural populations.">73. Implicit encoding of prior probabilities in optimal neural populations.</h3>
<p>【<a href="http://papers.nips.cc/paper/4130-implicit-encoding-of-prior-probabilities-in-optimal-neural-populations">Paper Link</a>】    【Pages】:658-666</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ganguli:Deep">Deep Ganguli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Simoncelli:Eero_P=">Eero P. Simoncelli</a></p>
<p>【Abstract】:
Optimal coding provides a guiding principle for understanding the representation of sensory variables in neural populations. Here we consider the influence of a prior probability distribution over sensory variables on the optimal allocation of cells and spikes in a neural population. We model the spikes of each cell as samples from an independent Poisson process with rate governed by an associated tuning curve. For this response model, we approximate the Fisher information in terms of the density and amplitude of the tuning curves, under the assumption that tuning width varies inversely with cell density. We consider a family of objective functions based on the expected value, over the sensory prior, of a functional of the Fisher information. This family includes lower bounds on mutual information and perceptual discriminability as special cases. In all cases, we find a closed form expression for the optimum, in which the density and gain of the cells in the population are power law functions of the stimulus prior. This also implies a power law relationship between the prior and perceptual discriminability. We show preliminary evidence that the theory successfully predicts the relationship between empirically measured stimulus priors, physiologically measured neural response properties (cell density, tuning widths, and firing rates), and psychophysically measured discrimination thresholds.</p>
<p>【Keywords】:</p>
<h3 id="74. Group Sparse Coding with a Laplacian Scale Mixture Prior.">74. Group Sparse Coding with a Laplacian Scale Mixture Prior.</h3>
<p>【<a href="http://papers.nips.cc/paper/3997-group-sparse-coding-with-a-laplacian-scale-mixture-prior">Paper Link</a>】    【Pages】:676-684</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Garrigues:Pierre">Pierre Garrigues</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Olshausen:Bruno_A=">Bruno A. Olshausen</a></p>
<p>【Abstract】:
We propose a class of sparse coding models that utilizes a Laplacian Scale Mixture (LSM) prior to model dependencies among coefficients. Each coefficient is modeled as a Laplacian distribution with a variable scale parameter, with a Gamma distribution prior over the scale parameter. We show that, due to the conjugacy of the Gamma prior, it is possible to derive efficient inference procedures for both the coefficients and the scale parameter. When the scale parameters of a group of coefficients are combined into a single variable, it is possible to describe the dependencies that occur due to common amplitude fluctuations among coefficients, which have been shown to constitute a large fraction of the redundancy in natural images. We show that, as a consequence of this group sparse coding, the resulting inference of the coefficients follows a divisive normalization rule, and that this may be efficiently implemented a network architecture similar to that which has been proposed to occur in primary visual cortex. We also demonstrate improvements in image coding and compressive sensing recovery using the LSM model.</p>
<p>【Keywords】:</p>
<h3 id="75. Improvements to the Sequence Memoizer.">75. Improvements to the Sequence Memoizer.</h3>
<p>【<a href="http://papers.nips.cc/paper/3938-improvements-to-the-sequence-memoizer">Paper Link</a>】    【Pages】:685-693</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gasthaus:Jan">Jan Gasthaus</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Teh:Yee_Whye">Yee Whye Teh</a></p>
<p>【Abstract】:
The sequence memoizer is a model for sequence data with state-of-the-art performance on language modeling and compression. We propose a number of improvements to the model and inference algorithm, including an enlarged range of hyperparameters, a memory-efficient representation, and inference algorithms operating on the new representation. Our derivations are based on precise definitions of the various processes that will also allow us to provide an elementary proof of the mysterious" coagulation and fragmentation properties used in the original paper on the sequence memoizer by Wood et al. (2009). We present some experimental results supporting our improvements."</p>
<p>【Keywords】:</p>
<h3 id="76. On Herding and the Perceptron Cycling Theorem.">76. On Herding and the Perceptron Cycling Theorem.</h3>
<p>【<a href="http://papers.nips.cc/paper/4004-on-herding-and-the-perceptron-cycling-theorem">Paper Link</a>】    【Pages】:694-702</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gelfand:Andrew">Andrew Gelfand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yutian">Yutian Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maaten:Laurens_van_der">Laurens van der Maaten</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Welling:Max">Max Welling</a></p>
<p>【Abstract】:
The paper develops a connection between traditional perceptron algorithms and recently introduced herding algorithms. It is shown that both algorithms can be viewed as an application of the perceptron cycling theorem. This connection strengthens some herding results and suggests new (supervised) herding algorithms that, like CRFs or discriminative RBMs, make predictions by conditioning on the input attributes. We develop and investigate variants of conditional herding, and show that conditional herding leads to practical algorithms that perform better than or on par with related classifiers such as the voted perceptron and the discriminative RBM.</p>
<p>【Keywords】:</p>
<h3 id="77. Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models.">77. Rescaling, thinning or complementing? On goodness-of-fit procedures for point process models and Generalized Linear Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3967-rescaling-thinning-or-complementing-on-goodness-of-fit-procedures-for-point-process-models-and-generalized-linear-models">Paper Link</a>】    【Pages】:703-711</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gerhard:Felipe">Felipe Gerhard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gerstner:Wulfram">Wulfram Gerstner</a></p>
<p>【Abstract】:
Generalized Linear Models (GLMs) are an increasingly popular framework for modeling neural spike trains. They have been linked to the theory of stochastic point processes and researchers have used this relation to assess goodness-of-fit using methods from point-process theory, e.g. the time-rescaling theorem. However, high neural firing rates or coarse discretization lead to a breakdown of the assumptions necessary for this connection. Here, we show how goodness-of-fit tests from point-process theory can still be applied to GLMs by constructing equivalent surrogate point processes out of time-series observations. Furthermore, two additional tests based on thinning and complementing point processes are introduced. They augment the instruments available for checking model adequacy of point processes as well as discretized models.</p>
<p>【Keywords】:</p>
<h3 id="78. The Neural Costs of Optimal Control.">78. The Neural Costs of Optimal Control.</h3>
<p>【<a href="http://papers.nips.cc/paper/4167-the-neural-costs-of-optimal-control">Paper Link</a>】    【Pages】:712-720</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gershman:Samuel">Samuel Gershman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wilson:Robert">Robert Wilson</a></p>
<p>【Abstract】:
Optimal control entails combining probabilities and utilities. However, for most practical problems probability densities can be represented only approximately. Choosing an approximation requires balancing the benefits of an accurate approximation against the costs of computing it. We propose a variational framework for achieving this balance and apply it to the problem of how a population code should optimally represent a distribution under resource constraints. The essence of our analysis is the conjecture that population codes are organized to maximize a lower bound on the log expected utility. This theory can account for a plethora of experimental data, including the reward-modulation of sensory receptive fields.</p>
<p>【Keywords】:</p>
<h3 id="79. LSTD with Random Projections.">79. LSTD with Random Projections.</h3>
<p>【<a href="http://papers.nips.cc/paper/3994-lstd-with-random-projections">Paper Link</a>】    【Pages】:721-729</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ghavamzadeh:Mohammad">Mohammad Ghavamzadeh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lazaric:Alessandro">Alessandro Lazaric</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maillard:Odalric=Ambrym">Odalric-Ambrym Maillard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Munos:R=eacute=mi">Rémi Munos</a></p>
<p>【Abstract】:
We consider the problem of reinforcement learning in high-dimensional spaces when the number of features is bigger than the number of samples. In particular, we study the least-squares temporal difference (LSTD) learning algorithm when a space of low dimension is generated with a random projection from a high-dimensional space. We provide a thorough theoretical analysis of the LSTD with random projections and derive performance bounds for the resulting algorithm. We also show how the error of LSTD with random projections is propagated through the iterations of a policy iteration algorithm and provide a performance bound for the resulting least-squares policy iteration (LSPI) algorithm.</p>
<p>【Keywords】:</p>
<h3 id="80. Humans Learn Using Manifolds, Reluctantly.">80. Humans Learn Using Manifolds, Reluctantly.</h3>
<p>【<a href="http://papers.nips.cc/paper/3905-humans-learn-using-manifolds-reluctantly">Paper Link</a>】    【Pages】:730-738</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gibson:Bryan_R=">Bryan R. Gibson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaojin">Xiaojin Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rogers:Timothy_T=">Timothy T. Rogers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kalish:Chuck">Chuck Kalish</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Harrison:Joseph">Joseph Harrison</a></p>
<p>【Abstract】:
When the distribution of unlabeled data in feature space lies along a manifold, the information it provides may be used by a learner to assist classification in a semi-supervised setting. While manifold learning is well-known in machine learning, the use of manifolds in human learning is largely unstudied. We perform a set of experiments which test a human's ability to use a manifold in a semi-supervised learning task, under varying conditions. We show that humans may be encouraged into using the manifold, overcoming the strong preference for a simple, axis-parallel linear boundary.</p>
<p>【Keywords】:</p>
<h3 id="81. Universal Consistency of Multi-Class Support Vector Classification.">81. Universal Consistency of Multi-Class Support Vector Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4042-universal-consistency-of-multi-class-support-vector-classification">Paper Link</a>】    【Pages】:739-747</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Glasmachers:Tobias">Tobias Glasmachers</a></p>
<p>【Abstract】:
Steinwart was the ﬁrst to prove universal consistency of support vector machine  classiﬁcation. His proof analyzed the ‘standard’ support vector machine classiﬁer,  which is restricted to binary classiﬁcation problems. In contrast, recent analysis  has resulted in the common belief that several extensions of SVM classiﬁcation to  more than two classes are inconsistent.  Countering this belief, we proof the universal consistency of the multi-class support vector machine by Crammer and Singer. Our proof extends Steinwart’s techniques to the multi-class case.</p>
<p>【Keywords】:</p>
<h3 id="82. Learning Efficient Markov Networks.">82. Learning Efficient Markov Networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4010-learning-efficient-markov-networks">Paper Link</a>】    【Pages】:748-756</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gogate:Vibhav">Vibhav Gogate</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Webb:William_Austin">William Austin Webb</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Domingos:Pedro_M=">Pedro M. Domingos</a></p>
<p>【Abstract】:
We present an algorithm for learning high-treewidth Markov networks where inference is still tractable. This is made possible by exploiting context specific independence and determinism in the domain. The class of models our algorithm can learn has the same desirable properties as thin junction trees: polynomial inference, closed form weight learning, etc., but is much broader. Our algorithm searches for a feature that divides the state space into subspaces where the remaining variables decompose into independent subsets (conditioned on the feature or its negation) and recurses on each subspace/subset of variables until no useful new features can be found. We provide probabilistic performance guarantees for our algorithm under the assumption that the maximum feature length is k (the treewidth can be much larger) and dependences are of bounded strength. We also propose a greedy version of the algorithm that, while forgoing these guarantees, is much more efficient.Experiments on a variety of domains show that our approach compares favorably with thin junction trees and other Markov network structure learners.</p>
<p>【Keywords】:</p>
<h3 id="83. Transduction with Matrix Completion: Three Birds with One Stone.">83. Transduction with Matrix Completion: Three Birds with One Stone.</h3>
<p>【<a href="http://papers.nips.cc/paper/3932-transduction-with-matrix-completion-three-birds-with-one-stone">Paper Link</a>】    【Pages】:757-765</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Goldberg:Andrew_B=">Andrew B. Goldberg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaojin">Xiaojin Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Recht:Ben">Ben Recht</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Jun=Ming">Jun-Ming Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nowak:Robert_D=">Robert D. Nowak</a></p>
<p>【Abstract】:
We pose transductive classification as a matrix completion problem. By assuming the underlying matrix has a low rank, our formulation is able to handle three problems simultaneously: i) multi-label learning, where each item has more than one label, ii) transduction, where most of these labels are unspecified, and iii) missing data, where a large number of features are missing. We obtained satisfactory results on several real-world tasks, suggesting that the low rank assumption may not be as restrictive as it seems. Our method allows for different loss functions to apply on the feature and label entries of the matrix. The resulting nuclear norm minimization problem is solved with a modified fixed-point continuation method that is guaranteed to find the global optimum.</p>
<p>【Keywords】:</p>
<h3 id="84. Near-Optimal Bayesian Active Learning with Noisy Observations.">84. Near-Optimal Bayesian Active Learning with Noisy Observations.</h3>
<p>【<a href="http://papers.nips.cc/paper/4073-near-optimal-bayesian-active-learning-with-noisy-observations">Paper Link</a>】    【Pages】:766-774</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Golovin:Daniel">Daniel Golovin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krause_0001:Andreas">Andreas Krause</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ray:Debajyoti">Debajyoti Ray</a></p>
<p>【Abstract】:
We tackle the fundamental problem of Bayesian active learning with noise, where we need to adaptively select from a number of expensive tests in order to identify an unknown hypothesis sampled from a known prior distribution. In the case of noise-free observations, a greedy algorithm called generalized binary search (GBS) is known to perform near-optimally. We show that if the observations are noisy, perhaps surprisingly, GBS can perform very poorly. We develop EC2, a novel, greedy active learning algorithm and prove that it is competitive with the optimal policy, thus obtaining the first competitiveness guarantees for Bayesian active learning with noisy observations. Our bounds rely on a recently discovered diminishing returns property called adaptive submodularity, generalizing the classical notion of submodular set functions to adaptive policies. Our results hold even if the tests have non–uniform cost and their noise is correlated. We also propose EffECXtive, a particularly fast approximation of EC2, and evaluate it on a Bayesian experimental design problem involving human subjects, intended to tease apart competing economic theories of how people make decisions under uncertainty.</p>
<p>【Keywords】:</p>
<h3 id="85. Discriminative Clustering by Regularized Information Maximization.">85. Discriminative Clustering by Regularized Information Maximization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4154-discriminative-clustering-by-regularized-information-maximization">Paper Link</a>】    【Pages】:775-783</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gomes:Ryan">Ryan Gomes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krause_0001:Andreas">Andreas Krause</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perona:Pietro">Pietro Perona</a></p>
<p>【Abstract】:
Is there a principled way to learn a probabilistic discriminative classifier from an unlabeled data set? We present a framework that simultaneously clusters the data and trains a discriminative classifier. We call it Regularized Information Maximization (RIM). RIM optimizes an intuitive information-theoretic objective function which balances class separation, class balance and classifier complexity. The approach can flexibly incorporate different likelihood functions, express prior assumptions about the relative size of different classes and incorporate partial labels for semi-supervised learning. In particular, we instantiate the framework to unsupervised, multi-class kernelized logistic regression. Our empirical evaluation indicates that RIM outperforms existing methods on several real data sets, and demonstrates that RIM is an effective model selection method.</p>
<p>【Keywords】:</p>
<h3 id="86. Learning to localise sounds with spiking neural networks.">86. Learning to localise sounds with spiking neural networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4127-learning-to-localise-sounds-with-spiking-neural-networks">Paper Link</a>】    【Pages】:784-792</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Goodman:Dan_F=_M=">Dan F. M. Goodman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brette:Romain">Romain Brette</a></p>
<p>【Abstract】:
To localise the source of a sound, we use location-specific properties of the signals received at the two ears caused by the asymmetric filtering of the original sound by our head and pinnae, the head-related transfer functions (HRTFs). These HRTFs change throughout an organism's lifetime, during development for example, and so the required neural circuitry cannot be entirely hardwired. Since HRTFs are not directly accessible from perceptual experience, they can only be inferred from filtered sounds. We present a spiking neural network model of sound localisation based on extracting location-specific synchrony patterns, and a simple supervised algorithm to learn the mapping between synchrony patterns and locations from a set of example sounds, with no previous knowledge of HRTFs. After learning, our model was able to accurately localise new sounds in both azimuth and elevation, including the difficult task of distinguishing sounds coming from the front and back.</p>
<p>【Keywords】:</p>
<h3 id="87. Feature Set Embedding for Incomplete Data.">87. Feature Set Embedding for Incomplete Data.</h3>
<p>【<a href="http://papers.nips.cc/paper/4047-feature-set-embedding-for-incomplete-data">Paper Link</a>】    【Pages】:793-801</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Grangier:David">David Grangier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Melvin:Iain">Iain Melvin</a></p>
<p>【Abstract】:
We present a new learning strategy for classification problems in which train and/or test data suffer from missing features. In previous work, instances are represented as vectors from some feature space and one is forced to impute missing values or to consider an instance-specific subspace. In contrast, our method considers instances as sets of (feature,value) pairs which naturally handle the missing value case. Building onto this framework, we propose a classification strategy for sets. Our proposal maps (feature,value) pairs into an embedding space and then non-linearly combines the set of embedded vectors. The embedding and the combination parameters are learned jointly on the final classification objective. This simple strategy allows great flexibility in encoding prior knowledge about the features in the embedding step and yields advantageous results compared to alternative solutions over several datasets.</p>
<p>【Keywords】:</p>
<h3 id="88. Active Instance Sampling via Matrix Partition.">88. Active Instance Sampling via Matrix Partition.</h3>
<p>【<a href="http://papers.nips.cc/paper/3919-active-instance-sampling-via-matrix-partition">Paper Link</a>】    【Pages】:802-810</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Yuhong">Yuhong Guo</a></p>
<p>【Abstract】:
Recently, batch-mode active learning has attracted a lot of attention. In this paper, we propose a novel batch-mode active learning approach that selects a batch of queries in each iteration by maximizing a natural form of mutual information criterion between the labeled and unlabeled instances. By employing a Gaussian process framework, this mutual information based instance selection problem can be formulated as a matrix partition problem. Although the matrix partition is an NP-hard combinatorial optimization problem, we show a good local solution can be obtained by exploiting an effective local optimization technique on the relaxed continuous optimization problem. The proposed active learning approach is independent of employed classification models. Our empirical studies show this approach can achieve comparable or superior performance to discriminative batch-mode active learning methods.</p>
<p>【Keywords】:</p>
<h3 id="89. Avoiding False Positive in Multi-Instance Learning.">89. Avoiding False Positive in Multi-Instance Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3941-avoiding-false-positive-in-multi-instance-learning">Paper Link</a>】    【Pages】:811-819</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Han:Yanjun">Yanjun Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tao:Qing">Qing Tao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jue">Jue Wang</a></p>
<p>【Abstract】:
In multi-instance learning, there are two kinds of prediction failure, i.e., false negative and false positive. Current research mainly focus on avoding the former. We attempt to utilize the geometric distribution of instances inside positive bags to avoid both the former and the latter. Based on kernel principal component analysis, we define a projection constraint for each positive bag to classify its constituent instances far away from the separating hyperplane while place positive instances and negative instances at opposite sides. We apply the Constrained Concave-Convex Procedure to solve the resulted problem. Empirical results demonstrate that our approach offers improved generalization performance.</p>
<p>【Keywords】:</p>
<h3 id="90. Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable.">90. Nonparametric Density Estimation for Stochastic Optimization with an Observable State Variable.</h3>
<p>【<a href="http://papers.nips.cc/paper/4098-nonparametric-density-estimation-for-stochastic-optimization-with-an-observable-state-variable">Paper Link</a>】    【Pages】:820-828</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hannah:Lauren">Lauren Hannah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Powell:Warren_B=">Warren B. Powell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blei:David_M=">David M. Blei</a></p>
<p>【Abstract】:
We study convex stochastic optimization problems where a noisy objective function value is observed after a decision is made. There are many stochastic optimization problems whose behavior depends on an exogenous state variable which affects the shape of the objective function. Currently, there is no general purpose algorithm to solve this class of problems. We use nonparametric density estimation for the joint distribution of state-outcome pairs to create weights for previous observations. The weights effectively group similar states. Those similar to the current state are used to create a convex, deterministic approximation of the objective function. We propose two solution methods that depend on the problem characteristics: function-based and gradient-based optimization. We offer two weighting schemes, kernel based weights and Dirichlet process based weights, for use with the solution methods. The weights and solution methods are tested on a synthetic multi-product newsvendor problem and the hour ahead wind commitment problem. Our results show Dirichlet process weights can offer substantial benefits over kernel based weights and, more generally, that nonparametric estimation methods provide good solutions to otherwise intractable problems.</p>
<p>【Keywords】:</p>
<h3 id="91. Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.">91. Space-Variant Single-Image Blind Deconvolution for Removing Camera Shake.</h3>
<p>【<a href="http://papers.nips.cc/paper/4007-space-variant-single-image-blind-deconvolution-for-removing-camera-shake">Paper Link</a>】    【Pages】:829-837</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Harmeling:Stefan">Stefan Harmeling</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hirsch:Michael">Michael Hirsch</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=ouml=lkopf:Bernhard">Bernhard Schölkopf</a></p>
<p>【Abstract】:
Modelling camera shake as a space-invariant convolution simplifies the problem of removing camera shake, but often insufficiently models actual motion blur such as those due to camera rotation and movements outside the sensor plane or when objects in the scene have different distances to the camera. In order to overcome such limitations we contribute threefold: (i) we introduce a taxonomy of camera shakes, (ii) we show how to combine a recently introduced framework for space-variant filtering based on overlap-add from Hirsch et al.~and a fast algorithm for single image blind deconvolution for space-invariant filters from Cho and Lee to introduce a method for blind deconvolution for space-variant blur. And (iii), we present an experimental setup for evaluation that allows us to take images with real camera shake while at the same time record the space-variant point spread function corresponding to that blur. Finally, we demonstrate that our method is able to deblur images degraded by spatially-varying blur originating from real camera shake.</p>
<p>【Keywords】:</p>
<h3 id="92. Double Q-learning.">92. Double Q-learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3964-double-q-learning">Paper Link</a>】    【Pages】:2613-2621</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hasselt:Hado_van">Hado van Hasselt</a></p>
<p>【Abstract】:
In some stochastic environments the well-known reinforcement learning algorithm Q-learning performs very poorly. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because Q-learning uses the maximum action value as an approximation for the maximum expected action value. We introduce an alternative way to approximate the maximum expected value for any set of random variables. The obtained double estimator method is shown to sometimes underestimate rather than overestimate the maximum expected value. We apply the double estimator to Q-learning to construct Double Q-learning, a new off-policy reinforcement learning algorithm. We show the new algorithm converges to the optimal policy and that it performs well in some settings in which Q-learning performs poorly due to its overestimation.</p>
<p>【Keywords】:</p>
<h3 id="93. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction.">93. A Primal-Dual Message-Passing Algorithm for Approximated Large Scale Structured Prediction.</h3>
<p>【<a href="http://papers.nips.cc/paper/3913-a-primal-dual-message-passing-algorithm-for-approximated-large-scale-structured-prediction">Paper Link</a>】    【Pages】:838-846</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hazan:Tamir">Tamir Hazan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Urtasun:Raquel">Raquel Urtasun</a></p>
<p>【Abstract】:
In this paper we propose an approximated learning framework for large scale graphical models and derive message passing algorithms for learning their parameters efficiently.  We first relate CRFs and structured SVMs  and show that in the CRF's primal a variant of the log-partition function, known as soft-max, smoothly approximates the hinge loss function of structured SVMs.  We then propose an intuitive approximation for structured prediction problems using Fenchel duality based on a local entropy approximation that computes the exact gradients of the approximated problem and is guaranteed to converge. Unlike existing approaches, this allow us to learn graphical models with cycles and very large number of parameters efficiently. We demonstrate the effectiveness of our approach  in an image denoising task. This task was previously solved by sharing parameters across cliques. In contrast, our algorithm is able to efficiently learn large number of parameters resulting in orders of magnitude better prediction.</p>
<p>【Keywords】:</p>
<h3 id="94. An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA.">94. An Inverse Power Method for Nonlinear Eigenproblems with Applications in 1-Spectral Clustering and Sparse PCA.</h3>
<p>【<a href="http://papers.nips.cc/paper/4110-an-inverse-power-method-for-nonlinear-eigenproblems-with-applications-in-1-spectral-clustering-and-sparse-pca">Paper Link</a>】    【Pages】:847-855</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hein_0001:Matthias">Matthias Hein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/B=uuml=hler:Thomas">Thomas Bühler</a></p>
<p>【Abstract】:
Many problems in machine learning and statistics can be formulated as (generalized) eigenproblems. In terms of the associated optimization problem, computing linear eigenvectors amounts to finding critical points of a quadratic function subject to quadratic constraints. In this paper we show that a certain class of constrained optimization problems with nonquadratic objective and constraints can be understood as nonlinear eigenproblems. We derive a generalization of the inverse power method which is guaranteed to converge to a nonlinear eigenvector. We apply the inverse power method to 1-spectral clustering and sparse PCA which can naturally be formulated as nonlinear eigenproblems. In both applications we achieve state-of-the-art results in terms of solution quality and runtime. Moving beyond the standard eigenproblem should be useful also in many other applications and our inverse power method can be easily adapted to new problems.</p>
<p>【Keywords】:</p>
<h3 id="95. Online Learning for Latent Dirichlet Allocation.">95. Online Learning for Latent Dirichlet Allocation.</h3>
<p>【<a href="http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation">Paper Link</a>】    【Pages】:856-864</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hoffman:Matthew_D=">Matthew D. Hoffman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blei:David_M=">David M. Blei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Francis_R=">Francis R. Bach</a></p>
<p>【Abstract】:
We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Allocation (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, including those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time.</p>
<p>【Keywords】:</p>
<h3 id="96. Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development.">96. Latent Variable Models for Predicting File Dependencies in Large-Scale Software Development.</h3>
<p>【<a href="http://papers.nips.cc/paper/4022-latent-variable-models-for-predicting-file-dependencies-in-large-scale-software-development">Paper Link</a>】    【Pages】:865-873</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Diane">Diane Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maaten:Laurens_van_der">Laurens van der Maaten</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cho:Youngmin">Youngmin Cho</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saul:Lawrence_K=">Lawrence K. Saul</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lerner:Sorin">Sorin Lerner</a></p>
<p>【Abstract】:
When software developers modify one or more files in a large code base, they must also identify and update other related files. Many file dependencies can be detected by mining the development history of the code base: in essence, groups of related files are revealed by the logs of previous workflows. From data of this form, we show how to detect dependent files by solving a problem in binary matrix completion. We explore different latent variable models (LVMs) for this problem, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. We evaluate these models on the development histories of three large, open-source software systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs improve the performance of related file prediction over current leading methods.</p>
<p>【Keywords】:</p>
<h3 id="97. Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.">97. Predicting Execution Time of Computer Programs Using Sparse Polynomial Regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/4145-predicting-execution-time-of-computer-programs-using-sparse-polynomial-regression">Paper Link</a>】    【Pages】:883-891</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Ling">Ling Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jia:Jinzhu">Jinzhu Jia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu_0001:Bin">Bin Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chun:Byung=Gon">Byung-Gon Chun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maniatis:Petros">Petros Maniatis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naik:Mayur">Mayur Naik</a></p>
<p>【Abstract】:
Predicting the execution time of computer programs is an important but challenging problem in the community of computer systems. Existing methods require experts to perform detailed analysis of program code in order to construct predictors or select important features. We recently developed a new system to automatically extract a large number of features from program execution on sample inputs, on which prediction models can be constructed without expert knowledge. In this paper we study the construction of predictive models for this problem. We propose the SPORE (Sparse POlynomial REgression) methodology to build accurate prediction models of program performance using feature data collected from program execution on sample inputs. Our two SPORE algorithms are able to build relationships between responses (e.g., the execution time of a computer program) and features, and select a few from hundreds of the retrieved features to construct an explicitly sparse and non-linear model to predict the response variable. The compact and explicitly polynomial form of the estimated model could reveal important insights into the computer program (e.g., features and their non-linear combinations that dominate the execution time), enabling a better understanding of the program’s behavior. Our evaluation on three widely used computer programs shows that SPORE methods can give accurate prediction with relative error less than 7% by using a moderate number of training data samples. In addition, we compare SPORE algorithms to state-of-the-art sparse regression algorithms, and show that SPORE methods, motivated by real applications, outperform the other methods in terms of both interpretability and prediction accuracy.</p>
<p>【Keywords】:</p>
<h3 id="98. Active Learning by Querying Informative and Representative Examples.">98. Active Learning by Querying Informative and Representative Examples.</h3>
<p>【<a href="http://papers.nips.cc/paper/4176-active-learning-by-querying-informative-and-representative-examples">Paper Link</a>】    【Pages】:892-900</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Sheng=Jun">Sheng-Jun Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Rong">Rong Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Zhi=Hua">Zhi-Hua Zhou</a></p>
<p>【Abstract】:
Most active learning approaches select either informative or representative unlabeled instances to query their labels. Although several active learning algorithms have been proposed to combine the two criterions for query selection, they are usually ad hoc in finding unlabeled instances that are both informative and representative. We address this challenge by a principled approach, termed QUIRE, based on the min-max view of active learning. The proposed approach provides a systematic way for measuring and combining the informativeness and representativeness of an instance. Extensive experimental results show that the proposed QUIRE approach outperforms several state-of -the-art active learning approaches.</p>
<p>【Keywords】:</p>
<h3 id="99. Exact inference and learning for cumulative distribution functions on loopy graphs.">99. Exact inference and learning for cumulative distribution functions on loopy graphs.</h3>
<p>【<a href="http://papers.nips.cc/paper/3899-exact-inference-and-learning-for-cumulative-distribution-functions-on-loopy-graphs">Paper Link</a>】    【Pages】:874-882</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jim_C=">Jim C. Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jojic:Nebojsa">Nebojsa Jojic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meek:Christopher">Christopher Meek</a></p>
<p>【Abstract】:
Probabilistic graphical models use local factors to represent dependence among sets of variables. For many problem domains, for instance climatology and epidemiology, in addition to local dependencies, we may also wish to model heavy-tailed statistics, where extreme deviations should not be treated as outliers. Specifying such distributions using graphical models for probability density functions (PDFs) generally lead to intractable inference and learning. Cumulative distribution networks (CDNs) provide a means to tractably specify multivariate heavy-tailed models as a product of cumulative distribution functions (CDFs). Currently, algorithms for inference and learning, which correspond to computing mixed derivatives, are exact only for tree-structured graphs. For graphs of arbitrary topology, an efficient algorithm is needed that takes advantage of the sparse structure of the model, unlike symbolic differentiation programs such as Mathematica and D<em> that do not. We present an algorithm for recursively decomposing the computation of derivatives for CDNs of arbitrary topology, where the decomposition is naturally described using junction trees. We compare the performance of the resulting algorithm to Mathematica and D</em>, and we apply our method to learning models for rainfall and H1N1 data, where we show that CDNs with cycles are able to provide a significantly better fits to the data as compared to tree-structured and unstructured CDNs and other heavy-tailed multivariate distributions such as the multivariate copula and logistic models.</p>
<p>【Keywords】:</p>
<h3 id="100. Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks.">100. Inter-time segment information sharing for non-homogeneous dynamic Bayesian networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/3944-inter-time-segment-information-sharing-for-non-homogeneous-dynamic-bayesian-networks">Paper Link</a>】    【Pages】:901-909</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Husmeier:Dirk">Dirk Husmeier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dondelinger:Frank">Frank Dondelinger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/L=egrave=bre:Sophie">Sophie Lèbre</a></p>
<p>【Abstract】:
Conventional dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption, which is too restrictive in many practical applications. Various approaches to relax the homogeneity assumption have therefore been proposed in the last few years. The present paper aims to improve the flexibility of two recent versions of non-homogeneous DBNs, which either (i) suffer from the need for data discretization, or (ii) assume a time-invariant network structure. Allowing the network structure to be fully flexible leads to the risk of overfitting and inflated inference uncertainty though, especially in the highly topical field of systems biology, where independent measurements tend to be sparse. In the present paper we investigate three conceptually different regularization schemes based on inter-segment information sharing. We assess the performance in a comparative evaluation study based on simulated data. We compare the predicted segmentation of gene expression time series obtained during embryogenesis in Drosophila melanogaster with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict a known regulatory network of five genes in Saccharomyces cerevisiae.</p>
<p>【Keywords】:</p>
<h3 id="101. Co-regularization Based Semi-supervised Domain Adaptation.">101. Co-regularization Based Semi-supervised Domain Adaptation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4009-co-regularization-based-semi-supervised-domain-adaptation">Paper Link</a>】    【Pages】:478-486</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Daum=eacute=_III:Hal">Hal Daumé III</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kumar_0001:Abhishek">Abhishek Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saha:Avishek">Avishek Saha</a></p>
<p>【Abstract】:
This paper presents a co-regularization based approach to semi-supervised domain adaptation. Our proposed approach (EA++) builds on the notion of augmented space (introduced in EASYADAPT (EA) [1]) and harnesses unlabeled data in target domain to further enable the transfer of information from source to target. This semi-supervised approach to domain adaptation is extremely simple to implement and can be applied as a pre-processing step to any supervised learner. Our theoretical analysis (in terms of Rademacher complexity) of EA and EA++ show that the hypothesis class of EA++ has lower complexity (compared to EA) and hence results in tighter generalization bounds. Experimental results on sentiment analysis tasks reinforce our theoretical findings and demonstrate the efficacy of the proposed method when compared to EA as well as a few other baseline approaches.</p>
<p>【Keywords】:</p>
<h3 id="102. Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication.">102. Deciphering subsampled data: adaptive compressive sampling as a principle of brain communication.</h3>
<p>【<a href="http://papers.nips.cc/paper/4093-deciphering-subsampled-data-adaptive-compressive-sampling-as-a-principle-of-brain-communication">Paper Link</a>】    【Pages】:910-918</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Isley:Guy">Guy Isley</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hillar:Christopher_J=">Christopher J. Hillar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sommer:Friedrich_T=">Friedrich T. Sommer</a></p>
<p>【Abstract】:
A new algorithm is proposed for a) unsupervised learning of sparse representations from subsampled measurements and b) estimating the parameters required for linearly reconstructing signals from the sparse codes. We verify that the new algorithm performs efficient data compression on par with the recent method of compressive sampling. Further, we demonstrate that the algorithm performs robustly when stacked in several stages or when applied in undercomplete or overcomplete situations. The new algorithm can explain how neural populations in the brain that receive subsampled input through fiber bottlenecks are able to form coherent response properties.</p>
<p>【Keywords】:</p>
<h3 id="103. Dynamic Infinite Relational Model for Time-varying Relational Data Analysis.">103. Dynamic Infinite Relational Model for Time-varying Relational Data Analysis.</h3>
<p>【<a href="http://papers.nips.cc/paper/4071-dynamic-infinite-relational-model-for-time-varying-relational-data-analysis">Paper Link</a>】    【Pages】:919-927</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Ishiguro:Katsuhiko">Katsuhiko Ishiguro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iwata:Tomoharu">Tomoharu Iwata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ueda:Naonori">Naonori Ueda</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tenenbaum:Joshua_B=">Joshua B. Tenenbaum</a></p>
<p>【Abstract】:
We propose a new probabilistic model for analyzing dynamic evolutions of relational data, such as additions, deletions and split &amp; merge, of relation clusters like communities in social networks. Our proposed model abstracts observed time-varying object-object relationships into relationships between object clusters. We extend the infinite Hidden Markov model to follow dynamic and time-sensitive changes in the structure of the relational data and to estimate a number of clusters simultaneously. We show the usefulness of the model through experiments with synthetic and real-world data sets.</p>
<p>【Keywords】:</p>
<h3 id="104. Inductive Regularized Learning of Kernel Functions.">104. Inductive Regularized Learning of Kernel Functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4159-inductive-regularized-learning-of-kernel-functions">Paper Link</a>】    【Pages】:946-954</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jain_0002:Prateek">Prateek Jain</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kulis:Brian">Brian Kulis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dhillon:Inderjit_S=">Inderjit S. Dhillon</a></p>
<p>【Abstract】:
In this paper we consider the fundamental problem of semi-supervised kernel function learning. We propose a general regularized framework for learning a kernel matrix, and then demonstrate an equivalence between our proposed kernel matrix learning framework and a general linear transformation learning problem. Our result shows that the learned kernel matrices parameterize a linear transformation kernel function and can be applied inductively to new data points. Furthermore, our result gives a constructive method for kernelizing most existing Mahalanobis metric learning formulations. To make our results practical for large-scale data, we modify our framework to limit the number of parameters in the optimization process. We also consider the problem of kernelized inductive dimensionality reduction in the semi-supervised setting. We introduce a novel method for this problem by considering a special case of our general kernel learning framework where we select the trace norm function as the regularizer. We empirically demonstrate that our framework learns useful kernel functions, improving the $k$-NN classification accuracy significantly in a variety of domains. Furthermore, our kernelized dimensionality reduction technique significantly reduces the dimensionality of the feature space while achieving competitive classification accuracies.</p>
<p>【Keywords】:</p>
<h3 id="105. Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning.">105. Hashing Hyperplane Queries to Near Points with Applications to Large-Scale Active Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4088-hashing-hyperplane-queries-to-near-points-with-applications-to-large-scale-active-learning">Paper Link</a>】    【Pages】:928-936</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jain_0002:Prateek">Prateek Jain</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vijayanarasimhan:Sudheendra">Sudheendra Vijayanarasimhan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grauman:Kristen">Kristen Grauman</a></p>
<p>【Abstract】:
We consider the problem of retrieving the database points nearest to a given {\em hyperplane} query without exhaustively scanning the database. We propose two hashing-based solutions. Our first approach maps the data to two-bit binary keys that are locality-sensitive for the angle between the hyperplane normal and a database point. Our second approach embeds the data into a vector space where the Euclidean norm reflects the desired distance between the original points and hyperplane query. Both use hashing to retrieve near points in sub-linear time. Our first method's preprocessing stage is more efficient, while the second has stronger accuracy guarantees. We apply both to pool-based active learning: taking the current hyperplane classifier as a query, our algorithm identifies those points (approximately) satisfying the well-known minimal distance-to-hyperplane selection criterion. We empirically demonstrate our methods' tradeoffs, and show that they make it practical to perform active selection with millions of unlabeled points.</p>
<p>【Keywords】:</p>
<h3 id="106. Guaranteed Rank Minimization via Singular Value Projection.">106. Guaranteed Rank Minimization via Singular Value Projection.</h3>
<p>【<a href="http://papers.nips.cc/paper/3904-guaranteed-rank-minimization-via-singular-value-projection">Paper Link</a>】    【Pages】:937-945</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jain_0002:Prateek">Prateek Jain</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meka:Raghu">Raghu Meka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dhillon:Inderjit_S=">Inderjit S. Dhillon</a></p>
<p>【Abstract】:
Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics. In this paper we propose a simple and fast algorithm SVP (Singular Value Projection) for rank minimization under affine constraints ARMP and show that SVP recovers the minimum rank solution for affine constraints that satisfy a Restricted Isometry Property} (RIP). Our method guarantees geometric convergence rate even in the presence of noise and requires strictly weaker assumptions on the RIP constants than the existing methods. We also introduce a Newton-step for our SVP framework to speed-up the convergence with substantial empirical gains. Next, we address a practically important application of ARMP - the problem of low-rank matrix completion, for which the defining affine constraints do not directly obey RIP, hence the guarantees of SVP do not hold. However, we provide partial progress towards a proof of exact recovery for our algorithm by showing a more restricted isometry property and observe empirically that our algorithm recovers low-rank Incoherent matrices from an almost optimal number of uniformly sampled entries. We also demonstrate empirically that our algorithms outperform existing methods, such as those of \cite{CaiCS2008,LeeB2009b, KeshavanOM2009}, for ARMP and the matrix completion problem by an order of magnitude and are also more robust to noise and sampling schemes. In particular, results show that our SVP-Newton method is significantly robust to noise and performs impressively on a more realistic power-law sampling scheme for the matrix completion problem.</p>
<p>【Keywords】:</p>
<h3 id="107. A Dirty Model for Multi-task Learning.">107. A Dirty Model for Multi-task Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4125-a-dirty-model-for-multi-task-learning">Paper Link</a>】    【Pages】:964-972</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jalali:Ali">Ali Jalali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ravikumar:Pradeep">Pradeep Ravikumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanghavi:Sujay">Sujay Sanghavi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ruan:Chao">Chao Ruan</a></p>
<p>【Abstract】:
We consider the multiple linear regression problem, in a setting where some of the set of relevant features could be shared across the tasks. A lot of recent research has studied the use of $\ell_1/\ell_q$ norm block-regularizations with $q &gt; 1$  for such (possibly) block-structured problems, establishing strong guarantees on recovery even under high-dimensional scaling where the number of features scale with the number of observations. However, these papers also caution that the performance of such block-regularized methods are very dependent on the {\em extent} to which the features are shared across tasks. Indeed they show~\citep{NWJoint} that if the extent of overlap is less than a threshold, or even if parameter {\em values} in the shared features are highly uneven, then block $\ell_1/\ell_q$ regularization could actually perform {\em worse} than simple separate elementwise $\ell_1$ regularization. We are far away from a realistic multi-task setting: not only do the set of relevant features have to be exactly the same across tasks, but their values have to as well.  Here, we ask the question: can we leverage support and parameter overlap when it exists, but not pay a penalty when it does not? Indeed, this falls under a more general question of whether we can model such \emph{dirty data} which may not fall into a single neat structural bracket (all block-sparse, or all low-rank and so on). Here, we take a first step, focusing on developing a dirty model for the multiple regression problem. Our method uses a very simple idea: we decompose  the parameters into two components and {\em regularize these differently.} We show both theoretically and empirically, our method strictly and noticeably outperforms both $\ell_1$ and $\ell_1/\ell_q$ methods, over the entire range of possible overlaps. We also provide theoretical guarantees that the method performs well under high-dimensional scaling.</p>
<p>【Keywords】:</p>
<h3 id="108. Lifted Inference Seen from the Other Side : The Tractable Features.">108. Lifted Inference Seen from the Other Side : The Tractable Features.</h3>
<p>【<a href="http://papers.nips.cc/paper/4067-lifted-inference-seen-from-the-other-side-the-tractable-features">Paper Link</a>】    【Pages】:973-981</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jha:Abhay_Kumar">Abhay Kumar Jha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gogate:Vibhav">Vibhav Gogate</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meliou:Alexandra">Alexandra Meliou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suciu:Dan">Dan Suciu</a></p>
<p>【Abstract】:
Lifted inference algorithms for representations that combine first-order logic and probabilistic graphical models have been the focus of much recent research. All lifted algorithms developed to date are based on the same underlying idea: take a standard probabilistic inference algorithm (e.g., variable elimination, belief propagation etc.) and improve its efficiency by exploiting repeated structure in the first-order model. In this paper, we propose an approach from the other side in that we use techniques from logic for probabilistic inference. In particular, we define a set of rules that look only at the logical representation to identify models for which exact efficient inference is possible. We show that our rules yield several new tractable classes that cannot be solved efficiently by any of the existing techniques.</p>
<p>【Keywords】:</p>
<h3 id="109. Factorized Latent Spaces with Structured Sparsity.">109. Factorized Latent Spaces with Structured Sparsity.</h3>
<p>【<a href="http://papers.nips.cc/paper/3953-factorized-latent-spaces-with-structured-sparsity">Paper Link</a>】    【Pages】:982-990</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jia:Yangqing">Yangqing Jia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Salzmann:Mathieu">Mathieu Salzmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Darrell:Trevor">Trevor Darrell</a></p>
<p>【Abstract】:
Recent approaches to multi-view learning have shown that factorizing the information into parts that are shared across all views and parts that are private to each view could effectively account for the dependencies and independencies between the different input modalities. Unfortunately, these approaches involve minimizing non-convex objective functions. In this paper, we propose an approach to learning such factorized representations inspired by sparse coding techniques. In particular, we show that structured sparsity allows us to address the multi-view learning problem by alternately solving two convex optimization problems. Furthermore, the resulting factorized latent spaces generalize over existing approaches in that they allow :having latent dimensions shared between any subset of the views instead of between all the views only. We show that our approach outperforms state-of-the-art methods on the task of human pose estimation.</p>
<p>【Keywords】:</p>
<h3 id="110. Bayesian Action-Graph Games.">110. Bayesian Action-Graph Games.</h3>
<p>【<a href="http://papers.nips.cc/paper/4171-bayesian-action-graph-games">Paper Link</a>】    【Pages】:991-999</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Albert_Xin">Albert Xin Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leyton=Brown:Kevin">Kevin Leyton-Brown</a></p>
<p>【Abstract】:
Games of incomplete information, or Bayesian games, are an important game-theoretic model and have many applications in economics. We propose Bayesian action-graph games (BAGGs), a novel graphical representation for Bayesian games. BAGGs can represent arbitrary Bayesian games, and furthermore can compactly express Bayesian games exhibiting commonly encountered types of structure including symmetry, action- and type-specific utility independence, and probabilistic independence of type distributions. We provide an algorithm for computing expected utility in BAGGs, and discuss conditions under which the algorithm runs in polynomial time.  Bayes-Nash equilibria of BAGGs can be computed by adapting existing algorithms for complete-information normal form games and leveraging our expected utility algorithm. We show both theoretically and empirically that our approaches improve significantly on the state of the art.</p>
<p>【Keywords】:</p>
<h3 id="111. On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.">111. On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient.</h3>
<p>【<a href="http://papers.nips.cc/paper/3922-on-a-connection-between-importance-sampling-and-the-likelihood-ratio-policy-gradient">Paper Link</a>】    【Pages】:1000-1008</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Abbeel:Pieter">Pieter Abbeel</a></p>
<p>【Abstract】:
Likelihood ratio policy gradient methods have been some of the most successful reinforcement learning algorithms, especially for learning on physical systems. We describe how the likelihood ratio policy gradient can be derived from an importance sampling perspective. This derivation highlights how likelihood ratio methods under-use past experience by (a) using the past experience to estimate {\em only} the gradient of the expected return $U(\theta)$ at the current policy parameterization $\theta$, rather than to obtain a more complete estimate of $U(\theta)$, and (b) using past experience under the current policy {\em only} rather than using all past experience to improve the estimates. We present a new policy search method, which leverages both of these observations as well as generalized baselines---a new technique which generalizes commonly used baseline techniques for policy gradient methods. Our algorithm outperforms standard likelihood ratio policy gradient algorithms on several testbeds.</p>
<p>【Keywords】:</p>
<h3 id="112. Linear Complementarity for Regularized Policy Evaluation and Improvement.">112. Linear Complementarity for Regularized Policy Evaluation and Improvement.</h3>
<p>【<a href="http://papers.nips.cc/paper/4086-linear-complementarity-for-regularized-policy-evaluation-and-improvement">Paper Link</a>】    【Pages】:1009-1017</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Johns:Jeffrey">Jeffrey Johns</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Painter=Wakefield:Christopher">Christopher Painter-Wakefield</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Parr:Ronald">Ronald Parr</a></p>
<p>【Abstract】:
Recent work in reinforcement learning has emphasized the power of L1 regularization to perform feature selection and prevent overfitting. We propose formulating the L1 regularized linear fixed point problem as a linear complementarity problem (LCP). This formulation offers several advantages over the LARS-inspired formulation, LARS-TD. The LCP formulation allows the use of efficient off-the-shelf solvers, leads to a new uniqueness result, and can be initialized with starting points from similar problems (warm starts). We demonstrate that warm starts, as well as the efficiency of LCP solvers, can speed up policy iteration. Moreover, warm starts permit a form of modified policy iteration that can be used to approximate a greedy" homotopy path, a generalization of the LARS-TD homotopy path that combines policy evaluation and optimization."</p>
<p>【Keywords】:</p>
<h3 id="113. Synergies in learning words and their referents.">113. Synergies in learning words and their referents.</h3>
<p>【<a href="http://papers.nips.cc/paper/3946-synergies-in-learning-words-and-their-referents">Paper Link</a>】    【Pages】:1018-1026</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Johnson:Mark">Mark Johnson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Demuth:Katherine">Katherine Demuth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Frank:Michael_C=">Michael C. Frank</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jones:Bevan_K=">Bevan K. Jones</a></p>
<p>【Abstract】:
This paper presents Bayesian non-parametric models that simultaneously learn to segment words from phoneme strings and learn the referents of some of those words, and shows that there is a synergistic interaction in the acquisition of these two kinds of linguistic information. The models themselves are novel kinds of Adaptor Grammars that are an extension of an embedding of topic models into PCFGs. These models simultaneously segment phoneme sequences into words and learn the relationship between non-linguistic objects to the words that refer to them. We show (i) that modelling inter-word dependencies not only improves the accuracy of the word segmentation but also of word-object relationships, and (ii) that a model that simultaneously learns word-object relationships and word segmentation segments more accurately than one that just learns word segmentation on its own. We argue that these results support an interactive view of language acquisition that can take advantage of synergies such as these.</p>
<p>【Keywords】:</p>
<h3 id="114. Structural epitome: a way to summarize one's visual experience.">114. Structural epitome: a way to summarize one's visual experience.</h3>
<p>【<a href="http://papers.nips.cc/paper/4092-structural-epitome-a-way-to-summarize-ones-visual-experience">Paper Link</a>】    【Pages】:1027-1035</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jojic:Nebojsa">Nebojsa Jojic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perina:Alessandro">Alessandro Perina</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Murino:Vittorio">Vittorio Murino</a></p>
<p>【Abstract】:
In order to study the properties of total visual input in humans, a single subject wore a camera for two weeks capturing, on average, an image every 20 seconds (www.research.microsoft.com/~jojic/aihs). The resulting new dataset contains a mix of indoor and outdoor scenes as well as numerous foreground objects. Our first analysis goal is to create a visual summary of the subject’s two weeks of life using unsupervised algorithms that would automatically discover recurrent scenes, familiar faces or common actions. Direct application of existing algorithms, such as panoramic stitching (e.g.  Photosynth) or appearance-based clustering models (e.g. the epitome), is impractical due to either the large dataset size or the dramatic variation in the lighting conditions.    As a remedy to these problems, we introduce a novel image representation, the “stel epitome,” and an associated efficient learning algorithm. In our model, each image or image patch is characterized by a hidden mapping T, which, as in previous epitome models, defines a mapping between the image-coordinates and the coordinates in the large all-I-have-seen" epitome matrix. The limited epitome real-estate forces the mappings of different images to overlap, with this overlap indicating image similarity. However, in our model the image similarity does not depend on direct pixel-to-pixel intensity/color/feature comparisons as in previous epitome models, but on spatial configuration of scene or object parts, as the model is based on the palette-invariant stel models. As a result, stel epitomes capture structure that is invariant to non-structural changes, such as illumination, that tend to uniformly affect pixels belonging to a single scene or object part."</p>
<p>【Keywords】:</p>
<h3 id="115. Probabilistic Belief Revision with Structural Constraints.">115. Probabilistic Belief Revision with Structural Constraints.</h3>
<p>【<a href="http://papers.nips.cc/paper/4053-probabilistic-belief-revision-with-structural-constraints">Paper Link</a>】    【Pages】:1036-1044</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jones:Peter">Peter Jones</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saligrama:Venkatesh">Venkatesh Saligrama</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitter:Sanjoy_K=">Sanjoy K. Mitter</a></p>
<p>【Abstract】:
Experts (human or computer) are often required to assess the probability of uncertain events. When a collection of experts independently assess events that are structurally interrelated, the resulting assessment may violate fundamental laws of probability. Such an assessment is termed incoherent. In this work we investigate how the problem of incoherence may be affected by allowing experts to specify likelihood models and then update their assessments based on the realization of a globally-observable random sequence.</p>
<p>【Keywords】:</p>
<h3 id="116. Efficient Optimization for Discriminative Latent Class Models.">116. Efficient Optimization for Discriminative Latent Class Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3907-efficient-optimization-for-discriminative-latent-class-models">Paper Link</a>】    【Pages】:1045-1053</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Joulin:Armand">Armand Joulin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Francis_R=">Francis R. Bach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Ponce:Jean">Jean Ponce</a></p>
<p>【Abstract】:
Dimensionality reduction is commonly used in the setting of multi-label supervised classification to control the learning capacity and to provide a meaningful representation of the data. We introduce a simple forward probabilistic model which is a multinomial extension of reduced rank regression; we show that this model provides a probabilistic interpretation of discriminative clustering methods with added benefits in terms of number of hyperparameters and optimization. While expectation-maximization (EM) algorithm is commonly used to learn these models, its optimization usually leads to local minimum because it relies on a non-convex cost function with many such local minima. To avoid this problem, we introduce a local approximation of this cost function, which leads to a quadratic non-convex optimization problem over a product of simplices. In order to minimize such functions, we propose an efficient algorithm based on convex relaxation and low-rank representation of our data, which allows to deal with large instances. Experiments on text document classification show that the new model outperforms other supervised dimensionality reduction methods, while simulations on unsupervised clustering show that our probabilistic formulation has better properties than existing discriminative clustering methods.</p>
<p>【Keywords】:</p>
<h3 id="117. Non-Stochastic Bandit Slate Problems.">117. Non-Stochastic Bandit Slate Problems.</h3>
<p>【<a href="http://papers.nips.cc/paper/3962-non-stochastic-bandit-slate-problems">Paper Link</a>】    【Pages】:1054-1062</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kale:Satyen">Satyen Kale</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Reyzin:Lev">Lev Reyzin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schapire:Robert_E=">Robert E. Schapire</a></p>
<p>【Abstract】:
We consider bandit problems, motivated by applications in online advertising and news story selection, in which the learner must repeatedly select a slate, that is, a subset of size s from K possible actions, and then receives rewards for just the selected actions. The goal is to minimize the regret with respect to total reward of the best slate computed in hindsight. We consider unordered and ordered versions of the problem, and give efficient algorithms which have regret O(sqrt(T)), where the constant depends on the specific nature of the problem. We also consider versions of the problem where we have access to a number of policies which make recommendations for slates in every round, and give algorithms with O(sqrt(T)) regret for competing with the best such policy as well. We make use of the technique of relative entropy projections combined with the usual multiplicative weight update algorithm to obtain our algorithms.</p>
<p>【Keywords】:</p>
<h3 id="118. Static Analysis of Binary Executables Using Structural SVMs.">118. Static Analysis of Binary Executables Using Structural SVMs.</h3>
<p>【<a href="http://papers.nips.cc/paper/3925-static-analysis-of-binary-executables-using-structural-svms">Paper Link</a>】    【Pages】:1063-1071</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Karampatziakis:Nikos">Nikos Karampatziakis</a></p>
<p>【Abstract】:
We cast the problem of identifying basic blocks of code in a binary executable as learning a mapping from a byte sequence to a segmentation of the sequence. In general, inference in segmentation models, such as semi-CRFs, can be cubic in the length of the sequence. By taking advantage of the structure of our problem, we derive a linear-time inference algorithm which makes our approach practical, given that even small programs are tens or hundreds of thousands bytes long. Furthermore, we introduce two loss functions which are appropriate for our problem and show how to use structural SVMs to optimize the learned mapping for these losses. Finally, we present experimental results that demonstrate the advantages of our method against a strong baseline.</p>
<p>【Keywords】:</p>
<h3 id="119. Using body-anchored priors for identifying actions in single images.">119. Using body-anchored priors for identifying actions in single images.</h3>
<p>【<a href="http://papers.nips.cc/paper/4012-using-body-anchored-priors-for-identifying-actions-in-single-images">Paper Link</a>】    【Pages】:1072-1080</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Karlinsky:Leonid">Leonid Karlinsky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dinerstein:Michael">Michael Dinerstein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ullman:Shimon">Shimon Ullman</a></p>
<p>【Abstract】:
This paper presents an approach to the visual recognition of human actions using only single images as input. The task is easy for humans but difficult for current approaches to object recognition, because action instances may be similar in terms of body pose, and often require detailed examination of relations between participating objects and body parts in order to be recognized. The proposed approach applies a two-stage interpretation procedure to each training and test image. The first stage produces accurate detection of the relevant body parts of the actor, forming a prior for the local evidence needed to be considered for identifying the action. The second stage extracts features that are ‘anchored’ to the detected body parts, and uses these features and their feature-to-part relations in order to recognize the action. The body anchored priors we propose apply to a large range of human actions. These priors allow focusing on the relevant regions and relations, thereby significantly simplifying the learning process and increasing recognition performance.</p>
<p>【Keywords】:</p>
<h3 id="120. Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks.">120. Effects of Synaptic Weight Diffusion on Learning in Decision Making Networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4029-effects-of-synaptic-weight-diffusion-on-learning-in-decision-making-networks">Paper Link</a>】    【Pages】:1081-1089</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Katahira:Kentaro">Kentaro Katahira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Okanoya:Kazuo">Kazuo Okanoya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Okada:Masato">Masato Okada</a></p>
<p>【Abstract】:
When animals repeatedly choose actions from multiple alternatives, they can allocate their choices stochastically depending on past actions and outcomes. It is commonly assumed that this ability is achieved by modifications in synaptic weights related to decision making. Choice behavior has been empirically found to follow Herrnstein’s matching law. Loewenstein &amp; Seung (2006) demonstrated that matching behavior is a steady state of learning in neural networks if the synaptic weights change proportionally to the covariance between reward and neural activities. However, their proof did not take into account the change in entire synaptic distributions. In this study, we show that matching behavior is not necessarily a steady state of the covariance-based learning rule when the synaptic strength is sufficiently strong so that the fluctuations in input from individual sensory neurons influence the net input to output neurons. This is caused by the increasing variance in the input potential due to the diffusion of synaptic weights. This effect causes an undermatching phenomenon, which has been observed in many behavioral experiments. We suggest that the synaptic diffusion effects provide a robust neural mechanism for stochastic choice behavior.</p>
<p>【Keywords】:</p>
<h3 id="121. Learning Convolutional Feature Hierarchies for Visual Recognition.">121. Learning Convolutional Feature Hierarchies for Visual Recognition.</h3>
<p>【<a href="http://papers.nips.cc/paper/4133-learning-convolutional-feature-hierarchies-for-visual-recognition">Paper Link</a>】    【Pages】:1090-1098</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kavukcuoglu:Koray">Koray Kavukcuoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sermanet:Pierre">Pierre Sermanet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boureau:Y=Lan">Y-Lan Boureau</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gregor:Karol">Karol Gregor</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mathieu:Micha=euml=l">Michaël Mathieu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/LeCun:Yann">Yann LeCun</a></p>
<p>【Abstract】:
We propose an unsupervised method for learning multi-stage   hierarchies of sparse convolutional features. While sparse coding   has become an increasingly popular method for learning visual   features, it is most often trained at the patch level. Applying the   resulting filters convolutionally results in highly redundant codes   because overlapping patches are encoded in isolation.  By training   convolutionally over large image windows, our method reduces the   redudancy between feature vectors at neighboring locations and   improves the efficiency of the overall representation. In addition   to a linear decoder that reconstructs the image from sparse   features, our method trains an efficient feed-forward encoder that   predicts quasi-sparse features from the input.  While patch-based   training rarely produces anything but oriented edge detectors, we   show that convolutional training produces highly diverse filters,   including center-surround filters, corner detectors, cross   detectors, and oriented grating detectors.  We show that using these   filters in multi-stage convolutional network architecture improves   performance on a number of visual recognition and detection tasks.</p>
<p>【Keywords】:</p>
<h3 id="122. Accounting for network effects in neuronal responses using L1 regularized point process models.">122. Accounting for network effects in neuronal responses using L1 regularized point process models.</h3>
<p>【<a href="http://papers.nips.cc/paper/4050-accounting-for-network-effects-in-neuronal-responses-using-l1-regularized-point-process-models">Paper Link</a>】    【Pages】:1099-1107</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kelly:Ryan_C=">Ryan C. Kelly</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smith:Matthew_A=">Matthew A. Smith</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kass:Robert_E=">Robert E. Kass</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Tai_Sing">Tai Sing Lee</a></p>
<p>【Abstract】:
Activity of a neuron, even in the early sensory areas, is not simply a function of its local receptive field or tuning properties, but depends on global context of the stimulus, as well as the neural context. This suggests the activity of the surrounding neurons and global brain states can exert considerable influence on the activity of a neuron. In this paper we implemented an L1 regularized point process model to assess the contribution of multiple factors to the firing rate of many individual units recorded simultaneously from V1 with a 96-electrode Utah" array. We found that the spikes of surrounding neurons indeed provide strong predictions of a neuron's response, in addition to the neuron's receptive field transfer function. We also found that the same spikes could be accounted for with the local field potentials, a surrogate measure of global network states. This work shows that accounting for network fluctuations can improve estimates of single trial firing rate and stimulus-response transfer functions."</p>
<p>【Keywords】:</p>
<h3 id="123. Variational bounds for mixed-data factor analysis.">123. Variational bounds for mixed-data factor analysis.</h3>
<p>【<a href="http://papers.nips.cc/paper/3947-variational-bounds-for-mixed-data-factor-analysis">Paper Link</a>】    【Pages】:1108-1116</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Khan:Mohammad_Emtiyaz">Mohammad Emtiyaz Khan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Marlin:Benjamin_M=">Benjamin M. Marlin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bouchard:Guillaume">Guillaume Bouchard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Murphy:Kevin_P=">Kevin P. Murphy</a></p>
<p>【Abstract】:
We propose a new variational EM algorithm for fitting factor analysis models with mixed continuous and categorical observations. The algorithm is based on a simple quadratic bound to the log-sum-exp function.  In the special case of fully observed binary data, the bound we propose is significantly faster than previous variational methods. We show that EM is significantly more robust in the presence of missing data compared to treating the latent factors as parameters, which is the approach used by exponential family PCA and other related matrix-factorization methods.  A further benefit of the variational approach is that it can easily be extended to the case of mixtures of factor analyzers, as we show. We present results on synthetic and real data sets demonstrating several desirable properties of our proposed method.</p>
<p>【Keywords】:</p>
<h3 id="124. Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.">124. Sparse Coding for Learning Interpretable Spatio-Temporal Primitives.</h3>
<p>【<a href="http://papers.nips.cc/paper/3930-sparse-coding-for-learning-interpretable-spatio-temporal-primitives">Paper Link</a>】    【Pages】:1117-1125</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim_0003:Taehwan">Taehwan Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shakhnarovich:Gregory">Gregory Shakhnarovich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Urtasun:Raquel">Raquel Urtasun</a></p>
<p>【Abstract】:
Sparse coding has recently become a popular approach in computer vision to learn dictionaries of natural images. In this paper we extend sparse coding to learn interpretable spatio-temporal primitives of human motion.  We cast the problem of learning spatio-temporal primitives as a tensor factorization problem  and introduce constraints to learn interpretable primitives. In particular, we use group norms over those tensors, diagonal constraints on the activations as well as smoothness constraints that are inherent to human motion.   We demonstrate the effectiveness of our approach to learn interpretable representations  of human motion from motion capture data, and show that our approach outperforms  recently developed matching pursuit and  sparse coding algorithms.</p>
<p>【Keywords】:</p>
<h3 id="125. Regularized estimation of image statistics by Score Matching.">125. Regularized estimation of image statistics by Score Matching.</h3>
<p>【<a href="http://papers.nips.cc/paper/4060-regularized-estimation-of-image-statistics-by-score-matching">Paper Link</a>】    【Pages】:1126-1134</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kingma:Diederik_P=">Diederik P. Kingma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/LeCun:Yann">Yann LeCun</a></p>
<p>【Abstract】:
Score Matching is a recently-proposed criterion for training high-dimensional density models for which maximum likelihood training is intractable. It has been applied to learning natural image statistics but has so-far been limited to simple models due to the difficulty of differentiating the loss with respect to the model parameters. We show how this differentiation can be automated with an extended version of the double-backpropagation algorithm. In addition, we introduce a regularization term for the Score Matching loss that enables its use for a broader range of problem by suppressing instabilities that occur with finite training sample sizes and quantized input values. Results are reported for image denoising and super-resolution.</p>
<p>【Keywords】:</p>
<h3 id="126. Random Conic Pursuit for Semidefinite Programming.">126. Random Conic Pursuit for Semidefinite Programming.</h3>
<p>【<a href="http://papers.nips.cc/paper/4026-random-conic-pursuit-for-semidefinite-programming">Paper Link</a>】    【Pages】:1135-1143</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kleiner:Ariel">Ariel Kleiner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rahimi:Ali">Ali Rahimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jordan:Michael_I=">Michael I. Jordan</a></p>
<p>【Abstract】:
We present a novel algorithm, Random Conic Pursuit, that solves semidefinite programs (SDPs) via repeated optimization over randomly selected two-dimensional subcones of the PSD cone. This scheme is simple, easily implemented, applicable to very general SDPs, scalable, and theoretically interesting. Its advantages are realized at the expense of an ability to readily compute highly exact solutions, though useful approximate solutions are easily obtained. This property renders Random Conic Pursuit of particular interest for machine learning applications, in which the relevant SDPs are generally based upon random data and so exact minima are often not a priority. Indeed, we present empirical results to this effect for various SDPs encountered in machine learning; these experiments demonstrate the potential practical usefulness of Random Conic Pursuit. We also provide a preliminary analysis that yields insight into the theoretical properties and convergence of the algorithm.</p>
<p>【Keywords】:</p>
<h3 id="127. Generalized roof duality and bisubmodular functions.">127. Generalized roof duality and bisubmodular functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4021-generalized-roof-duality-and-bisubmodular-functions">Paper Link</a>】    【Pages】:1144-1152</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kolmogorov:Vladimir">Vladimir Kolmogorov</a></p>
<p>【Abstract】:
Consider a convex relaxation $\hat f$ of a pseudo-boolean function $f$. We say that the relaxation is {\em totally half-integral} if $\hat f(\bx)$ is a polyhedral function with half-integral extreme points $\bx$, and this property is preserved after adding an arbitrary combination of constraints of the form $x_i=x_j$, $x_i=1-x_j$, and $x_i=\gamma$ where $\gamma\in{0,1,\frac{1}{2}}$ is a constant. A well-known example is the {\em roof duality} relaxation for quadratic pseudo-boolean functions $f$. We argue that total half-integrality is a natural requirement for generalizations of roof duality to arbitrary pseudo-boolean functions.  Our contributions are as follows. First, we provide a complete characterization of totally half-integral relaxations $\hat f$ by establishing a one-to-one correspondence with {\em bisubmodular functions}. Second, we give a new characterization of bisubmodular functions. Finally, we show some relationships between general totally half-integral relaxations and relaxations based on the roof duality.</p>
<p>【Keywords】:</p>
<h3 id="128. Energy Disaggregation via Discriminative Sparse Coding.">128. Energy Disaggregation via Discriminative Sparse Coding.</h3>
<p>【<a href="http://papers.nips.cc/paper/4054-energy-disaggregation-via-discriminative-sparse-coding">Paper Link</a>】    【Pages】:1153-1161</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kolter:J=_Zico">J. Zico Kolter</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Batra:Siddharth">Siddharth Batra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Andrew_Y=">Andrew Y. Ng</a></p>
<p>【Abstract】:
Energy disaggregation is the task of taking a whole-home energy signal and separating it into its component appliances. Studies have shown that having device-level energy information can cause users to conserve significant amounts of energy, but current electricity meters only report whole-home data. Thus, developing algorithmic methods for disaggregation presents a key technical challenge in the effort to maximize energy conservation. In this paper, we examine a large scale energy disaggregation task, and apply a novel extension of sparse coding to this problem. In particular, we develop a method, based upon structured prediction, for discriminatively training sparse coding algorithms specifically to maximize disaggregation performance. We show that this significantly improves the performance of sparse coding algorithms on the energy task and illustrate how these disaggregation results can provide useful information about energy usage.</p>
<p>【Keywords】:</p>
<h3 id="129. Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories.">129. Constructing Skill Trees for Reinforcement Learning Agents from Demonstration Trajectories.</h3>
<p>【<a href="http://papers.nips.cc/paper/3903-constructing-skill-trees-for-reinforcement-learning-agents-from-demonstration-trajectories">Paper Link</a>】    【Pages】:1162-1170</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Konidaris:George">George Konidaris</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kuindersma:Scott">Scott Kuindersma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Barto:Andrew_G=">Andrew G. Barto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grupen:Roderic_A=">Roderic A. Grupen</a></p>
<p>【Abstract】:
We introduce CST, an algorithm for constructing skill trees from demonstration trajectories in continuous reinforcement learning domains. CST uses a changepoint detection method to segment each trajectory into a skill chain by detecting a change of appropriate abstraction, or that a segment is too complex to model as a single skill. The skill chains from each trajectory are then merged to form a skill tree. We demonstrate that CST constructs an appropriate skill tree that can be further refined through learning in a challenging continuous domain, and that it can be used to segment demonstration trajectories on a mobile manipulator into chains of skills where each skill is assigned an appropriate abstraction.</p>
<p>【Keywords】:</p>
<h3 id="130. Structured Determinantal Point Processes.">130. Structured Determinantal Point Processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/3969-structured-determinantal-point-processes">Paper Link</a>】    【Pages】:1171-1179</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kulesza:Alex">Alex Kulesza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Taskar:Ben">Ben Taskar</a></p>
<p>【Abstract】:
We present a novel probabilistic model for distributions over sets of structures -- for example, sets of sequences, trees, or graphs. The critical characteristic of our model is a preference for diversity: sets containing dissimilar structures are more likely. Our model is a marriage of structured probabilistic models, like Markov random fields and context free grammars, with determinantal point processes, which arise in quantum physics as models of particles with repulsive interactions. We extend the determinantal point process model to handle an exponentially-sized set of particles (structures) via a natural factorization of the model into parts. We show how this factorization leads to tractable algorithms for exact inference, including computing marginals, computing conditional probabilities, and sampling. Our algorithms exploit a novel polynomially-sized dual representation of determinantal point processes, and use message passing over a special semiring to compute relevant quantities. We illustrate the advantages of the model on tracking and articulated pose estimation problems.</p>
<p>【Keywords】:</p>
<h3 id="131. MAP Estimation for Graphical Models by Likelihood Maximization.">131. MAP Estimation for Graphical Models by Likelihood Maximization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4165-map-estimation-for-graphical-models-by-likelihood-maximization">Paper Link</a>】    【Pages】:1180-1188</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:Akshat">Akshat Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zilberstein:Shlomo">Shlomo Zilberstein</a></p>
<p>【Abstract】:
Computing a {\em maximum a posteriori} (MAP) assignment in graphical models is a crucial inference problem for many practical applications. Several provably convergent approaches have been successfully developed using linear programming (LP) relaxation of the MAP problem. We present an alternative approach, which transforms the MAP problem into that of inference in a finite mixture of simple Bayes nets. We then derive the Expectation Maximization (EM) algorithm for this mixture that also monotonically increases a lower bound on the MAP assignment until convergence. The update equations for the EM algorithm are remarkably simple, both conceptually and computationally, and can be implemented using a graph-based message passing paradigm similar to max-product computation. We experiment on the real-world protein design dataset and show that EM's convergence rate is significantly higher than the previous LP relaxation based approach MPLP. EM achieves a solution quality within $95$\% of optimal for most instances and is often an order-of-magnitude faster than MPLP.</p>
<p>【Keywords】:</p>
<h3 id="132. Self-Paced Learning for Latent Variable Models.">132. Self-Paced Learning for Latent Variable Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3923-self-paced-learning-for-latent-variable-models">Paper Link</a>】    【Pages】:1189-1197</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:M=_Pawan">M. Pawan Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Packer:Benjamin">Benjamin Packer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koller:Daphne">Daphne Koller</a></p>
<p>【Abstract】:
Latent variable models are a powerful tool for addressing several tasks in machine learning. However, the algorithms for learning the parameters of latent variable models are prone to getting stuck in a bad local optimum. To alleviate this problem, we build on the intuition that, rather than considering all samples simultaneously, the algorithm should be presented with the training data in a meaningful order that facilitates learning. The order of the samples is determined by how easy they are. The main challenge is that often we are not provided with a readily computable measure of the easiness of samples. We address this issue by  proposing a novel, iterative self-paced learning algorithm where each iteration simultaneously selects easy samples and learns a new parameter vector. The number of samples selected is governed by a weight that is annealed until the entire training data has been considered. We empirically demonstrate that the self-paced learning algorithm outperforms the state of the art method for learning a latent structural SVM on four applications: object localization, noun phrase coreference, motif finding and handwritten digit recognition.</p>
<p>【Keywords】:</p>
<h3 id="133. Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions.">133. Efficient algorithms for learning kernels from multiple similarity matrices with general convex loss functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4131-efficient-algorithms-for-learning-kernels-from-multiple-similarity-matrices-with-general-convex-loss-functions">Paper Link</a>】    【Pages】:1198-1206</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kundu:Achintya">Achintya Kundu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tankasali:Vikram">Vikram Tankasali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Chiranjib">Chiranjib Bhattacharyya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Ben=Tal:Aharon">Aharon Ben-Tal</a></p>
<p>【Abstract】:
In this paper we consider the problem of learning an n x n Kernel matrix from m similarity matrices under general convex loss. Past research have extensively studied the m =1 case and have derived several algorithms which require sophisticated techniques like ACCP, SOCP, etc. The existing algorithms do not apply if one uses arbitrary losses and often can not handle m &gt; 1 case. We present several provably convergent iterative algorithms, where each iteration requires either an SVM or a Multiple Kernel Learning (MKL) solver for m &gt; 1 case. One of the major contributions of the paper is to extend the well known Mirror Descent(MD) framework to handle Cartesian product of psd matrices. This novel extension leads to an algorithm, called EMKL, which solves the problem in O(m^2 log n) iterations; in each iteration one solves an MKL involving m kernels and m eigen-decomposition of n x n matrices. By suitably defining a restriction on the objective function, a faster version of EMKL is proposed, called REKL, which avoids the eigen-decomposition. An alternative to both EMKL and REKL is also suggested which requires only an SVM solver. Experimental results on real world protein data set involving several similarity matrices illustrate the efficacy of the proposed algorithms.</p>
<p>【Keywords】:</p>
<h3 id="134. Beyond Actions: Discriminative Models for Contextual Group Activities.">134. Beyond Actions: Discriminative Models for Contextual Group Activities.</h3>
<p>【<a href="http://papers.nips.cc/paper/4158-beyond-actions-discriminative-models-for-contextual-group-activities">Paper Link</a>】    【Pages】:1216-1224</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lan:Tian">Tian Lan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0003:Yang">Yang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Weilong">Weilong Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mori:Greg">Greg Mori</a></p>
<p>【Abstract】:
We propose a discriminative model for recognizing group activities. Our model jointly captures the group activity, the individual person actions, and the interactions among them. Two new types of contextual information, group-person interaction and person-person interaction, are explored in a latent variable framework. Different from most of the previous latent structured models which assume a predefined structure for the hidden layer, e.g. a tree structure, we treat the structure of the hidden layer as a latent variable and implicitly infer it during learning and inference. Our experimental results demonstrate that by inferring this contextual information together with adaptive structures, the proposed model can significantly improve activity recognition performance.</p>
<p>【Keywords】:</p>
<h3 id="135. Functional Geometry Alignment and Localization of Brain Areas.">135. Functional Geometry Alignment and Localization of Brain Areas.</h3>
<p>【<a href="http://papers.nips.cc/paper/4059-functional-geometry-alignment-and-localization-of-brain-areas">Paper Link</a>】    【Pages】:1225-1233</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Langs:Georg">Georg Langs</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tie:Yanmei">Yanmei Tie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rigolo:Laura">Laura Rigolo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Golby:Alexandra_J=">Alexandra J. Golby</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Golland:Polina">Polina Golland</a></p>
<p>【Abstract】:
Matching functional brain regions across individuals is a challenging task, largely due to the variability in their location and extent. It is particularly difficult, but highly relevant, for patients with pathologies such as brain tumors, which can cause substantial reorganization of functional systems. In such cases spatial registration based on anatomical data is only of limited value if the goal is to establish correspondences of functional areas among different individuals, or to localize potentially displaced active regions. Rather than rely on spatial alignment, we propose to perform registration in an alternative space whose geometry is governed by the functional interaction patterns in the brain. We first embed each brain into a functional map that reflects connectivity patterns during a fMRI experiment. The resulting functional maps are then registered, and the obtained correspondences are propagated back to the two brains. In application to a language fMRI experiment, our preliminary results suggest that the proposed method yields improved functional correspondences across subjects. This advantage is pronounced for subjects with tumors that affect the language areas and thus cause spatial reorganization of the functional regions.</p>
<p>【Keywords】:</p>
<h3 id="136. Efficient Relational Learning with Hidden Variable Detection.">136. Efficient Relational Learning with Hidden Variable Detection.</h3>
<p>【<a href="http://papers.nips.cc/paper/4175-efficient-relational-learning-with-hidden-variable-detection">Paper Link</a>】    【Pages】:1234-1242</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lao:Ni">Ni Lao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jun">Jun Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xinwang">Xinwang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yandong">Yandong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a></p>
<p>【Abstract】:
Markov networks (MNs) can incorporate arbitrarily complex features in modeling relational data. However, this flexibility comes at a sharp price of training an exponentially complex model. To address this challenge, we propose a novel relational learning approach, which consists of a restricted class of relational MNs (RMNs) called relation tree-based RMN (treeRMN), and an efficient Hidden Variable Detection algorithm called Contrastive Variable Induction (CVI). On one hand, the restricted treeRMN only considers simple (e.g., unary and pairwise) features in relational data and thus achieves computational efficiency; and on the other hand, the CVI algorithm efficiently detects hidden variables which can capture long range dependencies. Therefore, the resultant approach is highly efficient yet does not sacrifice its expressive power. Empirical results on four real datasets show that the proposed relational learning method can achieve similar prediction quality as the state-of-the-art approaches, but is significantly more efficient in training; and the induced hidden variables are semantically meaningful and crucial to improve the training speed and prediction qualities of treeRMNs.</p>
<p>【Keywords】:</p>
<h3 id="137. Learning to combine foveal glimpses with a third-order Boltzmann machine.">137. Learning to combine foveal glimpses with a third-order Boltzmann machine.</h3>
<p>【<a href="http://papers.nips.cc/paper/4089-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine">Paper Link</a>】    【Pages】:1243-1251</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Larochelle:Hugo">Hugo Larochelle</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hinton:Geoffrey_E=">Geoffrey E. Hinton</a></p>
<p>【Abstract】:
We describe a model based on a Boltzmann machine with third-order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must decide on a sequence of fixations and it must combine the glimpse" at each fixation with the location of the fixation before integrating the information with information from other glimpses of the same object. We evaluate this model on a synthetic dataset and two image classification datasets, showing that it can perform at least as well as a model trained on whole images."</p>
<p>【Keywords】:</p>
<h3 id="138. Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations.">138. Categories and Functional Units: An Infinite Hierarchical Model for Brain Activations.</h3>
<p>【<a href="http://papers.nips.cc/paper/3912-categories-and-functional-units-an-infinite-hierarchical-model-for-brain-activations">Paper Link</a>】    【Pages】:1252-1260</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lashkari:Danial">Danial Lashkari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sridharan:Ramesh">Ramesh Sridharan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Golland:Polina">Polina Golland</a></p>
<p>【Abstract】:
We present a model that describes the structure in the responses of different brain areas to a set of stimuli in terms of stimulus categories" (clusters of stimuli) and "functional units" (clusters of voxels). We assume that voxels within a unit respond similarly to all stimuli from the same category, and design a nonparametric hierarchical model to capture inter-subject variability among the units. The model explicitly captures the relationship between brain activations and fMRI time courses. A variational inference algorithm derived based on the model can learn categories, units, and a set of unit-category activation probabilities from data. When applied to data from an fMRI study of object recognition, the method finds meaningful and consistent clusterings of stimuli into categories and voxels into units."</p>
<p>【Keywords】:</p>
<h3 id="139. Identifying Dendritic Processing.">139. Identifying Dendritic Processing.</h3>
<p>【<a href="http://papers.nips.cc/paper/4070-identifying-dendritic-processing">Paper Link</a>】    【Pages】:1261-1269</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lazar:Aurel_A=">Aurel A. Lazar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Slutskiy:Yevgeniy_B=">Yevgeniy B. Slutskiy</a></p>
<p>【Abstract】:
In system identification both the input and the output of a system are available to an observer and an algorithm is sought to identify parameters of a hypothesized model of that system. Here we present a novel formal methodology for identifying dendritic processing in a neural circuit consisting of a linear dendritic processing filter in cascade with a spiking neuron model. The input to the circuit is an analog signal that belongs to the space of bandlimited functions. The output is a time sequence associated with the spike train. We derive an algorithm for identification of the dendritic processing filter and reconstruct its kernel with arbitrary precision.</p>
<p>【Keywords】:</p>
<h3 id="140. Tiled convolutional neural networks.">140. Tiled convolutional neural networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4136-tiled-convolutional-neural-networks">Paper Link</a>】    【Pages】:1279-1287</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Le:Quoc_V=">Quoc V. Le</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ngiam:Jiquan">Jiquan Ngiam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhenghao">Zhenghao Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chia:Daniel_Jin_hao">Daniel Jin hao Chia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koh:Pang_Wei">Pang Wei Koh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Andrew_Y=">Andrew Y. Ng</a></p>
<p>【Abstract】:
Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. Using convolutional (tied) weights signiﬁcantly reduces the number of parameters that have to be learned, and also allows translational invariance to be hard-coded into the architecture. In this paper, we consider the problem of learning invariances, rather than relying on hard-coding. We propose tiled convolution neural networks (Tiled CNNs), which use a regular “tiled” pattern of tied weights that does not require that adjacent hidden units share identical weights, but instead requires only that hidden units k steps away from each other to have tied weights. By pooling over neighboring units, this architecture is able to learn complex invariances (such as scale and rotational invariance) beyond translational invariance. Further, it also enjoys much of CNNs’ advantage of having a relatively small number of learned parameters (such as ease of learning and greater scalability). We provide an efficient learning algorithm for Tiled CNNs based on Topographic ICA, and show that learning complex invariant features allows us to achieve highly competitive results for both the NORB and CIFAR-10 datasets.</p>
<p>【Keywords】:</p>
<h3 id="141. Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings.">141. Cross Species Expression Analysis using a Dirichlet Process Mixture Model with Latent Matchings.</h3>
<p>【<a href="http://papers.nips.cc/paper/4153-cross-species-expression-analysis-using-a-dirichlet-process-mixture-model-with-latent-matchings">Paper Link</a>】    【Pages】:1270-1278</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Le:Hai=Son">Hai-Son Le</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bar=Joseph:Ziv">Ziv Bar-Joseph</a></p>
<p>【Abstract】:
Recent studies compare gene expression data across species to identify core and species specific genes in biological systems. To perform such comparisons researchers need to match genes across species. This is a challenging task since the correct matches (orthologs) are not known for most genes. Previous work in this area used deterministic matchings or reduced multidimensional expression data to binary representation. Here we develop a new method that can utilize soft matches (given as priors) to infer both, unique and similar expression patterns across species and a matching for the genes in both species. Our method uses a Dirichlet process mixture model which includes a latent data matching variable. We present learning and inference algorithms based on variational methods for this model. Applying our method to immune response data we show that it can accurately identify common and unique response patterns by improving the matchings between human and mouse genes.</p>
<p>【Keywords】:</p>
<h3 id="142. Evaluation of Rarity of Fingerprints in Forensics.">142. Evaluation of Rarity of Fingerprints in Forensics.</h3>
<p>【<a href="http://papers.nips.cc/paper/3945-evaluation-of-rarity-of-fingerprints-in-forensics">Paper Link</a>】    【Pages】:1207-1215</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Su:Chang">Chang Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srihari:Sargur_N=">Sargur N. Srihari</a></p>
<p>【Abstract】:
A method for computing the rarity of latent fingerprints represented by minutiae is given. It allows determining the probability of finding a match for an evidence print in a database of n known prints. The probability of random correspondence between evidence and database is determined in three procedural steps. In the registration step the latent print is aligned by finding its core point; which is done using a procedure based on a machine learning approach based on Gaussian processes. In the evidence probability evaluation step a generative model based on Bayesian networks is used to determine the probability of the evidence; it takes into account both the dependency of each minutia on nearby minutiae and the confidence of their presence in the evidence. In the specific probability of random correspondence step the evidence probability is used to determine the probability of match among n for a given tolerance; the last evaluation is similar to the birthday correspondence probability for a specific birthday. The generative model is validated using a goodness-of-fit test evaluated with a standard database of fingerprints. The probability of random correspondence for several latent fingerprints are evaluated for varying numbers of minutiae.</p>
<p>【Keywords】:</p>
<h3 id="143. Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces.">143. Estimating Spatial Layout of Rooms using Volumetric Reasoning about Objects and Surfaces.</h3>
<p>【<a href="http://papers.nips.cc/paper/4120-estimating-spatial-layout-of-rooms-using-volumetric-reasoning-about-objects-and-surfaces">Paper Link</a>】    【Pages】:1288-1296</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:David_C=">David C. Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gupta:Abhinav">Abhinav Gupta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hebert:Martial">Martial Hebert</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kanade:Takeo">Takeo Kanade</a></p>
<p>【Abstract】:
There has been a recent push in extraction of 3D spatial layout of scenes. However, none of these approaches model the 3D interaction between objects and the spatial layout. In this paper, we argue for a parametric representation of objects in 3D, which allows us to incorporate volumetric constraints of the physical world. We show that augmenting current structured prediction techniques with volumetric reasoning significantly improves the performance of the state-of-the-art.</p>
<p>【Keywords】:</p>
<h3 id="144. Adaptive Multi-Task Lasso: with Application to eQTL Detection.">144. Adaptive Multi-Task Lasso: with Application to eQTL Detection.</h3>
<p>【<a href="http://papers.nips.cc/paper/4038-adaptive-multi-task-lasso-with-application-to-eqtl-detection">Paper Link</a>】    【Pages】:1306-1314</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Seunghak">Seunghak Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jun">Jun Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
To understand the relationship between genomic variations among population and complex diseases, it is essential to detect eQTLs which are associated with phenotypic effects. However, detecting eQTLs remains a challenge due to complex underlying mechanisms and the very large number of genetic loci involved compared to the number of samples. Thus, to address the problem, it is desirable to take advantage of the structure of the data and prior information about genomic locations such as conservation scores and transcription factor binding sites.  In this paper, we propose a novel regularized regression approach for detecting eQTLs which takes into account related traits simultaneously while incorporating many regulatory features. We first present a Bayesian network for a multi-task learning problem that includes priors on SNPs, making it possible to estimate the significance of each covariate adaptively. Then we find the maximum a posteriori (MAP) estimation of regression coefficients and estimate weights of covariates jointly. This optimization procedure is efficient since it can be achieved by using convex optimization and a coordinate descent procedure iteratively. Experimental results on simulated and real yeast datasets confirm that our model outperforms previous methods for finding eQTLs.</p>
<p>【Keywords】:</p>
<h3 id="145. Practical Large-Scale Optimization for Max-norm Regularization.">145. Practical Large-Scale Optimization for Max-norm Regularization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4124-practical-large-scale-optimization-for-max-norm-regularization">Paper Link</a>】    【Pages】:1297-1305</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Jason_D=">Jason D. Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Recht:Ben">Ben Recht</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srebro:Nathan">Nathan Srebro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tropp:Joel_A=">Joel A. Tropp</a></p>
<p>【Abstract】:
The max-norm was proposed as a convex matrix regularizer by Srebro et al (2004) and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro (2003) to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas.</p>
<p>【Keywords】:</p>
<h3 id="146. Joint Cascade Optimization Using A Product Of Boosted Classifiers.">146. Joint Cascade Optimization Using A Product Of Boosted Classifiers.</h3>
<p>【<a href="http://papers.nips.cc/paper/4148-joint-cascade-optimization-using-a-product-of-boosted-classifiers">Paper Link</a>】    【Pages】:1315-1323</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lefakis:Leonidas">Leonidas Lefakis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fleuret:Fran=ccedil=ois">François Fleuret</a></p>
<p>【Abstract】:
The standard strategy for efficient object detection consists of building a cascade composed of several binary classifiers. The detection process takes the form of a lazy evaluation of the conjunction of the responses of these classifiers, and concentrates the computation on difficult parts of the image which can not be trivially rejected.  We introduce a novel algorithm to construct jointly the classifiers of such a cascade. We interpret the response of a classifier as a probability of a positive prediction, and the overall response of the cascade as the probability that all the predictions are positive. From this noisy-AND model, we derive a consistent loss and a Boosting procedure to optimize that global probability on the training set.  Such a joint learning allows the individual predictors to focus on a more restricted modeling problem, and improves the performance compared to a standard cascade. We demonstrate the efficiency of this approach on face and pedestrian detection with standard data-sets and comparisons with reference baselines.</p>
<p>【Keywords】:</p>
<h3 id="147. Learning To Count Objects in Images.">147. Learning To Count Objects in Images.</h3>
<p>【<a href="http://papers.nips.cc/paper/4043-learning-to-count-objects-in-images">Paper Link</a>】    【Pages】:1324-1332</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lempitsky:Victor_S=">Victor S. Lempitsky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zisserman:Andrew">Andrew Zisserman</a></p>
<p>【Abstract】:
We propose a new supervised learning framework for visual object counting tasks, such as estimating the number of cells in a microscopic image or the number of humans in surveillance video frames. We focus on the practically-attractive case when the training images are annotated with dots (one dot per object).   Our goal is to accurately estimate the count. However, we evade the hard task of learning to detect and localize individual object instances. Instead, we cast the problem as that of estimating an image density whose integral over any image region gives the count of objects within that region. Learning to infer such density can be formulated as a minimization of a regularized risk quadratic cost function. We introduce a new loss function, which is well-suited for such learning, and at the same time can be computed efficiently via a maximum subarray algorithm. The learning can then be posed as a convex quadratic program solvable with cutting-plane optimization.  The proposed framework is very flexible as it can accept any domain-specific visual features. Once trained, our system provides accurate object counts and requires a very small time overhead over the feature extraction step, making it a good candidate for applications involving real-time processing or dealing with huge amount of visual data.</p>
<p>【Keywords】:</p>
<h3 id="148. Optimal Web-Scale Tiering as a Flow Problem.">148. Optimal Web-Scale Tiering as a Flow Problem.</h3>
<p>【<a href="http://papers.nips.cc/paper/3986-optimal-web-scale-tiering-as-a-flow-problem">Paper Link</a>】    【Pages】:1333-1341</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Leung:Gilbert">Gilbert Leung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Quadrianto:Novi">Novi Quadrianto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tsioutsiouliklis:Kostas">Kostas Tsioutsiouliklis</a></p>
<p>【Abstract】:
We present a fast online solver for large scale maximum-flow problems as they occur in portfolio optimization, inventory management, computer vision, and logistics. Our algorithm solves an integer linear program in an online fashion. It exploits total unimodularity of the constraint matrix and a Lagrangian relaxation to solve the problem as a convex online game. The algorithm generates approximate solutions of max-flow problems by performing stochastic gradient descent on a set of flows. We apply the algorithm to optimize tier arrangement of over 80 Million web pages on a layered set of caches to serve an incoming query stream optimally. We provide an empirical demonstration of the effectiveness of our method on real query-pages data.</p>
<p>【Keywords】:</p>
<h3 id="149. Feature Construction for Inverse Reinforcement Learning.">149. Feature Construction for Inverse Reinforcement Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3918-feature-construction-for-inverse-reinforcement-learning">Paper Link</a>】    【Pages】:1342-1350</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Levine:Sergey">Sergey Levine</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Popovic:Zoran">Zoran Popovic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koltun:Vladlen">Vladlen Koltun</a></p>
<p>【Abstract】:
The goal of inverse reinforcement learning is to find a reward function for a Markov decision process, given example traces from its optimal policy. Current IRL techniques generally rely on user-supplied features that form a concise basis for the reward. We present an algorithm that instead constructs reward features from a large collection of component features, by building logical conjunctions of those component features that are relevant to the example policy. Given example traces, the algorithm returns a reward function as well as the constructed features. The reward function can be used to recover a full, deterministic, stationary policy, and the features can be used to transplant the reward function into any novel environment on which the component features are well defined.</p>
<p>【Keywords】:</p>
<h3 id="150. Convex Multiple-Instance Learning by Estimating Likelihood Ratio.">150. Convex Multiple-Instance Learning by Estimating Likelihood Ratio.</h3>
<p>【<a href="http://papers.nips.cc/paper/3926-convex-multiple-instance-learning-by-estimating-likelihood-ratio">Paper Link</a>】    【Pages】:1360-1368</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Fuxin">Fuxin Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sminchisescu:Cristian">Cristian Sminchisescu</a></p>
<p>【Abstract】:
Multiple-Instance learning has been long known as a hard non-convex problem.
 In this work, we propose an approach that recasts it as a convex likelihood ratio
 estimation problem. Firstly, the constraint in multiple-instance learning is reformulated
 into a convex constraint on the likelihood ratio. Then we show that a joint
 estimation of a likelihood ratio function and the likelihood on training instances
 can be learned convexly. Theoretically, we prove a quantitative relationship between
 the risk estimated under the 0-1 classification loss, and under a loss function
 for likelihood ratio estimation. It is shown that our likelihood ratio estimation is
 generally a good surrogate for the 0-1 loss, and separates positive and negative
 instances well. However with the joint estimation it tends to underestimate the
 likelihood of an example to be positive. We propose to use these likelihood ratio
 estimates as features, and learn a linear combination on them to classify the bags.
 Experiments on synthetic and real datasets show the superiority of the approach.</p>
<p>【Keywords】:</p>
<h3 id="151. Object Bank: A High-Level Image Representation for Scene Classification & Semantic Feature Sparsification.">151. Object Bank: A High-Level Image Representation for Scene Classification &amp; Semantic Feature Sparsification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4008-object-bank-a-high-level-image-representation-for-scene-classification-semantic-feature-sparsification">Paper Link</a>】    【Pages】:1378-1386</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Li=Jia">Li-Jia Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Hao">Hao Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Fei=Fei">Fei-Fei Li</a></p>
<p>【Abstract】:
Robust low-level image features have been proven to be effective representations for a variety of visual recognition tasks such as object recognition and scene classification; but pixels, or even local image patches, carry little semantic meanings. For high level visual tasks, such low-level image representations are potentially not enough. In this paper, we propose a high-level image representation, called the Object Bank, where an image is represented as a scale invariant response map of a large number of pre-trained generic object detectors, blind to the testing dataset or visual task. Leveraging on the Object Bank representation, superior performances on high level visual recognition tasks can be achieved with simple off-the-shelf classifiers such as logistic regression and linear SVM. Sparsity algorithms make our representation more efficient and scalable for large scene datasets, and reveal semantically meaningful feature patterns.</p>
<p>【Keywords】:</p>
<h3 id="152. Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles.">152. Individualized ROI Optimization via Maximization of Group-wise Consistency of Structural and Functional Profiles.</h3>
<p>【<a href="http://papers.nips.cc/paper/4016-individualized-roi-optimization-via-maximization-of-group-wise-consistency-of-structural-and-functional-profiles">Paper Link</a>】    【Pages】:1369-1377</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Kaiming">Kaiming Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo_0002:Lei">Lei Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faraco:Carlos">Carlos Faraco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Dajiang">Dajiang Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Fan">Fan Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Tuo">Tuo Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Xi">Xi Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Degang">Degang Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Hanbo">Hanbo Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Xintao">Xintao Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Miller:L=_Stephen">L. Stephen Miller</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tianming">Tianming Liu</a></p>
<p>【Abstract】:
Functional segregation and integration are fundamental characteristics of the human brain. Studying the connectivity among segregated regions and the dynamics of integrated brain networks has drawn increasing interest. A very controversial, yet fundamental issue in these studies is how to determine the best functional brain regions or ROIs (regions of interests) for individuals. Essentially, the computed connectivity patterns and dynamics of brain networks are very sensitive to the locations, sizes, and shapes of the ROIs. This paper presents a novel methodology to optimize the locations of an individual's ROIs in the working memory system. Our strategy is to formulate the individual ROI optimization as a group variance minimization problem, in which group-wise functional and structural connectivity patterns, and anatomic profiles are defined as optimization constraints. The optimization problem is solved via the simulated annealing approach. Our experimental results show that the optimized ROIs have significantly improved consistency in structural and functional profiles across subjects, and have more reasonable localizations and more consistent morphological and anatomic profiles.</p>
<p>【Keywords】:</p>
<h3 id="153. Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models.">153. Towards Holistic Scene Understanding: Feedback Enabled Cascaded Classification Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/4003-towards-holistic-scene-understanding-feedback-enabled-cascaded-classification-models">Paper Link</a>】    【Pages】:1351-1359</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Congcong">Congcong Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kowdle:Adarsh">Adarsh Kowdle</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saxena:Ashutosh">Ashutosh Saxena</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Tsuhan">Tsuhan Chen</a></p>
<p>【Abstract】:
In many machine learning domains (such as scene understanding), several related sub-tasks (such as scene categorization, depth estimation, object detection) operate on the same raw data and provide correlated outputs. Each of these tasks is often notoriously hard, and state-of-the-art classifiers already exist for many sub-tasks. It is desirable to have an algorithm that can capture such correlation without requiring to make any changes to the inner workings of any classifier.  We propose Feedback Enabled Cascaded Classification Models (FE-CCM), that maximizes the joint likelihood of the sub-tasks, while requiring only a ‘black-box’ interface to the original classifier for each sub-task. We use a two-layer cascade of classifiers, which are repeated instantiations of the original ones, with the output of the first layer fed into the second layer as input. Our training method involves a feedback step that allows later classifiers to provide earlier classifiers information about what error modes to focus on. We show that our method significantly improves performance in all the sub-tasks in two different domains: (i) scene understanding, where we consider depth estimation, scene categorization, event categorization, object detection, geometric labeling and saliency detection, and (ii) robotic grasping, where we consider grasp point detection and object classification.</p>
<p>【Keywords】:</p>
<h3 id="154. b-Bit Minwise Hashing for Estimating Three-Way Similarities.">154. b-Bit Minwise Hashing for Estimating Three-Way Similarities.</h3>
<p>【<a href="http://papers.nips.cc/paper/4119-b-bit-minwise-hashing-for-estimating-three-way-similarities">Paper Link</a>】    【Pages】:1387-1395</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li_0001:Ping">Ping Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/K=ouml=nig:Arnd_Christian">Arnd Christian König</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gui:Wenhao">Wenhao Gui</a></p>
<p>【Abstract】:
Computing two-way and multi-way set similarities is a fundamental problem. This study focuses on estimating 3-way resemblance (Jaccard similarity) using b-bit minwise hashing. While traditional minwise hashing methods store each hashed value using 64 bits, b-bit minwise hashing only stores the lowest b bits (where b&gt;= 2 for 3-way). The extension to 3-way similarity from the prior work on 2-way similarity is technically non-trivial. We develop the precise estimator which is accurate and very complicated; and we recommend a much simplified estimator suitable for sparse data. Our analysis shows that $b$-bit minwise hashing can normally achieve a 10 to 25-fold improvement in the storage space required for a given estimator accuracy of the 3-way resemblance.</p>
<p>【Keywords】:</p>
<h3 id="155. Construction of Dependent Dirichlet Processes based on Poisson Processes.">155. Construction of Dependent Dirichlet Processes based on Poisson Processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/4151-construction-of-dependent-dirichlet-processes-based-on-poisson-processes">Paper Link</a>】    【Pages】:1396-1404</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Dahua">Dahua Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grimson:Eric">Eric Grimson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fisher_III:John_W=">John W. Fisher III</a></p>
<p>【Abstract】:
Abstract Missing</p>
<p>【Keywords】:</p>
<h3 id="156. Deep Coding Network.">156. Deep Coding Network.</h3>
<p>【<a href="http://papers.nips.cc/paper/3929-deep-coding-network">Paper Link</a>】    【Pages】:1405-1413</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Yuanqing">Yuanqing Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Tong">Tong Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Shenghuo">Shenghuo Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Kai">Kai Yu</a></p>
<p>【Abstract】:
This paper proposes a principled extension of the traditional single-layer flat sparse coding scheme, where a two-layer coding scheme is derived based on theoretical analysis of nonlinear functional approximation that extends recent results for local coordinate coding. The two-layer approach can be easily generalized to deeper structures in a hierarchical multiple-layer manner. Empirically, it is shown that the deep coding approach yields improved performance in benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="157. Moreau-Yosida Regularization for Grouped Tree Structure Learning.">157. Moreau-Yosida Regularization for Grouped Tree Structure Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3931-moreau-yosida-regularization-for-grouped-tree-structure-learning">Paper Link</a>】    【Pages】:1459-1467</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Jun">Jun Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Jieping">Jieping Ye</a></p>
<p>【Abstract】:
We consider the tree structured group Lasso where the structure over the features can be represented as a tree with leaf nodes as features and internal nodes as clusters of the features. The structured regularization with a pre-defined tree structure is based on a group-Lasso penalty, where one group is defined for each node in the tree. Such a regularization can help uncover the structured sparsity, which is desirable for applications with some meaningful tree structures on the features. However, the tree structured group Lasso is challenging to solve due to the complex regularization. In this paper, we develop an efficient algorithm for the tree structured group Lasso. One of the key steps in the proposed algorithm is to solve the Moreau-Yosida regularization associated with the grouped tree structure. The main technical contributions of this paper include (1) we show that the associated Moreau-Yosida regularization admits an analytical solution, and (2) we develop an efficient algorithm for determining the effective interval for the regularization parameter. Our experimental results on the AR and JAFFE face data sets demonstrate the efficiency and effectiveness of the proposed algorithm.</p>
<p>【Keywords】:</p>
<h3 id="158. Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models.">158. Stability Approach to Regularization Selection (StARS) for High Dimensional Graphical Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3966-stability-approach-to-regularization-selection-stars-for-high-dimensional-graphical-models">Paper Link</a>】    【Pages】:1432-1440</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roeder:Kathryn">Kathryn Roeder</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wasserman:Larry_A=">Larry A. Wasserman</a></p>
<p>【Abstract】:
A challenging problem in estimating high-dimensional graphical models is to choose the regularization parameter in a data-dependent way. The standard techniques include $K$-fold cross-validation ($K$-CV), Akaike information criterion (AIC), and Bayesian information criterion (BIC). Though these methods work well for low-dimensional problems, they are not suitable in high dimensional settings. In this paper, we present StARS: a new stability-based method for choosing the regularization parameter in high dimensional inference for undirected graphs. The method has a clear interpretation: we use the least amount of regularization that simultaneously makes a graph sparse and replicable under random sampling. This interpretation requires essentially no conditions. Under mild conditions, we show that StARS is partially sparsistent in terms of graph estimation: i.e. with high probability, all the true edges will be included in the selected model even when the graph size asymptotically increases with the sample size. Empirically, the performance of StARS is compared with the state-of-the-art model selection procedures, including $K$-CV, AIC, and BIC, on both synthetic data and a real microarray dataset. StARS outperforms all competing procedures.</p>
<p>【Keywords】:</p>
<h3 id="159. Multivariate Dyadic Regression Trees for Sparse Learning Problems.">159. Multivariate Dyadic Regression Trees for Sparse Learning Problems.</h3>
<p>【<a href="http://papers.nips.cc/paper/4178-multivariate-dyadic-regression-trees-for-sparse-learning-problems">Paper Link</a>】    【Pages】:1441-1449</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Xi">Xi Chen</a></p>
<p>【Abstract】:
We propose a new nonparametric learning method based on multivariate dyadic regression trees (MDRTs).  Unlike traditional dyadic decision trees (DDTs) or classification and regression trees (CARTs), MDRTs are constructed using penalized empirical risk minimization with a novel sparsity-inducing penalty.  Theoretically, we show that MDRTs can simultaneously adapt to the unknown sparsity and smoothness of the true regression functions, and achieve the nearly optimal rates of convergence (in a minimax sense) for the class of $(\alpha, C)$-smooth functions. Empirically, MDRTs can simultaneously conduct function estimation and variable selection in high dimensions. To make MDRTs applicable  for large-scale learning problems, we propose a greedy heuristics. The superior performance of MDRTs are demonstrated on both synthetic and real datasets.</p>
<p>【Keywords】:</p>
<h3 id="160. Robust Clustering as Ensembles of Affinity Relations.">160. Robust Clustering as Ensembles of Affinity Relations.</h3>
<p>【<a href="http://papers.nips.cc/paper/4045-robust-clustering-as-ensembles-of-affinity-relations">Paper Link</a>】    【Pages】:1414-1422</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Hairong">Hairong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Latecki:Longin_Jan">Longin Jan Latecki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Shuicheng">Shuicheng Yan</a></p>
<p>【Abstract】:
In this paper, we regard clustering as ensembles of k-ary affinity relations and clusters correspond to subsets of objects with maximal average affinity relations. The average affinity relation of a cluster is relaxed and well approximated by a constrained homogenous function. We present an efficient procedure to solve this optimization problem, and show that the underlying clusters can be robustly revealed by using priors systematically constructed from the data. Our method can automatically select some points to form clusters, leaving other points un-grouped; thus it is inherently robust to large numbers of outliers, which has seriously limited the applicability of classical methods. Our method also provides a unified solution to clustering from k-ary affinity relations with k ≥ 2, that is, it applies to both graph-based and hypergraph-based clustering problems. Both theoretical analysis and experimental results show the superiority of our method over classical solutions to the clustering problem, especially when there exists a large number of outliers.</p>
<p>【Keywords】:</p>
<h3 id="161. Graph-Valued Regression.">161. Graph-Valued Regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/3916-graph-valued-regression">Paper Link</a>】    【Pages】:1423-1431</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Xi">Xi Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lafferty:John_D=">John D. Lafferty</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wasserman:Larry_A=">Larry A. Wasserman</a></p>
<p>【Abstract】:
Undirected graphical models encode in a graph $G$ the dependency structure of a random vector $Y$. In many applications, it is of interest to model $Y$ given another random vector $X$ as input. We refer to the problem of estimating the graph $G(x)$ of $Y$ conditioned on $X=x$ as <code>graph-valued regression''. In this paper, we propose a semiparametric method for estimating $G(x)$ that builds a tree on the $X$ space just as in CART (classification and regression trees), but at each leaf of the tree estimates a graph. We call the method</code>Graph-optimized CART'', or Go-CART. We study the theoretical properties of Go-CART using dyadic partitioning trees, establishing oracle inequalities on risk minimization and tree partition consistency. We also demonstrate the application of Go-CART to a meteorological dataset, showing how graph-valued regression can provide a useful tool for analyzing complex data.</p>
<p>【Keywords】:</p>
<h3 id="162. Multi-Stage Dantzig Selector.">162. Multi-Stage Dantzig Selector.</h3>
<p>【<a href="http://papers.nips.cc/paper/4015-multi-stage-dantzig-selector">Paper Link</a>】    【Pages】:1450-1458</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu_0002:Ji">Ji Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wonka:Peter">Peter Wonka</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Jieping">Jieping Ye</a></p>
<p>【Abstract】:
We consider the following sparse signal recovery (or feature selection) problem: given a design matrix $X\in \mathbb{R}^{n\times m}$ $(m\gg n)$ and a noisy observation vector $y\in \mathbb{R}^{n}$ satisfying $y=X\beta^<em>+\epsilon$ where $\epsilon$ is the noise vector following a Gaussian distribution $N(0,\sigma^2I)$, how to recover the signal (or parameter vector) $\beta^</em>$ when the signal is sparse?  The Dantzig selector has been proposed for sparse signal recovery with strong theoretical guarantees. In this paper, we propose a multi-stage Dantzig selector method, which iteratively refines the target signal $\beta^<em>$. We show that if $X$ obeys a certain condition, then with a large probability the difference between the solution $\hat\beta$ estimated by the proposed method and the true solution $\beta^</em>$ measured in terms of the $l_p$ norm ($p\geq 1$) is bounded as \begin{equation<em>} |\hat\beta-\beta^</em>|_p\leq \left(C(s-N)^{1/p}\sqrt{\log m}+\Delta\right)\sigma, \end{equation<em>} $C$ is a constant, $s$ is the number of nonzero entries in $\beta^</em>$, $\Delta$ is independent of $m$ and is much smaller than the first term, and $N$ is the number of entries of $\beta^<em>$ larger than a certain value in the order of $\mathcal{O}(\sigma\sqrt{\log m})$. The proposed method improves the estimation bound of the standard Dantzig selector approximately from $Cs^{1/p}\sqrt{\log m}\sigma$ to $C(s-N)^{1/p}\sqrt{\log m}\sigma$ where the value $N$ depends on the number of large entries in $\beta^</em>$. When $N=s$, the proposed algorithm achieves the oracle solution with a high probability. In addition, with a large probability, the proposed method can select the same number of correct features under a milder condition than the Dantzig selector.</p>
<p>【Keywords】:</p>
<h3 id="163. Decoding Ipsilateral Finger Movements from ECoG Signals in Humans.">163. Decoding Ipsilateral Finger Movements from ECoG Signals in Humans.</h3>
<p>【<a href="http://papers.nips.cc/paper/4121-decoding-ipsilateral-finger-movements-from-ecog-signals-in-humans">Paper Link</a>】    【Pages】:1468-1476</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yuzong">Yuzong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sharma:Mohit">Mohit Sharma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gaona:Charles_M=">Charles M. Gaona</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Breshears:Jonathan">Jonathan Breshears</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Roland:Jarod">Jarod Roland</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Freudenburg:Zachary">Zachary Freudenburg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leuthardt:Eric">Eric Leuthardt</a></p>
<p>【Abstract】:
Several motor related Brain Computer Interfaces (BCIs) have been developed over the years that use activity decoded from the contralateral hemisphere to operate devices. Many recent studies have also talked about the importance of ipsilateral activity in planning of motor movements. For successful upper limb BCIs, it is important to decode finger movements from brain activity. This study uses ipsilateral cortical signals from humans (using ECoG) to decode finger movements. We demonstrate, for the first time, successful finger movement detection using machine learning algorithms. Our results show high decoding accuracies in all cases which are always above chance. We also show that significant accuracies can be achieved with the use of only a fraction of all the features recorded and that these core features also make sense physiologically. The results of this study have a great potential in the emerging world of motor neuroprosthetics and other BCIs.</p>
<p>【Keywords】:</p>
<h3 id="164. Approximate Inference by Compilation to Arithmetic Circuits.">164. Approximate Inference by Compilation to Arithmetic Circuits.</h3>
<p>【<a href="http://papers.nips.cc/paper/4011-approximate-inference-by-compilation-to-arithmetic-circuits">Paper Link</a>】    【Pages】:1477-1485</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lowd:Daniel">Daniel Lowd</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Domingos:Pedro_M=">Pedro M. Domingos</a></p>
<p>【Abstract】:
Arithmetic circuits (ACs) exploit context-specific independence and determinism to allow exact inference even in networks with high treewidth. In this paper, we introduce the first ever approximate inference methods using ACs, for domains where exact inference remains intractable. We propose and evaluate a variety of techniques based on exact compilation, forward sampling, AC structure learning, Markov network parameter learning, variational inference, and Gibbs sampling. In experiments on eight challenging real-world domains, we find that the methods based on sampling and learning work best: one such method (AC2-F) is faster and usually more accurate than loopy belief propagation, mean field, and Gibbs sampling; another (AC2-G) has a running time similar to Gibbs sampling but is consistently more accurate than all baselines.</p>
<p>【Keywords】:</p>
<h3 id="165. Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference.">165. Block Variable Selection in Multivariate Regression and High-dimensional Causal Inference.</h3>
<p>【<a href="http://papers.nips.cc/paper/3993-block-variable-selection-in-multivariate-regression-and-high-dimensional-causal-inference">Paper Link</a>】    【Pages】:1486-1494</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lozano:Aurelie_C=">Aurelie C. Lozano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sindhwani:Vikas">Vikas Sindhwani</a></p>
<p>【Abstract】:
We consider multivariate regression problems involving high-dimensional predictor and response spaces. To efficiently address such problems, we propose a variable selection method, Multivariate Group Orthogonal Matching Pursuit, which extends the standard Orthogonal Matching Pursuit technique to account for arbitrary sparsity patterns induced by domain-specific groupings over both input and output variables, while also taking advantage of the correlation that may exist between the multiple outputs. We illustrate the utility of this framework for inferring causal relationships over a collection of high-dimensional time series variables. When applied to time-evolving social media content, our models yield a new family of causality-based influence measures that may be seen as an alternative to PageRank. Theoretical guarantees, extensive simulations and empirical studies confirm the generality and value of our framework.</p>
<p>【Keywords】:</p>
<h3 id="166. Functional form of motion priors in human motion perception.">166. Functional form of motion priors in human motion perception.</h3>
<p>【<a href="http://papers.nips.cc/paper/3948-functional-form-of-motion-priors-in-human-motion-perception">Paper Link</a>】    【Pages】:1495-1503</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Hongjing">Hongjing Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Tungyou">Tungyou Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Alan_L=_F=">Alan L. F. Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vese:Luminita_A=">Luminita A. Vese</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuille:Alan_L=">Alan L. Yuille</a></p>
<p>【Abstract】:
It has been speculated that the human motion system combines noisy measurements with prior expectations in an optimal, or rational, manner. The basic goal of our work is to discover experimentally which prior distribution is used. More specifically, we seek to infer the functional form of the motion prior from the performance of human subjects on motion estimation tasks. We restricted ourselves to priors which combine three terms for motion slowness, first-order smoothness, and second-order smoothness. We focused on two functional forms for prior distributions: L2-norm and L1-norm regularization corresponding to the Gaussian and Laplace distributions respectively. In our first experimental session we estimate the weights of the three terms for each functional form to maximize the fit to human performance. We then measured human performance for motion tasks and found that we obtained better fit for the L1-norm (Laplace) than for the L2-norm (Gaussian). We note that the L1-norm is also a better fit to the statistics of motion in natural environments. In addition, we found large weights for the second-order smoothness term, indicating the importance of high-order smoothness compared to slowness and lower-order smoothness. To validate our results further, we used the best fit models using the L1-norm to predict human performance in a second session with different experimental setups. Our results showed excellent agreement between human performance and model prediction -- ranging from 3\% to 8\% for five human subjects over ten experimental conditions -- and give further support that the human visual system uses an L1-norm (Laplace) prior.</p>
<p>【Keywords】:</p>
<h3 id="167. Learning from Candidate Labeling Sets.">167. Learning from Candidate Labeling Sets.</h3>
<p>【<a href="http://papers.nips.cc/paper/4041-learning-from-candidate-labeling-sets">Paper Link</a>】    【Pages】:1504-1512</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Jie">Jie Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Orabona:Francesco">Francesco Orabona</a></p>
<p>【Abstract】:
In many real world applications we do not have access to fully-labeled training data, but only to a list of possible labels. This is the case, e.g., when learning visual classifiers from images downloaded from the web, using just their text captions or tags as learning oracles. In general, these problems can be very difficult. However most of the time there exist different implicit sources of information, coming from the relations between instances and labels, which are usually dismissed. In this paper, we propose a semi-supervised framework to model this kind of problems. Each training sample is a bag containing multi-instances, associated with a set of candidate labeling vectors. Each labeling vector encodes the possible labels for the instances in the bag, with only one being fully correct. The use of the labeling vectors provides a principled way not to exclude any information. We propose a large margin discriminative formulation, and an efficient algorithm to solve it. Experiments conducted on artificial datasets and a real-world images and captions dataset show that our approach achieves performance comparable to SVM trained with the ground-truth labels, and outperforms other baselines.</p>
<p>【Keywords】:</p>
<h3 id="168. Decomposing Isotonic Regression for Efficiently Solving Large Problems.">168. Decomposing Isotonic Regression for Efficiently Solving Large Problems.</h3>
<p>【<a href="http://papers.nips.cc/paper/4079-decomposing-isotonic-regression-for-efficiently-solving-large-problems">Paper Link</a>】    【Pages】:1513-1521</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Luss:Ronny">Ronny Luss</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rosset:Saharon">Saharon Rosset</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shahar:Moni">Moni Shahar</a></p>
<p>【Abstract】:
A new algorithm for isotonic regression is presented based on recursively partitioning the solution space. We develop efficient methods for each partitioning subproblem through an equivalent representation as a network flow problem, and prove that this sequence of partitions converges to the global solution. These network flow problems can further be decomposed in order to solve very large problems. Success of isotonic regression in prediction and our algorithm's favorable computational properties are demonstrated through simulated examples as large as 2x10^5 variables and 10^7 constraints.</p>
<p>【Keywords】:</p>
<h3 id="169. Getting lost in space: Large sample analysis of the resistance distance.">169. Getting lost in space: Large sample analysis of the resistance distance.</h3>
<p>【<a href="http://papers.nips.cc/paper/3891-getting-lost-in-space-large-sample-analysis-of-the-resistance-distance">Paper Link</a>】    【Pages】:2622-2630</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Luxburg:Ulrike_von">Ulrike von Luxburg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Radl:Agnes">Agnes Radl</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hein_0001:Matthias">Matthias Hein</a></p>
<p>【Abstract】:
The commute distance between two vertices in a graph is the expected   time it takes a random walk to travel from the first to the second   vertex and back. We study the behavior of the commute distance as the size of the underlying graph   increases. We prove that the commute distance converges to an   expression that does not take into account the structure of the   graph at all and that is completely meaningless as a distance   function on the graph. Consequently, the use of the raw commute   distance for machine learning purposes is strongly discouraged for   large graphs and in high dimensions. As an alternative we introduce   the amplified commute distance that corrects for   the undesired large sample effects.</p>
<p>【Keywords】:</p>
<h3 id="170. Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform.">170. Divisive Normalization: Justification and Effectiveness as Efficient Coding Transform.</h3>
<p>【<a href="http://papers.nips.cc/paper/4065-divisive-normalization-justification-and-effectiveness-as-efficient-coding-transform">Paper Link</a>】    【Pages】:1522-1530</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lyu:Siwei">Siwei Lyu</a></p>
<p>【Abstract】:
Divisive normalization (DN) has been advocated as an effective nonlinear {\em efficient coding} transform for natural sensory signals with applications in biology and engineering. In this work, we aim to establish a connection between the DN transform and the statistical properties of natural sensory signals. Our analysis is based on the use of multivariate {\em t} model to capture some important statistical properties of natural sensory signals. The multivariate {\em t} model justifies DN as an approximation to the transform that completely eliminates its statistical dependency. Furthermore, using the multivariate {\em t} model and measuring statistical dependency with multi-information, we can precisely quantify the statistical dependency that is reduced by the DN transform. We compare this with the actual performance of the DN transform in reducing statistical dependencies of natural sensory signals. Our theoretical analysis and quantitative evaluations confirm DN as an effective efficient coding transform for natural sensory signals. On the other hand, we also observe a previously unreported phenomenon that DN may increase statistical dependencies when the size of pooling is small.</p>
<p>【Keywords】:</p>
<h3 id="171. Permutation Complexity Bound on Out-Sample Error.">171. Permutation Complexity Bound on Out-Sample Error.</h3>
<p>【<a href="http://papers.nips.cc/paper/3908-permutation-complexity-bound-on-out-sample-error">Paper Link</a>】    【Pages】:1531-1539</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Magdon=Ismail:Malik">Malik Magdon-Ismail</a></p>
<p>【Abstract】:
We define a data dependent permutation complexity for a hypothesis set \math{\hset}, which is similar to a Rademacher complexity or maximum discrepancy. The permutation complexity is based like the maximum discrepancy on (dependent) sampling. We prove a uniform bound on the generalization error, as well as a concentration result which means that the permutation estimate can be efficiently estimated.</p>
<p>【Keywords】:</p>
<h3 id="172. Basis Construction from Power Series Expansions of Value Functions.">172. Basis Construction from Power Series Expansions of Value Functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/3898-basis-construction-from-power-series-expansions-of-value-functions">Paper Link</a>】    【Pages】:1540-1548</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mahadevan:Sridhar">Sridhar Mahadevan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Bo">Bo Liu</a></p>
<p>【Abstract】:
This paper explores links between basis construction methods in Markov decision processes and power series expansions of value functions. This perspective provides a useful framework to analyze properties of existing bases, as well as provides insight into constructing more effective bases. Krylov and Bellman error bases are based on the Neumann series expansion. These bases incur very large initial Bellman errors, and can converge rather slowly as the discount factor approaches unity. The Laurent series expansion, which relates discounted and average-reward formulations, provides both an explanation for this slow convergence as well as suggests a way to construct more efficient basis representations. The first two terms in the Laurent series represent the scaled average-reward and the average-adjusted sum of rewards, and subsequent terms expand the discounted value function using powers of a generalized inverse called the Drazin (or group inverse) of a singular matrix derived from the transition matrix. Experiments show that Drazin bases converge considerably more quickly than several other bases, particularly for large values of the discount factor. An incremental variant of Drazin bases called Bellman average-reward bases (BARBs) is described, which provides some of the same benefits at lower computational cost.</p>
<p>【Keywords】:</p>
<h3 id="173. Scrambled Objects for Least-Squares Regression.">173. Scrambled Objects for Least-Squares Regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/3973-scrambled-objects-for-least-squares-regression">Paper Link</a>】    【Pages】:1549-1557</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maillard:Odalric=Ambrym">Odalric-Ambrym Maillard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Munos:R=eacute=mi">Rémi Munos</a></p>
<p>【Abstract】:
We consider least-squares regression using a randomly generated subspace G<em>P\subset F of finite dimension P, where F is a function space of infinite dimension, e.g.~L_2([0,1]^d).  G_P is defined as the span of P random features  that are linear combinations of the basis functions of F weighted by random Gaussian i.i.d.~coefficients. In particular, we consider multi-resolution random combinations at all scales of a given mother function,  such as a hat function or a wavelet. In this latter case, the resulting Gaussian objects are called {\em scrambled wavelets} and we show that they enable to approximate functions in Sobolev spaces H^s([0,1]^d). As a result, given N data, the least-squares estimate \hat g built from P scrambled wavelets has excess risk ||f^* - \hat g||</em>\P^2 = O(||f^<em>||^2_{H^s([0,1]^d)}(\log N)/P + P(\log N )/N) for target functions f^</em>\in H^s([0,1]^d) of smoothness order s&gt;d/2. An interesting aspect of the resulting bounds is that they do not depend on the distribution \P from which the data are generated, which is important in a statistical regression setting considered here. Randomization enables to adapt to any possible distribution.   We conclude by describing an efficient numerical implementation using lazy expansions with numerical complexity \tilde O(2^d N^{3/2}\log N + N^2), where d is the dimension of the input space.</p>
<p>【Keywords】:</p>
<h3 id="174. Network Flow Algorithms for Structured Sparsity.">174. Network Flow Algorithms for Structured Sparsity.</h3>
<p>【<a href="http://papers.nips.cc/paper/3965-network-flow-algorithms-for-structured-sparsity">Paper Link</a>】    【Pages】:1558-1566</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mairal:Julien">Julien Mairal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jenatton:Rodolphe">Rodolphe Jenatton</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Obozinski:Guillaume">Guillaume Obozinski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Francis_R=">Francis R. Bach</a></p>
<p>【Abstract】:
We consider a class of learning problems that involve a structured sparsity-inducing norm defined as the sum of $\ell_\infty$-norms over groups of variables. Whereas a lot of effort has been put in developing fast optimization methods when the groups are disjoint or embedded in a specific hierarchical structure, we address here the case of general overlapping groups. To this end, we show that the corresponding optimization problem is related to network flow optimization. More precisely, the proximal problem associated with the norm we consider is dual to a quadratic min-cost flow problem. We propose an efficient procedure which computes its solution exactly in polynomial time. Our algorithm scales up to millions of groups and variables, and opens up a whole new range of applications for structured sparse models. We present several experiments on image and video data, demonstrating the applicability and scalability of our approach for various problems.</p>
<p>【Keywords】:</p>
<h3 id="175. Sphere Embedding: An Application to Part-of-Speech Induction.">175. Sphere Embedding: An Application to Part-of-Speech Induction.</h3>
<p>【<a href="http://papers.nips.cc/paper/3979-sphere-embedding-an-application-to-part-of-speech-induction">Paper Link</a>】    【Pages】:1567-1575</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maron:Yariv">Yariv Maron</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lamar:Michael">Michael Lamar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bienenstock:Elie">Elie Bienenstock</a></p>
<p>【Abstract】:
Motivated by an application to unsupervised part-of-speech tagging, we present an algorithm for the Euclidean embedding of large sets of categorical data based on co-occurrence statistics. We use the CODE model of Globerson et al. but constrain the embedding to lie on a high-dimensional unit sphere. This constraint allows for efficient optimization, even in the case of large datasets and high embedding dimensionality. Using k-means clustering of the embedded data, our approach efficiently produces state-of-the-art results. We analyze the reasons why the sphere constraint is beneficial in this application, and conjecture that these reasons might apply quite generally to other large-scale tasks.</p>
<p>【Keywords】:</p>
<h3 id="176. Variable margin losses for classifier design.">176. Variable margin losses for classifier design.</h3>
<p>【<a href="http://papers.nips.cc/paper/4024-variable-margin-losses-for-classifier-design">Paper Link</a>】    【Pages】:1576-1584</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Masnadi=Shirazi:Hamed">Hamed Masnadi-Shirazi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vasconcelos:Nuno">Nuno Vasconcelos</a></p>
<p>【Abstract】:
The problem of controlling the margin of a classifier is studied. A detailed analytical study is presented on how properties of the classification risk, such as its optimal link and minimum risk functions, are related to the shape of the loss, and its margin enforcing properties. It is shown that for a class of risks, denoted canonical risks, asymptotic Bayes consistency is compatible with simple analytical relationships between these functions. These enable a precise characterization of the loss for a popular class of link functions. It is shown that, when the risk is in canonical form and the link is inverse sigmoidal, the margin properties of the loss are determined by a single parameter. Novel families of Bayes consistent loss functions, of variable margin, are derived. These families are then used to design boosting style algorithms with explicit control of the classification margin. The new algorithms generalize well established approaches, such as LogitBoost. Experimental results show that the proposed variable margin losses outperform the fixed margin counterparts used by existing algorithms. Finally, it is shown that best performance can be achieved by cross-validating the margin parameter.</p>
<p>【Keywords】:</p>
<h3 id="177. Why are some word orders more common than others? A uniform information density account.">177. Why are some word orders more common than others? A uniform information density account.</h3>
<p>【<a href="http://papers.nips.cc/paper/4085-why-are-some-word-orders-more-common-than-others-a-uniform-information-density-account">Paper Link</a>】    【Pages】:1585-1593</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maurits:Luke">Luke Maurits</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Navarro:Daniel_J=">Daniel J. Navarro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perfors:Amy">Amy Perfors</a></p>
<p>【Abstract】:
Languages vary widely in many ways, including their canonical word order. A basic aspect of the observed variation is the fact that some word orders are much more common than others. Although this regularity has been recognized for some time, it has not been well-explained. In this paper we offer an information-theoretic explanation for the observed word-order distribution across languages, based on the concept of Uniform Information Density (UID). We suggest that object-first languages are particularly disfavored because they are highly non-optimal if the goal is to distribute information content approximately evenly throughout a sentence, and that the rest of the observed word-order distribution is at least partially explainable in terms of UID. We support our theoretical analysis with data from child-directed speech and experimental work.</p>
<p>【Keywords】:</p>
<h3 id="178. Direct Loss Minimization for Structured Prediction.">178. Direct Loss Minimization for Structured Prediction.</h3>
<p>【<a href="http://papers.nips.cc/paper/4069-direct-loss-minimization-for-structured-prediction">Paper Link</a>】    【Pages】:1594-1602</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/McAllester:David_A=">David A. McAllester</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hazan:Tamir">Tamir Hazan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keshet:Joseph">Joseph Keshet</a></p>
<p>【Abstract】:
In discriminative machine learning one is interested in training a system to optimize a certain desired measure of performance, or loss. In binary classification one typically tries to minimizes the error rate. But in structured prediction each task often has its own measure of performance such as the BLEU score in machine translation or the intersection-over-union score in PASCAL segmentation. The most common approaches to structured prediction, structural SVMs and CRFs, do not minimize the task loss: the former minimizes a surrogate loss with no guarantees for task loss and the latter minimizes log loss independent of task loss. The main contribution of this paper is a theorem stating that a certain perceptron-like learning rule, involving features vectors derived from loss-adjusted inference, directly corresponds to the gradient of task loss. We give empirical results on phonetic alignment of a standard test set from the TIMIT corpus, which surpasses all previously reported results on this problem.</p>
<p>【Keywords】:</p>
<h3 id="179. Gated Softmax Classification.">179. Gated Softmax Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/3895-gated-softmax-classification">Paper Link</a>】    【Pages】:1603-1611</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Memisevic:Roland">Roland Memisevic</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zach:Christopher">Christopher Zach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hinton:Geoffrey_E=">Geoffrey E. Hinton</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pollefeys:Marc">Marc Pollefeys</a></p>
<p>【Abstract】:
We describe a log-bilinear" model that computes class probabilities by combining an input vector multiplicatively with a vector of binary latent variables. Even though the latent variables can take on exponentially many possible combinations of values, we can efficiently compute the exact probability of each class by marginalizing over the latent variables. This makes it possible to get the exact gradient of the log likelihood. The bilinear score-functions are defined using a three-dimensional weight tensor, and we show that factorizing this tensor allows the model to encode invariances inherent in a task by learning a dictionary of invariant basis functions. Experiments on a set of benchmark problems show that this fully probabilistic model can achieve classification performance that is competitive with (kernel) SVMs, backpropagation, and deep belief nets."</p>
<p>【Keywords】:</p>
<h3 id="180. A Family of Penalty Functions for Structured Sparsity.">180. A Family of Penalty Functions for Structured Sparsity.</h3>
<p>【<a href="http://papers.nips.cc/paper/4137-a-family-of-penalty-functions-for-structured-sparsity">Paper Link</a>】    【Pages】:1612-1623</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Micchelli:Charles_A=">Charles A. Micchelli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Morales:Jean">Jean Morales</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pontil:Massimiliano">Massimiliano Pontil</a></p>
<p>【Abstract】:
We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the $\ell_1$ norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.</p>
<p>【Keywords】:</p>
<h3 id="181. Subgraph Detection Using Eigenvector L1 Norms.">181. Subgraph Detection Using Eigenvector L1 Norms.</h3>
<p>【<a href="http://papers.nips.cc/paper/4044-subgraph-detection-using-eigenvector-l1-norms">Paper Link</a>】    【Pages】:1633-1641</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Miller:Benjamin_A=">Benjamin A. Miller</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bliss:Nadya_T=">Nadya T. Bliss</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wolfe:Patrick_J=">Patrick J. Wolfe</a></p>
<p>【Abstract】:
When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to determine the detectability of small, anomalous graphs embedded into background networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a detection theory" for graph-valued data. Its focus is the detection of anomalies in unweighted, undirected graphs through L1 properties of the eigenvectors of the graph’s so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the background graph. An analysis of subgraphs in real network datasets confirms the efficacy of this approach."</p>
<p>【Keywords】:</p>
<h3 id="182. A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model.">182. A VLSI Implementation of the Adaptive Exponential Integrate-and-Fire Neuron Model.</h3>
<p>【<a href="http://papers.nips.cc/paper/3995-a-vlsi-implementation-of-the-adaptive-exponential-integrate-and-fire-neuron-model">Paper Link</a>】    【Pages】:1642-1650</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Millner:Sebastian">Sebastian Millner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gr=uuml=bl:Andreas">Andreas Grübl</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meier:Karlheinz">Karlheinz Meier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schemmel:Johannes">Johannes Schemmel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schwartz:Marc=Olivier">Marc-Olivier Schwartz</a></p>
<p>【Abstract】:
We describe an accelerated hardware neuron being capable of emulating the adap-tive exponential integrate-and-fire neuron model. Firing patterns of the membrane stimulated by a step current are analyzed in transistor level simulation and in silicon on a prototype chip. The neuron is destined to be the hardware neuron of a highly integrated wafer-scale system reaching out for new computational paradigms and opening new experimentation possibilities. As the neuron is dedicated as a universal device for neuroscientific experiments, the focus lays on parameterizability and reproduction of the analytical model.</p>
<p>【Keywords】:</p>
<h3 id="183. Large-Scale Matrix Factorization with Missing Data under Additional Constraints.">183. Large-Scale Matrix Factorization with Missing Data under Additional Constraints.</h3>
<p>【<a href="http://papers.nips.cc/paper/4111-large-scale-matrix-factorization-with-missing-data-under-additional-constraints">Paper Link</a>】    【Pages】:1651-1659</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Kaushik">Kaushik Mitra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sheorey:Sameer">Sameer Sheorey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chellappa:Rama">Rama Chellappa</a></p>
<p>【Abstract】:
Matrix factorization in the presence of missing data is at the core of many computer vision problems such as structure from motion (SfM), non-rigid SfM and photometric stereo. We formulate the problem of matrix factorization with missing data as a low-rank semidefinite program (LRSDP) with the advantage that: $1)$ an efficient quasi-Newton implementation of the LRSDP enables us to solve large-scale factorization problems, and $2)$ additional constraints such as ortho-normality, required in orthographic SfM, can be directly incorporated in the new formulation. Our empirical evaluations suggest that, under the conditions of matrix completion theory, the proposed algorithm finds the optimal solution, and also requires fewer observations compared to the current state-of-the-art algorithms. We further demonstrate the effectiveness of the proposed algorithm in solving the affine SfM problem, non-rigid SfM and photometric stereo problems.</p>
<p>【Keywords】:</p>
<h3 id="184. Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks.">184. Natural Policy Gradient Methods with Parameter-based Exploration for Control Tasks.</h3>
<p>【<a href="http://papers.nips.cc/paper/3987-natural-policy-gradient-methods-with-parameter-based-exploration-for-control-tasks">Paper Link</a>】    【Pages】:1660-1668</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Miyamae:Atsushi">Atsushi Miyamae</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nagata:Yuichi">Yuichi Nagata</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ono:Isao">Isao Ono</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kobayashi:Shigenobu">Shigenobu Kobayashi</a></p>
<p>【Abstract】:
In this paper, we propose an efficient algorithm for estimating the natural policy gradient with parameter-based exploration; this algorithm samples directly in the parameter space. Unlike previous methods based on natural gradients, our algorithm calculates the natural policy gradient using the inverse of the exact Fisher information matrix. The computational cost of this algorithm is equal to that of conventional policy gradients whereas previous natural policy gradient methods have a prohibitive computational cost. Experimental results show that the proposed method outperforms several policy gradient methods.</p>
<p>【Keywords】:</p>
<h3 id="185. An analysis on negative curvature induced by singularity in multi-layer neural-network learning.">185. An analysis on negative curvature induced by singularity in multi-layer neural-network learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4046-an-analysis-on-negative-curvature-induced-by-singularity-in-multi-layer-neural-network-learning">Paper Link</a>】    【Pages】:1669-1677</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mizutani:Eiji">Eiji Mizutani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dreyfus:Stuart">Stuart Dreyfus</a></p>
<p>【Abstract】:
In the neural-network parameter space,  an attractive field is likely to be induced by singularities. In such a singularity region, first-order gradient learning typically causes a long plateau with very little change  in the objective function value E (hence, a flat region). Therefore, it may be confused with ``attractive'' local minima. Our analysis shows that the Hessian matrix of E tends to be indefinite in the vicinity of (perturbed) singular points, suggesting a promising strategy that exploits negative curvature so as to escape from the singularity plateaus. For numerical evidence, we limit the scope to small examples (some of which are found in journal papers)  that allow us to confirm singularities and the eigenvalues of the Hessian matrix, and for which computation using a descent direction of negative curvature encounters no plateau.  Even for those small problems, no efficient methods have been previously developed that avoided plateaus.</p>
<p>【Keywords】:</p>
<h3 id="186. Layer-wise analysis of deep networks with Gaussian kernels.">186. Layer-wise analysis of deep networks with Gaussian kernels.</h3>
<p>【<a href="http://papers.nips.cc/paper/4061-layer-wise-analysis-of-deep-networks-with-gaussian-kernels">Paper Link</a>】    【Pages】:1678-1686</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Montavon:Gr=eacute=goire">Grégoire Montavon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Braun:Mikio_L=">Mikio L. Braun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/M=uuml=ller:Klaus=Robert">Klaus-Robert Müller</a></p>
<p>【Abstract】:
Deep networks can potentially express a learning problem more efficiently than local learning machines. While deep networks outperform local learning machines on some problems, it is still unclear how their nice representation emerges from their complex structure. We present an analysis based on Gaussian kernels that measures how the representation of the learning problem evolves layer after layer as the deep network builds higher-level abstract representations of the input. We use this analysis to show empirically that deep networks build progressively better representations of the learning problem and that the best representations are obtained when the deep network discriminates only in the last layers.</p>
<p>【Keywords】:</p>
<h3 id="187. Probabilistic latent variable models for distinguishing between cause and effect.">187. Probabilistic latent variable models for distinguishing between cause and effect.</h3>
<p>【<a href="http://papers.nips.cc/paper/4173-probabilistic-latent-variable-models-for-distinguishing-between-cause-and-effect">Paper Link</a>】    【Pages】:1687-1695</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mooij:Joris_M=">Joris M. Mooij</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Stegle:Oliver">Oliver Stegle</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Janzing:Dominik">Dominik Janzing</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Kun">Kun Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=ouml=lkopf:Bernhard">Bernhard Schölkopf</a></p>
<p>【Abstract】:
We propose a novel method for inferring whether X causes Y or vice versa from joint observations of X and Y. The basic idea is to model the observed data using probabilistic latent variable models, which incorporate the effects of unobserved noise. To this end, we consider the hypothetical effect variable to be a function of the hypothetical cause variable and an independent noise term (not necessarily additive). An important novel aspect of our work is that we do not restrict the model class, but instead put general non-parametric priors on this function and on the distribution of the cause. The causal direction can then be inferred by using standard Bayesian model selection. We evaluate our approach on synthetic data and real-world data and report encouraging results.</p>
<p>【Keywords】:</p>
<h3 id="188. A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups.">188. A Primal-Dual Algorithm for Group Sparse Regularization with Overlapping Groups.</h3>
<p>【<a href="http://papers.nips.cc/paper/3974-a-primal-dual-algorithm-for-group-sparse-regularization-with-overlapping-groups">Paper Link</a>】    【Pages】:2604-2612</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mosci:Sofia">Sofia Mosci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Villa:Silvia">Silvia Villa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Verri:Alessandro">Alessandro Verri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rosasco:Lorenzo">Lorenzo Rosasco</a></p>
<p>【Abstract】:
We deal with the problem of variable selection when  variables must be selected group-wise, with possibly overlapping groups defined a priori. In particular we propose a new optimization procedure  for solving the regularized algorithm presented in Jacob et al. 09, where the group lasso  penalty is generalized to overlapping groups of variables. While in Jacob et al. 09 the proposed implementation requires explicit replication of the variables belonging to more than one group, our iterative procedure is based on a combination of proximal methods in the primal space and constrained Newton method in a reduced dual space, corresponding to the active groups.  This procedure provides a scalable alternative with no need for data duplication, and allows to deal with high dimensional problems without pre-processing  to reduce the  dimensionality of the data.  The computational advantages of our scheme with respect to state-of-the-art algorithms  using data duplication are shown empirically with numerical simulations.</p>
<p>【Keywords】:</p>
<h3 id="189. Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures.">189. Epitome driven 3-D Diffusion Tensor image segmentation: on extracting specific structures.</h3>
<p>【<a href="http://papers.nips.cc/paper/3955-epitome-driven-3-d-diffusion-tensor-image-segmentation-on-extracting-specific-structures">Paper Link</a>】    【Pages】:1696-1704</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Motwani:Kamiya">Kamiya Motwani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Adluru:Nagesh">Nagesh Adluru</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hinrichs:Chris">Chris Hinrichs</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Alexander:Andrew_L=">Andrew L. Alexander</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Vikas">Vikas Singh</a></p>
<p>【Abstract】:
We study the problem of segmenting specific white matter structures of interest from Diffusion Tensor (DT-MR) images of the human brain. This is an important requirement in many Neuroimaging studies: for instance, to evaluate whether a brain structure exhibits group level differences as a function of disease in a set of images. Typically, interactive expert guided segmentation has been the method of choice for such applications, but this is tedious for large datasets common today. To address this problem, we endow an image segmentation algorithm with 'advice' encoding some global characteristics of the region(s) we want to extract. This is accomplished by constructing (using expert-segmented images) an epitome of a specific region - as a histogram over a bag of 'words' (e.g.,suitable feature descriptors). Now, given such a representation, the problem reduces to segmenting new brain image with additional constraints that enforce consistency between the segmented foreground and the pre-specified histogram over features. We present combinatorial approximation algorithms to incorporate such domain specific constraints for Markov Random Field (MRF) segmentation. Making use of recent results on image co-segmentation, we derive effective solution strategies for our problem. We provide an analysis of solution quality, and present promising experimental evidence showing that many structures of interest in Neuroscience can be extracted reliably from 3-D brain image volumes using our algorithm.</p>
<p>【Keywords】:</p>
<h3 id="190. Improving Human Judgments by Decontaminating Sequential Dependencies.">190. Improving Human Judgments by Decontaminating Sequential Dependencies.</h3>
<p>【<a href="http://papers.nips.cc/paper/4162-improving-human-judgments-by-decontaminating-sequential-dependencies">Paper Link</a>】    【Pages】:1705-1713</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mozer:Michael">Michael Mozer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pashler:Harold">Harold Pashler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wilder:Matthew_H=">Matthew H. Wilder</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lindsey:Robert_V=">Robert V. Lindsey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jones:Matt">Matt Jones</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jones:Michael">Michael Jones</a></p>
<p>【Abstract】:
For over half a century, psychologists have been struck by how poor people are at expressing their internal sensations, impressions, and evaluations via rating scales. When individuals make judgments, they are incapable of using an absolute rating scale, and instead rely on reference points from recent experience. This relativity of judgment limits the usefulness of responses provided by individuals to surveys, questionnaires, and evaluation forms. Fortunately, the cognitive processes that transform internal states to responses are not simply noisy, but rather are influenced by recent experience in a lawful manner. We explore techniques to remove sequential dependencies, and thereby decontaminate a series of ratings to obtain more meaningful human judgments. In our formulation, decontamination is fundamentally a problem of inferring latent states (internal sensations) which, because of the relativity of judgment, have temporal dependencies. We propose a decontamination solution using a conditional random field with constraints motivated by psychological theories of relative judgment. Our exploration of decontamination models is supported by two experiments we conducted to obtain ground-truth rating data on a simple length estimation task. Our decontamination techniques yield an over 20% reduction in the error of human judgments.</p>
<p>【Keywords】:</p>
<h3 id="191. A Theory of Multiclass Boosting.">191. A Theory of Multiclass Boosting.</h3>
<p>【<a href="http://papers.nips.cc/paper/4135-a-theory-of-multiclass-boosting">Paper Link</a>】    【Pages】:1714-1722</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mukherjee:Indraneel">Indraneel Mukherjee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schapire:Robert_E=">Robert E. Schapire</a></p>
<p>【Abstract】:
Abstract Missing</p>
<p>【Keywords】:</p>
<h3 id="192. A biologically plausible network for the computation of orientation dominance.">192. A biologically plausible network for the computation of orientation dominance.</h3>
<p>【<a href="http://papers.nips.cc/paper/3982-a-biologically-plausible-network-for-the-computation-of-orientation-dominance">Paper Link</a>】    【Pages】:1723-1731</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Muralidharan:Kritika">Kritika Muralidharan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vasconcelos:Nuno">Nuno Vasconcelos</a></p>
<p>【Abstract】:
The determination of dominant orientation at a given image location is formulated as a decision-theoretic question. This leads to a novel measure for the dominance of a given orientation $\theta$, which is similar to that used by SIFT. It is then shown that the new measure can be computed with a network that implements the sequence of operations of the standard neurophysiological model of V1. The measure can thus be seen as a biologically plausible version of SIFT, and is denoted as bioSIFT. The network units are shown to exhibit trademark properties of V1 neurons, such as cross-orientation suppression, sparseness and independence. The connection between SIFT and biological vision provides a justification for the success of SIFT-like features and reinforces the importance of contrast normalization in computer vision. We illustrate this by replacing the Gabor units of an HMAX network with the new bioSIFT units. This is shown to lead to significant gains for classification tasks, leading to state-of-the-art performance among biologically inspired network models and performance competitive with the best non-biological object recognition systems.</p>
<p>【Keywords】:</p>
<h3 id="193. Slice sampling covariance hyperparameters of latent Gaussian models.">193. Slice sampling covariance hyperparameters of latent Gaussian models.</h3>
<p>【<a href="http://papers.nips.cc/paper/4114-slice-sampling-covariance-hyperparameters-of-latent-gaussian-models">Paper Link</a>】    【Pages】:1732-1740</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Murray:Iain">Iain Murray</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Adams:Ryan_Prescott">Ryan Prescott Adams</a></p>
<p>【Abstract】:
The Gaussian process (GP) is a popular way to specify dependencies between random variables in a probabilistic model. In the Bayesian framework the covariance structure can be specified using unknown hyperparameters. Integrating over these hyperparameters considers different possible explanations for the data when making predictions. This integration is often performed using Markov chain Monte Carlo (MCMC) sampling. However, with non-Gaussian observations standard hyperparameter sampling approaches require careful tuning and may converge slowly. In this paper we present a slice sampling approach that requires little tuning while mixing well in both strong- and weak-data regimes.</p>
<p>【Keywords】:</p>
<h3 id="194. On the Convexity of Latent Social Network Inference.">194. On the Convexity of Latent Social Network Inference.</h3>
<p>【<a href="http://papers.nips.cc/paper/4113-on-the-convexity-of-latent-social-network-inference">Paper Link</a>】    【Pages】:1741-1749</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Myers:Seth_A=">Seth A. Myers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leskovec:Jure">Jure Leskovec</a></p>
<p>【Abstract】:
In many real-world scenarios, it is nearly impossible to collect explicit social network data. In such cases, whole networks must be inferred from underlying observations. Here, we formulate the problem of inferring latent social networks based on network diffusion or disease propagation data. We consider contagions propagating over the edges of an unobserved social network, where we only observe the times when nodes became infected, but not who infected them. Given such node infection times, we then identify the optimal network that best explains the observed data. We present a maximum likelihood approach based on convex programming with a l1-like penalty term that encourages sparsity. Experiments on real and synthetic data reveal that our method near-perfectly recovers the underlying network structure as well as the parameters of the contagion propagation model. Moreover, our approach scales well as it can infer optimal networks on thousands of nodes in a matter of minutes.</p>
<p>【Keywords】:</p>
<h3 id="195. Infinite Relational Modeling of Functional Connectivity in Resting State fMRI.">195. Infinite Relational Modeling of Functional Connectivity in Resting State fMRI.</h3>
<p>【<a href="http://papers.nips.cc/paper/4057-infinite-relational-modeling-of-functional-connectivity-in-resting-state-fmri">Paper Link</a>】    【Pages】:1750-1758</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/M=oslash=rup:Morten">Morten Mørup</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Madsen:Kristoffer_Hougaard">Kristoffer Hougaard Madsen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dogonowski:Anne=Marie">Anne-Marie Dogonowski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Siebner:Hartwig_R=">Hartwig R. Siebner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hansen:Lars_Kai">Lars Kai Hansen</a></p>
<p>【Abstract】:
Functional magnetic resonance imaging (fMRI) can be applied to study the functional connectivity of the neural elements which form complex network at a whole brain level. Most analyses of functional resting state networks (RSN) have been based on the analysis of correlation between the temporal dynamics of various regions of the brain. While these models can identify coherently behaving groups in terms of correlation they give little insight into how these groups interact. In this paper we take a different view on the analysis of functional resting state networks. Starting from the definition of resting state as functional coherent groups we search for functional units of the brain that communicate with other parts of the brain in a coherent manner as measured by mutual information. We use the infinite relational model (IRM) to quantify functional coherent groups of resting state networks and demonstrate how the extracted component interactions can be used to discriminate between functional resting state activity in multiple sclerosis and normal subjects.</p>
<p>【Keywords】:</p>
<h3 id="196. Minimum Average Cost Clustering.">196. Minimum Average Cost Clustering.</h3>
<p>【<a href="http://papers.nips.cc/paper/4106-minimum-average-cost-clustering">Paper Link</a>】    【Pages】:1759-1767</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nagano:Kiyohito">Kiyohito Nagano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kawahara:Yoshinobu">Yoshinobu Kawahara</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iwata:Satoru">Satoru Iwata</a></p>
<p>【Abstract】:
A number of objective functions in clustering problems can be described with submodular functions. In this paper, we introduce the minimum average cost criterion, and show that the theory of intersecting submodular functions can be used for clustering with submodular objective functions. The proposed algorithm does not require the number of clusters in advance, and it will be determined by the property of a given set of data points. The minimum average cost clustering problem is parameterized with a real variable, and surprisingly, we show that all information about optimal clusterings for all parameters can be computed in polynomial time in total. Additionally, we evaluate the performance of the proposed algorithm through computational experiments.</p>
<p>【Keywords】:</p>
<h3 id="197. Global Analytic Solution for Variational Bayesian Matrix Factorization.">197. Global Analytic Solution for Variational Bayesian Matrix Factorization.</h3>
<p>【<a href="http://papers.nips.cc/paper/4063-global-analytic-solution-for-variational-bayesian-matrix-factorization">Paper Link</a>】    【Pages】:1768-1776</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nakajima:Shinichi">Shinichi Nakajima</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sugiyama:Masashi">Masashi Sugiyama</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tomioka:Ryota">Ryota Tomioka</a></p>
<p>【Abstract】:
Bayesian methods of matrix factorization (MF) have been actively explored recently as promising alternatives to classical singular value decomposition. In this paper, we show that, despite the fact that the optimization problem is non-convex, the global optimal solution of variational Bayesian (VB) MF can be computed analytically by solving a quartic equation. This is highly advantageous over a popular VBMF algorithm based on iterated conditional modes since it can only find a local optimal solution after iterations. We further show that the global optimal solution of empirical VBMF (hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments.</p>
<p>【Keywords】:</p>
<h3 id="198. Random Walk Approach to Regret Minimization.">198. Random Walk Approach to Regret Minimization.</h3>
<p>【<a href="http://papers.nips.cc/paper/3972-random-walk-approach-to-regret-minimization">Paper Link</a>】    【Pages】:1777-1785</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Narayanan:Hariharan">Hariharan Narayanan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rakhlin:Alexander">Alexander Rakhlin</a></p>
<p>【Abstract】:
We propose a computationally efficient random walk on a convex body which rapidly mixes to a time-varying Gibbs distribution. In the setting of online convex optimization and repeated games, the algorithm yields low regret and presents a novel efficient method for implementing mixture forecasting strategies.</p>
<p>【Keywords】:</p>
<h3 id="199. Sample Complexity of Testing the Manifold Hypothesis.">199. Sample Complexity of Testing the Manifold Hypothesis.</h3>
<p>【<a href="http://papers.nips.cc/paper/3958-sample-complexity-of-testing-the-manifold-hypothesis">Paper Link</a>】    【Pages】:1786-1794</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Narayanan:Hariharan">Hariharan Narayanan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitter:Sanjoy_K=">Sanjoy K. Mitter</a></p>
<p>【Abstract】:
The hypothesis that high dimensional data tends to lie in the vicinity of a low dimensional manifold is the basis of a collection of methodologies termed Manifold Learning. In this paper, we study statistical aspects of the question of fitting a manifold with a nearly optimal least squared error.   Given upper bounds on the dimension, volume, and curvature, we show that Empirical Risk Minimization can produce a nearly optimal manifold using a number of random samples that is {\it independent} of the ambient dimension of the space in which data lie. We obtain an upper bound on the required number of samples that depends polynomially on the curvature, exponentially on the intrinsic dimension, and linearly on the intrinsic volume. For constant error, we prove a matching minimax lower bound on the sample complexity that shows that this dependence on intrinsic dimension, volume and curvature is unavoidable. Whether the known lower bound of $O(\frac{k}{\eps^2} + \frac{\log \frac{1}{\de}}{\eps^2})$ for the sample complexity of Empirical Risk minimization on $k-$means applied to data in a unit ball of arbitrary dimension is tight, has been an open question since 1997 \cite{bart2}. Here $\eps$ is the desired bound on the error and $\de$ is a bound on the probability of failure. We improve the best currently known upper bound \cite{pontil} of $O(\frac{k^2}{\eps^2} + \frac{\log \frac{1}{\de}}{\eps^2})$ to $O\left(\frac{k}{\eps^2}\left(\min\left(k, \frac{\log^4 \frac{k}{\eps}}{\eps^2}\right)\right) + \frac{\log \frac{1}{\de}}{\eps^2}\right)$. Based on these results, we devise a simple algorithm for $k-$means and another that uses a family of convex programs to fit a piecewise linear curve of a specified length to  high dimensional data, where the sample complexity is independent of the ambient dimension.</p>
<p>【Keywords】:</p>
<h3 id="200. Learning the context of a category.">200. Learning the context of a category.</h3>
<p>【<a href="http://papers.nips.cc/paper/4139-learning-the-context-of-a-category">Paper Link</a>】    【Pages】:1795-1803</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Navarro:Daniel_J=">Daniel J. Navarro</a></p>
<p>【Abstract】:
This paper outlines a hierarchical Bayesian model for human category learning that learns both the organization of objects into categories, and the context in which this knowledge should be applied. The model is fit to multiple data sets, and provides a parsimonious method for describing how humans learn context specific conceptual representations.</p>
<p>【Keywords】:</p>
<h3 id="201. Online Markov Decision Processes under Bandit Feedback.">201. Online Markov Decision Processes under Bandit Feedback.</h3>
<p>【<a href="http://papers.nips.cc/paper/4048-online-markov-decision-processes-under-bandit-feedback">Paper Link</a>】    【Pages】:1804-1812</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Neu:Gergely">Gergely Neu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gy=ouml=rgy:Andr=aacute=s">András György</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Antos:Andr=aacute=s">András Antos</a></p>
<p>【Abstract】:
We consider online learning in finite stochastic Markovian environments where in each time step a new reward function is chosen by an oblivious adversary. The goal of the learning agent is to compete with the best stationary policy in terms of the total reward received. In each time step the agent observes the current state and the reward associated with the last transition, however, the agent does not observe the rewards associated with other state-action pairs. The agent is assumed to know the transition probabilities. The state of the art result for this setting is a no-regret algorithm. In this paper we propose a new learning algorithm and assuming that stationary policies mix uniformly fast, we show that after T time steps, the expected regret of the new algorithm is O(T^{2/3} (ln T)^{1/3}), giving the first rigorously proved convergence rate result for the problem.</p>
<p>【Keywords】:</p>
<h3 id="202. Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization.">202. Efficient and Robust Feature Selection via Joint ℓ2, 1-Norms Minimization.</h3>
<p>【<a href="http://papers.nips.cc/paper/3988-efficient-and-robust-feature-selection-via-joint-l21-norms-minimization">Paper Link</a>】    【Pages】:1813-1821</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nie:Feiping">Feiping Nie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Heng">Heng Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Xiao">Xiao Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Chris_H=_Q=">Chris H. Q. Ding</a></p>
<p>【Abstract】:
Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint ℓ2,1-norm minimization on both loss function and regularization. The ℓ2,1-norm based loss function is robust to outliers in data points and the ℓ2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies were performed on six data sets to demonstrate the effectiveness of our feature selection method.</p>
<p>【Keywords】:</p>
<h3 id="203. Generative Local Metric Learning for Nearest Neighbor Classification.">203. Generative Local Metric Learning for Nearest Neighbor Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4040-generative-local-metric-learning-for-nearest-neighbor-classification">Paper Link</a>】    【Pages】:1822-1830</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Noh:Yung=Kyun">Yung-Kyun Noh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Byoung=Tak">Byoung-Tak Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Daniel_D=">Daniel D. Lee</a></p>
<p>【Abstract】:
We consider the problem of learning a local metric to enhance the performance of nearest neighbor classification. Conventional metric learning methods attempt to separate data distributions in a purely discriminative manner; here we show how to take advantage of information from parametric generative models. We focus on the bias in the information-theoretic error arising from finite sampling effects, and find an appropriate local metric that maximally reduces the bias based upon knowledge from generative models. As a byproduct, the asymptotic theoretical analysis in this work relates metric learning with dimensionality reduction, which was not understood from previous discriminative approaches. Empirical experiments show that this learned local metric enhances the discriminative nearest neighbor performance on various datasets using simple class conditional generative models.</p>
<p>【Keywords】:</p>
<h3 id="204. Approximate inference in continuous time Gaussian-Jump processes.">204. Approximate inference in continuous time Gaussian-Jump processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/4023-approximate-inference-in-continuous-time-gaussian-jump-processes">Paper Link</a>】    【Pages】:1831-1839</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Opper:Manfred">Manfred Opper</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ruttor:Andreas">Andreas Ruttor</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanguinetti:Guido">Guido Sanguinetti</a></p>
<p>【Abstract】:
We present a novel approach to inference in conditionally Gaussian continuous time stochastic processes, where the latent process is a Markovian jump process. We first consider the case of jump-diffusion processes, where the drift of a linear stochastic differential equation can jump at arbitrary time points. We derive partial differential equations for exact inference and present a very efficient mean field approximation. By introducing a novel lower bound on the free energy, we then generalise our approach to Gaussian processes with arbitrary covariance, such as the non-Markovian RBF covariance. We present results on both simulated and real data, showing that the approach is very accurate in capturing latent dynamics and can be useful in a number of real data modelling tasks.</p>
<p>【Keywords】:</p>
<h3 id="205. New Adaptive Algorithms for Online Classification.">205. New Adaptive Algorithms for Online Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4017-new-adaptive-algorithms-for-online-classification">Paper Link</a>】    【Pages】:1840-1848</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Orabona:Francesco">Francesco Orabona</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crammer:Koby">Koby Crammer</a></p>
<p>【Abstract】:
We propose a general framework to online learning for   classification problems with time-varying potential functions in the   adversarial setting. This framework allows to design and prove   relative mistake bounds for any generic loss function. The mistake   bounds can be specialized for the hinge loss, allowing to recover   and improve the bounds of known online classification   algorithms. By optimizing the general bound we derive a new online   classification algorithm, called NAROW, that hybridly uses adaptive- and fixed- second order   information. We analyze the properties of the algorithm and   illustrate its performance using synthetic dataset.</p>
<p>【Keywords】:</p>
<h3 id="206. Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs.">206. Estimation of Renyi Entropy and Mutual Information Based on Generalized Nearest-Neighbor Graphs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4072-estimation-of-renyi-entropy-and-mutual-information-based-on-generalized-nearest-neighbor-graphs">Paper Link</a>】    【Pages】:1849-1857</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/P=aacute=l:D=aacute=vid">Dávid Pál</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/P=oacute=czos:Barnab=aacute=s">Barnabás Póczos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a></p>
<p>【Abstract】:
We present simple and computationally efficient nonparametric estimators of R\'enyi entropy and mutual information based on an i.i.d. sample drawn from an unknown, absolutely continuous distribution over $\R^d$. The estimators are calculated as the sum of $p$-th powers of the Euclidean lengths of the edges of the `generalized nearest-neighbor' graph of the sample and the empirical copula of the sample respectively. For the first time, we prove the almost sure consistency of these estimators and upper bounds on their rates of convergence, the latter of which under the assumption that the density underlying the sample is Lipschitz continuous. Experiments demonstrate their usefulness in independent subspace analysis.</p>
<p>【Keywords】:</p>
<h3 id="207. Gaussian sampling by local perturbations.">207. Gaussian sampling by local perturbations.</h3>
<p>【<a href="http://papers.nips.cc/paper/3940-gaussian-sampling-by-local-perturbations">Paper Link</a>】    【Pages】:1858-1866</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Papandreou:George">George Papandreou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuille:Alan_L=">Alan L. Yuille</a></p>
<p>【Abstract】:
We present a technique for exact simulation of Gaussian Markov random fields (GMRFs), which can be interpreted as locally injecting noise to each Gaussian factor independently, followed by computing the mean/mode of the perturbed GMRF. Coupled with standard iterative techniques for the solution of symmetric positive definite systems, this yields a very efficient sampling algorithm with essentially linear complexity in terms of speed and memory requirements, well suited to extremely large scale probabilistic models. Apart from synthesizing data under a Gaussian model, the proposed technique directly leads to an efficient unbiased estimator of marginal variances. Beyond Gaussian models, the proposed algorithm is also very useful for handling highly non-Gaussian continuously-valued MRFs such as those arising in statistical image modeling or in the first layer of deep belief networks describing real-valued data, where the non-quadratic potentials coupling different sites can be represented as finite or infinite mixtures of Gaussians with the help of local or distributed latent mixture assignment variables. The Bayesian treatment of such models most naturally involves a block Gibbs sampler which alternately draws samples of the conditionally independent latent mixture assignments and the conditionally multivariate Gaussian continuous vector and we show that it can directly benefit from the proposed methods.</p>
<p>【Keywords】:</p>
<h3 id="208. Large Margin Multi-Task Metric Learning.">208. Large Margin Multi-Task Metric Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3935-large-margin-multi-task-metric-learning">Paper Link</a>】    【Pages】:1867-1875</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Parameswaran:Shibin">Shibin Parameswaran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a></p>
<p>【Abstract】:
Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to svms by Evgeniou et al. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (lmnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multitask learning. We evaluate the resulting multi-task lmnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.</p>
<p>【Keywords】:</p>
<h3 id="209. Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers.">209. Multiparty Differential Privacy via Aggregation of Locally Trained Classifiers.</h3>
<p>【<a href="http://papers.nips.cc/paper/4034-multiparty-differential-privacy-via-aggregation-of-locally-trained-classifiers">Paper Link</a>】    【Pages】:1876-1884</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pathak:Manas_A=">Manas A. Pathak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rane:Shantanu">Shantanu Rane</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raj:Bhiksha">Bhiksha Raj</a></p>
<p>【Abstract】:
As increasing amounts of sensitive personal information finds its way into data repositories, it is important to develop analysis mechanisms that can derive aggregate information from these repositories without revealing information about individual data instances. Though the differential privacy model  provides a framework to analyze such mechanisms for databases belonging to a single party, this framework has not yet been considered in a multi-party setting. In this paper, we propose a privacy-preserving protocol for composing a differentially private aggregate classifier  using classifiers trained locally by separate mutually untrusting parties. The protocol allows these parties to interact with an untrusted curator to construct additive shares of a perturbed aggregate classifier. We also present a detailed theoretical analysis containing a proof of differential privacy  of the perturbed aggregate classifier and a bound on the excess risk introduced by the perturbation. We verify the bound with an experimental evaluation on a real dataset.</p>
<p>【Keywords】:</p>
<h3 id="210. (RF)^2 - Random Forest Random Field.">210. (RF)^2 - Random Forest Random Field.</h3>
<p>【<a href="http://papers.nips.cc/paper/4140-rf2-random-forest-random-field">Paper Link</a>】    【Pages】:1885-1893</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Payet:Nadia">Nadia Payet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Todorovic:Sinisa">Sinisa Todorovic</a></p>
<p>【Abstract】:
We combine random forest (RF) and conditional random field (CRF) into a new computational framework, called random forest random field (RF)^2. Inference of (RF)^2 uses the Swendsen-Wang cut algorithm, characterized by Metropolis-Hastings jumps. A jump from one state to another depends on the ratio of the proposal distributions, and on the ratio of the posterior distributions of the two states. Prior work typically resorts to a parametric estimation of these four distributions, and then computes their ratio. Our key idea is to instead directly estimate these ratios using RF. RF collects in leaf nodes of each decision tree the class histograms of training examples. We use these class histograms for a non-parametric estimation of the distribution ratios. We derive the theoretical error bounds of a two-class (RF)^2. (RF)^2 is applied to a challenging task of multiclass object recognition and segmentation over a random field of input image regions. In our empirical evaluation, we use only the visual information provided by image regions (e.g., color, texture, spatial layout), whereas the competing methods additionally use higher-level cues about the horizon location and 3D layout of surfaces in the scene. Nevertheless, (RF)^2 outperforms the state of the art on benchmark datasets, in terms of accuracy and computation time.</p>
<p>【Keywords】:</p>
<h3 id="211. On the Theory of Learnining with Privileged Information.">211. On the Theory of Learnining with Privileged Information.</h3>
<p>【<a href="http://papers.nips.cc/paper/3960-on-the-theory-of-learnining-with-privileged-information">Paper Link</a>】    【Pages】:1894-1902</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pechyony:Dmitry">Dmitry Pechyony</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vapnik:Vladimir">Vladimir Vapnik</a></p>
<p>【Abstract】:
In Learning Using Privileged Information (LUPI) paradigm, along with the standard training data in the decision space, a teacher supplies a learner with the privileged information in the correcting space. The goal of the learner is to find a classifier with a low generalization error in the decision space. We consider a new version of  empirical risk minimization algorithm, called Privileged ERM, that takes into account the privileged information in order to find a good function in the decision space. We outline the conditions on the correcting space that, if satisfied, allow Privileged ERM to have much faster learning rate in the decision space than the one of the regular empirical risk minimization.</p>
<p>【Keywords】:</p>
<h3 id="212. Empirical Bernstein Inequalities for U-Statistics.">212. Empirical Bernstein Inequalities for U-Statistics.</h3>
<p>【<a href="http://papers.nips.cc/paper/4081-empirical-bernstein-inequalities-for-u-statistics">Paper Link</a>】    【Pages】:1903-1911</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Peel:Thomas">Thomas Peel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Anthoine:Sandrine">Sandrine Anthoine</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ralaivola:Liva">Liva Ralaivola</a></p>
<p>【Abstract】:
We present original empirical Bernstein inequalities for U-statistics with bounded symmetric kernels q. They are expressed with respect to empirical estimates of either the variance of q or the conditional variance that appears in the Bernstein-type inequality for U-statistics derived by Arcones [2]. Our result subsumes other existing empirical Bernstein inequalities, as it reduces to them when U-statistics of order 1 are considered. In addition, it is based on a rather direct argument using two applications of the same (non-empirical) Bernstein inequality for U-statistics. We discuss potential applications of our new inequalities, especially in the realm of learning ranking/scoring functions. In the process, we exhibit an efficient procedure to compute the variance estimates for the special case of bipartite ranking that rests on a sorting argument. We also argue that our results may provide test set bounds and particularly interesting empirical racing algorithms for the problem of online learning of scoring functions.</p>
<p>【Keywords】:</p>
<h3 id="213. Learning Networks of Stochastic Differential Equations.">213. Learning Networks of Stochastic Differential Equations.</h3>
<p>【<a href="http://papers.nips.cc/paper/4055-learning-networks-of-stochastic-differential-equations">Paper Link</a>】    【Pages】:172-180</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bento:Jos=eacute=">José Bento</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ibrahimi:Morteza">Morteza Ibrahimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Montanari:Andrea">Andrea Montanari</a></p>
<p>【Abstract】:
We consider linear models for stochastic dynamics. Any such model can be associated a network (namely a directed graph) describing which degrees of freedom interact under the dynamics. We tackle the problem of learning such a network from observation of the system trajectory over a time interval T. We analyse the l1-regularized least squares algorithm and, in the setting in which the underlying network is sparse, we prove performance guarantees that are uniform in the sampling rate as long as this is sufficiently high. This result substantiates the notion of a well defined ‘time complexity’ for the network inference problem.</p>
<p>【Keywords】:</p>
<h3 id="214. Word Features for Latent Dirichlet Allocation.">214. Word Features for Latent Dirichlet Allocation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4094-word-features-for-latent-dirichlet-allocation">Paper Link</a>】    【Pages】:1921-1929</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Petterson:James">James Petterson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caetano:Tib=eacute=rio_S=">Tibério S. Caetano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Buntine:Wray_L=">Wray L. Buntine</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Narayanamurthy:Shravan_M=">Shravan M. Narayanamurthy</a></p>
<p>【Abstract】:
We extend Latent Dirichlet Allocation (LDA) by explicitly allowing for the encoding of side information in the distribution over words. This results in a variety of new capabilities, such as improved estimates for infrequently occurring words, as well as the ability to leverage thesauri and dictionaries in order to boost topic cohesion within and across languages. We present experiments on multi-language topic synchronisation where dictionary information is used to bias corresponding words towards similar topics. Results indicate that our model substantially improves topic cohesion when compared to the standard LDA model.</p>
<p>【Keywords】:</p>
<h3 id="215. Reverse Multi-Label Learning.">215. Reverse Multi-Label Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3920-reverse-multi-label-learning">Paper Link</a>】    【Pages】:1912-1920</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Petterson:James">James Petterson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caetano:Tib=eacute=rio_S=">Tibério S. Caetano</a></p>
<p>【Abstract】:
Multi-label classification is the task of predicting potentially multiple labels for a given instance. This is common in several applications such as image annotation, document classification and gene function prediction. In this paper we present a formulation for this problem based on reverse prediction: we predict sets of instances given the labels. By viewing the problem from this perspective, the most popular quality measures for assessing the performance of multi-label classification admit relaxations that can be efficiently optimised. We optimise these relaxations with standard algorithms and compare our results with several state-of-the-art methods, showing excellent performance.</p>
<p>【Keywords】:</p>
<h3 id="216. Probabilistic Deterministic Infinite Automata.">216. Probabilistic Deterministic Infinite Automata.</h3>
<p>【<a href="http://papers.nips.cc/paper/4161-probabilistic-deterministic-infinite-automata">Paper Link</a>】    【Pages】:1930-1938</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pfau:David">David Pfau</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bartlett:Nicholas">Nicholas Bartlett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wood:Frank">Frank Wood</a></p>
<p>【Abstract】:
We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop and sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.</p>
<p>【Keywords】:</p>
<h3 id="217. The Maximal Causes of Natural Scenes are Edge Filters.">217. The Maximal Causes of Natural Scenes are Edge Filters.</h3>
<p>【<a href="http://papers.nips.cc/paper/4132-the-maximal-causes-of-natural-scenes-are-edge-filters">Paper Link</a>】    【Pages】:1939-1947</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Puertas:Gervasio">Gervasio Puertas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bornschein:J=ouml=rg">Jörg Bornschein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/L=uuml=cke:J=ouml=rg">Jörg Lücke</a></p>
<p>【Abstract】:
We study the application of a strongly non-linear generative model to image patches. As in standard approaches such as Sparse Coding or Independent Component Analysis, the model assumes a sparse prior with independent hidden variables. However, in the place where standard approaches use the sum to combine basis functions we use the maximum. To derive tractable approximations for parameter estimation we apply a novel approach based on variational Expectation Maximization. The derived learning algorithm can be applied to large-scale problems with hundreds of observed and hidden variables. Furthermore, we can infer all model parameters including observation noise and the degree of sparseness. In applications to image patches we find that Gabor-like basis functions are obtained. Gabor-like functions are thus not a feature exclusive to approaches assuming linear superposition. Quantitatively, the inferred basis functions show a large diversity of shapes with many strongly elongated and many circular symmetric functions. The distribution of basis function shapes reflects properties of simple cell receptive fields that are not reproduced by standard linear approaches. In the study of natural image statistics, the implications of using different superposition assumptions have so far not been investigated systematically because models with strong non-linearities have been found analytically and computationally challenging. The presented algorithm represents the first large-scale application of such an approach.</p>
<p>【Keywords】:</p>
<h3 id="218. A New Probabilistic Model for Rank Aggregation.">218. A New Probabilistic Model for Rank Aggregation.</h3>
<p>【<a href="http://papers.nips.cc/paper/3906-a-new-probabilistic-model-for-rank-aggregation">Paper Link</a>】    【Pages】:1948-1956</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qin:Tao">Tao Qin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Geng:Xiubo">Xiubo Geng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a></p>
<p>【Abstract】:
This paper is concerned with rank aggregation, which aims to combine multiple input rankings to get a better ranking. A popular approach to rank aggregation is based on probabilistic models on permutations, e.g., the Luce model and the Mallows model. However, these models have their limitations in either poor expressiveness or high computational complexity. To avoid these limitations, in this paper, we propose a new model, which is defined with a coset-permutation distance, and models the generation of a permutation as a stagewise process. We refer to the new model as coset-permutation distance based stagewise (CPS) model. The CPS model has rich expressiveness and can therefore be used in versatile applications, because many different permutation distances can be used to induce the coset-permutation distance. The complexity of the CPS model is low because of the stagewise decomposition of the permutation probability and the efficient computation of most coset-permutation distances. We apply the CPS model to supervised rank aggregation, derive the learning and inference algorithms, and empirically study their effectiveness and efficiency. Experiments on public datasets show that the derived algorithms based on the CPS model can achieve state-of-the-art ranking accuracy, and are much more efficient than previous algorithms.</p>
<p>【Keywords】:</p>
<h3 id="219. Multitask Learning without Label Correspondences.">219. Multitask Learning without Label Correspondences.</h3>
<p>【<a href="http://papers.nips.cc/paper/3990-multitask-learning-without-label-correspondences">Paper Link</a>】    【Pages】:1957-1965</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Quadrianto:Novi">Novi Quadrianto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caetano:Tib=eacute=rio_S=">Tibério S. Caetano</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:S=_V=_N=">S. V. N. Vishwanathan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Petterson:James">James Petterson</a></p>
<p>【Abstract】:
We propose an algorithm to perform multitask learning where each task has potentially distinct label sets and label correspondences are not readily available. This is in contrast with existing methods which either assume that the label sets shared by different tasks are the same or that there exists a label mapping oracle. Our method directly maximizes the mutual information among the labels, and we show that the resulting objective function can be efficiently optimized using existing algorithms. Our proposed approach has a direct application for data integration with different label spaces for the purpose of classification, such as integrating Yahoo! and DMOZ web directories.</p>
<p>【Keywords】:</p>
<h3 id="220. Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics.">220. Inferring Stimulus Selectivity from the Spatial Structure of Neural Network Dynamics.</h3>
<p>【<a href="http://papers.nips.cc/paper/4051-inferring-stimulus-selectivity-from-the-spatial-structure-of-neural-network-dynamics">Paper Link</a>】    【Pages】:1975-1983</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rajan:Kanaka">Kanaka Rajan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Abbott:L=">L. Abbott</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sompolinsky:Haim">Haim Sompolinsky</a></p>
<p>【Abstract】:
How are the spatial patterns of spontaneous and evoked population responses related? We study the impact of connectivity on the spatial pattern of fluctuations in the input-generated response of a neural network, by comparing the distribution of evoked and intrinsically generated activity across the different units. We develop a complementary approach to principal component analysis in which separate high-variance directions are typically derived for each input condition. We analyze subspace angles to compute the difference between the shapes of trajectories corresponding to different network states, and the orientation of the low-dimensional subspaces that driven trajectories occupy within the full space of neuronal activity. In addition to revealing how the spatiotemporal structure of spontaneous activity affects input-evoked responses, these methods can be used to infer input selectivity induced by network dynamics from experimentally accessible measures of spontaneous activity (e.g. from voltage- or calcium-sensitive optical imaging experiments). We conclude that the absence of a detailed spatial map of afferent inputs and cortical connectivity does not limit our ability to design spatially extended stimuli that evoke strong responses.</p>
<p>【Keywords】:</p>
<h3 id="221. Online Learning: Random Averages, Combinatorial Parameters, and Learnability.">221. Online Learning: Random Averages, Combinatorial Parameters, and Learnability.</h3>
<p>【<a href="http://papers.nips.cc/paper/4116-online-learning-random-averages-combinatorial-parameters-and-learnability">Paper Link</a>】    【Pages】:1984-1992</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rakhlin:Alexander">Alexander Rakhlin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sridharan:Karthik">Karthik Sridharan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tewari:Ambuj">Ambuj Tewari</a></p>
<p>【Abstract】:
We develop a theory of online learning by defining several complexity measures. Among them are analogues of Rademacher complexity, covering numbers and fat-shattering dimension from statistical learning theory. Relationship among these complexity measures, their connection to online learning, and tools for bounding them are provided. We apply these results to various learning problems. We provide a complete characterization of online learnability in the supervised setting.</p>
<p>【Keywords】:</p>
<h3 id="222. Evaluating neuronal codes for inference using Fisher information.">222. Evaluating neuronal codes for inference using Fisher information.</h3>
<p>【<a href="http://papers.nips.cc/paper/4066-evaluating-neuronal-codes-for-inference-using-fisher-information">Paper Link</a>】    【Pages】:1993-2001</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Haefner:Ralf_M=">Ralf M. Haefner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bethge:Matthias">Matthias Bethge</a></p>
<p>【Abstract】:
Many studies have explored the impact of response variability on the quality of sensory codes. The source of this variability is almost always assumed to be intrinsic to the brain. However, when inferring a particular stimulus property, variability associated with other stimulus attributes also effectively act as noise. Here we study the impact of such stimulus-induced response variability for the case of binocular disparity inference. We characterize the response distribution for the binocular energy model in response to random dot stereograms and find it to be very different from the Poisson-like noise usually assumed. We then compute the Fisher information with respect to binocular disparity, present in the monocular inputs to the standard model of early binocular processing, and thereby obtain an upper bound on how much information a model could theoretically extract from them. Then we analyze the information loss incurred by the different ways of combining those inputs to produce a scalar single-neuron response. We find that in the case of depth inference, monocular stimulus variability places a greater limit on the extractable information than intrinsic neuronal noise for typical spike counts. Furthermore, the largest loss of information is incurred by the standard model for position disparity neurons (tuned-excitatory), that are the most ubiquitous in monkey primary visual cortex, while more information from the inputs is preserved in phase-disparity neurons (tuned-near or tuned-far) primarily found in higher cortical regions.</p>
<p>【Keywords】:</p>
<h3 id="223. Generating more realistic images using gated MRF's.">223. Generating more realistic images using gated MRF's.</h3>
<p>【<a href="http://papers.nips.cc/paper/4138-generating-more-realistic-images-using-gated-mrfs">Paper Link</a>】    【Pages】:2002-2010</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ranzato:Marc=Aurelio">Marc'Aurelio Ranzato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mnih:Volodymyr">Volodymyr Mnih</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hinton:Geoffrey_E=">Geoffrey E. Hinton</a></p>
<p>【Abstract】:
Probabilistic models of natural images are usually evaluated by measuring performance on rather indirect tasks, such as denoising and inpainting. A more direct way to evaluate a generative model is to draw samples from it and to check whether statistical properties of the samples match the statistics of natural images. This method is seldom used with high-resolution images, because current models produce samples that are very different from natural images, as assessed by even simple visual inspection. We investigate the reasons for this failure and we show that by augmenting existing models so that there are two sets of latent variables, one set modelling pixel intensities and the other set modelling image-specific pixel covariances, we are able to generate high-resolution images that look much more realistic than before. The overall model can be interpreted as a gated MRF where both pair-wise dependencies and mean intensities of pixels are modulated by the states of latent variables. Finally, we confirm that if we disallow weight-sharing between receptive fields that overlap each other, the gated MRF learns more efficient internal representations, as demonstrated in several recognition tasks.</p>
<p>【Keywords】:</p>
<h3 id="224. An Approximate Inference Approach to Temporal Optimization in Optimal Control.">224. An Approximate Inference Approach to Temporal Optimization in Optimal Control.</h3>
<p>【<a href="http://papers.nips.cc/paper/4018-an-approximate-inference-approach-to-temporal-optimization-in-optimal-control">Paper Link</a>】    【Pages】:2011-2019</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rawlik:Konrad">Konrad Rawlik</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Toussaint:Marc">Marc Toussaint</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vijayakumar:Sethu">Sethu Vijayakumar</a></p>
<p>【Abstract】:
Algorithms based on iterative local approximations present a practical approach to optimal control in robotic systems. However, they generally require the temporal parameters (for e.g. the movement duration or the time point of reaching an intermediate goal) to be specified \textit{a priori}. Here, we present a methodology that is capable of jointly optimising the temporal parameters in addition to the control command profiles. The presented approach is based on a Bayesian canonical time formulation of the optimal control problem, with the temporal mapping from canonical to real time parametrised by an additional control variable. An approximate EM algorithm is derived that efficiently optimises both the movement duration and control commands offering, for the first time, a practical approach to tackling generic via point problems in a systematic way under the optimal control framework. The proposed approach is evaluated on simulations of a redundant robotic plant.</p>
<p>【Keywords】:</p>
<h3 id="225. MAP estimation in Binary MRFs via Bipartite Multi-cuts.">225. MAP estimation in Binary MRFs via Bipartite Multi-cuts.</h3>
<p>【<a href="http://papers.nips.cc/paper/4170-map-estimation-in-binary-mrfs-via-bipartite-multi-cuts">Paper Link</a>】    【Pages】:955-963</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Reddi:Sashank_Jakkam">Sashank Jakkam Reddi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarawagi:Sunita">Sunita Sarawagi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:Sundar">Sundar Vishwanathan</a></p>
<p>【Abstract】:
Abstract Missing</p>
<p>【Keywords】:</p>
<h3 id="226. Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model.">226. Hallucinations in Charles Bonnet Syndrome Induced by Homeostasis: a Deep Boltzmann Machine Model.</h3>
<p>【<a href="http://papers.nips.cc/paper/4097-hallucinations-in-charles-bonnet-syndrome-induced-by-homeostasis-a-deep-boltzmann-machine-model">Paper Link</a>】    【Pages】:2020-2028</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Reichert:David_P=">David P. Reichert</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seri=egrave=s:Peggy">Peggy Seriès</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Storkey:Amos_J=">Amos J. Storkey</a></p>
<p>【Abstract】:
The Charles Bonnet Syndrome (CBS) is characterized by complex vivid visual hallucinations in people with, primarily, eye diseases and no other neurological pathology. We present a Deep Boltzmann Machine model of CBS, exploring two core hypotheses: First, that the visual cortex learns a generative or predictive model of sensory input, thus explaining its capability to generate internal imagery. And second, that homeostatic mechanisms stabilize neuronal activity levels, leading to hallucinations being formed when input is lacking. We reproduce a variety of qualitative findings in CBS. We also introduce a modification to the DBM that allows us to model a possible role of acetylcholine in CBS as mediating the balance of feed-forward and feed-back processing. Our model might provide new insights into CBS and also demonstrates that generative frameworks are promising as hypothetical models of cortical learning and perception.</p>
<p>【Keywords】:</p>
<h3 id="227. Link Discovery using Graph Feature Tracking.">227. Link Discovery using Graph Feature Tracking.</h3>
<p>【<a href="http://papers.nips.cc/paper/3911-link-discovery-using-graph-feature-tracking">Paper Link</a>】    【Pages】:1966-1974</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Richard:Emile">Emile Richard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Baskiotis:Nicolas">Nicolas Baskiotis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Evgeniou:Theodoros">Theodoros Evgeniou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vayatis:Nicolas">Nicolas Vayatis</a></p>
<p>【Abstract】:
We consider the problem of discovering links of an evolving undirected graph given a series of past snapshots of that graph. The graph is observed through the time sequence of its adjacency matrix and only the presence of edges is observed. The absence of an edge on a certain snapshot cannot be distinguished from a missing entry in the adjacency matrix. Additional information can be provided by examining the dynamics of the graph through a set of topological features, such as the degrees of the vertices.  We develop a novel methodology by building on both static matrix completion methods and the estimation of the future state of relevant graph features. Our procedure relies on the formulation of an optimization problem which can be approximately solved by a fast alternating linearized algorithm whose properties are examined. We  show experiments with both simulated and real data which reveal the interest of our methodology.</p>
<p>【Keywords】:</p>
<h3 id="228. An Alternative to Low-level-Sychrony-Based Methods for Speech Detection.">228. An Alternative to Low-level-Sychrony-Based Methods for Speech Detection.</h3>
<p>【<a href="http://papers.nips.cc/paper/3915-an-alternative-to-low-level-sychrony-based-methods-for-speech-detection">Paper Link</a>】    【Pages】:2029-2037</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ruvolo:Paul">Paul Ruvolo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Movellan:Javier_R=">Javier R. Movellan</a></p>
<p>【Abstract】:
Determining whether someone is talking has applications in many areas such as speech recognition, speaker diarization, social robotics, facial expression recognition, and human computer interaction.  One popular approach to this problem is audio-visual synchrony detection. A candidate speaker is deemed to be talking if the visual signal around that speaker correlates with the auditory signal.  Here we show that with the proper visual features (in this case movements of various facial muscle groups), a very accurate detector of speech can be created that does not use the audio signal at all.  Further we show that this person independent visual-only detector can be used to train very accurate audio-based person dependent voice models.  The voice model has the advantage of being able to identify when a particular person is speaking even when they are not visible to the camera (e.g. in the case of a mobile robot).  Moreover, we show that a simple sensory fusion scheme between the auditory and visual models improves performance on the task of talking detection.  The work here provides dramatic evidence about the efficacy of two very different approaches to multimodal speech detection on a challenging database.</p>
<p>【Keywords】:</p>
<h3 id="229. Tight Sample Complexity of Large-Margin Learning.">229. Tight Sample Complexity of Large-Margin Learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/4032-tight-sample-complexity-of-large-margin-learning">Paper Link</a>】    【Pages】:2038-2046</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sabato:Sivan">Sivan Sabato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srebro:Nathan">Nathan Srebro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tishby:Naftali">Naftali Tishby</a></p>
<p>【Abstract】:
We obtain a tight distribution-specific characterization of the sample complexity of large-margin classification with L2 regularization: We introduce the gamma-adapted-dimension, which is a simple function of the spectrum of a distribution's covariance matrix, and show distribution-specific upper and lower bounds on the sample complexity, both governed by the gamma-adapted-dimension of the source distribution. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classification. The bounds hold for a rich family of sub-Gaussian distributions.</p>
<p>【Keywords】:</p>
<h3 id="230. Boosting Classifier Cascades.">230. Boosting Classifier Cascades.</h3>
<p>【<a href="http://papers.nips.cc/paper/4033-boosting-classifier-cascades">Paper Link</a>】    【Pages】:2047-2055</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Saberian:Mohammad_J=">Mohammad J. Saberian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vasconcelos:Nuno">Nuno Vasconcelos</a></p>
<p>【Abstract】:
The problem of optimal and automatic design of a detector cascade is considered. A novel mathematical model is introduced for a cascaded detector. This model is analytically tractable, leads to recursive computation, and accounts for both classification and complexity. A boosting algorithm, FCBoost, is proposed for fully automated cascade design. It exploits the new cascade model, minimizes a Lagrangian cost that accounts for both classification risk and complexity. It searches the space of cascade configurations to automatically determine the optimal number of stages and their predictors, and is compatible with bootstrapping of negative examples and cost sensitive learning. Experiments show that the resulting cascades have state-of-the-art performance in various computer vision problems.</p>
<p>【Keywords】:</p>
<h3 id="231. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm.">231. Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm.</h3>
<p>【<a href="http://papers.nips.cc/paper/4102-collaborative-filtering-in-a-non-uniform-world-learning-with-the-weighted-trace-norm">Paper Link</a>】    【Pages】:2056-2064</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srebro:Nathan">Nathan Srebro</a></p>
<p>【Abstract】:
We show that matrix completion with trace-norm regularization can be significantly hurt when entries of the matrix are sampled non-uniformly, but that a properly weighted version of the trace-norm regularizer works well with non-uniform sampling. We show that the weighted trace-norm regularization indeed yields significant gains on the highly non-uniformly sampled Netflix dataset.</p>
<p>【Keywords】:</p>
<h3 id="232. Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation.">232. Implicitly Constrained Gaussian Process Regression for Monocular Non-Rigid Pose Estimation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4179-implicitly-constrained-gaussian-process-regression-for-monocular-non-rigid-pose-estimation">Paper Link</a>】    【Pages】:2065-2073</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Salzmann:Mathieu">Mathieu Salzmann</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Urtasun:Raquel">Raquel Urtasun</a></p>
<p>【Abstract】:
Estimating 3D pose from monocular images is a highly ambiguous problem. Physical constraints can be exploited to restrict the space of feasible configurations. In this paper we propose an approach to constraining the prediction of a discriminative predictor. We first show that the mean prediction of a Gaussian process implicitly satisfies linear constraints if those constraints are satisfied by the training examples. We then show how, by performing a change of variables, a GP can be forced to satisfy quadratic constraints. As evidenced by the experiments, our method outperforms state-of-the-art approaches on the tasks of rigid and non-rigid pose estimation.</p>
<p>【Keywords】:</p>
<h3 id="233. Deterministic Single-Pass Algorithm for LDA.">233. Deterministic Single-Pass Algorithm for LDA.</h3>
<p>【<a href="http://papers.nips.cc/paper/3959-deterministic-single-pass-algorithm-for-lda">Paper Link</a>】    【Pages】:2074-2082</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sato:Issei">Issei Sato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurihara:Kenichi">Kenichi Kurihara</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nakagawa:Hiroshi">Hiroshi Nakagawa</a></p>
<p>【Abstract】:
We develop a deterministic single-pass algorithm for latent Dirichlet allocation (LDA) in order to process received documents one at a time and then discard them in an excess text stream. Our algorithm does not need to store old statistics for all data. The proposed algorithm is much faster than a batch algorithm and is comparable to the batch algorithm in terms of perplexity in experiments.</p>
<p>【Keywords】:</p>
<h3 id="234. Active Estimation of F-Measures.">234. Active Estimation of F-Measures.</h3>
<p>【<a href="http://papers.nips.cc/paper/3999-active-estimation-of-f-measures">Paper Link</a>】    【Pages】:2083-2091</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sawade:Christoph">Christoph Sawade</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Landwehr:Niels">Niels Landwehr</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scheffer:Tobias">Tobias Scheffer</a></p>
<p>【Abstract】:
We address the problem of estimating the F-measure of a given model as accurately as possible on a fixed labeling budget. This problem occurs whenever an estimate cannot be obtained from held-out training data; for instance, when data that have been used to train the model are held back for reasons of privacy or do not reflect the test distribution. In this case, new test instances have to be drawn and labeled at a cost. An active estimation procedure selects instances according to an instrumental sampling distribution. An analysis of the sources of estimation error leads to an optimal sampling distribution that minimizes estimator variance. We explore conditions under which active estimates of F-measures are more accurate than estimates based on instances sampled from the test distribution.</p>
<p>【Keywords】:</p>
<h3 id="235. Trading off Mistakes and Don't-Know Predictions.">235. Trading off Mistakes and Don't-Know Predictions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4142-trading-off-mistakes-and-dont-know-predictions">Paper Link</a>】    【Pages】:2092-2100</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sayedi:Amin">Amin Sayedi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zadimoghaddam:Morteza">Morteza Zadimoghaddam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blum:Avrim">Avrim Blum</a></p>
<p>【Abstract】:
We discuss an online learning framework in which the agent is allowed to say <code>I don't know'' as well as making incorrect predictions on given examples. We analyze the trade off between saying</code>I don't know'' and making mistakes. If the number of don't know predictions is forced to be zero, the model reduces to the well-known mistake-bound model introduced by Littlestone [Lit88]. On the other hand, if no mistakes are allowed, the model reduces to KWIK framework introduced by Li et. al. [LLW08]. We propose a general, though inefficient, algorithm for general finite concept classes that minimizes the number of don't-know predictions if a certain number of mistakes are allowed. We then present specific polynomial-time algorithms for the concept classes of monotone disjunctions and linear separators.</p>
<p>【Keywords】:</p>
<h3 id="236. Sparse Inverse Covariance Selection via Alternating Linearization Methods.">236. Sparse Inverse Covariance Selection via Alternating Linearization Methods.</h3>
<p>【<a href="http://papers.nips.cc/paper/4099-sparse-inverse-covariance-selection-via-alternating-linearization-methods">Paper Link</a>】    【Pages】:2101-2109</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Scheinberg:Katya">Katya Scheinberg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Shiqian">Shiqian Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goldfarb:Donald">Donald Goldfarb</a></p>
<p>【Abstract】:
Gaussian graphical models are of great interest in statistical learning.  Because the conditional independencies between different nodes correspond to zero entries in the inverse covariance matrix of the Gaussian distribution, one can learn the structure of the graph by estimating a sparse inverse covariance matrix from sample data, by solving a convex maximum likelihood problem with an $\ell_1$-regularization term. In this paper, we propose a first-order method based on an alternating linearization technique that exploits the problem's special structure; in particular, the subproblems solved in each iteration have closed-form solutions. Moreover, our algorithm obtains an $\epsilon$-optimal solution in $O(1/\epsilon)$ iterations. Numerical experiments on both synthetic  and real data from gene association networks show that a practical version of  this algorithm outperforms other competitive algorithms.</p>
<p>【Keywords】:</p>
<h3 id="237. Spike timing-dependent plasticity as dynamic filter.">237. Spike timing-dependent plasticity as dynamic filter.</h3>
<p>【<a href="http://papers.nips.cc/paper/3917-spike-timing-dependent-plasticity-as-dynamic-filter">Paper Link</a>】    【Pages】:2110-2118</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Schmiedt:Joscha">Joscha Schmiedt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Albers:Christian">Christian Albers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pawelzik:Klaus">Klaus Pawelzik</a></p>
<p>【Abstract】:
When stimulated with complex action potential sequences synapses exhibit spike timing-dependent plasticity (STDP) with attenuated and enhanced pre- and postsynaptic contributions to long-term synaptic modifications. In order to investigate the functional consequences of these contribution dynamics (CD) we propose a minimal model formulated in terms of differential equations. We find that our model reproduces a wide range of experimental results with a small number of biophysically interpretable parameters. The model allows to investigate the susceptibility of STDP to arbitrary time courses of pre- and postsynaptic activities, i.e. its nonlinear filter properties. We demonstrate this for the simple example of small periodic modulations of pre- and postsynaptic firing rates for which our model can be solved. It predicts synaptic strengthening for synchronous rate modulations. For low baseline rates modifications are dominant in the theta frequency range, a result which underlines the well known relevance of theta activities in hippocampus and cortex for learning. We also find emphasis of low baseline spike rates and suppression for high baseline rates. The latter suggests a mechanism of network activity regulation inherent in STDP. Furthermore, our novel formulation provides a general framework for investigating the joint dynamics of neuronal activity and the CD of STDP in both spike-based as well as rate-based neuronal network models.</p>
<p>【Keywords】:</p>
<h3 id="238. A novel family of non-parametric cumulative based divergences for point processes.">238. A novel family of non-parametric cumulative based divergences for point processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/4126-a-novel-family-of-non-parametric-cumulative-based-divergences-for-point-processes">Paper Link</a>】    【Pages】:2119-2127</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Seth:Sohan">Sohan Seth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Park:Il_Memming">Il Memming Park</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brockmeier:Austin_J=">Austin J. Brockmeier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Semework:Mulugeta">Mulugeta Semework</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Choi:John_S=">John S. Choi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Francis:Joseph_T=">Joseph T. Francis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pr=iacute=ncipe:Jos=eacute=_Carlos">José Carlos Príncipe</a></p>
<p>【Abstract】:
Hypothesis testing on point processes has several applications such as model fitting, plasticity detection, and non-stationarity detection. Standard tools for hypothesis testing include tests on mean firing rate and time varying rate function. However, these statistics do not fully describe a point process and thus the tests can be misleading. In this paper, we introduce a family of non-parametric divergence measures for hypothesis testing. We extend the traditional Kolmogorov--Smirnov and Cramer--von-Mises tests for point process via stratification. The proposed divergence measures compare the underlying probability structure and, thus, is zero if and only if the point processes are the same. This leads to a more robust test of hypothesis. We prove consistency and show that these measures can be efficiently estimated from data. We demonstrate an application of using the proposed divergence as a cost function to find optimally matched spike trains.</p>
<p>【Keywords】:</p>
<h3 id="239. Online Learning in The Manifold of Low-Rank Matrices.">239. Online Learning in The Manifold of Low-Rank Matrices.</h3>
<p>【<a href="http://papers.nips.cc/paper/4084-online-learning-in-the-manifold-of-low-rank-matrices">Paper Link</a>】    【Pages】:2128-2136</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shalit:Uri">Uri Shalit</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinshall:Daphna">Daphna Weinshall</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chechik:Gal">Gal Chechik</a></p>
<p>【Abstract】:
When learning models that are represented in matrix forms, enforcing   a low-rank constraint can dramatically improve the memory and run   time complexity, while providing a natural regularization of the   model.  However, naive approaches for minimizing functions over the   set of low-rank matrices are either prohibitively time   consuming (repeated singular value decomposition of the matrix) or   numerically unstable (optimizing a factored representation of the   low rank matrix). We build on recent advances in optimization over   manifolds, and describe an iterative online learning procedure, consisting of a   gradient step, followed by a second-order retraction back to the   manifold. While the ideal retraction is hard to compute, and so is   the projection operator that approximates it, we describe another   second-order retraction that can be computed efficiently, with run   time and memory complexity of O((n+m)k) for a rank-k   matrix of dimension m x n, given rank one gradients.  We use   this algorithm, LORETA, to learn a matrix-form similarity measure over pairs of   documents represented as high dimensional vectors. LORETA   improves the mean average precision over a passive-   aggressive approach in a factorized model, and also improves over   a full model trained over pre-selected features using the same   memory requirements. LORETA also showed consistent improvement over   standard methods in a large (1600 classes) multi-label image classification task.</p>
<p>【Keywords】:</p>
<h3 id="240. Identifying graph-structured activation patterns in networks.">240. Identifying graph-structured activation patterns in networks.</h3>
<p>【<a href="http://papers.nips.cc/paper/4075-identifying-graph-structured-activation-patterns-in-networks">Paper Link</a>】    【Pages】:2137-2145</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sharpnack:James">James Sharpnack</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Aarti">Aarti Singh</a></p>
<p>【Abstract】:
We consider the problem of identifying an activation pattern in a complex, large-scale network that is embedded in very noisy measurements. This problem is relevant to several applications, such as identifying traces of a biochemical spread by a sensor network, expression levels of genes, and anomalous activity or congestion in the Internet. Extracting such patterns is a challenging task specially if the network is large (pattern is very high-dimensional) and the noise is so excessive that it masks the activity at any single node. However, typically there are statistical dependencies in the network activation process that can be leveraged to fuse the measurements of multiple nodes and enable reliable extraction of high-dimensional noisy patterns. In this paper, we analyze an estimator based on the graph Laplacian eigenbasis, and establish the limits of mean square error recovery of noisy patterns arising from a probabilistic (Gaussian or Ising) model based on an arbitrary graph structure. We consider both deterministic and probabilistic network evolution models, and our results indicate that by leveraging the network interaction structure, it is possible to consistently recover high-dimensional patterns even when the noise variance increases with network size.</p>
<p>【Keywords】:</p>
<h3 id="241. A rational decision making framework for inhibitory control.">241. A rational decision making framework for inhibitory control.</h3>
<p>【<a href="http://papers.nips.cc/paper/3937-a-rational-decision-making-framework-for-inhibitory-control">Paper Link</a>】    【Pages】:2146-2154</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shenoy:Pradeep">Pradeep Shenoy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rao:Rajesh_P=_N=">Rajesh P. N. Rao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Angela_J=">Angela J. Yu</a></p>
<p>【Abstract】:
Intelligent agents are often faced with the need to choose actions with uncertain consequences, and to modify those actions according to ongoing sensory processing and changing task demands. The requisite ability to dynamically modify or cancel planned actions is known as inhibitory control in psychology. We formalize inhibitory control as a rational decision-making problem, and apply to it to the classical stop-signal task. Using Bayesian inference and stochastic control tools, we show that the optimal policy systematically depends on various parameters of the problem, such as the relative costs of different action choices, the noise level of sensory inputs, and the dynamics of changing environmental demands. Our normative model accounts for a range of behavioral data in humans and animals in the stop-signal task, suggesting that the brain implements statistically optimal, dynamically adaptive, and reward-sensitive decision-making in the context of inhibitory control problems.</p>
<p>【Keywords】:</p>
<h3 id="242. Penalized Principal Component Regression on Graphs for Analysis of Subnetworks.">242. Penalized Principal Component Regression on Graphs for Analysis of Subnetworks.</h3>
<p>【<a href="http://papers.nips.cc/paper/3957-penalized-principal-component-regression-on-graphs-for-analysis-of-subnetworks">Paper Link</a>】    【Pages】:2155-2163</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shojaie:Ali">Ali Shojaie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Michailidis:George">George Michailidis</a></p>
<p>【Abstract】:
Network models are widely used to capture interactions among component of complex systems, such as social and biological. To understand their behavior, it is often necessary to analyze functionally related components of the system, corresponding to subsystems. Therefore, the analysis of subnetworks may provide additional insight into the behavior of the system, not evident from individual components. We propose a novel approach for incorporating available network information into the analysis of arbitrary subnetworks. The proposed method offers an efficient dimension reduction strategy using Laplacian eigenmaps with Neumann boundary conditions, and provides a flexible inference framework for analysis of subnetworks, based on a group-penalized principal component regression model on graphs. Asymptotic properties of the proposed inference method, as well as the choice of the tuning parameter for control of the false positive rate are discussed in high dimensional settings. The performance of the proposed methodology is illustrated using simulated and real data examples from biology.</p>
<p>【Keywords】:</p>
<h3 id="243. Monte-Carlo Planning in Large POMDPs.">243. Monte-Carlo Planning in Large POMDPs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4031-monte-carlo-planning-in-large-pomdps">Paper Link</a>】    【Pages】:2164-2172</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Silver:David">David Silver</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Veness:Joel">Joel Veness</a></p>
<p>【Abstract】:
This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, Rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10x10 Battleship and Partially Observable PacMan, with approximately 10^18 and 10^56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.</p>
<p>【Keywords】:</p>
<h3 id="244. Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models.">244. Sodium entry efficiency during action potentials: A novel single-parameter family of Hodgkin-Huxley models.</h3>
<p>【<a href="http://papers.nips.cc/paper/3970-sodium-entry-efficiency-during-action-potentials-a-novel-single-parameter-family-of-hodgkin-huxley-models">Paper Link</a>】    【Pages】:2173-2180</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Anand">Anand Singh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jolivet:Renaud">Renaud Jolivet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Magistretti:Pierre_J=">Pierre J. Magistretti</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Bruno">Bruno Weber</a></p>
<p>【Abstract】:
Sodium entry during an action potential determines the energy efficiency of a neuron. The classic Hodgkin-Huxley model of action potential generation is notoriously inefficient in that regard with about 4 times more charges flowing through the membrane than the theoretical minimum required to achieve the observed depolarization. Yet, recent experimental results show that mammalian neurons are close to the optimal metabolic efficiency and that the dynamics of their voltage-gated channels is significantly different than the one exhibited by the classic Hodgkin-Huxley model during the action potential. Nevertheless, the original Hodgkin-Huxley model is still widely used and rarely to model the squid giant axon from which it was extracted. Here, we introduce a novel family of Hodgkin-Huxley models that correctly account for sodium entry, action potential width and whose voltage-gated channels display a dynamics very similar to the most recent experimental observations in mammalian neurons. We speak here about a family of models because the model is parameterized by a unique parameter the variations of which allow to reproduce the entire range of experimental observations from cortical pyramidal neurons to Purkinje cells, yielding a very economical framework to model a wide range of different central neurons. The present paper demonstrates the performances and discuss the properties of this new family of models.</p>
<p>【Keywords】:</p>
<h3 id="245. More data means less inference: A pseudo-max approach to structured learning.">245. More data means less inference: A pseudo-max approach to structured learning.</h3>
<p>【<a href="http://papers.nips.cc/paper/3921-more-data-means-less-inference-a-pseudo-max-approach-to-structured-learning">Paper Link</a>】    【Pages】:2181-2189</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sontag:David">David Sontag</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meshi:Ofer">Ofer Meshi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jaakkola:Tommi_S=">Tommi S. Jaakkola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Globerson:Amir">Amir Globerson</a></p>
<p>【Abstract】:
The problem of learning to predict structured labels is of key importance in many applications. However, for general graph structure both learning and inference in this setting are intractable. Here we show that it is possible to circumvent this difficulty when the input distribution is rich enough via a method similar in spirit to pseudo-likelihood. We show how our new method achieves consistency, and illustrate empirically that it indeed performs as well as exact methods when sufficiently large training sets are used.</p>
<p>【Keywords】:</p>
<h3 id="246. Reward Design via Online Gradient Ascent.">246. Reward Design via Online Gradient Ascent.</h3>
<p>【<a href="http://papers.nips.cc/paper/4146-reward-design-via-online-gradient-ascent">Paper Link</a>】    【Pages】:2190-2198</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sorg:Jonathan">Jonathan Sorg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Satinder_P=">Satinder P. Singh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lewis:Richard_L=">Richard L. Lewis</a></p>
<p>【Abstract】:
Recent work has demonstrated that when artificial agents are limited in their ability to achieve their goals, the agent designer can benefit by making the agent's goals different from the designer's. This gives rise to the optimization problem of designing the artificial agent's goals---in the RL framework, designing the agent's reward function. Existing attempts at solving this optimal reward problem do not leverage experience gained online during the agent's lifetime nor do they take advantage of knowledge about the agent's structure. In this work, we develop a gradient ascent approach with formal convergence guarantees for approximately solving the optimal reward problem online during an agent's lifetime. We show that our method generalizes a standard policy gradient approach, and we demonstrate its ability to improve reward functions in agents with various forms of limitations.</p>
<p>【Keywords】:</p>
<h3 id="247. Smoothness, Low Noise and Fast Rates.">247. Smoothness, Low Noise and Fast Rates.</h3>
<p>【<a href="http://papers.nips.cc/paper/3894-smoothness-low-noise-and-fast-rates">Paper Link</a>】    【Pages】:2199-2207</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Srebro:Nathan">Nathan Srebro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sridharan:Karthik">Karthik Sridharan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tewari:Ambuj">Ambuj Tewari</a></p>
<p>【Abstract】:
We establish an excess risk bound of O(H R_n^2 + sqrt{H L<em>} R_n) for ERM with an H-smooth loss function and a hypothesis class with Rademacher complexity R_n, where L</em> is the best risk achievable by the hypothesis class. For typical hypothesis classes where R_n = sqrt{R/n}, this translates to a learning rate of ̃ O(RH/n) in the separable (L<em> = 0) case and O(RH/n + sqrt{L</em> RH/n}) more generally. We also provide similar guarantees for online and stochastic convex optimization of a smooth non-negative objective.</p>
<p>【Keywords】:</p>
<h3 id="248. Efficient Minimization of Decomposable Submodular Functions.">248. Efficient Minimization of Decomposable Submodular Functions.</h3>
<p>【<a href="http://papers.nips.cc/paper/4028-efficient-minimization-of-decomposable-submodular-functions">Paper Link</a>】    【Pages】:2208-2216</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Stobbe:Peter">Peter Stobbe</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krause_0001:Andreas">Andreas Krause</a></p>
<p>【Abstract】:
Many combinatorial problems arising in machine learning can be reduced to the problem of minimizing a submodular function. Submodular functions are a natural discrete analog of convex functions, and can be minimized in strongly polynomial time. Unfortunately, state-of-the-art algorithms for general submodular minimization are intractable for practical problems. In this paper, we introduce a novel subclass of submodular minimization problems that we call decomposable. Decomposable submodular functions are those that can be represented as sums of concave functions applied to linear functions. We develop an algorithm, SLG, that can efficiently minimize decomposable submodular functions with tens of thousands of variables. Our algorithm exploits recent results in smoothed convex minimization. We apply SLG to synthetic benchmarks and a joint classification-and-segmentation task, and show that it outperforms the state-of-the-art general purpose submodular minimization algorithms by several orders of magnitude.</p>
<p>【Keywords】:</p>
<h3 id="249. Learning from Logged Implicit Exploration Data.">249. Learning from Logged Implicit Exploration Data.</h3>
<p>【<a href="http://papers.nips.cc/paper/3977-learning-from-logged-implicit-exploration-data">Paper Link</a>】    【Pages】:2217-2225</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Strehl:Alexander_L=">Alexander L. Strehl</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Langford:John">John Langford</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lihong">Lihong Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kakade:Sham">Sham Kakade</a></p>
<p>【Abstract】:
We provide a sound and consistent foundation for the use of \emph{nonrandom} exploration data in <code>contextual bandit'' or</code>partially labeled'' settings where only the value of a chosen action is learned.                                                       The primary challenge in a variety of settings is that the exploration policy, in which ``offline'' data is logged, is not explicitly known. Prior solutions here require either control of the actions during the learning process, recorded random exploration, or actions chosen obliviously in a repeated manner.  The techniques reported here lift these restrictions, allowing the learning of a policy for choosing actions given features from historical data where no randomization occurred or was logged.  We empirically verify our solution on two reasonably sized sets of real-world data obtained from an Internet %online advertising company.</p>
<p>【Keywords】:</p>
<h3 id="250. Layered image motion with explicit occlusions, temporal consistency, and depth ordering.">250. Layered image motion with explicit occlusions, temporal consistency, and depth ordering.</h3>
<p>【<a href="http://papers.nips.cc/paper/4030-layered-image-motion-with-explicit-occlusions-temporal-consistency-and-depth-ordering">Paper Link</a>】    【Pages】:2226-2234</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Deqing">Deqing Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sudderth:Erik_B=">Erik B. Sudderth</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Black:Michael_J=">Michael J. Black</a></p>
<p>【Abstract】:
Layered models are a powerful way of describing natural scenes containing smooth surfaces that may overlap and occlude each other. For image motion estimation, such models have a long history but have not achieved the wide use or accuracy of non-layered methods. We present a new probabilistic model of optical flow in layers that addresses many of the shortcomings of previous approaches. In particular, we define a probabilistic graphical model that explicitly captures: 1) occlusions and disocclusions; 2) depth ordering of the layers; 3) temporal consistency of the layer segmentation. Additionally the optical flow in each layer is modeled by a combination of a parametric model and a smooth deviation based on an MRF with a robust spatial prior; the resulting model allows roughness in layers. Finally, a key contribution is the formulation of the layers using an image-dependent hidden field prior based on recent models for static scene segmentation. The method achieves state-of-the-art results on the Middlebury benchmark and produces meaningful scene segmentations as well as detected occlusion regions.</p>
<p>【Keywords】:</p>
<h3 id="251. Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices.">251. Improving the Asymptotic Performance of Markov Chain Monte-Carlo by Inserting Vortices.</h3>
<p>【<a href="http://papers.nips.cc/paper/4037-improving-the-asymptotic-performance-of-markov-chain-monte-carlo-by-inserting-vortices">Paper Link</a>】    【Pages】:2235-2243</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Yi">Yi Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gomez:Faustino_J=">Faustino J. Gomez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schmidhuber:J=uuml=rgen">Jürgen Schmidhuber</a></p>
<p>【Abstract】:
We present a new way of converting a reversible finite Markov chain into a nonreversible one, with a theoretical guarantee that the asymptotic variance of the MCMC estimator based on the non-reversible chain is reduced. The method is applicable to any reversible chain whose states are not connected through a tree, and can be interpreted graphically as inserting vortices into the state transition graph. Our result confirms that non-reversible chains are fundamentally better than reversible ones in terms of asymptotic performance, and suggests interesting directions for further improving MCMC.</p>
<p>【Keywords】:</p>
<h3 id="252. A Reduction from Apprenticeship Learning to Classification.">252. A Reduction from Apprenticeship Learning to Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4180-a-reduction-from-apprenticeship-learning-to-classification">Paper Link</a>】    【Pages】:2253-2261</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Syed:Umar">Umar Syed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schapire:Robert_E=">Robert E. Schapire</a></p>
<p>【Abstract】:
We provide new theoretical results for apprenticeship learning, a variant of reinforcement learning in which the true reward function is unknown, and the goal is to perform well relative to an observed expert. We study a common approach to learning from expert demonstrations: using a classification algorithm to learn to imitate the expert's behavior. Although this straightforward learning strategy is widely-used in practice, it has been subject to very little formal analysis. We prove that, if the learned classifier has error rate $\eps$, the difference between the value of the apprentice's policy and the expert's policy is $O(\sqrt{\eps})$. Further, we prove that this difference is only $O(\eps)$ when the expert's policy is close to optimal. This latter result has an important practical consequence: Not only does imitating a near-optimal expert result in a better policy, but far fewer demonstrations are required to successfully imitate such an expert. This suggests an opportunity for substantial savings whenever the expert is known to be good, but demonstrations are expensive or difficult to obtain.</p>
<p>【Keywords】:</p>
<h3 id="253. Semi-Supervised Learning with Adversarially Missing Label Information.">253. Semi-Supervised Learning with Adversarially Missing Label Information.</h3>
<p>【<a href="http://papers.nips.cc/paper/4035-semi-supervised-learning-with-adversarially-missing-label-information">Paper Link</a>】    【Pages】:2244-2252</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Syed:Umar">Umar Syed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Taskar:Ben">Ben Taskar</a></p>
<p>【Abstract】:
We address the problem of semi-supervised learning in an adversarial setting. Instead of assuming that labels are missing at random, we analyze a less favorable scenario where the label information can be missing partially and arbitrarily, which is motivated by several practical examples. We present nearly matching upper and lower generalization bounds for learning in this setting under reasonable assumptions about available label information. Motivated by the analysis, we formulate a convex optimization problem for parameter estimation, derive an efficient algorithm, and analyze its convergence. We provide experimental results on several standard data sets showing the robustness of our algorithm to the pattern of missing label information, outperforming several strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="254. Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch.">254. Identifying Patients at Risk of Major Adverse Cardiovascular Events Using Symbolic Mismatch.</h3>
<p>【<a href="http://papers.nips.cc/paper/4078-identifying-patients-at-risk-of-major-adverse-cardiovascular-events-using-symbolic-mismatch">Paper Link</a>】    【Pages】:2262-2270</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Syed:Zeeshan">Zeeshan Syed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guttag:John_V=">John V. Guttag</a></p>
<p>【Abstract】:
Cardiovascular disease is the leading cause of death globally, resulting in 17 million deaths each year. Despite the availability of various treatment options, existing techniques based upon conventional medical knowledge often fail to identify patients who might have benefited from more aggressive therapy. In this paper, we describe and evaluate a novel unsupervised machine learning approach for cardiac risk stratification. The key idea of our approach is to avoid specialized medical knowledge, and assess patient risk using symbolic mismatch, a new metric to assess similarity in long-term time-series activity. We hypothesize that high risk patients can be identified using symbolic mismatch, as individuals in a population with unusual long-term physiological activity. We describe related approaches that build on these ideas to provide improved medical decision making for patients who have recently suffered coronary attacks. We first describe how to compute the symbolic mismatch between pairs of long term electrocardiographic (ECG) signals. This algorithm maps the original signals into a symbolic domain, and provides a quantitative assessment of the difference between these symbolic representations of the original signals. We then show how this measure can be used with each of a one-class SVM, a nearest neighbor classifier, and hierarchical clustering to improve risk stratification. We evaluated our methods on a population of 686 cardiac patients with available long-term electrocardiographic data. In a univariate analysis, all of the methods provided a statistically significant association with the occurrence of a major adverse cardiac event in the next 90 days. In a multivariate analysis that incorporated the most widely used clinical risk variables, the nearest neighbor and hierarchical clustering approaches were able to statistically significantly distinguish patients with a roughly two-fold risk of suffering a major adverse cardiac event in the next 90 days.</p>
<p>【Keywords】:</p>
<h3 id="255. Switching state space model for simultaneously estimating state transitions and nonstationary firing rates.">255. Switching state space model for simultaneously estimating state transitions and nonstationary firing rates.</h3>
<p>【<a href="http://papers.nips.cc/paper/4172-switching-state-space-model-for-simultaneously-estimating-state-transitions-and-nonstationary-firing-rates">Paper Link</a>】    【Pages】:2271-2279</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Takiyama:Ken">Ken Takiyama</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Okada:Masato">Masato Okada</a></p>
<p>【Abstract】:
We propose an algorithm for simultaneously estimating state transitions among neural states, the number of neural states, and nonstationary firing rates using a switching state space model (SSSM).  This model enables us to detect state transitions based not only on the discontinuous changes of mean firing rates but also on discontinuous changes in temporal profiles of firing rates, e.g., temporal correlation.  We derive a variational Bayes algorithm for a non-Gaussian SSSM whose non-Gaussian property is caused by binary spike events.  Synthetic data analysis reveals the high performance of our algorithm in estimating state transitions, the number of neural states, and nonstationary firing rates compared to previous methods.  We also analyze neural data recorded from the medial temporal area.  The statistically detected neural states probably coincide with transient and sustained states, which have been detected heuristically. Estimated parameters suggest that our algorithm detects the state transition based on discontinuous change in the temporal correlation of firing rates, which transitions previous methods cannot detect.  This result suggests the advantage of our algorithm in real-data analysis.</p>
<p>【Keywords】:</p>
<h3 id="256. Pose-Sensitive Embedding by Nonlinear NCA Regression.">256. Pose-Sensitive Embedding by Nonlinear NCA Regression.</h3>
<p>【<a href="http://papers.nips.cc/paper/4143-pose-sensitive-embedding-by-nonlinear-nca-regression">Paper Link</a>】    【Pages】:2280-2288</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Taylor:Graham_W=">Graham W. Taylor</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fergus:Rob">Rob Fergus</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Williams:George">George Williams</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Spiro:Ian">Ian Spiro</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bregler:Christoph">Christoph Bregler</a></p>
<p>【Abstract】:
This paper tackles the complex problem of visually matching people in similar pose but with different clothes, background, and other appearance changes. We achieve this with a novel method for learning a nonlinear embedding based on several extensions to the Neighborhood Component Analysis (NCA) framework. Our method is convolutional, enabling it to scale to realistically-sized images. By cheaply labeling the head and hands in large video databases through Amazon Mechanical Turk (a crowd-sourcing service), we can use the task of localizing the head and hands as a proxy for determining body pose. We apply our method to challenging real-world data and show that it can generalize beyond hand localization to infer a more general notion of body pose. We evaluate our method quantitatively against other embedding methods. We also demonstrate that real-world performance can be improved through the use of synthetic data.</p>
<p>【Keywords】:</p>
<h3 id="257. Fast Large-scale Mixture Modeling with Component-specific Data Partitions.">257. Fast Large-scale Mixture Modeling with Component-specific Data Partitions.</h3>
<p>【<a href="http://papers.nips.cc/paper/3924-fast-large-scale-mixture-modeling-with-component-specific-data-partitions">Paper Link</a>】    【Pages】:2289-2297</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Thiesson:Bo">Bo Thiesson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Chong">Chong Wang</a></p>
<p>【Abstract】:
Remarkably easy implementation and guaranteed convergence has made the EM algorithm one of the most used algorithms for mixture modeling. On the downside, the E-step is linear in both the sample size and the number of mixture components, making it impractical for large-scale data. Based on the variational EM framework, we propose a fast alternative that uses component-specific data partitions to obtain a sub-linear E-step in sample size, while the algorithm still maintains provable convergence. Our approach builds on previous work, but is significantly faster and scales much better in the number of mixture components. We demonstrate this speedup by experiments on large-scale synthetic and real data.</p>
<p>【Keywords】:</p>
<h3 id="258. Policy gradients in linearly-solvable MDPs.">258. Policy gradients in linearly-solvable MDPs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4013-policy-gradients-in-linearly-solvable-mdps">Paper Link</a>】    【Pages】:2298-2306</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Todorov:Emanuel">Emanuel Todorov</a></p>
<p>【Abstract】:
We present policy gradient results within the framework of linearly-solvable MDPs. For the first time, compatible function approximators and natural policy gradients are obtained by estimating the cost-to-go function, rather than the (much larger) state-action advantage function as is necessary in traditional MDPs. We also develop the first compatible function approximators and natural policy gradients for continuous-time stochastic systems.</p>
<p>【Keywords】:</p>
<h3 id="259. Phoneme Recognition with Large Hierarchical Reservoirs.">259. Phoneme Recognition with Large Hierarchical Reservoirs.</h3>
<p>【<a href="http://papers.nips.cc/paper/4056-phoneme-recognition-with-large-hierarchical-reservoirs">Paper Link</a>】    【Pages】:2307-2315</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Triefenbach:Fabian">Fabian Triefenbach</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jalalvand:Azarakhsh">Azarakhsh Jalalvand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schrauwen:Benjamin">Benjamin Schrauwen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Martens:Jean=Pierre">Jean-Pierre Martens</a></p>
<p>【Abstract】:
Automatic speech recognition has gradually improved over the years, but the reliable recognition of unconstrained speech is still not within reach. In order to achieve a breakthrough, many research groups are now investigating new methodologies that have potential to outperform the Hidden Markov Model technology that is at the core of all present commercial systems. In this paper, it is shown that the recently introduced concept of Reservoir Computing might form the basis of such a methodology. In a limited amount of time, a reservoir system that can recognize the elementary sounds of continuous speech has been built. The system already achieves a state-of-the-art performance, and there is evidence that the margin for further improvements is still significant.</p>
<p>【Keywords】:</p>
<h3 id="260. Exact learning curves for Gaussian process regression on large random graphs.">260. Exact learning curves for Gaussian process regression on large random graphs.</h3>
<p>【<a href="http://papers.nips.cc/paper/3981-exact-learning-curves-for-gaussian-process-regression-on-large-random-graphs">Paper Link</a>】    【Pages】:2316-2324</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/u/Urry:Matthew">Matthew Urry</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sollich:Peter">Peter Sollich</a></p>
<p>【Abstract】:
We study learning curves for Gaussian process regression which characterise performance in terms of the Bayes error averaged over datasets of a given size. Whilst learning curves are in general very difficult to calculate we show that for discrete input domains, where similarity between input points is characterised in terms of a graph, accurate predictions can be obtained. These should in fact become exact for large graphs drawn from a broad range of random graph ensembles with arbitrary degree distributions where each input (node) is connected only to a finite number of others. The method is based on translating the appropriate belief propagation equations to the graph ensemble. We demonstrate the accuracy of the predictions for Poisson (Erdos-Renyi) and regular random graphs, and discuss when and why previous approximations to the learning curve fail.</p>
<p>【Keywords】:</p>
<h3 id="261. Brain covariance selection: better individual functional connectivity models using population prior.">261. Brain covariance selection: better individual functional connectivity models using population prior.</h3>
<p>【<a href="http://papers.nips.cc/paper/4080-brain-covariance-selection-better-individual-functional-connectivity-models-using-population-prior">Paper Link</a>】    【Pages】:2334-2342</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Varoquaux:Ga=euml=l">Gaël Varoquaux</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gramfort:Alexandre">Alexandre Gramfort</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Poline:Jean=Baptiste">Jean-Baptiste Poline</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thirion:Bertrand">Bertrand Thirion</a></p>
<p>【Abstract】:
Spontaneous brain activity, as observed in functional neuroimaging, has been shown to display reproducible structure that expresses brain architecture and carries markers of brain pathologies. An important view of modern neuroscience is that such large-scale structure of coherent activity reflects modularity properties of brain connectivity graphs. However, to date, there has been no demonstration that the limited and noisy data available in spontaneous activity observations could be used to learn full-brain probabilistic models that generalize to new data.  Learning such models entails two main challenges:  i) modeling full brain connectivity is a difficult estimation problem that faces the curse of dimensionality and  ii) variability between subjects, coupled with the variability of functional signals between experimental runs, makes the use of multiple datasets challenging.  We describe subject-level brain functional connectivity structure as a multivariate Gaussian process and introduce a new strategy to estimate it from group data, by imposing a common structure on the graphical model in the population. We show that individual models learned from functional Magnetic Resonance Imaging (fMRI) data using this population prior generalize better to unseen data than models based on alternative regularization schemes. To our knowledge, this is the first report of a cross-validated model of spontaneous brain activity. Finally, we use the estimated graphical model to explore the large-scale characteristics of functional architecture and show for the first time that known cognitive networks appear as the integrated communities of functional connectivity graph.</p>
<p>【Keywords】:</p>
<h3 id="262. Fast detection of multiple change-points shared by many signals using group LARS.">262. Fast detection of multiple change-points shared by many signals using group LARS.</h3>
<p>【<a href="http://papers.nips.cc/paper/4157-fast-detection-of-multiple-change-points-shared-by-many-signals-using-group-lars">Paper Link</a>】    【Pages】:2343-2351</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vert:Jean=Philippe">Jean-Philippe Vert</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bleakley:Kevin">Kevin Bleakley</a></p>
<p>【Abstract】:
We present a fast algorithm for the detection of multiple change-points when each is frequently shared by members of a set of co-occurring one-dimensional signals. We give conditions on consistency of the method when the number of signals increases, and provide empirical evidence to support the consistency results.</p>
<p>【Keywords】:</p>
<h3 id="263. Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.">263. Optimal Bayesian Recommendation Sets and Myopically Optimal Choice Query Sets.</h3>
<p>【<a href="http://papers.nips.cc/paper/3943-optimal-bayesian-recommendation-sets-and-myopically-optimal-choice-query-sets">Paper Link</a>】    【Pages】:2352-2360</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Viappiani:Paolo">Paolo Viappiani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boutilier:Craig">Craig Boutilier</a></p>
<p>【Abstract】:
Bayesian approaches to utility elicitation typically adopt (myopic) expected value of information (EVOI) as a natural criterion for selecting queries.  However, EVOI-optimization is  usually computationally prohibitive.  In this paper, we examine EVOI optimization using \emph{choice queries}, queries in which a user  is ask to select her most preferred product from a set. We show that,  under very general assumptions, the optimal choice query w.r.t.\ EVOI coincides with \emph{optimal recommendation set}, that is, a set maximizing expected utility of the user selection. Since recommendation set optimization is a simpler, submodular problem, this can greatly reduce the complexity of both exact and approximate (greedy) computation of optimal choice queries.  We  also examine the case where user responses to choice queries are error-prone (using both constant and follow mixed multinomial logit  noise models) and provide worst-case guarantees.   Finally we present a local search technique that works well with large outcome spaces.</p>
<p>【Keywords】:</p>
<h3 id="264. Worst-case bounds on the quality of max-product fixed-points.">264. Worst-case bounds on the quality of max-product fixed-points.</h3>
<p>【<a href="http://papers.nips.cc/paper/3978-worst-case-bounds-on-the-quality-of-max-product-fixed-points">Paper Link</a>】    【Pages】:2325-2333</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vinyals:Meritxell">Meritxell Vinyals</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cerquides:Jes=uacute=s">Jesús Cerquides</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Farinelli:Alessandro">Alessandro Farinelli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rodr=iacute=guez=Aguilar:Juan_A=">Juan A. Rodríguez-Aguilar</a></p>
<p>【Abstract】:
We study worst-case bounds on the quality of any fixed point assignment of the max-product algorithm for Markov Random Fields (MRF). We start proving a bound   independent of the MRF structure and parameters. Afterwards, we show how this bound can be improved for MRFs with particular structures such as bipartite graphs or grids.  Our results provide interesting insight into the behavior of max-product. For example, we prove that max-product provides very good results (at least 90% of the optimal) on MRFs  with large variable-disjoint cycles (MRFs in which all cycles are variable-disjoint, namely that they do not share any edge and in which each cycle contains at least 20 variables).</p>
<p>【Keywords】:</p>
<h3 id="265. Multiple Kernel Learning and the SMO Algorithm.">265. Multiple Kernel Learning and the SMO Algorithm.</h3>
<p>【<a href="http://papers.nips.cc/paper/3985-multiple-kernel-learning-and-the-smo-algorithm">Paper Link</a>】    【Pages】:2361-2369</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:S=_V=_N=">S. V. N. Vishwanathan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/sun:Zhaonan">Zhaonan sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Ampornpunt:Nawanol">Nawanol Ampornpunt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Varma:Manik">Manik Varma</a></p>
<p>【Abstract】:
Our objective is to train $p$-norm Multiple Kernel Learning (MKL) and, more generally, linear MKL regularised by the Bregman divergence, using the Sequential Minimal Optimization (SMO) algorithm. The SMO algorithm is simple, easy to implement and adapt, and efficiently scales to large problems. As a result, it has gained widespread acceptance and SVMs are routinely trained using SMO in diverse real world applications. Training using SMO has been a long standing goal in MKL for the very same reasons. Unfortunately, the standard MKL dual is not differentiable, and therefore can not be optimised using SMO style co-ordinate ascent. In this paper, we demonstrate that linear MKL regularised with the $p$-norm squared, or with certain Bregman divergences, can indeed be trained using SMO. The resulting algorithm retains both simplicity and efficiency and is significantly faster than the state-of-the-art specialised $p$-norm MKL solvers. We show that we can train on a hundred thousand kernels in approximately seven minutes and on fifty thousand points in less than half an hour on a single core.</p>
<p>【Keywords】:</p>
<h3 id="266. Spectral Regularization for Support Estimation.">266. Spectral Regularization for Support Estimation.</h3>
<p>【<a href="http://papers.nips.cc/paper/4062-spectral-regularization-for-support-estimation">Paper Link</a>】    【Pages】:487-495</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vito:Ernesto_De">Ernesto De Vito</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rosasco:Lorenzo">Lorenzo Rosasco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Toigo:Alessandro">Alessandro Toigo</a></p>
<p>【Abstract】:
In this paper we consider the problem of learning from data  the  support of a probability distribution when  the distribution {\em does not} have a density (with respect to some reference measure). We  propose a  new class of regularized spectral estimators based on a new notion of reproducing kernel Hilbert space,  which we call {\em   ``completely regular''}. Completely regular kernels   allow to capture the relevant  geometric and topological properties  of an arbitrary probability space.  In particular, they are the key ingredient to prove the  universal consistency  of the spectral  estimators and in this respect they are the analogue of  universal kernels for supervised problems. Numerical  experiments show that spectral estimators compare favorably to state of the art machine learning algorithms for  density support  estimation.</p>
<p>【Keywords】:</p>
<h3 id="267. Unsupervised Kernel Dimension Reduction.">267. Unsupervised Kernel Dimension Reduction.</h3>
<p>【<a href="http://papers.nips.cc/paper/4122-unsupervised-kernel-dimension-reduction">Paper Link</a>】    【Pages】:2379-2387</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Meihong">Meihong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sha:Fei">Fei Sha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jordan:Michael_I=">Michael I. Jordan</a></p>
<p>【Abstract】:
We apply the framework of kernel dimension reduction, originally designed for supervised problems, to unsupervised dimensionality reduction. In this framework, kernel-based measures of independence are used to derive low-dimensional representations that maximally capture information in covariates in order to predict responses. We extend this idea and develop similarly motivated measures for unsupervised problems where covariates and responses are the same. Our empirical studies show that the resulting compact representation yields meaningful and appealing visualization and clustering of data. Furthermore, when used in conjunction with supervised learners for classification, our methods lead to lower classification errors than state-of-the-art methods, especially when embedding data in spaces of very few dimensions.</p>
<p>【Keywords】:</p>
<h3 id="268. Multi-View Active Learning in the Non-Realizable Case.">268. Multi-View Active Learning in the Non-Realizable Case.</h3>
<p>【<a href="http://papers.nips.cc/paper/3991-multi-view-active-learning-in-the-non-realizable-case">Paper Link</a>】    【Pages】:2388-2396</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0028:Wei">Wei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Zhi=Hua">Zhi-Hua Zhou</a></p>
<p>【Abstract】:
The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multi-view setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multi-view active learning can be $\widetilde{O}(\log \frac{1}{\epsilon})$, contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is $\widetilde{O}(\frac{1}{\epsilon})$, where the order of $1/\epsilon$ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of $1/\epsilon$ is related to the parameter in Tsybakov noise.</p>
<p>【Keywords】:</p>
<h3 id="269. A Discriminative Latent Model of Image Region and Object Tag Correspondence.">269. A Discriminative Latent Model of Image Region and Object Tag Correspondence.</h3>
<p>【<a href="http://papers.nips.cc/paper/3968-a-discriminative-latent-model-of-image-region-and-object-tag-correspondence">Paper Link</a>】    【Pages】:2397-2405</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0003:Yang">Yang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mori:Greg">Greg Mori</a></p>
<p>【Abstract】:
We propose a discriminative latent model for annotating images with unaligned object-level textual annotations. Instead of using the bag-of-words image representation currently popular in the computer vision community, our model explicitly captures more intricate relationships underlying visual and textual information. In particular, we model the mapping that translates image regions to annotations. This mapping allows us to relate image regions to their corresponding annotation terms. We also model the overall scene label as latent information. This allows us to cluster test images. Our training data consist of images and their associated annotations. But we do not have access to the ground-truth region-to-annotation mapping or the overall scene label. We develop a novel variant of the latent SVM framework to model them as latent variables. Our experimental results demonstrate the effectiveness of the proposed model compared with other baseline methods.</p>
<p>【Keywords】:</p>
<h3 id="270. Joint Analysis of Time-Evolving Binary Matrices and Associated Documents.">270. Joint Analysis of Time-Evolving Binary Matrices and Associated Documents.</h3>
<p>【<a href="http://papers.nips.cc/paper/4152-joint-analysis-of-time-evolving-binary-matrices-and-associated-documents">Paper Link</a>】    【Pages】:2370-2378</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Eric">Eric Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Dehong">Dehong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silva:Jorge_G=">Jorge G. Silva</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dunson:David_B=">David B. Dunson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
We consider problems for which one has incomplete binary matrices that evolve with time (e.g., the votes of legislators on particular legislation, with each year characterized by a different such matrix). An objective of such analysis is to infer structure and inter-relationships underlying the matrices, here defined by latent features associated with each axis of the matrix. In addition, it is assumed that documents are available for the entities associated with at least one of the matrix axes. By jointly analyzing the matrices and documents, one may be used to inform the other within the analysis, and the model offers the opportunity to predict matrix values (e.g., votes) based only on an associated document (e.g., legislation). The research presented here merges two areas of machine-learning that have previously been investigated separately: incomplete-matrix analysis and topic modeling. The analysis is performed from a Bayesian perspective, with efficient inference constituted via Gibbs sampling. The framework is demonstrated by considering all voting data and available documents (legislation) during the 220-year lifetime of the United States Senate and House of Representatives.</p>
<p>【Keywords】:</p>
<h3 id="271. Heavy-Tailed Process Priors for Selective Shrinkage.">271. Heavy-Tailed Process Priors for Selective Shrinkage.</h3>
<p>【<a href="http://papers.nips.cc/paper/3996-heavy-tailed-process-priors-for-selective-shrinkage">Paper Link</a>】    【Pages】:2406-2414</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wauthier:Fabian_L=">Fabian L. Wauthier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jordan:Michael_I=">Michael I. Jordan</a></p>
<p>【Abstract】:
Heavy-tailed distributions are often used to enhance the robustness of regression and classification methods to outliers in output space.  Often, however, we are confronted with ``outliers'' in input space, which are isolated observations in sparsely populated regions. We show that heavy-tailed process priors (which we construct from Gaussian processes via a copula), can be used to improve robustness of regression and classification estimators to such outliers by selectively shrinking them more strongly in sparse regions than in dense regions. We carry out a theoretical analysis to show that selective shrinkage occurs provided the marginals of the heavy-tailed process have sufficiently heavy tails. The analysis is complemented by experiments on biological data which indicate significant improvements of estimates in sparse regions while producing competitive results in dense regions.</p>
<p>【Keywords】:</p>
<h3 id="272. Sidestepping Intractable Inference with Structured Ensemble Cascades.">272. Sidestepping Intractable Inference with Structured Ensemble Cascades.</h3>
<p>【<a href="http://papers.nips.cc/paper/4134-sidestepping-intractable-inference-with-structured-ensemble-cascades">Paper Link</a>】    【Pages】:2415-2423</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weiss:David_J=">David J. Weiss</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sapp:Benjamin">Benjamin Sapp</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Taskar:Ben">Ben Taskar</a></p>
<p>【Abstract】:
For many structured prediction problems, complex models often require adopting approximate inference techniques such as variational methods or sampling, which generally provide no satisfactory accuracy guarantees. In this work, we propose sidestepping intractable inference altogether by learning ensembles of tractable sub-models as part of a structured prediction cascade. We focus in particular on problems with high-treewidth and large state-spaces, which occur in many computer vision tasks. Unlike other variational methods, our ensembles do not enforce agreement between sub-models, but filter the space of possible outputs by simply adding and thresholding the max-marginals of each constituent model. Our framework jointly estimates parameters for all models in the ensemble for each level of the cascade by minimizing a novel, convex loss function, yet requires only a linear increase in computation over learning or inference in a single tractable sub-model. We provide a generalization bound on the filtering loss of the ensemble as a theoretical justification of our approach, and we evaluate our method on both synthetic data and the task of estimating articulated human pose from challenging videos. We find that our approach significantly outperforms loopy belief propagation on the synthetic data and a state-of-the-art model on the pose estimation/tracking problem.</p>
<p>【Keywords】:</p>
<h3 id="273. The Multidimensional Wisdom of Crowds.">273. The Multidimensional Wisdom of Crowds.</h3>
<p>【<a href="http://papers.nips.cc/paper/4074-the-multidimensional-wisdom-of-crowds">Paper Link</a>】    【Pages】:2424-2432</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Welinder:Peter">Peter Welinder</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Branson:Steve">Steve Branson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Belongie:Serge_J=">Serge J. Belongie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perona:Pietro">Pietro Perona</a></p>
<p>【Abstract】:
Abstract Missing</p>
<p>【Keywords】:</p>
<h3 id="274. Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains.">274. Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains.</h3>
<p>【<a href="http://papers.nips.cc/paper/4090-interval-estimation-for-reinforcement-learning-algorithms-in-continuous-state-domains">Paper Link</a>】    【Pages】:2433-2441</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/White:Martha">Martha White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Adam_M=">Adam M. White</a></p>
<p>【Abstract】:
The reinforcement learning community has explored many approaches to obtain- ing value estimates and models to guide decision making; these approaches, how- ever, do not usually provide a measure of confidence in the estimate. Accurate estimates of an agent’s confidence are useful for many applications, such as bi- asing exploration and automatically adjusting parameters to reduce dependence on parameter-tuning. Computing confidence intervals on reinforcement learning value estimates, however, is challenging because data generated by the agent- environment interaction rarely satisfies traditional assumptions. Samples of value- estimates are dependent, likely non-normally distributed and often limited, partic- ularly in early learning when confidence estimates are pivotal. In this work, we investigate how to compute robust confidences for value estimates in continuous Markov decision processes. We illustrate how to use bootstrapping to compute confidence intervals online under a changing policy (previously not possible) and prove validity under a few reasonable assumptions. We demonstrate the applica- bility of our confidence estimation algorithms with experiments on exploration, parameter estimation and tracking.</p>
<p>【Keywords】:</p>
<h3 id="275. Active Learning Applied to Patient-Adaptive Heartbeat Classification.">275. Active Learning Applied to Patient-Adaptive Heartbeat Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/4091-active-learning-applied-to-patient-adaptive-heartbeat-classification">Paper Link</a>】    【Pages】:2442-2450</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wiens:Jenna">Jenna Wiens</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guttag:John_V=">John V. Guttag</a></p>
<p>【Abstract】:
While clinicians can accurately identify different types of heartbeats in electrocardiograms (ECGs) from different patients, researchers have had limited success in applying supervised machine learning to the same task. The problem is made challenging by the variety of tasks, inter- and intra-patient differences, an often severe class imbalance, and the high cost of getting cardiologists to label data for individual patients. We address these difficulties using active learning to perform patient-adaptive and task-adaptive heartbeat classification. When tested on a benchmark database of cardiologist annotated ECG recordings, our method had considerably better performance than other recently proposed methods on the two primary classification tasks recommended by the Association for the Advancement of Medical Instrumentation. Additionally, our method required over 90% less patient-specific training data than the methods to which we compared it.</p>
<p>【Keywords】:</p>
<h3 id="276. Probabilistic Inference and Differential Privacy.">276. Probabilistic Inference and Differential Privacy.</h3>
<p>【<a href="http://papers.nips.cc/paper/3897-probabilistic-inference-and-differential-privacy">Paper Link</a>】    【Pages】:2451-2459</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Williams:Oliver">Oliver Williams</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/McSherry:Frank">Frank McSherry</a></p>
<p>【Abstract】:
We identify and investigate a strong connection between probabilistic inference and differential privacy, the latter being a recent privacy definition that permits only indirect observation of data through noisy measurement. Previous research on differential privacy has focused on designing measurement processes whose output is likely to be useful on its own. We consider the potential of applying probabilistic inference to the measurements and measurement process to derive posterior distributions over the data sets and model parameters thereof. We find that probabilistic inference can improve accuracy, integrate multiple observations, measure uncertainty, and even provide posterior distributions over quantities that were not directly measured.</p>
<p>【Keywords】:</p>
<h3 id="277. Copula Processes.">277. Copula Processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/4082-copula-processes">Paper Link</a>】    【Pages】:2460-2468</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wilson:Andrew_Gordon">Andrew Gordon Wilson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghahramani:Zoubin">Zoubin Ghahramani</a></p>
<p>【Abstract】:
We define a copula process which describes the dependencies between arbitrarily many random variables independently of their marginal distributions. As an example, we develop a stochastic volatility model, Gaussian Copula Process Volatility (GCPV), to predict the latent standard deviations of a sequence of random variables. To make predictions we use Bayesian inference, with the Laplace approximation, and with Markov chain Monte Carlo as an alternative. We find our model can outperform GARCH on simulated and financial data. And unlike GARCH, GCPV can easily handle missing data, incorporate covariates other than time, and model a rich class of covariance structures.</p>
<p>【Keywords】:</p>
<h3 id="278. Linear readout from a neural population with partial correlation data.">278. Linear readout from a neural population with partial correlation data.</h3>
<p>【<a href="http://papers.nips.cc/paper/4076-linear-readout-from-a-neural-population-with-partial-correlation-data">Paper Link</a>】    【Pages】:2469-2477</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wohrer:Adrien">Adrien Wohrer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Romo:Ranulfo">Ranulfo Romo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Machens:Christian_K=">Christian K. Machens</a></p>
<p>【Abstract】:
How much information does a neural population convey about a stimulus? Answers to this question are known to strongly depend on the correlation of response variability in neural populations. These noise correlations, however, are essentially immeasurable as the number of parameters in a noise correlation matrix grows quadratically with population size. Here, we suggest to bypass this problem by imposing a parametric model on a noise correlation matrix. Our basic assumption is that noise correlations arise due to common inputs between neurons. On average, noise correlations will therefore reflect signal correlations, which can be measured in neural populations. We suggest an explicit parametric dependency between signal and noise correlations. We show how this dependency can be used to fill the gaps" in noise correlations matrices using an iterative application of the Wishart distribution over positive definitive matrices. We apply our method to data from the primary somatosensory cortex of monkeys performing a two-alternative-forced choice task. We compare the discrimination thresholds read out from the population of recorded neurons with the discrimination threshold of the monkey and show that our method predicts different results than simpler, average schemes of noise correlations."</p>
<p>【Keywords】:</p>
<h3 id="279. A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration.">279. A Log-Domain Implementation of the Diffusion Network in Very Large Scale Integration.</h3>
<p>【<a href="http://papers.nips.cc/paper/3950-a-log-domain-implementation-of-the-diffusion-network-in-very-large-scale-integration">Paper Link</a>】    【Pages】:2487-2495</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Yi=Da">Yi-Da Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Shi=Jie">Shi-Jie Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Hsin">Hsin Chen</a></p>
<p>【Abstract】:
The Diffusion Network(DN) is a stochastic recurrent network which has been shown capable of modeling the distributions of continuous-valued, continuous-time paths. However, the dynamics of the DN are governed by stochastic differential equations, making the DN unfavourable for simulation in a digital computer. This paper presents the implementation of the DN in analogue Very Large Scale Integration, enabling the DN to be simulated in real time. Moreover, the log-domain representation is applied to the DN, allowing the supply voltage and thus the power consumption to be reduced without limiting the dynamic ranges for diffusion processes. A VLSI chip containing a DN with two stochastic units has been designed and fabricated. The design of component circuits will be described, so will the simulation of the full system be presented. The simulation results demonstrate that the DN in VLSI is able to regenerate various types of continuous paths in real-time.</p>
<p>【Keywords】:</p>
<h3 id="280. A unified model of short-range and long-range motion perception.">280. A unified model of short-range and long-range motion perception.</h3>
<p>【<a href="http://papers.nips.cc/paper/3910-a-unified-model-of-short-range-and-long-range-motion-perception">Paper Link</a>】    【Pages】:2478-2486</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Shuang">Shuang Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Xuming">Xuming He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Hongjing">Hongjing Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuille:Alan_L=">Alan L. Yuille</a></p>
<p>【Abstract】:
The human vision system is able to effortlessly perceive both short-range and long-range motion patterns in complex dynamic scenes. Previous work has assumed that two different mechanisms are involved in processing these two types of motion. In this paper, we propose a hierarchical model as a unified framework for modeling both short-range and long-range motion perception. Our model consists of two key components: a data likelihood that proposes multiple motion hypotheses using nonlinear matching, and a hierarchical prior that imposes slowness and spatial smoothness constraints on the motion field at multiple scales. We tested our model on two types of stimuli, random dot kinematograms and multiple-aperture stimuli, both commonly used in human vision research. We demonstrate that the hierarchical model adequately accounts for human performance in psychophysical experiments.</p>
<p>【Keywords】:</p>
<h3 id="281. Distributionally Robust Markov Decision Processes.">281. Distributionally Robust Markov Decision Processes.</h3>
<p>【<a href="http://papers.nips.cc/paper/3927-distributionally-robust-markov-decision-processes">Paper Link</a>】    【Pages】:2505-2513</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Huan">Huan Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mannor:Shie">Shie Mannor</a></p>
<p>【Abstract】:
We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level so that a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a-priori information of the distribution of parameters, and arises naturally in practice where methods to estimate the confidence region of parameters abound. We propose a decision criterion based on <em>distributional robustness</em>: the optimal policy maximizes the expected total reward under the most adversarial probability distribution over realizations of the uncertain parameters that is admissible (i.e., it agrees with the a-priori information). We show that finding the optimal distributionally robust policy can be reduced to a standard robust MDP where the parameters belong to a single uncertainty set, hence it can be computed in polynomial time under mild technical conditions.</p>
<p>【Keywords】:</p>
<h3 id="282. Inference and communication in the game of Password.">282. Inference and communication in the game of Password.</h3>
<p>【<a href="http://papers.nips.cc/paper/3951-inference-and-communication-in-the-game-of-password">Paper Link</a>】    【Pages】:2514-2522</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kemp:Charles">Charles Kemp</a></p>
<p>【Abstract】:
Communication between a speaker and hearer will be most efficient when both parties make accurate inferences about the other. We study inference and communication in a television game called Password, where speakers must convey secret words to hearers by providing one-word clues. Our working hypothesis is that human communication is relatively efficient, and we use game show data to examine three predictions. First, we predict that speakers and hearers are both considerate, and that both take the other’s perspective into account. Second, we predict that speakers and hearers are calibrated, and that both make accurate assumptions about the strategy used by the other. Finally, we predict that speakers and hearers are collaborative, and that they tend to share the cognitive burden of communication equally. We find evidence in support of all three predictions, and demonstrate in addition that efficient communication tends to break down when speakers and hearers are placed under time pressure.</p>
<p>【Keywords】:</p>
<h3 id="283. Robust PCA via Outlier Pursuit.">283. Robust PCA via Outlier Pursuit.</h3>
<p>【<a href="http://papers.nips.cc/paper/4005-robust-pca-via-outlier-pursuit">Paper Link</a>】    【Pages】:2496-2504</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Huan">Huan Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caramanis:Constantine">Constantine Caramanis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanghavi:Sujay">Sujay Sanghavi</a></p>
<p>【Abstract】:
Singular Value Decomposition (and Principal Component Analysis)  is one of the most widely used techniques for dimensionality reduction: successful and efficiently computable, it is nevertheless plagued by a well-known, well-documented sensitivity to outliers. Recent work has considered the setting where each point has a few arbitrarily corrupted components. Yet, in applications of SVD or PCA such as robust collaborative filtering or bioinformatics, malicious agents, defective genes, or simply corrupted or contaminated experiments may effectively yield entire points that are completely corrupted.  We present an efficient convex optimization-based  algorithm we call Outlier Pursuit, that under some mild assumptions on the uncorrupted points (satisfied, e.g., by the standard generative assumption in PCA problems) recovers the <em>exact</em> optimal low-dimensional subspace, and identifies the corrupted points. Such identification of corrupted points that do not conform to the low-dimensional approximation, is of paramount interest in bioinformatics and financial applications, and beyond. Our techniques involve matrix decomposition using nuclear norm minimization, however, our results, setup, and approach, necessarily differ considerably from the existing line of work in matrix completion and matrix decomposition, since we develop an approach to recover the correct <em>column space</em> of the uncorrupted matrix, rather than the exact matrix itself.</p>
<p>【Keywords】:</p>
<h3 id="284. Relaxed Clipping: A Global Training Method for Robust Regression and Classification.">284. Relaxed Clipping: A Global Training Method for Robust Regression and Classification.</h3>
<p>【<a href="http://papers.nips.cc/paper/3936-relaxed-clipping-a-global-training-method-for-robust-regression-and-classification">Paper Link</a>】    【Pages】:2532-2540</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yaoliang">Yaoliang Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Min">Min Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Linli">Linli Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Martha">Martha White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schuurmans:Dale">Dale Schuurmans</a></p>
<p>【Abstract】:
Robust regression and classification are often thought to require non-convex loss functions that prevent scalable, global training. However, such a view neglects the possibility of reformulated training methods that can yield practically solvable alternatives. A natural way to make a loss function more robust to outliers is to truncate loss values that exceed a maximum threshold. We demonstrate that a relaxation of this form of ``loss clipping'' can be made globally solvable and applicable to any standard loss while guaranteeing robustness against outliers. We present a generic procedure that can be applied to standard loss functions and demonstrate improved robustness in regression and classification problems.</p>
<p>【Keywords】:</p>
<h3 id="285. Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike.">285. Feature Transitions with Saccadic Search: Size, Color, and Orientation Are Not Alike.</h3>
<p>【<a href="http://papers.nips.cc/paper/4112-feature-transitions-with-saccadic-search-size-color-and-orientation-are-not-alike">Paper Link</a>】    【Pages】:2523-2531</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Stella_X=">Stella X. Yu</a></p>
<p>【Abstract】:
Size, color, and orientation have long been considered elementary features whose attributes are extracted in parallel and available to guide the deployment of attention. If each is processed in the same fashion with simply a different set of local detectors, one would expect similar search behaviours on localizing an equivalent flickering change among identically laid out disks. We analyze feature transitions associated with saccadic search and find out that size, color, and orientation are not alike in dynamic attribute processing over time. The Markovian feature transition is attractive for size, repulsive for color, and largely reversible for orientation.</p>
<p>【Keywords】:</p>
<h3 id="286. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty.">286. Learning Multiple Tasks with a Sparse Matrix-Normal Penalty.</h3>
<p>【<a href="http://papers.nips.cc/paper/4095-learning-multiple-tasks-with-a-sparse-matrix-normal-penalty">Paper Link</a>】    【Pages】:2550-2558</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0010:Yi">Yi Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schneider:Jeff_G=">Jeff G. Schneider</a></p>
<p>【Abstract】:
In this paper, we propose a matrix-variate normal penalty with sparse inverse covariances to couple multiple tasks. Learning multiple (parametric) models can be viewed as estimating a matrix of parameters, where rows and columns of the matrix correspond to tasks and features, respectively. Following the matrix-variate normal density, we design a penalty that decomposes the full covariance of matrix elements into the Kronecker product of row covariance and column covariance, which characterizes both task relatedness and feature representation. Several recently proposed methods are variants of the special cases of this formulation. To address the overfitting issue and select meaningful task and feature structures, we include sparse covariance selection into our matrix-normal regularization via L-1 penalties on task and feature inverse covariances. We empirically study the proposed method and compare with related models in two real-world problems: detecting landmines in multiple fields and recognizing faces between different subjects. Experimental results show that the proposed framework provides an effective and flexible way to model various different structures of multiple tasks.</p>
<p>【Keywords】:</p>
<h3 id="287. Lower Bounds on Rate of Convergence of Cutting Plane Methods.">287. Lower Bounds on Rate of Convergence of Cutting Plane Methods.</h3>
<p>【<a href="http://papers.nips.cc/paper/4144-lower-bounds-on-rate-of-convergence-of-cutting-plane-methods">Paper Link</a>】    【Pages】:2541-2549</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xinhua">Xinhua Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saha:Ankan">Ankan Saha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:S=_V=_N=">S. V. N. Vishwanathan</a></p>
<p>【Abstract】:
In a recent paper Joachims (2006) presented SVM-Perf, a cutting plane method (CPM) for training linear Support Vector Machines (SVMs) which converges to an $\epsilon$ accurate solution in $O(1/\epsilon^{2})$ iterations. By tightening the analysis, Teo et al. (2010) showed that $O(1/\epsilon)$ iterations suffice. Given the impressive convergence speed of CPM on a number of practical problems, it was conjectured that these rates could be further improved. In this paper we disprove this conjecture. We present counter examples which are not only applicable for training linear SVMs with hinge loss, but also hold for support vector methods which optimize a \emph{multivariate} performance score. However, surprisingly, these problems are not inherently hard. By exploiting the structure of the objective function we can devise an algorithm that converges in $O(1/\sqrt{\epsilon})$ iterations.</p>
<p>【Keywords】:</p>
<h3 id="288. Probabilistic Multi-Task Feature Selection.">288. Probabilistic Multi-Task Feature Selection.</h3>
<p>【<a href="http://papers.nips.cc/paper/4150-probabilistic-multi-task-feature-selection">Paper Link</a>】    【Pages】:2559-2567</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0006:Yu">Yu Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yeung:Dit=Yan">Dit-Yan Yeung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Qian">Qian Xu</a></p>
<p>【Abstract】:
Recently, some variants of the $l<em>1$ norm, particularly matrix norms such as the $l</em>{1,2}$ and $l<em>{1,\infty}$ norms, have been widely used in multi-task learning, compressed sensing and other related areas to enforce sparsity via joint regularization. In this paper, we unify the $l</em>{1,2}$ and $l<em>{1,\infty}$ norms by considering a family of $l</em>{1,q}$ norms for $1 &lt; q\le\infty$ and study the problem of determining the most appropriate sparsity enforcing norm to use in the context of multi-task feature selection. Using the generalized normal distribution, we provide a probabilistic interpretation of the general multi-task feature selection problem using the $l_{1,q}$ norm. Based on this probabilistic interpretation, we develop a probabilistic model using the noninformative Jeffreys prior. We also extend the model to learn and exploit more general types of pairwise relationships between tasks. For both versions of the model, we devise expectation-maximization~(EM) algorithms to learn all model parameters, including $q$, automatically. Experiments have been conducted on two cancer classification applications using microarray gene expression data.</p>
<p>【Keywords】:</p>
<h3 id="289. Worst-Case Linear Discriminant Analysis.">289. Worst-Case Linear Discriminant Analysis.</h3>
<p>【<a href="http://papers.nips.cc/paper/3998-worst-case-linear-discriminant-analysis">Paper Link</a>】    【Pages】:2568-2576</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0006:Yu">Yu Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yeung:Dit=Yan">Dit-Yan Yeung</a></p>
<p>【Abstract】:
Dimensionality reduction is often needed in many applications due to the high dimensionality of the data involved. In this paper, we first analyze the scatter measures used in the conventional linear discriminant analysis~(LDA) model and note that the formulation is based on the average-case view. Based on this analysis, we then propose a new dimensionality reduction method called worst-case linear discriminant analysis~(WLDA) by defining new between-class and within-class scatter measures. This new model adopts the worst-case view which arguably is more suitable for applications such as classification. When the number of training data points or the number of features is not very large, we relax the optimization problem involved and formulate it as a metric learning problem. Otherwise, we take a greedy approach by finding one direction of the transformation at a time. Moreover, we also analyze a special case of WLDA to show its relationship with conventional LDA. Experiments conducted on several benchmark datasets demonstrate the effectiveness of WLDA when compared with some related dimensionality reduction methods.</p>
<p>【Keywords】:</p>
<h3 id="290. Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework.">290. Sufficient Conditions for Generating Group Level Sparsity in a Robust Minimax Framework.</h3>
<p>【<a href="http://papers.nips.cc/paper/4000-sufficient-conditions-for-generating-group-level-sparsity-in-a-robust-minimax-framework">Paper Link</a>】    【Pages】:2577-2585</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Hongbo">Hongbo Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Qiang">Qiang Cheng</a></p>
<p>【Abstract】:
Regularization technique has become a principle tool for statistics and machine learning research and practice. However, in most situations, these regularization terms are not well interpreted, especially on how they are related to the loss function and data. In this paper, we propose a robust minimax framework to interpret the relationship between data and regularization terms for a large class of loss functions. We show that various regularization terms are essentially corresponding to different distortions to the original data matrix. This minimax framework includes ridge regression, lasso, elastic net, fused lasso, group lasso, local coordinate coding, multiple kernel learning, etc., as special cases. Within this minimax framework, we further gave mathematically exact definition for a novel representation called sparse grouping representation (SGR), and proved sufficient conditions for generating such group level sparsity. Under these sufficient conditions, a large set of consistent regularization terms can be designed. This SGR is essentially different from group lasso in the way of using class or group information, and it outperforms group lasso when there appears group label noise. We also gave out some generalization bounds in a classification setting.</p>
<p>【Keywords】:</p>
<h3 id="291. Large Margin Learning of Upstream Scene Understanding Models.">291. Large Margin Learning of Upstream Scene Understanding Models.</h3>
<p>【<a href="http://papers.nips.cc/paper/4149-large-margin-learning-of-upstream-scene-understanding-models">Paper Link</a>】    【Pages】:2586-2594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jun">Jun Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Li=Jia">Li-Jia Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Fei=Fei">Fei-Fei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
Upstream supervised topic models have been widely used for complicated scene understanding. However, existing maximum likelihood estimation (MLE) schemes can make the prediction model learning independent of latent topic discovery and result in an imbalanced prediction rule for scene classification. This paper presents a joint max-margin and max-likelihood learning method for upstream scene understanding models, in which latent topic discovery and prediction model estimation are closely coupled and well-balanced. The optimization problem is efficiently solved with a variational EM procedure, which iteratively solves an online loss-augmented SVM. We demonstrate the advantages of the large-margin approach on both an 8-category sports dataset and the 67-class MIT indoor scene dataset for scene categorization.</p>
<p>【Keywords】:</p>
<h3 id="292. Parallelized Stochastic Gradient Descent.">292. Parallelized Stochastic Gradient Descent.</h3>
<p>【<a href="http://papers.nips.cc/paper/4006-parallelized-stochastic-gradient-descent">Paper Link</a>】    【Pages】:2595-2603</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zinkevich:Martin">Martin Zinkevich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weimer:Markus">Markus Weimer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lihong">Lihong Li</a></p>
<p>【Abstract】:
With the increase in available data parallel machine learning has   become an increasingly pressing problem. In this paper we present   the first parallel stochastic gradient descent algorithm including a   detailed analysis and experimental evidence. Unlike prior work on   parallel optimization algorithms our   variant comes with parallel acceleration guarantees and it poses no   overly tight latency constraints, which might only be available in   the multicore setting. Our analysis introduces a novel proof   technique --- contractive mappings to quantify the   speed of convergence of parameter distributions to their asymptotic   limits. As a side effect this answers the question of how quickly   stochastic gradient descent algorithms reach the asymptotically   normal regime.</p>
<p>【Keywords】:</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
