 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="light" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/light.css">
<link id="dark" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/dark.css" disabled/>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/skin.js"></script>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/classie.js"></script>

<!-- This is for Mathjax -->

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["$","$"] ],
			displayMath: [ ['$$','$$'], ["$$","$$"] ],
			processEscapes: true
			},
		TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
		"HTML-CSS": {linebreaks: {automatic: true}},
		SVG: {linebreaks: {automatic: true}}
	});
</script>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#5. WSDM 2012:Seattle, WA, USA">5. WSDM 2012:Seattle, WA, USA</a><ul>
<li><a href="#Paper Num: 86 || Session Num: 17">Paper Num: 86 || Session Num: 17</a></li>
<li><a href="#Keynote address    1">Keynote address    1</a><ul>
<li><a href="#1. Nowcasting the macroeconomy with search engine data.">1. Nowcasting the macroeconomy with search engine data.</a></li>
</ul>
</li>
<li><a href="#Multimedia and geo-mining    5">Multimedia and geo-mining    5</a><ul>
<li><a href="#2. Spatially-aware indexing for image object retrieval.">2. Spatially-aware indexing for image object retrieval.</a></li>
<li><a href="#3. Auralist: introducing serendipity into music recommendation.">3. Auralist: introducing serendipity into music recommendation.</a></li>
<li><a href="#4. Efficient misbehaving user detection in online video chat services.">4. Efficient misbehaving user detection in online video chat services.</a></li>
<li><a href="#5. Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities.">5. Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities.</a></li>
<li><a href="#6. Object matching in tweets with spatial models.">6. Object matching in tweets with spatial models.</a></li>
</ul>
</li>
<li><a href="#Spotlight on mining    15">Spotlight on mining    15</a><ul>
<li><a href="#7. Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data.">7. Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data.</a></li>
<li><a href="#8. Mining contrastive opinions on political texts using cross-perspective topic model.">8. Mining contrastive opinions on political texts using cross-perspective topic model.</a></li>
<li><a href="#9. Coupled temporal scoping of relational facts.">9. Coupled temporal scoping of relational facts.</a></li>
<li><a href="#10. Overcoming browser cookie churn with clustering.">10. Overcoming browser cookie churn with clustering.</a></li>
<li><a href="#11. mTrust: discerning multi-faceted trust in a connected world.">11. mTrust: discerning multi-faceted trust in a connected world.</a></li>
<li><a href="#12. Of hammers and nails: an empirical comparison of three paradigms for processing large graphs.">12. Of hammers and nails: an empirical comparison of three paradigms for processing large graphs.</a></li>
<li><a href="#13. Pairwise cross-domain factor model for heterogeneous transfer ranking.">13. Pairwise cross-domain factor model for heterogeneous transfer ranking.</a></li>
<li><a href="#14. Scalable inference in latent variable models.">14. Scalable inference in latent variable models.</a></li>
<li><a href="#15. Learning recommender systems with adaptive regularization.">15. Learning recommender systems with adaptive regularization.</a></li>
<li><a href="#16. Collaborative ranking.">16. Collaborative ranking.</a></li>
<li><a href="#17. From chatter to headlines: harnessing the real-time web for personalized news recommendation.">17. From chatter to headlines: harnessing the real-time web for personalized news recommendation.</a></li>
<li><a href="#18. ETF: extended tensor factorization model for personalizing prediction of review helpfulness.">18. ETF: extended tensor factorization model for personalizing prediction of review helpfulness.</a></li>
<li><a href="#19. Multi-relational matrix factorization using bayesian personalized ranking for social network data.">19. Multi-relational matrix factorization using bayesian personalized ranking for social network data.</a></li>
<li><a href="#20. Comment spam detection by sequence mining.">20. Comment spam detection by sequence mining.</a></li>
<li><a href="#21. Mining slang and urban opinion words and phrases from cQA services: an optimization approach.">21. Mining slang and urban opinion words and phrases from cQA services: an optimization approach.</a></li>
</ul>
</li>
<li><a href="#Web search I    4">Web search I    4</a><ul>
<li><a href="#22. No search result left behind: branching behavior with browser tabs.">22. No search result left behind: branching behavior with browser tabs.</a></li>
<li><a href="#23. Characterizing web content, user interests, and search behavior by reading level and topic.">23. Characterizing web content, user interests, and search behavior by reading level and topic.</a></li>
<li><a href="#24. Topical clustering of search results.">24. Topical clustering of search results.</a></li>
<li><a href="#25. To each his own: personalized content selection based on text comprehensibility.">25. To each his own: personalized content selection based on text comprehensibility.</a></li>
</ul>
</li>
<li><a href="#Web information extraction and data mining    4">Web information extraction and data mining    4</a><ul>
<li><a href="#26. WebSets: extracting sets of entities from the web using unsupervised information extraction.">26. WebSets: extracting sets of entities from the web using unsupervised information extraction.</a></li>
<li><a href="#27. Selecting actions for resource-bounded information extraction using reinforcement learning.">27. Selecting actions for resource-bounded information extraction using reinforcement learning.</a></li>
<li><a href="#28. Online selection of diverse results.">28. Online selection of diverse results.</a></li>
<li><a href="#29. Overlapping clusters for distributed computation.">29. Overlapping clusters for distributed computation.</a></li>
</ul>
</li>
<li><a href="#Spotlight on search and advertising    15">Spotlight on search and advertising    15</a><ul>
<li><a href="#30. Sponsored search auctions with conflict constraints.">30. Sponsored search auctions with conflict constraints.</a></li>
<li><a href="#31. Post-click conversion modeling and analysis for non-guaranteed delivery display advertising.">31. Post-click conversion modeling and analysis for non-guaranteed delivery display advertising.</a></li>
<li><a href="#32. Incorporating revisiting behaviors into click models.">32. Incorporating revisiting behaviors into click models.</a></li>
<li><a href="#33. A noise-aware click model for web search.">33. A noise-aware click model for web search.</a></li>
<li><a href="#34. Personalized click model through collaborative filtering.">34. Personalized click model through collaborative filtering.</a></li>
<li><a href="#35. Fair and balanced: learning to present news stories.">35. Fair and balanced: learning to present news stories.</a></li>
<li><a href="#36. Extracting search-focused key n-grams for relevance ranking in web search.">36. Extracting search-focused key n-grams for relevance ranking in web search.</a></li>
<li><a href="#37. Query suggestion by constructing term-transition graphs.">37. Query suggestion by constructing term-transition graphs.</a></li>
<li><a href="#38. Language models for keyword search over data graphs.">38. Language models for keyword search over data graphs.</a></li>
<li><a href="#39. Large-scale analysis of individual and task differences in search result page examination strategies.">39. Large-scale analysis of individual and task differences in search result page examination strategies.</a></li>
<li><a href="#40. Sequence clustering and labeling for unsupervised query intent discovery.">40. Sequence clustering and labeling for unsupervised query intent discovery.</a></li>
<li><a href="#41. IR system evaluation using nugget-based test collections.">41. IR system evaluation using nugget-based test collections.</a></li>
<li><a href="#42. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries.">42. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries.</a></li>
<li><a href="#43. Domain bias in web search.">43. Domain bias in web search.</a></li>
<li><a href="#44. Optimized top-k processing with global page scores on block-max indexes.">44. Optimized top-k processing with global page scores on block-max indexes.</a></li>
</ul>
</li>
<li><a href="#Web search II    4">Web search II    4</a><ul>
<li><a href="#45. Probabilistic models for personalizing web search.">45. Probabilistic models for personalizing web search.</a></li>
<li><a href="#46. Effective query formulation with multiple information sources.">46. Effective query formulation with multiple information sources.</a></li>
<li><a href="#47. Learning to rank with multi-aspect relevance for vertical search.">47. Learning to rank with multi-aspect relevance for vertical search.</a></li>
<li><a href="#48. Beyond ten blue links: enabling user click modeling in federated web search.">48. Beyond ten blue links: enabling user click modeling in federated web search.</a></li>
</ul>
</li>
<li><a href="#Web advertising and finance    4">Web advertising and finance    4</a><ul>
<li><a href="#49. Finding the right consumer: optimizing for conversion in display advertising campaigns.">49. Finding the right consumer: optimizing for conversion in display advertising campaigns.</a></li>
<li><a href="#50. Fast top-k retrieval for model based recommendation.">50. Fast top-k retrieval for model based recommendation.</a></li>
<li><a href="#51. Relational click prediction for sponsored search.">51. Relational click prediction for sponsored search.</a></li>
<li><a href="#52. "I loan because...": understanding motivations for pro-social lending.">52. "I loan because...": understanding motivations for pro-social lending.</a></li>
</ul>
</li>
<li><a href="#Social I    4">Social I    4</a><ul>
<li><a href="#53. Correlating financial time series with micro-blogging activity.">53. Correlating financial time series with micro-blogging activity.</a></li>
<li><a href="#54. Harmony and dissonance: organizing the people's voices on political controversies.">54. Harmony and dissonance: organizing the people's voices on political controversies.</a></li>
<li><a href="#55. Identifying content for planned events across social media sites.">55. Identifying content for planned events across social media sites.</a></li>
<li><a href="#56. Daily deals: prediction, social diffusion, and reputational ramifications.">56. Daily deals: prediction, social diffusion, and reputational ramifications.</a></li>
</ul>
</li>
<li><a href="#Spotlight on social    9">Spotlight on social    9</a><ul>
<li><a href="#57. On clustering heterogeneous social media objects with outlier links.">57. On clustering heterogeneous social media objects with outlier links.</a></li>
<li><a href="#58. Adding semantics to microblog posts.">58. Adding semantics to microblog posts.</a></li>
<li><a href="#59. Exploring social influence via posterior effect of word-of-mouth recommendations.">59. Exploring social influence via posterior effect of word-of-mouth recommendations.</a></li>
<li><a href="#60. Find me opinion sources in blogosphere: a unified framework for opinionated blog feed retrieval.">60. Find me opinion sources in blogosphere: a unified framework for opinionated blog feed retrieval.</a></li>
<li><a href="#61. Understanding cyclic trends in social choices.">61. Understanding cyclic trends in social choices.</a></li>
<li><a href="#62. Maximizing product adoption in social networks.">62. Maximizing product adoption in social networks.</a></li>
<li><a href="#63. Answers, not links: extracting tips from yahoo! answers to address how-to web queries.">63. Answers, not links: extracting tips from yahoo! answers to address how-to web queries.</a></li>
<li><a href="#64. A straw shows which way the wind blows: ranking potentially popular items from early votes.">64. A straw shows which way the wind blows: ranking potentially popular items from early votes.</a></li>
<li><a href="#65. A large-scale sentiment analysis for Yahoo! answers.">65. A large-scale sentiment analysis for Yahoo! answers.</a></li>
</ul>
</li>
<li><a href="#Spotlight on mining    1">Spotlight on mining    1</a><ul>
<li><a href="#66. What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities.">66. What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities.</a></li>
</ul>
</li>
<li><a href="#Spotlight on social    5">Spotlight on social    5</a><ul>
<li><a href="#67. Tips, dones and todos: uncovering user profiles in foursquare.">67. Tips, dones and todos: uncovering user profiles in foursquare.</a></li>
<li><a href="#68. When will it happen?: relationship prediction in heterogeneous information networks.">68. When will it happen?: relationship prediction in heterogeneous information networks.</a></li>
<li><a href="#69. The life and death of online groups: predicting group growth and longevity.">69. The life and death of online groups: predicting group growth and longevity.</a></li>
<li><a href="#70. Evaluating search in personal social media collections.">70. Evaluating search in personal social media collections.</a></li>
<li><a href="#71. Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization.">71. Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization.</a></li>
</ul>
</li>
<li><a href="#Social II    5">Social II    5</a><ul>
<li><a href="#72. Effects of user similarity in social media.">72. Effects of user similarity in social media.</a></li>
<li><a href="#73. How user behavior is related to social affinity.">73. How user behavior is related to social affinity.</a></li>
<li><a href="#74. Finding your friends and following them to where you are.">74. Finding your friends and following them to where you are.</a></li>
<li><a href="#75. How to win friends and influence people, truthfully: influence maximization mechanisms for social networks.">75. How to win friends and influence people, truthfully: influence maximization mechanisms for social networks.</a></li>
<li><a href="#76. Inferring social ties across heterogenous networks.">76. Inferring social ties across heterogenous networks.</a></li>
</ul>
</li>
<li><a href="#Keynote address    1">Keynote address    1</a><ul>
<li><a href="#77. The secret life of social links.">77. The secret life of social links.</a></li>
</ul>
</li>
<li><a href="#Doctoral consortium    4">Doctoral consortium    4</a><ul>
<li><a href="#78. Exploration and discovery of user-generated content in large information spaces.">78. Exploration and discovery of user-generated content in large information spaces.</a></li>
<li><a href="#79. Computational advertising: leveraging user interaction & contextual factors for improved ad relevance & targeting.">79. Computational advertising: leveraging user interaction &amp; contextual factors for improved ad relevance &amp; targeting.</a></li>
<li><a href="#80. The early bird gets the buzz: detecting anomalies and emerging trends in information networks.">80. The early bird gets the buzz: detecting anomalies and emerging trends in information networks.</a></li>
<li><a href="#81. Characterizing and harnessing peer-production of information in social tagging systems.">81. Characterizing and harnessing peer-production of information in social tagging systems.</a></li>
</ul>
</li>
<li><a href="#Tutorials    3">Tutorials    3</a><ul>
<li><a href="#82. Mining, searching and exploiting collaboratively generated content on the web.">82. Mining, searching and exploiting collaboratively generated content on the web.</a></li>
<li><a href="#83. Collaborative information seeking: understanding users, systems, and content.">83. Collaborative information seeking: understanding users, systems, and content.</a></li>
<li><a href="#84. Machine learning for query-document matching in search.">84. Machine learning for query-document matching in search.</a></li>
</ul>
</li>
<li><a href="#Workshops    2">Workshops    2</a><ul>
<li><a href="#85. 2nd international workshop on diversity in document retrieval (DDR 2012">85. 2nd international workshop on diversity in document retrieval (DDR 2012).</a>.)</li>
<li><a href="#86. WSCD 2012: workshop on web search click data 2012.">86. WSCD 2012: workshop on web search click data 2012.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="5. WSDM 2012:Seattle, WA, USA">5. WSDM 2012:Seattle, WA, USA</h1>
<p><a href="">Proceedings of the Fifth International Conference on Web Search and Web Data Mining, WSDM 2012, Seattle, WA, USA, February 8-12, 2012.</a> ACM
【<a href="http://dblp.uni-trier.de/db/conf/wsdm/wsdm2012.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 86 || Session Num: 17">Paper Num: 86 || Session Num: 17</h2>
<h2 id="Keynote address    1">Keynote address    1</h2>
<h3 id="1. Nowcasting the macroeconomy with search engine data.">1. Nowcasting the macroeconomy with search engine data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124297">Paper Link</a>】    【Pages】:1-2</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Varian:Hal_R=">Hal R. Varian</a></p>
<p>【Abstract】:
It is now possible to acquire real time information on economic variables of interest from various commercial sources. I illustrate how one can use Google Trends data to measure the state of the macroeconomy in various sectors, and discuss some of the ramifications for research and policy.</p>
<p>【Keywords】:
forecasting</p>
<h2 id="Multimedia and geo-mining    5">Multimedia and geo-mining    5</h2>
<h3 id="2. Spatially-aware indexing for image object retrieval.">2. Spatially-aware indexing for image object retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124299">Paper Link</a>】    【Pages】:3-12</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zwol:Roelof_van">Roelof van Zwol</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pueyo:Lluis_Garcia">Lluis Garcia Pueyo</a></p>
<p>【Abstract】:
The success of image object retrieval systems relies on the visual bag-of-words paradigm, which allows image retrieval systems to adopt a retrieval strategy analogous to text retrieval. In this paper we propose two spatially-aware retrieval strategies for image object retrieval that replaces the vector space model. The advantage of the proposed spatially-aware indexing and retrieval strategies are threefold: (1) It allows for the deployment of small visual vocabularies, (2) the number of images evaluated at retrieval time is significantly reduced, and (3) it eliminates the need for a post-retrieval phase, which is normally used to test the spatial composition of the visual words in the retrieved images. The first spatially-aware retrieval strategy explores the direct neighbourhood of two local features for common visual words to determine the similarity of the region surrounding the local features. The second strategy embeds the spatial composition of its neighbourhood directly in the index using edge signatures. Both strategies rely on the coherence of the neighbourhood of points in different images containing similar objects. The comparison of the spatially-aware retrieval strategies against the vector space baseline shows a significant improvement in terms of early precision, and at the same time significantly reduce the number of candidates to be considered at retrieval time.</p>
<p>【Keywords】:
image retrieval; quantization error; spatially-aware indexing</p>
<h3 id="3. Auralist: introducing serendipity into music recommendation.">3. Auralist: introducing serendipity into music recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124300">Paper Link</a>】    【Pages】:13-22</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yuan_Cao">Yuan Cao Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/S=eacute=aghdha:Diarmuid_=Oacute=">Diarmuid Ó Séaghdha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/q/Quercia:Daniele">Daniele Quercia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jambor:Tamas">Tamas Jambor</a></p>
<p>【Abstract】:
Recommendation systems exist to help users discover content in a large body of items. An ideal recommendation system should mimic the actions of a trusted friend or expert, producing a personalised collection of recommendations that balance between the desired goals of accuracy, diversity, novelty and serendipity. We introduce the Auralist recommendation framework, a system that - in contrast to previous work - attempts to balance and improve all four factors simultaneously. Using a collection of novel algorithms inspired by principles of "serendipitous discovery", we demonstrate a method of successfully injecting serendipity, novelty and diversity into recommendations whilst limiting the impact on accuracy. We evaluate Auralist quantitatively over a broad set of metrics and, with a user study on music recommendation, show that Auralist's emphasis on serendipity indeed improves user satisfaction.</p>
<p>【Keywords】:
accuracy; collaborative filtering; diversification; metrics; novelty; recommender systems; serendipity</p>
<h3 id="4. Efficient misbehaving user detection in online video chat services.">4. Efficient misbehaving user detection in online video chat services.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124301">Paper Link</a>】    【Pages】:23-32</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Hanqiang">Hanqiang Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liang:Yu=Li">Yu-Li Liang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Xinyu">Xinyu Xing</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xue">Xue Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Richard">Richard Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Qin">Qin Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mishra:Shivakant">Shivakant Mishra</a></p>
<p>【Abstract】:
Online video chat services, such as Chatroulette, Omegle, and vChatter are becoming increasingly popular and have attracted millions of users. One critical problem encountered in such applications is the presence of misbehaving users ("flashers") and obscene content. Automatically filtering out obscene content from these systems in an efficient manner poses a difficult challenge. This paper presents a novel Fine-Grained Cascaded (FGC) classification solution that significantly speeds up the compute-intensive process of classifying misbehaving users by dividing image feature extraction into multiple stages and filtering out easily classified images in earlier stages, thus saving unnecessary computation costs of feature extraction in later stages. Our work is further enhanced by integrating new webcam-related contextual information (illumination and color) into the classification process, and a 2-stage soft margin SVM algorithm for combining multiple features. Evaluation results using real-world data set obtained from Chatroulette show that the proposed FGC based classification solution significantly outperforms state-of-the-art techniques.</p>
<p>【Keywords】:
misbehaving user; online video chat</p>
<h3 id="5. Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities.">5. Beyond co-occurrence: discovering and visualizing tag relationships from geo-spatial and temporal similarities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124302">Paper Link</a>】    【Pages】:33-42</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Haipeng">Haipeng Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Korayem:Mohammed">Mohammed Korayem</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/You:Erkang">Erkang You</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crandall:David_J=">David J. Crandall</a></p>
<p>【Abstract】:
Studying relationships between keyword tags on social sharing websites has become a popular topic of research, both to improve tag suggestion systems and to discover connections between the concepts that the tags represent. Existing approaches have largely relied on tag co-occurrences. In this paper, we show how to find connections between tags by comparing their distributions over time and space, discovering tags with similar geographic and temporal patterns of use. Geo-spatial, temporal and geo-temporal distributions of tags are extracted and represented as vectors which can then be compared and clustered. Using a dataset of tens of millions of geo-tagged Flickr photos, we show that we can cluster Flickr photo tags based on their geographic and temporal patterns, and we evaluate the results both qualitatively and quantitatively using a panel of human judges. We also develop visualizations of temporal and geographic tag distributions, and show that they help humans recognize semantic relationships between tags. This approach to finding and visualizing similar tags is potentially useful for exploring any data having geographic and temporal annotations.</p>
<p>【Keywords】:
flickr; geo-spatial and temporal clustering; tag semantics and visualization</p>
<h3 id="6. Object matching in tweets with spatial models.">6. Object matching in tweets with spatial models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124303">Paper Link</a>】    【Pages】:43-52</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dalvi:Nilesh_N=">Nilesh N. Dalvi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:Ravi">Ravi Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pang:Bo">Bo Pang</a></p>
<p>【Abstract】:
Despite their 140-character limitation, tweets embody a lot of valuable information, especially temporal and spatial. In this paper we study the geographic aspects of tweets, for a given object domain. We propose a user-level model for spatial encoding in tweets that goes beyond the explicit geo-coding or place name mentions; this model can be used to match objects to tweets. We illustrate our model and methodology using restaurants as the objects, and show a significant improvement in performance over using standard language models. En route, we obtain a method to geolocate users who tweet about geolocated objects; this may be of independent interest.</p>
<p>【Keywords】:
language model; object matching; spatial model; tweets</p>
<h2 id="Spotlight on mining    15">Spotlight on mining    15</h2>
<h3 id="7. Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data.">7. Beyond 100 million entities: large-scale blocking-based resolution for heterogeneous data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124305">Paper Link</a>】    【Pages】:53-62</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Papadakis_0001:George">George Papadakis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ioannou:Ekaterini">Ekaterini Ioannou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nieder=eacute=e:Claudia">Claudia Niederée</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Palpanas:Themis">Themis Palpanas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nejdl:Wolfgang">Wolfgang Nejdl</a></p>
<p>【Abstract】:
A prerequisite for leveraging the vast amount of data available on the Web is Entity Resolution, i.e., the process of identifying and linking data that describe the same real-world objects. To make this inherently quadratic process applicable to large data sets, blocking is typically employed: entities (records) are grouped into clusters - the blocks - of matching candidates and only entities of the same block are compared. However, novel blocking techniques are required for dealing with the noisy, heterogeneous, semi-structured, user-generateddata in the Web, as traditional blocking techniques are inapplicable due to their reliance on schema information. The introduction of redundancy, improves the robustness of blocking methods but comes at the price of additional computational cost. In this paper, we present methods for enhancing the efficiency of redundancy-bearing blocking methods, such as our attribute-agnostic blocking approach. We introduce novel blocking schemes that build blocks based on a variety of evidences, including entity identifiers and relationships between entities; they significantly reduce the required number of comparisons, while maintaining blocking effectiveness at very high levels. We also introduce two theoretical measures that provide a reliable estimation of the performance of a blocking method, without requiring the analytical processing of its blocks. Based on these measures, we develop two techniques for improving the performance of blocking: combining individual, complementary blocking schemes, and purging blocks until given criteria are satisfied. We test our methods through an extensive experimental evaluation, using a voluminous data set with 182 million heterogeneous entities. The outcomes of our study show the applicability and the high performance of our approach.</p>
<p>【Keywords】:
attribute-agnostic blocking; data cleaning; entity resolution</p>
<h3 id="8. Mining contrastive opinions on political texts using cross-perspective topic model.">8. Mining contrastive opinions on political texts using cross-perspective topic model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124306">Paper Link</a>】    【Pages】:63-72</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fang:Yi">Yi Fang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Somasundaram:Naveen">Naveen Somasundaram</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Zhengtao">Zhengtao Yu</a></p>
<p>【Abstract】:
This paper presents a novel opinion mining research problem, which is called Contrastive Opinion Modeling (COM). Given any query topic and a set of text collections from multiple perspectives, the task of COM is to present the opinions of the individual perspectives on the topic, and furthermore to quantify their difference. This general problem subsumes many interesting applications, including opinion summarization and forecasting, government intelligence and cross-cultural studies. We propose a novel unsupervised topic model for contrastive opinion modeling. It simulates the generative process of how opinion words occur in the documents of different collections. The ad hoc opinion search process can be efficiently accomplished based on the learned parameters in the model. The difference of perspectives can be quantified in a principled way by the Jensen-Shannon divergence among the individual topic-opinion distributions. An extensive set of experiments have been conducted to evaluate the proposed model on two datasets in the political domain: 1) statement records of U.S. senators; 2) world news reports from three representative media in U.S., China and India, respectively. The experimental results with both qualitative and quantitative analysis have shown the effectiveness of the proposed model.</p>
<p>【Keywords】:
contrastive opinions; opinion mining; opinion retrieval; topic modeling</p>
<h3 id="9. Coupled temporal scoping of relational facts.">9. Coupled temporal scoping of relational facts.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124307">Paper Link</a>】    【Pages】:73-82</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_Pratim">Partha Pratim Talukdar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wijaya:Derry_Tanti">Derry Tanti Wijaya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>【Abstract】:
Recent research has made significant advances in automatically constructing knowledge bases by extracting relational facts (e.g., Bill Clinton-presidentOf-US) from large text corpora. Temporally scoping such relational facts in the knowledge base (i.e., determining that Bill Clinton-presidentOf-US is true only during the period 1993 - 2001) is an important, but relatively unexplored problem. In this paper, we propose a joint inference framework for this task, which leverages fact-specific temporal constraints, and weak supervision in the form of a few labeled examples. Our proposed framework, CoTS (Coupled Temporal Scoping), exploits temporal containment, alignment, succession, and mutual exclusion constraints among facts from within and across relations. Our contribution is multi-fold. Firstly, while most previous research has focused on micro-reading approaches for temporal scoping, we pose it in a macro-reading fashion, as a change detection in a time series of facts' features computed from a large number of documents. Secondly, to the best of our knowledge, there is no other work that has used joint inference for temporal scoping. We show that joint inference is effective compared to doing temporal scoping of individual facts independently. We conduct our experiments on large scale open-domain publicly available time-stamped datasets, such as English Gigaword Corpus and Google Books Ngrams, demonstrating CoTS's effectiveness.</p>
<p>【Keywords】:
joint inference; knowledge base; temporal scoping</p>
<h3 id="10. Overcoming browser cookie churn with clustering.">10. Overcoming browser cookie churn with clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124308">Paper Link</a>】    【Pages】:83-92</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dasgupta_0001:Anirban">Anirban Dasgupta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gurevich:Maxim">Maxim Gurevich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Liang">Liang Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tseng:Belle_L=">Belle L. Tseng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thomas:Achint_Oommen">Achint Oommen Thomas</a></p>
<p>【Abstract】:
Many large Internet websites are accessed by users anonymously, without requiring registration or logging-in. However, to provide personalized service these sites build anonymous, yet persistent, user models based on repeated user visits. Cookies, issued when a web browser first visits a site, are typically employed to anonymously associate a website visit with a distinct user (web browser). However, users may reset cookies, making such association short-lived and noisy. In this paper we propose a solution to the cookie churn problem: a novel algorithm for grouping similar cookies into clusters that are more persistent than individual cookies. Such clustering could potentially allow more robust estimation of the number of unique visitors of the site over a certain long time period, and also better user modeling which is key to plenty of web applications such as advertising and recommender systems. We present a novel method to cluster browser cookies into groups that are likely to belong to the same browser based on a statistical model of browser visitation patterns. We address each step of the clustering as a binary classification problem estimating the probability that two different subsets of cookies belong to the same browser. We observe that our clustering problem is a generalized interval graph coloring problem, and propose a greedy heuristic algorithm for solving it. The scalability of this method allows us to cluster hundreds of millions of browser cookies and provides significant improvements over baselines such as constrained K-means.</p>
<p>【Keywords】:
bayes factor; browser cookie churn; clustering algorithms; distributed computing; similarity measure</p>
<h3 id="11. mTrust: discerning multi-faceted trust in a connected world.">11. mTrust: discerning multi-faceted trust in a connected world.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124309">Paper Link</a>】    【Pages】:93-102</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jiliang">Jiliang Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Huiji">Huiji Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Huan">Huan Liu</a></p>
<p>【Abstract】:
Traditionally, research about trust assumes a single type of trust between users. However, trust, as a social concept, inherently has many facets indicating multiple and heterogeneous trust relationships between users. Due to the presence of a large trust network for an online user, it is necessary to discern multi-faceted trust as there are naturally experts of different types. Our study in product review sites reveals that people place trust differently to different people. Since the widely used adjacency matrix cannot capture multi-faceted trust relationships between users, we propose a novel approach by incorporating these relationships into traditional rating prediction algorithms to reliably estimate their strengths. Our work results in interesting findings such as heterogeneous pairs of reciprocal links. Experimental results on real-world data from Epinions and Ciao show that our work of discerning multi-faceted trust can be applied to improve the performance of tasks such as rating prediction, facet-sensitive ranking, and status theory.</p>
<p>【Keywords】:
heterogeneous trust; multi-dimension tie strength; multi-faceted trust; trust network</p>
<h3 id="12. Of hammers and nails: an empirical comparison of three paradigms for processing large graphs.">12. Of hammers and nails: an empirical comparison of three paradigms for processing large graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124310">Paper Link</a>】    【Pages】:103-112</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Najork:Marc">Marc Najork</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fetterly:Dennis">Dennis Fetterly</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Halverson:Alan">Alan Halverson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kenthapadi:Krishnaram">Krishnaram Kenthapadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gollapudi:Sreenivas">Sreenivas Gollapudi</a></p>
<p>【Abstract】:
Many phenomena and artifacts such as road networks, social networks and the web can be modeled as large graphs and analyzed using graph algorithms. However, given the size of the underlying graphs, efficient implementation of basic operations such as connected component analysis, approximate shortest paths, and link-based ranking (e.g. PageRank) becomes challenging. This paper presents an empirical study of computations on such large graphs in three well-studied platform models, viz., a relational model, a data-parallel model, and a special-purpose in-memory model. We choose a prototypical member of each platform model and analyze the computational efficiencies and requirements for five basic graph operations used in the analysis of real-world graphs viz., PageRank, SALSA, Strongly Connected Components (SCC), Weakly Connected Components (WCC), and Approximate Shortest Paths (ASP). Further, we characterize each platform in terms of these computations using model-specific implementations of these algorithms on a large web graph. Our experiments show that there is no single platform that performs best across different classes of operations on large graphs. While relational databases are powerful and flexible tools that support a wide variety of computations, there are computations that benefit from using special-purpose storage systems and others that can exploit data-parallel platforms.</p>
<p>【Keywords】:
data-parallel computing; databases; graph algorithms; graph servers; very large graphs</p>
<h3 id="13. Pairwise cross-domain factor model for heterogeneous transfer ranking.">13. Pairwise cross-domain factor model for heterogeneous transfer ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124311">Paper Link</a>】    【Pages】:113-122</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Long:Bo">Bo Long</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dong:Anlei">Anlei Dong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Jianzhang">Jianzhang He</a></p>
<p>【Abstract】:
Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation systems. Traditional ranking mainly focuses on one type of data source, and effective modeling relies on a sufficiently large number of labeled examples, which require expensive and time-consuming labeling process. However, in many real-world applications, ranking over multiple related heterogeneous domains becomes a common situation, where in some domains we may have a relatively large amount of training data while in some other domains we can only collect very little. Theretofore, how to leverage labeled information from related heterogeneous domain to improve ranking in a target domain has become a problem of great interests. In this paper, we propose a novel probabilistic model, pairwise cross-domain factor model, to address this problem. The proposed model learns latent factors(features) for multi-domain data in partially-overlapped heterogeneous feature spaces. It is capable of learning homogeneous feature correlation, heterogeneous feature correlation, and pairwise preference correlation for cross-domain knowledge transfer. We also derive two PCDF variations to address two important special cases. Under the PCDF model, we derive a stochastic gradient based algorithm, which facilitates distributed optimization and is flexible to adopt different loss functions and regularization functions to accommodate different data distributions. The extensive experiments on real world data sets demonstrate the effectiveness of the proposed model and algorithm.</p>
<p>【Keywords】:
heterogeneous transfer ranking; homogeneous transfer ranking; pairwise cross-domain factor model; ranking; source domain; stochastic gradient descent; target domain</p>
<h3 id="14. Scalable inference in latent variable models.">14. Scalable inference in latent variable models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124312">Paper Link</a>】    【Pages】:123-132</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ahmed:Amr">Amr Ahmed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aly:Mohamed">Mohamed Aly</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gonzalez:Joseph">Joseph Gonzalez</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Narayanamurthy:Shravan_M=">Shravan M. Narayanamurthy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a></p>
<p>【Abstract】:
Latent variable techniques are pivotal in tasks ranging from predicting user click patterns and targeting ads to organizing the news and managing user generated content. Latent variable techniques like topic modeling, clustering, and subspace estimation provide substantial insight into the latent structure of complex data with little or no external guidance making them ideal for reasoning about large-scale, rapidly evolving datasets. Unfortunately, due to the data dependencies and global state introduced by latent variables and the iterative nature of latent variable inference, latent-variable techniques are often prohibitively expensive to apply to large-scale, streaming datasets. In this paper we present a scalable parallel framework for efficient inference in latent variable models over streaming web-scale data. Our framework addresses three key challenges: 1) synchronizing the global state which includes global latent variables (e.g., cluster centers and dictionaries); 2) efficiently storing and retrieving the large local state which includes the data-points and their corresponding latent variables (e.g., cluster membership); and 3) sequentially incorporating streaming data (e.g., the news). We address these challenges by introducing: 1) a novel delta-based aggregation system with a bandwidth-efficient communication protocol; 2) schedule-aware out-of-core storage; and 3) approximate forward sampling to rapidly incorporate new data. We demonstrate state-of-the-art performance of our framework by easily tackling datasets two orders of magnitude larger than those addressed by the current state-of-the-art. Furthermore, we provide an optimized and easily customizable open-source implementation of the framework1.</p>
<p>【Keywords】:
graphical models; inference; large-scale systems; latent models</p>
<h3 id="15. Learning recommender systems with adaptive regularization.">15. Learning recommender systems with adaptive regularization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124313">Paper Link</a>】    【Pages】:133-142</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rendle:Steffen">Steffen Rendle</a></p>
<p>【Abstract】:
Many factorization models like matrix or tensor factorization have been proposed for the important application of recommender systems. The success of such factorization models depends largely on the choice of good values for the regularization parameters. Without a careful selection they result in poor prediction quality as they either underfit or overfit the data. Regularization values are typically determined by an expensive search that requires learning the model parameters several times: once for each tuple of candidate values for the regularization parameters. In this paper, we present a new method that adapts the regularization automatically while training the model parameters. To achieve this, we optimize simultaneously for two criteria: (1) as usual the model parameters for the regularized objective and (2) the regularization of future parameter updates for the best predictive quality on a validation set. We develop this for the generic model class of Factorization Machines which subsumes a wide variety of factorization models. We show empirically, that the advantages of our adaptive regularization method compared to expensive hyperparameter search do not come to the price of worse predictive quality. In total with our method, learning regularization parameters is as easy as learning model parameters and thus there is no need for any time-consuming search of regularization values because they are found on-the-fly. This makes our method highly attractive for practical use.</p>
<p>【Keywords】:
matrix factorization; regularization; tensor factorization</p>
<h3 id="16. Collaborative ranking.">16. Collaborative ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124314">Paper Link</a>】    【Pages】:143-152</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Balakrishnan:Suhrid">Suhrid Balakrishnan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chopra:Sumit">Sumit Chopra</a></p>
<p>【Abstract】:
Typical recommender systems use the root mean squared error (RMSE) between the predicted and actual ratings as the evaluation metric. We argue that RMSE is not an optimal choice for this task, especially when we will only recommend a few (top) items to any user. Instead, we propose using a ranking metric, namely normalized discounted cumulative gain (NDCG), as a better evaluation metric for this task. Borrowing ideas from the learning to rank community for web search, we propose novel models which approximately optimize NDCG for the recommendation task. Our models are essentially variations on matrix factorization models where we also additionally learn the features associated with the users and the items for the ranking task. Experimental results on a number of standard collaborative filtering data sets validate our claims. The results also show the accuracy and efficiency of our models and the benefits of learning features for ranking.</p>
<p>【Keywords】:
collaborative ranking; learning to rank; ndcg; recommender systems; rmse</p>
<h3 id="17. From chatter to headlines: harnessing the real-time web for personalized news recommendation.">17. From chatter to headlines: harnessing the real-time web for personalized news recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124315">Paper Link</a>】    【Pages】:153-162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Morales:Gianmarco_De_Francisci">Gianmarco De Francisci Morales</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gionis:Aristides">Aristides Gionis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lucchese:Claudio">Claudio Lucchese</a></p>
<p>【Abstract】:
We propose a new methodology for recommending interesting news to users by exploiting the information in their twitter persona. We model relevance between users and news articles using a mix of signals drawn from the news stream and from twitter: the profile of the social neighborhood of the users, the content of their own tweet stream, and topic popularity in the news and in the whole twitter-land. We validate our approach on a real-world dataset of approximately 40k articles coming from Yahoo! News and one month of crawled twitter data. We train our model using a learning-to-rank approach and support-vector machines. The train and test set are drawn from Yahoo! toolbar log data. We heuristically identify 3214 users of twitter in the log and use their clicks on news articles to train our system. Our methodology is able to predict with good accuracy the news articles clicked by the users and rank them higher than other news articles. The results show that the combination of various signals from real-time Web and micro-blogging platforms can be a useful resource to understand user behavior.</p>
<p>【Keywords】:
micro-blogging applications; news recommendation; personalization; real-time web; recommendation systems</p>
<h3 id="18. ETF: extended tensor factorization model for personalizing prediction of review helpfulness.">18. ETF: extended tensor factorization model for personalizing prediction of review helpfulness.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124316">Paper Link</a>】    【Pages】:163-172</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moghaddam:Samaneh">Samaneh Moghaddam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jamali:Mohsen">Mohsen Jamali</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ester:Martin">Martin Ester</a></p>
<p>【Abstract】:
Online reviews are valuable sources of information for a variety of decision-making processes such as purchasing products. As the number of online reviews is growing rapidly, it becomes increasingly difficult for users to identify those that are helpful. This has motivated research into the problem of identifying high quality and helpful reviews automatically. The current methods assume that the helpfulness of a review is independent from the readers of that review. However, we argue that the quality of a review may not be the same for different users. For example, a professional and an amateur photographer may rate the helpfulness of a review very differently. In this paper, we introduce the problem of predicting a personalized review quality for recommendation of helpful reviews. To address this problem, we propose a series of increasingly sophisticated probabilistic graphical models, based on Matrix Factorization and Tensor Factorization. We evaluate the proposed models using a database of 1.5 million reviews and more than 13 million quality ratings obtained from Epinions.com. The experiments demonstrate that the proposed latent factor models outperform the state-of-the art approaches using textual and social features. Finally, our experiments confirm that the helpfulness of a review is indeed not the same for all users and that there are some latent factors that affect a user's evaluation of the review quality.</p>
<p>【Keywords】:
matrix factorization; personalized review quality prediction; review recommendation; tensor factorization</p>
<h3 id="19. Multi-relational matrix factorization using bayesian personalized ranking for social network data.">19. Multi-relational matrix factorization using bayesian personalized ranking for social network data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124317">Paper Link</a>】    【Pages】:173-182</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Krohn=Grimberghe:Artus">Artus Krohn-Grimberghe</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Drumond:Lucas">Lucas Drumond</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Freudenthaler:Christoph">Christoph Freudenthaler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schmidt=Thieme:Lars">Lars Schmidt-Thieme</a></p>
<p>【Abstract】:
A key element of the social networks on the internet such as Facebook and Flickr is that they encourage users to create connections between themselves, other users and objects. One important task that has been approached in the literature that deals with such data is to use social graphs to predict user behavior (e.g. joining a group of interest). More specifically, we study the cold-start problem, where users only participate in some relations, which we will call social relations, but not in the relation on which the predictions are made, which we will refer to as target relations. We propose a formalization of the problem and a principled approach to it based on multi-relational factorization techniques. Furthermore, we derive a principled feature extraction scheme from the social data to extract predictors for a classifier on the target relation. Experiments conducted on real world datasets show that our approach outperforms current methods.</p>
<p>【Keywords】:
cold-start; item prediction; item recommendation; joint factorization; matrix factorization; multi-relational learning; ranking; recommender systems; social network</p>
<h3 id="20. Comment spam detection by sequence mining.">20. Comment spam detection by sequence mining.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124318">Paper Link</a>】    【Pages】:183-192</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kant:Ravi">Ravi Kant</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sengamedu:Srinivasan_H=">Srinivasan H. Sengamedu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:Krishnan_S=">Krishnan S. Kumar</a></p>
<p>【Abstract】:
Comments are supported by several web sites to increase user participation. Users can usually comment on a variety of media types - photos, videos, news articles, blogs, etc. Comment spam is one of the biggest challenges facing this feature. The traditional approach to combat spam is to train classifiers using various machine learning techniques. Since the commonly used classifiers work on the entire comment text, it is easy to mislead them by embedding spam content in good content. In this paper, we make several contributions towards comment spam detection. (1) We propose a new framework for spam detection that is immune to embed attacks. We characterize spam by a set of frequently occurring sequential patterns. (2) We introduce a variant (called min-closed) of the frequent closed sequence mining problem that succinctly captures all the frequently occurring patterns. We prove as well as experimentally show that the set of min-closed sequences is an order of magnitude smaller than the set of closed sequences and yet has exactly the same coverage. (3) We describe MCPRISM, extension of the recently published PRISM algorithm that effectively mines min-closed sequences, using prime encoding. In the process, we solve the open problem of using the prime-encoding technique to speed up traditional closed sequence mining. (4) We finally need to whittle down the set of frequent subsequences to a small set without sacrificing coverage. This problem is NP-Hard but we show that the coverage function is submodular and hence the greedy heuristic gives a fast algorithm that is close to optimal. We then describe the experiments that were carried out on a large real world comment data and the publicly available Gazelle dataset. (1) We show that nearly 80% of spam on real world data can be effectively captured by the mined sequences at very low false positive rates. (2) The sequences mined are highly discriminative. (3) On Gazelle data, the proposed algorithmic enhancements are faster by at least by a factor and by an order of magnitude on the larger comment dataset.</p>
<p>【Keywords】:
comments spam detection; frequent subsequence mining</p>
<h3 id="21. Mining slang and urban opinion words and phrases from cQA services: an optimization approach.">21. Mining slang and urban opinion words and phrases from cQA services: an optimization approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124319">Paper Link</a>】    【Pages】:193-202</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Amiri:Hadi">Hadi Amiri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a></p>
<p>【Abstract】:
Current opinion lexicons contain most of the common opinion words, but they miss slang and so-called urban opinion words and phrases (e.g. delish, cozy, yummy, nerdy, and yuck). These subjectivity clues are frequently used in community questions and are useful for opinion question analysis. This paper introduces a principled approach to constructing an opinion lexicon for community-based question answering (cQA) services. We formulate the opinion lexicon induction as a semi-supervised learning task in the graph context. Our method makes use of existing opinion words to extract new opinion entities (slang and urban words/phrases) from community questions. It then models the opinion entities in a graph context to learn the polarity of the new opinion entities based on the graph connectivity information. In contrast to previous approaches, our method not only learns such polarities from the labeled data but also from the unlabeled data and is more feasible in the web context where the dictionary-based relations (such as synonym, antonym, or hyponym) between most words are not available for constructing a high quality graph. The experiments show that our approach is effective both in terms of the quality of the discovered new opinion entities as well as its ability in inferring their polarity. Furthermore, since the value of opinion lexicons lies in their usefulness in applications, we show the utility of the constructed lexicon in the sentiment classification task.</p>
<p>【Keywords】:
opinion lexicon; opinion mining; sentiment analysis; sentiment orientation; slang; urban word</p>
<h2 id="Web search I    4">Web search I    4</h2>
<h3 id="22. No search result left behind: branching behavior with browser tabs.">22. No search result left behind: branching behavior with browser tabs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124322">Paper Link</a>】    【Pages】:203-212</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jeff">Jeff Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Thomas">Thomas Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a></p>
<p>【Abstract】:
Today's Web browsers allow users to open links in new windows or tabs. This action, which we call 'branching', is sometimes performed on search results when the user plans to eventually visit multiple results. We detect branching behavior on a large commercial search engine with a client-side script on the results page. Two-fifths of all users spawned new tabs on search results in the timeframe of our study; branching usage varied with different query types and vertical. Both branching and backtracking are viable methods for visiting multiple search results. To understand user search strategies, we treat multiple result clicks following a query as ordered events to understand user search strategies. Users branching in a query are more likely to click search results from top to bottom, while users who backtrack are less likely to do so; this is especially true for queries involving more than two clicks. These findings inform an experiment in which we take a popular click model and modify it to account for the differing user behavior when branching. By understanding that users continue examining search results before viewing a branched result, we can improve the click model for branching queries.</p>
<p>【Keywords】:
browser tabs; click models; examining search results</p>
<h3 id="23. Characterizing web content, user interests, and search behavior by reading level and topic.">23. Characterizing web content, user interests, and search behavior by reading level and topic.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124323">Paper Link</a>】    【Pages】:213-222</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Jin_Young">Jin Young Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Collins=Thompson:Kevyn">Kevyn Collins-Thompson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bennett:Paul_N=">Paul N. Bennett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a></p>
<p>【Abstract】:
A user's expertise or ability to understand a document on a given topic is an important aspect of that document's relevance. However, this aspect has not been well-explored in information retrieval systems, especially those at Web scale where the great diversity of content, users, and tasks presents an especially challenging search problem. To help improve our modeling and understanding of this diversity, we apply automatic text classifiers, based on reading difficulty and topic prediction, to estimate a novel type of profile for important entities in Web search -- users, websites, and queries. These profiles capture topic and reading level distributions, which we then use in conjunction with search log data to characterize and compare different entities. We find that reading level and topic distributions provide an important new representation of Web content and user interests, and that using both together is more effective than using either one separately. In particular we find that: 1) the reading level of Web content and the diversity of visitors to a website can vary greatly by topic; 2) the degree to which a user's profile matches with a site's profile is closely correlated with the user's preference of the website in search results, and 3) site or URL profiles can be used to predict 'expertness' whether a given site or URL is oriented toward expert vs. non-expert users. Our findings provide strong evidence in favor of jointly incorporating reading level and topic distribution metadata into a variety of critical tasks in Web information systems.</p>
<p>【Keywords】:
domain expertise; log analysis; reading level prediction; topic prediction; web search</p>
<h3 id="24. Topical clustering of search results.">24. Topical clustering of search results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124324">Paper Link</a>】    【Pages】:223-232</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Scaiella:Ugo">Ugo Scaiella</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Ferragina:Paolo">Paolo Ferragina</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Marino:Andrea">Andrea Marino</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Ciaramita:Massimiliano">Massimiliano Ciaramita</a></p>
<p>【Abstract】:
Search results clustering (SRC) is a challenging algorithmic problem that requires grouping together the results returned by one or more search engines in topically coherent clusters, and labeling the clusters with meaningful phrases describing the topics of the results included in them. In this paper we propose to solve SRC via an innovative approach that consists of modeling the problem as the labeled clustering of the nodes of a newly introduced graph of topics. The topics are Wikipedia-pages identified by means of recently proposed topic annotators [9, 11, 16, 20] applied to the search results, and the edges denote the relatedness among these topics computed by taking into account the linkage of the Wikipedia-graph. We tackle this problem by designing a novel algorithm that exploits the spectral properties and the labels of that graph of topics. We show the superiority of our approach with respect to academic state-of-the-art work [6] and well-known commercial systems (CLUSTY and LINGO3G) by performing an extensive set of experiments on standard datasets and user studies via Amazon Mechanical Turk. We test several standard measures for evaluating the performance of all systems and show a relative improvement of up to 20%.</p>
<p>【Keywords】:
search result clustering; spectral clustering; topical annotation; user study</p>
<h3 id="25. To each his own: personalized content selection based on text comprehensibility.">25. To each his own: personalized content selection based on text comprehensibility.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124325">Paper Link</a>】    【Pages】:233-242</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Chenhao">Chenhao Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gabrilovich:Evgeniy">Evgeniy Gabrilovich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pang:Bo">Bo Pang</a></p>
<p>【Abstract】:
Imagine a physician and a patient doing a search on antibiotic resistance. Or a chess amateur and a grandmaster conducting a search on Alekhine's Defence. Although the topic is the same, arguably the two users in each case will satisfy their information needs with very different texts. Yet today search engines mostly adopt the one-size-fits-all solution, where personalization is restricted to topical preference. We found that users do not uniformly prefer simple texts, and that the text comprehensibility level should match the user's level of preparedness. Consequently, we propose to model the comprehensibility of texts as well as the users' reading proficiency in order to better explain how different users choose content for further exploration. We also model topic-specific reading proficiency, which allows us to better explain why a physician might choose to read sophisticated medical articles yet simple descriptions of SLR cameras. We explore different ways to build user profiles, and use collaborative filtering techniques to overcome data sparsity. We conducted experiments on large-scale datasets from a major Web search engine and a community question answering forum. Our findings confirm that explicitly modeling text comprehensibility can significantly improve content ranking (search results or answers, respectively).</p>
<p>【Keywords】:
personalization; re-ranking; text comprehensibility; user modeling</p>
<h2 id="Web information extraction and data mining    4">Web information extraction and data mining    4</h2>
<h3 id="26. WebSets: extracting sets of entities from the web using unsupervised information extraction.">26. WebSets: extracting sets of entities from the web using unsupervised information extraction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124327">Paper Link</a>】    【Pages】:243-252</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dalvi:Bhavana_Bharat">Bhavana Bharat Dalvi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cohen:William_W=">William W. Cohen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Callan:Jamie">Jamie Callan</a></p>
<p>【Abstract】:
We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.</p>
<p>【Keywords】:
clustering; hyponymy relation acquisition; web mining</p>
<h3 id="27. Selecting actions for resource-bounded information extraction using reinforcement learning.">27. Selecting actions for resource-bounded information extraction using reinforcement learning.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124328">Paper Link</a>】    【Pages】:253-262</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanani:Pallika_H=">Pallika H. Kanani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>【Abstract】:
Given a database with missing or uncertain content, our goal is to correct and fill the database by extracting specific information from a large corpus such as the Web, and to do so under resource limitations. We formulate the information gathering task as a series of choices among alternative, resource-consuming actions and use reinforcement learning to select the best action at each time step. We use temporal difference q-learning method to train the function that selects these actions, and compare it to an online, error-driven algorithm called SampleRank. We present a system that finds information such as email, job title and department affiliation for the faculty at our university, and show that the learning-based approach accomplishes this task efficiently under a limited action budget. Our evaluations show that we can obtain 92.4% of the final F1, by only using 14.3% of all possible actions.</p>
<p>【Keywords】:
active information acquisition; reinforcement learning; resource-bounded information extraction; web mining</p>
<h3 id="28. Online selection of diverse results.">28. Online selection of diverse results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124329">Paper Link</a>】    【Pages】:263-272</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Panigrahi:Debmalya">Debmalya Panigrahi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarma:Atish_Das">Atish Das Sarma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Gagan">Gagan Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tomkins:Andrew">Andrew Tomkins</a></p>
<p>【Abstract】:
The phenomenal growth in the volume of easily accessible information via various web-based services has made it essential for service providers to provide users with personalized representative summaries of such information. Further, online commercial services including social networking and micro-blogging websites, e-commerce portals, leisure and entertainment websites, etc. recommend interesting content to users that is simultaneously diverse on many different axes such as topic, geographic specificity, etc. The key algorithmic question in all these applications is the generation of a succinct, representative, and relevant summary from a large stream of data coming from a variety of sources. In this paper, we formally model this optimization problem, identify its key structural characteristics, and use these observations to design an extremely scalable and efficient algorithm. We analyze the algorithm using theoretical techniques to show that it always produces a nearly optimal solution. In addition, we perform large-scale experiments on both real-world and synthetically generated datasets, which confirm that our algorithm performs even better than its analytical guarantees in practice, and also outperforms other candidate algorithms for the problem by a wide margin.</p>
<p>【Keywords】:
online algorithm; result diversity</p>
<h3 id="29. Overlapping clusters for distributed computation.">29. Overlapping clusters for distributed computation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124330">Paper Link</a>】    【Pages】:273-282</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Andersen:Reid">Reid Andersen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gleich:David_F=">David F. Gleich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mirrokni:Vahab_S=">Vahab S. Mirrokni</a></p>
<p>【Abstract】:
Most graph decomposition procedures seek to partition a graph into disjoint sets of vertices. Motivated by applications of clustering in distributed computation, we describe a graph decomposition algorithm for the paradigm where the partitions intersect. This algorithm covers the vertex set with a collection of overlapping clusters. Each vertex in the graph is well-contained within some cluster in the collection. We then describe a framework for distributed computation across a collection of overlapping clusters and describe how this framework can be used in various algorithms based on the graph diffusion process. In particular, we focus on two illustrative examples: (i) the simulation of a randomly walking particle and (ii) the solution of a linear system, e.g. PageRank. Our simulation results for these two cases show a significant reduction in swapping between clusters in a random walk, a significant decrease in communication volume during a linear system solve in a geometric mesh, and some ability to reduce the communication volume during a linear system solve in an information network.</p>
<p>【Keywords】:
conductance; covering; diffusion; leave-time; pagerank</p>
<h2 id="Spotlight on search and advertising    15">Spotlight on search and advertising    15</h2>
<h3 id="30. Sponsored search auctions with conflict constraints.">30. Sponsored search auctions with conflict constraints.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124332">Paper Link</a>】    【Pages】:283-292</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Papadimitriou_0002:Panagiotis">Panagiotis Papadimitriou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Garcia=Molina:Hector">Hector Garcia-Molina</a></p>
<p>【Abstract】:
In sponsored search auctions advertisers compete for ad slots in the search engine results page, by bidding on keywords of interest. To improve advertiser expressiveness, we augment the bidding process with conflict constraints. With such constraints, advertisers can condition their bids on the non-appearance of certain undesired ads on the results page. We study the complexity of the allocation problem in these augmented SSA and we introduce an algorithm that can efficiently allocate the ad slots to advertisers. We evaluate the algorithm run time in simulated conflict scenarios and we study the implications of the conflict constraints on search engine revenue. Our results show that the allocation problem can be solved within few tens of milliseconds and that the adoption of conflict constraints can potentially increase search engine revenue.</p>
<p>【Keywords】:
auction design; branch-and-bound; conflicts; sponsored search</p>
<h3 id="31. Post-click conversion modeling and analysis for non-guaranteed delivery display advertising.">31. Post-click conversion modeling and analysis for non-guaranteed delivery display advertising.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124333">Paper Link</a>】    【Pages】:293-302</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rosales:R=oacute=mer">Rómer Rosales</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Haibin">Haibin Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Manavoglu:Eren">Eren Manavoglu</a></p>
<p>【Abstract】:
In on-line search and display advertising, the click-trough rate (CTR) has been traditionally a key measure of ad/campaign effectiveness. More recently, the market has gained interest in more direct measures of profitability, one early alternative is the conversion rate (CVR). CVRs measure the proportion of certain users who take a predefined, desirable action, such as a purchase, registration, download, etc.; as compared to simply page browsing. We provide a detailed analysis of conversion rates in the context of non-guaranteed delivery targeted advertising. In particular we focus on the post-click conversion (PCC) problem or the analysis of conversions after a user click on a referring ad. The key elements we study are the probability of a conversion given a click in a user/page context, P(conversion | click, context). We provide various fundamental properties of this process based on contextual information, formalize the problem of predicting PCC, and propose an approach for measuring attribute relevance when the underlying attribute distribution is non-stationary. We provide experimental analyses based on logged events from a large-scale advertising platform.</p>
<p>【Keywords】:
conversion modeling; conversion rate; display advertising; non-guaranteed delivery; post-click conversion</p>
<h3 id="32. Incorporating revisiting behaviors into click models.">32. Incorporating revisiting behaviors into click models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124334">Paper Link</a>】    【Pages】:303-312</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Danqing">Danqing Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yiqun">Yiqun Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0006:Min">Min Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Shaoping">Shaoping Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ru:Liyun">Liyun Ru</a></p>
<p>【Abstract】:
Click-through behaviors are treated as invaluable sources of user feedback and they have been leveraged in several commercial search engines in recent years. However, estimating unbiased relevance is always a challenging task because of position bias. To solve this problem, many researchers have proposed a variety of assumptions to model click-through behaviors. Most of these models share a common examination hypothesis, which is that users examine search results from the top to the bottom. Nevertheless, this model cannot draw a complete picture of information-seeking behaviors. Many eye-tracking studies find that user interactions are not sequential but contain revisiting patterns. If a user clicks on a higher ranked document after having clicked on a lower-ranked one, we call this scenario a revisiting pattern, and we believe that the revisiting patterns are important signals regarding a user's click preferences. This paper incorporates revisiting behaviors into click models and introduces a novel click model named Temporal Hidden Click Model (THCM). This model dynamically models users' click behaviors with a temporal order. In our experiment, we collect over 115 million query sessions from a widely-used commercial search engine and then conduct a comparative analysis between our model and several state-of-the-art click models. The experimental results show that the THCM model achieves a significant improvement in the Normalized Discounted Cumulative Gain (NDCG), the click perplexity and click distributions metrics.</p>
<p>【Keywords】:
click-through behavior; document relevance; revisit; temporal hidden click model</p>
<h3 id="33. A noise-aware click model for web search.">33. A noise-aware click model for web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124335">Paper Link</a>】    【Pages】:313-322</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Weizhu">Weizhu Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dong">Dong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yuchen">Yuchen Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singla:Adish">Adish Singla</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a></p>
<p>【Abstract】:
Recent advances in click model have established it as an attractive approach to infer document relevance. Most of these advances consider the user click/skip behavior as binary events but neglect the context in which a click happens. We show that real click behavior in industrial search engines is often noisy and not always a good indication of relevance. For a considerable percentage of clicks, users select what turn out to be irrelevant documents and these clicks should not be directly used as evidence for relevance inference. Thus in this paper, we put forward an observation that the relevance indication degree of a click is not a constant, but can be differentiated by user preferences and the context in which the user makes her click decision. In particular, to interpret the click behavior discriminatingly, we propose a Noise-aware Click Model (NCM) by characterizing the noise degree of a click, which indicates the quality of the click for inferring relevance. Specifically, the lower the click noise is, the more important the click is in its role for relevance inference. To verify the necessity of explicitly accounting for the uninformative noise in a user click, we conducted experiments on a billion-scale dataset. Extensive experimental results demonstrate that as compared with two state-of-the-art click models in Web Search, NCM can better interpret user click behavior and achieve significant improvements in terms of both perplexity and NDCG.</p>
<p>【Keywords】:
click model; click noise; log analysis</p>
<h3 id="34. Personalized click model through collaborative filtering.">34. Personalized click model through collaborative filtering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124336">Paper Link</a>】    【Pages】:323-332</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Si">Si Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Botao">Botao Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Weizhu">Weizhu Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a></p>
<p>【Abstract】:
Click modeling aims to interpret the users' search click data in order to predict their clicking behavior. Existing models can well characterize the position bias of documents and snippets in relation to users' mainstream click behavior. Yet, current advances depict users' search actions only in a general setting by implicitly assuming that all users act in the same way, regardless of the fact that anyone, motivated with some individual interest, is more likely to click on a link than others. It is in light of this that we put forward a novel personalized click model to describe the user-oriented click preferences, which applies and extends matrix / tensor factorization from the view of collaborative filtering to connect users, queries and documents together. Our model serves as a generalized personalization framework that can be incorporated to the previously proposed click models and, in many cases, to their future extensions. Despite the sparsity of search click data, our personalized model demonstrates its advantage over the best click models previously discussed in the Web-search literature, supported by our large-scale experiments on a real dataset. A delightful bonus is the model's ability to gain insights into queries and documents through latent feature vectors, and hence to handle rare and even new query-document pairs much better than previous click models.</p>
<p>【Keywords】:
click log analysis; click model; personalization; search engine; user behavior</p>
<h3 id="35. Fair and balanced: learning to present news stories.">35. Fair and balanced: learning to present news stories.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124337">Paper Link</a>】    【Pages】:333-342</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ahmed:Amr">Amr Ahmed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Teo:Choon_Hui">Choon Hui Teo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vishwanathan:S=_V=_N=">S. V. N. Vishwanathan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a></p>
<p>【Abstract】:
Relevance, diversity and personalization are key issues when presenting content which is apt to pique a user's interest. This is particularly true when presenting an engaging set of news stories. In this paper we propose an efficient algorithm for selecting a small subset of relevant articles from a streaming news corpus. It offers three key pieces of improvement over past work: 1) It is based on a detailed model of a user's viewing behavior which does not require explicit feedback. 2) We use the notion of submodularity to estimate the propensity of interacting with content. This improves over the classical context independent relevance ranking algorithms. Unlike existing methods, we learn the submodular function from the data. 3) We present an efficient online algorithm which can be adapted for personalization, story adaptation, and factorization models. Experiments show that our system yields a significant improvement over a retrieval system deployed in production.</p>
<p>【Keywords】:
graphical models; online learning; personalization; submodularity</p>
<h3 id="36. Extracting search-focused key n-grams for relevance ranking in web search.">36. Extracting search-focused key n-grams for relevance ranking in web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124338">Paper Link</a>】    【Pages】:343-352</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Chen">Chen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bi:Keping">Keping Bi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hu:Yunhua">Yunhua Hu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cao:Guihong">Guihong Cao</a></p>
<p>【Abstract】:
In web search, relevance ranking of popular pages is relatively easy, because of the inclusion of strong signals such as anchor text and search log data. In contrast, with less popular pages, relevance ranking becomes very challenging due to a lack of information. In this paper the former is referred to as head pages, and the latter tail pages. We address the challenge by learning a model that can extract search-focused key n-grams from web pages, and using the key n-grams for searches of the pages, particularly, the tail pages. To the best of our knowledge, this problem has not been previously studied. Our approach has four characteristics. First, key n-grams are search-focused in the sense that they are defined as those which can compose "good queries" for searching the page. Second, key n-grams are learned in a relative sense using learning to rank techniques. Third, key n-grams are learned using search log data, such that the characteristics of key n-grams in the search log data, particularly in the heads; can be applied to the other data, particularly to the tails. Fourth, the extracted key n-grams are used as features of the relevance ranking model also trained with learning to rank techniques. Experiments validate the effectiveness of the proposed approach with large-scale web search datasets. The results show that our approach can significantly improve relevance ranking performance on both heads and tails; and particularly tails, compared with baseline approaches. Characteristics of our approach have also been fully investigated through comprehensive experiments.</p>
<p>【Keywords】:
key n-gram extraction; learning to rank; ranking; search relevance; tail page</p>
<h3 id="37. Query suggestion by constructing term-transition graphs.">37. Query suggestion by constructing term-transition graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124339">Paper Link</a>】    【Pages】:353-362</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Song:Yang">Yang Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Dengyong">Dengyong Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Li=wei">Li-wei He</a></p>
<p>【Abstract】:
Query suggestion is an interactive approach for search engines to better understand users information need. In this paper, we propose a novel query suggestion framework which leverages user re-query feedbacks from search engine logs. Specifically, we mined user query reformulation activities where the user only modifies part of the query by (1) adding terms after the query, (2) deleting terms within the query, or (3) modifying terms to new terms. We build a term-transition graph based on the mined data. Two models are proposed which address topic-level and term-level query suggestions, respectively. In the first topic-based unsupervised Pagerank model, we perform random walk on each of the topic-based term-transition graph and calculate the Pagerank for each term within a topic. Given a new query, we suggest relevant queries based on its topic distribution and term-transition probability within each topic. Our second model resembles the supervised learning-to-rank (LTR) framework, in which term modifications are treated as documents so that each query reformulation is treated as a training instance. A rich set of features are constructed for each (query, document) pair from Pagerank, Wikipedia, N-gram, ODP and so on. This supervised model is capable of suggesting new queries on a term level which addresses the limitation of previous methods. Experiments are conducted on a large data set from a commercial search engine. By comparing the with state-of-the-art query suggestion methods [4, 2], our proposals exhibit significant performance increase for all categories of queries.</p>
<p>【Keywords】:
learning-to-rank; pagerank; query suggestion</p>
<h3 id="38. Language models for keyword search over data graphs.">38. Language models for keyword search over data graphs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124340">Paper Link</a>】    【Pages】:363-372</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mass:Yosi">Yosi Mass</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sagiv:Yehoshua">Yehoshua Sagiv</a></p>
<p>【Abstract】:
In keyword search over data graphs, an answer is a non-redundant subtree that includes the given keywords. This paper focuses on improving the effectiveness of that type of search. A novel approach that combines language models with structural relevance is described. The proposed approach consists of three steps. First, language models are used to assign dynamic, query-dependent weights to the graph. Those weights complement static weights that are pre-assigned to the graph. Second, an existing algorithm returns candidate answers based on their weights. Third, the candidate answers are re-ranked by creating a language model for each one. The effectiveness of the proposed approach is verified on a benchmark of three datasets: IMDB, Wikipedia and Mondial. The proposed approach outperforms all existing systems on the three datasets, which is a testament to its robustness. It is also shown that the effectiveness can be further improved by augmenting keyword queries with very basic knowledge about the structure.</p>
<p>【Keywords】:
data graphs; language models; ranking; semantic weights</p>
<h3 id="39. Large-scale analysis of individual and task differences in search result page examination strategies.">39. Large-scale analysis of individual and task differences in search result page examination strategies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124341">Paper Link</a>】    【Pages】:373-382</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Buscher:Georg">Georg Buscher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jeff">Jeff Huang</a></p>
<p>【Abstract】:
Understanding the impact of individual and task differences on search result page examination strategies is important in developing improved search engines. Characterizing these effects using query and click data alone is common but insufficient since they provide an incomplete picture of result examination behavior. Cursor- or gaze-tracking studies reveal richer interaction patterns but are often done in small-scale laboratory settings. In this paper we leverage large-scale rich behavioral log data in a naturalistic setting. We examine queries, clicks, cursor movements, scrolling, and text highlighting for millions of queries on the Bing commercial search engine to better understand the impact of user, task, and user-task interactions on user behavior on search result pages (SERPs). By clustering users based on cursor features, we identify individual, task, and user-task differences in how users examine results which are similar to those observed in small-scale studies. Our findings have implications for developing search support for behaviorally-similar searcher cohorts, modeling search behavior, and designing search systems that leverage implicit feedback.</p>
<p>【Keywords】:
individual differences; rich interaction logging; task differences</p>
<h3 id="40. Sequence clustering and labeling for unsupervised query intent discovery.">40. Sequence clustering and labeling for unsupervised query intent discovery.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124342">Paper Link</a>】    【Pages】:383-392</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiao">Xiao Li</a></p>
<p>【Abstract】:
One popular form of semantic search observed in several modern search engines is to recognize query patterns that trigger instant answers or domain-specific search, producing semantically enriched search results. This often requires understanding the query intent in addition to the meaning of the query terms in order to access structured data sources. A major challenge in intent understanding is to construct a domain-dependent schema and to annotate search queries based on such a schema, a process that to date has required much manual annotation effort. We present an unsupervised method for clustering queries with similar intent and for producing a pattern consisting of a sequence of semantic concepts and/or lexical items for each intent. Furthermore, we leverage the discovered intent patterns to automatically annotate a large number of queries beyond those used in clustering. We evaluated our method on 10 selected domains, discovering over 1400 intent patterns and automatically annotating 125K (and potentially many more) queries. We found that over 90% of patterns and 80% of instance annotations tested are judged to be correct by a majority of annotators.</p>
<p>【Keywords】:
clustering; query intent discovery; semantic search</p>
<h3 id="41. IR system evaluation using nugget-based test collections.">41. IR system evaluation using nugget-based test collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124343">Paper Link</a>】    【Pages】:393-402</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pavlu:Virgiliu">Virgiliu Pavlu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rajput:Shahzad">Shahzad Rajput</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Golbus:Peter_B=">Peter B. Golbus</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aslam:Javed_A=">Javed A. Aslam</a></p>
<p>【Abstract】:
The development of information retrieval systems such as search engines relies on good test collections, including assessments of retrieved content. The widely employed Cranfield paradigm dictates that the information relevant to a topic be encoded at the level of documents, therefore requiring effectively complete document relevance assessments. As this is no longer practical for modern corpora, numerous problems arise, including scalability, reusability, and applicability. We propose a new method for relevance assessment based on relevant information, not relevant documents. Once the relevant 'nuggets' are collected, our matching method can assess any document for relevance with high accuracy, and so any retrieved list of documents can be assessed for performance. In this paper we analyze the performance of the matching function by looking at specific cases and by comparing with other methods. We then show how these inferred relevance assessments can be used to perform IR system evaluation, and we discuss in particular reusability and scalability. Our main contribution is a methodology for producing test collections that are highly accurate, more complete, scalable, reusable, and can be generated with similar amounts of effort as existing methods, with great potential for future applications.</p>
<p>【Keywords】:
nuggets; test collection</p>
<h3 id="42. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries.">42. Tapping into knowledge base for concept feedback: leveraging conceptnet to improve search results for difficult queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124344">Paper Link</a>】    【Pages】:403-412</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kotov:Alexander">Alexander Kotov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
Query expansion is an important and commonly used technique for improving Web search results. Existing methods for query expansion have mostly relied on global or local analysis of document collection, click-through data, or simple ontologies such as WordNet. In this paper, we present the results of a systematic study of the methods leveraging the ConceptNet knowledge base, an emerging new Web resource, for query expansion. Specifically, we focus on the methods leveraging ConceptNet to improve the search results for poorly performing (or difficult) queries. Unlike other lexico-semantic resources, such as WordNet and Wikipedia, which have been extensively studied in the past, ConceptNet features a graph-based representation model of commonsense knowledge, in which the terms are conceptually related through rich relational ontology. Such representation structure enables complex, multi-step inferences between the concepts, which can be applied to query expansion. We first demonstrate through simulation experiments that expanding queries with the related concepts from ConceptNet has great potential for improving the search results for difficult queries. We then propose and study several supervised and unsupervised methods for selecting the concepts from ConceptNet for automatic query expansion. The experimental results on multiple data sets indicate that the proposed methods can effectively leverage ConceptNet to improve the retrieval performance of difficult queries both when used in isolation as well as in combination with pseudo-relevance feedback.</p>
<p>【Keywords】:
conceptnet; knowledge bases; query analysis; query expansion</p>
<h3 id="43. Domain bias in web search.">43. Domain bias in web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124345">Paper Link</a>】    【Pages】:413-422</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Ieong:Samuel">Samuel Ieong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mishra:Nina">Nina Mishra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sadikov:Eldar">Eldar Sadikov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Li">Li Zhang</a></p>
<p>【Abstract】:
This paper uncovers a new phenomenon in web search that we call domain bias --- a user's propensity to believe that a page is more relevant just because it comes from a particular domain. We provide evidence of the existence of domain bias in click activity as well as in human judgments via a comprehensive collection of experiments. We begin by studying the difference between domains that a search engine surfaces and that users click. Surprisingly, we find that despite changes in the overall distribution of surfaced domains, there has not been a comparable shift in the distribution of clicked domains. Users seem to have learned the landscape of the internet and their click behavior has thus become more predictable over time. Next, we run a blind domain test, akin to a Pepsi/Coke taste test, to determine whether domains can shift a user's opinion of which page is more relevant. We find that domains can actually flip a user's preference about 25% of the time. Finally, we demonstrate the existence of systematic domain preferences, even after factoring out confounding issues such as position bias and relevance, two factors that have been used extensively in past work to explain user behavior. The existence of domain bias has numerous consequences including, for example, the importance of discounting click activity from reputable domains.</p>
<p>【Keywords】:
domain bias; user behavior; web search</p>
<h3 id="44. Optimized top-k processing with global page scores on block-max indexes.">44. Optimized top-k processing with global page scores on block-max indexes.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124346">Paper Link</a>】    【Pages】:423-432</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shan:Dongdong">Dongdong Shan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Shuai">Shuai Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Jing">Jing He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Hongfei">Hongfei Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a></p>
<p>【Abstract】:
Large web search engines are facing formidable performance challenges because they have to process thousands of queries per second on tens of billions of documents, within interactive response time. Among many others, Top-k query processing (also called early termination or dynamic pruning) is an important class of optimization techniques that can improve the search efficiency and achieve faster query processing by avoiding the scoring of documents that are unlikely to be in the top results. One recent technique is using Block-Max index. In the Block-Max index, the posting lists are organized as blocks and the maximum score for each block is stored to improve the query efficiency. Although query processing speedup is achieved with Block-Max index, the ranking function for the Top-k results is the term-based approach. It is well known that documents' static scores are also important for a good ranking function. In this paper, we show that the performance of the state-of-the-art algorithms with the Block-Max index is degraded when the static score is added in the ranking function. Then we study efficient techniques for Top-k query processing in the case where a page's static score is given, such as PageRank, in addition to the term-based approach. In particular, we propose a set of new algorithms based on the WAND and MaxScore with Block-Max index using local score, which outperform the existing ones. Then we propose new techniques to estimate a better score upper bound for each block. We also study the search efficiency on different index structures where the document identifiers are assigned by URL sorting or by static document scores. Experiments on TREC GOV2 and ClueWeb09B show that considerable performance gains are achieved.</p>
<p>【Keywords】:
block-max inverted index; early termination; top-k query processing</p>
<h2 id="Web search II    4">Web search II    4</h2>
<h3 id="45. Probabilistic models for personalizing web search.">45. Probabilistic models for personalizing web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124348">Paper Link</a>】    【Pages】:433-442</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sontag:David">David Sontag</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Collins=Thompson:Kevyn">Kevyn Collins-Thompson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bennett:Paul_N=">Paul N. Bennett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Billerbeck:Bodo">Bodo Billerbeck</a></p>
<p>【Abstract】:
We present a new approach for personalizing Web search results to a specific user. Ranking functions for Web search engines are typically trained by machine learning algorithms using either direct human relevance judgments or indirect judgments obtained from click-through data from millions of users. The rankings are thus optimized to this generic population of users, not to any specific user. We propose a generative model of relevance which can be used to infer the relevance of a document to a specific user for a search query. The user-specific parameters of this generative model constitute a compact user profile. We show how to learn these profiles from a user's long-term search history. Our algorithm for computing the personalized ranking is simple and has little computational overhead. We evaluate our personalization approach using historical search data from thousands of users of a major Web search engine. Our findings demonstrate gains in retrieval performance for queries with high ambiguity, with particularly large improvements for acronym queries.</p>
<p>【Keywords】:
machine learning; personalization; probabilistic models; re-ranking</p>
<h3 id="46. Effective query formulation with multiple information sources.">46. Effective query formulation with multiple information sources.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124349">Paper Link</a>】    【Pages】:443-452</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bendersky:Michael">Michael Bendersky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metzler:Donald">Donald Metzler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
Most standard information retrieval models use a single source of information (e.g., the retrieval corpus) for query formulation tasks such as term and phrase weighting and query expansion. In contrast, in this paper, we present a unified framework that automatically optimizes the combination of information sources used for effective query formulation. The proposed framework produces fully weighted and expanded queries that are both more effective and more compact than those produced by the current state-of-the-art query expansion and weighting methods. We conduct an empirical evaluation of our framework for both newswire and web corpora. In all cases, our combination of multiple information sources for query formulation is found to be more effective than using any single source. The proposed query formulations are especially advantageous for large scale web corpora, where they also reduce the number of terms required for effective query expansion, and improve the diversity of the retrieved results.</p>
<p>【Keywords】:
external sources; query expansion; query formulation</p>
<h3 id="47. Learning to rank with multi-aspect relevance for vertical search.">47. Learning to rank with multi-aspect relevance for vertical search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124350">Paper Link</a>】    【Pages】:453-462</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kang:Changsung">Changsung Kang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Xuanhui">Xuanhui Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tseng:Belle_L=">Belle L. Tseng</a></p>
<p>【Abstract】:
Many vertical search tasks such as local search focus on specific domains. The meaning of relevance in these verticals is domain-specific and usually consists of multiple well-defined aspects (e.g., text matching and distance in local search). Thus the overall relevance between a query and a document is a tradeoff between multiple relevance aspects. Such a tradeoff can vary for different types of queries or in different contexts. In this paper, we explore these vertical-specific aspects in the learning to rank setting. We propose a novel formulation in which the relevance between a query and a document is assessed with respect to each aspect, forming the multi-aspect relevance. In order to compute a ranking function, we study two types of learning-based approaches to estimate the tradeoff between these relevance aspects: a label aggregation method and a model aggregation method. Since there are only a few aspects, a minimal amount of training data is needed to learn the tradeoff. We conduct both offline and online test experiments on a local search engine and the experimental results show that our proposed multi-aspect relevance formulation is very promising. The two types of aggregation methods perform more effectively than a set of baseline methods including a conventional learning to rank method.</p>
<p>【Keywords】:
aggregation; multi-aspect relevance; web search</p>
<h3 id="48. Beyond ten blue links: enabling user click modeling in federated web search.">48. Beyond ten blue links: enabling user click modeling in federated web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124351">Paper Link</a>】    【Pages】:463-472</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Danqi">Danqi Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Weizhu">Weizhu Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Haixun">Haixun Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a></p>
<p>【Abstract】:
Click models have been positioned as an effective approach to interpret user click behavior in search engines. Existing click models mostly focus on traditional Web search that considers only ten homogeneous Web HTML documents that appear on the first search-result page. However, in modern commercial search engines, more and more Web search results are federated from multiple sources and contain non-HTML results returned by other heterogeneous vertical engines, such as video or image search engines. In this paper, we study user click behavior in federated search. We observed that user click behavior in federated search is highly different from that in traditional Web search, making it difficult to interpret using existing click models. In response, we propose a novel federated click model (FCM) to interpret user click behavior in federated search. In particular, we take into considerations two new biases in FCM. The first comes from the observation that users tend to be attracted by vertical results and their visual attention on them may increase the examination probability of other nearby web results. The other illustrates that user click behavior on vertical results may lead to more clues of search relevance due to their presentation style in federated search. With these biases and an effective model to correct them, FCM is more accurate in characterizing user click behavior in federated search. Our extensive experimental results show that FCM can outperform other click models in interpreting user click behavior in federated search and achieve significant improvements in terms of both perplexity and log-likelihood.</p>
<p>【Keywords】:
click model; federated search; log analysis</p>
<h2 id="Web advertising and finance    4">Web advertising and finance    4</h2>
<h3 id="49. Finding the right consumer: optimizing for conversion in display advertising campaigns.">49. Finding the right consumer: optimizing for conversion in display advertising campaigns.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124353">Paper Link</a>】    【Pages】:473-482</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yandong">Yandong Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pandey:Sandeep">Sandeep Pandey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Deepak">Deepak Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Josifovski:Vanja">Vanja Josifovski</a></p>
<p>【Abstract】:
The ultimate goal of advertisers are conversions representing desired user actions on the advertisers' websites in the form of purchases and product information request. In this paper we address the problem of finding the right audience for display campaigns by finding the users that are most likely to convert. This challenging problem is at the heart of display campaign optimization and has to deal with several issues such as very small percentage of converters in the general population, high-dimensional representation of the user profiles, large churning rate of users and advertisers. To overcome these difficulties, in our approach we use two sources of information: a seed set of users that have converted for a campaign in the past; and a description of the campaign based on the advertiser's website. We explore the importance of the information provided by each of these two sources in a principled manner and then combine them to propose models for predicting converters. In particular, we show how seed set can be used to capture the campaign-specific targeting constraints, while the campaign metadata allows to share targeting knowledge across campaigns. We give methods for learning these models and perform experiments on real-world advertising campaigns. Our findings show that the seed set and the campaign metadata are complimentary to each other and both sources provide valuable information for conversion optimization.</p>
<p>【Keywords】:
advertising; conversions; modeling</p>
<h3 id="50. Fast top-k retrieval for model based recommendation.">50. Fast top-k retrieval for model based recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124354">Paper Link</a>】    【Pages】:483-492</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal:Deepak">Deepak Agarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gurevich:Maxim">Maxim Gurevich</a></p>
<p>【Abstract】:
A crucial task in many recommender problems like computational advertising, content optimization, and others is to retrieve a small set of items by scoring a large item inventory through some elaborate statistical/machine-learned model. This is challenging since the retrieval has to be fast (few milliseconds) to load the page quickly. Fast retrieval is well studied in the information retrieval (IR) literature, especially in the context of document retrieval for queries. When queries and documents have sparse representation and relevance is measured through cosine similarity (or some variant thereof), one could build highly efficient retrieval algorithms that scale gracefully to increasing item inventory. The key components exploited by such algorithms is sparse query-document representation and the special form of the relevance function. Many machine-learned models used in modern recommender problems do not satisfy these properties and since brute force evaluation is not an option with large item inventory, heuristics that filter out some items are often employed to reduce model computations at runtime. In this paper, we take a two-stage approach where the first stage retrieves top-K items using our approximate procedures and the second stage selects the desired top-k using brute force model evaluation on the K retrieved items. The main idea of our approach is to reduce the first stage to a standard IR problem, where each item is represented by a sparse feature vector (a.k.a. the vector-space representation) and the query-item relevance score is given by vector dot product. The sparse item representation is learnt to closely approximate the original machine-learned score by using retrospective data. Such a reduction allows leveraging extensive work in IR that resulted in highly efficient retrieval systems. Our approach is model-agnostic, relying only on data generated from the machine-learned model. We obtain significant improvements in the computational cost vs. accuracy tradeoff compared to several baselines in our empirical evaluation on both synthetic models and on a click-through (CTR) model used in online advertising.</p>
<p>【Keywords】:
inverted index; lasso; machine learning; support vector machines; top-k retrieval</p>
<h3 id="51. Relational click prediction for sponsored search.">51. Relational click prediction for sponsored search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124355">Paper Link</a>】    【Pages】:493-502</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xiong:Chenyan">Chenyan Xiong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Taifeng">Taifeng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Wenkui">Wenkui Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Yidong">Yidong Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Tie=Yan">Tie-Yan Liu</a></p>
<p>【Abstract】:
This paper is concerned with the prediction of clicking an ad in sponsored search. The accurate prediction of user's click on an ad plays an important role in sponsored search, because it is widely used in both ranking and pricing of the ads. Previous work on click prediction usually takes a single ad as input, and ignores its relationship to the other ads shown in the same page. This independence assumption here, however, might not be valid in the real scenario. In this paper, we first perform an analysis on this issue by looking at the click-through rates (CTR) of the same ad, in the same position and for the same query, but surrounded by different ads. We found that in most cases the CTR varies largely, which suggests that the relationship between ads is really an important factor in predicting click probability. Furthermore, our investigation shows that the more similar the surrounding ads are to an ad, the lower the CTR of the ad is. Based on this observation, we design a continuous conditional random fields (CRF) based model for click prediction, which considers both the features of an ad and its similarity to the surrounding ads. We show that the model can be effectively learned using maximum likelihood estimation, and can also be efficiently inferred due to its closed form solution. Our experimental results on the click-through log from a commercial search engine show that the proposed model can predict clicks more accurately than previous independent models. To our best knowledge this is the first work that predicts ad clicks by considering the relationship between ads.</p>
<p>【Keywords】:
continuous crf; online advertising; relational click prediction; sponsored search</p>
<h3 id="52. "I loan because...": understanding motivations for pro-social lending.">52. "I loan because...": understanding motivations for pro-social lending.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124356">Paper Link</a>】    【Pages】:503-512</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu_0019:Yang">Yang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Roy">Roy Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yan">Yan Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mei:Qiaozhu">Qiaozhu Mei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Salib:Suzy">Suzy Salib</a></p>
<p>【Abstract】:
As a new paradigm of online communities, microfinance sites such as Kiva.org have attracted much public attention. To understand lender motivations on Kiva, we classify the lenders' self-stated motivations into ten categories with human coders and machine learning based classifiers. We employ text classifiers using lexical features, along with social features based on lender activity information on Kiva, to predict the categories of lender motivation statements. Although the task appears to be much more challenging than traditional topic-based categorization, our classifiers can achieve high precision in most categories. Using the results of this classification along with Kiva teams information, we predict lending activity from lender motivation and team affiliations. Finally, we make design recommendations regarding Kiva practices which might increase pro-social lending.</p>
<p>【Keywords】:
kiva; lending motivation; microfinance; pro-social lending; text classification</p>
<h2 id="Social I    4">Social I    4</h2>
<h3 id="53. Correlating financial time series with micro-blogging activity.">53. Correlating financial time series with micro-blogging activity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124358">Paper Link</a>】    【Pages】:513-522</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ruiz:Eduardo_J=">Eduardo J. Ruiz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hristidis:Vagelis">Vagelis Hristidis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Castillo:Carlos">Carlos Castillo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gionis:Aristides">Aristides Gionis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jaimes:Alejandro">Alejandro Jaimes</a></p>
<p>【Abstract】:
We study the problem of correlating micro-blogging activity with stock-market events, defined as changes in the price and traded volume of stocks. Specifically, we collect messages related to a number of companies, and we search for correlations between stock-market events for those companies and features extracted from the micro-blogging messages. The features we extract can be categorized in two groups. Features in the first group measure the overall activity in the micro-blogging platform, such as number of posts, number of re-posts, and so on. Features in the second group measure properties of an induced interaction graph, for instance, the number of connected components, statistics on the degree distribution, and other graph-based properties. We present detailed experimental results measuring the correlation of the stock market events with these features, using Twitter as a data source. Our results show that the most correlated features are the number of connected components and the number of nodes of the interaction graph. The correlation is stronger with the traded volume than with the price of the stock. However, by using a simulator we show that even relatively small correlations between price and micro-blogging features can be exploited to drive a stock trading strategy that outperforms other baseline strategies.</p>
<p>【Keywords】:
financial time series; micro-blogging; social networks</p>
<h3 id="54. Harmony and dissonance: organizing the people's voices on political controversies.">54. Harmony and dissonance: organizing the people's voices on political controversies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124359">Paper Link</a>】    【Pages】:523-532</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Awadallah:Rawia">Rawia Awadallah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ramanath:Maya">Maya Ramanath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weikum:Gerhard">Gerhard Weikum</a></p>
<p>【Abstract】:
The wikileaks documents about the death of Osama Bin Laden and the debates about the economic crisis in Greece and other European countries are some of the controversial topics being played on the news everyday. Each of these topics has many different aspects, and there is no absolute, simple truth in answering questions such as: should the EU guarantee the financial stability of each member country, or should the countries themselves be solely responsible? To understand the landscape of opinions, it would be helpful to know which politician or other stakeholder takes which position - support or opposition - on these aspects of controversial topics.</p>
<p>【Keywords】:
political opinion mining; web information extraction</p>
<h3 id="55. Identifying content for planned events across social media sites.">55. Identifying content for planned events across social media sites.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124360">Paper Link</a>】    【Pages】:533-542</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Becker:Hila">Hila Becker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Iter:Dan">Dan Iter</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naaman:Mor">Mor Naaman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gravano:Luis">Luis Gravano</a></p>
<p>【Abstract】:
User-contributed Web data contains rich and diverse information about a variety of events in the physical world, such as shows, festivals, conferences and more. This information ranges from known event features (e.g., title, time, location) posted on event aggregation platforms (e.g., Last.fm events, EventBrite, Facebook events) to discussions and reactions related to events shared on different social media sites (e.g., Twitter, YouTube, Flickr). In this paper, we focus on the challenge of automatically identifying user-contributed content for events that are planned and, therefore, known in advance, across different social media sites. We mine event aggregation platforms to extract event features, which are often noisy or missing. We use these features to develop query formulation strategies for retrieving content associated with an event on different social media sites. Further, we explore ways in which event content identified on one social media site can be used to retrieve additional relevant event content on other social media sites. We apply our strategies to a large set of user-contributed events, and analyze their effectiveness in retrieving relevant event content from Twitter, YouTube, and Flickr.</p>
<p>【Keywords】:
cross-site document retrieval; event identification; social media</p>
<h3 id="56. Daily deals: prediction, social diffusion, and reputational ramifications.">56. Daily deals: prediction, social diffusion, and reputational ramifications.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124361">Paper Link</a>】    【Pages】:543-552</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Byers:John_W=">John W. Byers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitzenmacher:Michael">Michael Mitzenmacher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zervas:Georgios">Georgios Zervas</a></p>
<p>【Abstract】:
Daily deal sites have become the latest Internet sensation, providing discounted offers to customers for restaurants, ticketed events, services, and other items. We begin by undertaking a study of the economics of daily deals on the web, based on a dataset we compiled by monitoring Groupon and LivingSocial sales in 20 large cities over several months. We use this dataset to characterize deal purchases; glean insights about operational strategies of these firms; and evaluate customers' sensitivity to factors such as price, deal scheduling, and limited inventory. We then marry our daily deals dataset with additional datasets we compiled from Facebook and Yelp users to study the interplay between social networks and daily deal sites. First, by studying user activity on Facebook while a deal is running, we provide evidence that daily deal sites benefit from significant word-of-mouth effects during sales events, consistent with results predicted by cascade models. Second, we consider the effects of daily deals on the longer-term reputation of merchants, based on their Yelp reviews before and after they run a daily deal. Our analysis shows that while the number of reviews increases significantly due to daily deals, average rating scores from reviewers who mention daily deals are 10% lower than scores of their peers on average.</p>
<p>【Keywords】:
daily deals; reputation; social diffusion</p>
<h2 id="Spotlight on social    9">Spotlight on social    9</h2>
<h3 id="57. On clustering heterogeneous social media objects with outlier links.">57. On clustering heterogeneous social media objects with outlier links.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124363">Paper Link</a>】    【Pages】:553-562</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qi:Guo=Jun">Guo-Jun Qi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Charu_C=">Charu C. Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Thomas_S=">Thomas S. Huang</a></p>
<p>【Abstract】:
The clustering of social media objects provides intrinsic understanding of the similarity relationships between documents, images, and their contextual sources. Both content and link structure provide important cues for an effective clustering algorithm of the underlying objects. While link information provides useful hints for improving the clustering process, it also contains a significant amount of noisy information. Therefore, a robust clustering algorithm is required to reduce the impact of noisy links. In order to address the aforementioned problems, we propose heterogeneous random fields to model the structure and content of social media networks. We design a probability measure on the social media networks which output a configuration of clusters that are consistent with both content and link structure. Furthermore, noisy links can also be detected, and their impact on the clustering algorithm can be significantly reduced. We conduct experiments on a real social media network and show the advantage of the method over other state-of-the-art algorithms.</p>
<p>【Keywords】:
noisy links; robust clustering; social media networks</p>
<h3 id="58. Adding semantics to microblog posts.">58. Adding semantics to microblog posts.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124364">Paper Link</a>】    【Pages】:563-572</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Meij:Edgar">Edgar Meij</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weerkamp:Wouter">Wouter Weerkamp</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
Microblogs have become an important source of information for the purpose of marketing, intelligence, and reputation management. Streams of microblogs are of great value because of their direct and real-time nature. Determining what an individual microblog post is about, however, can be non-trivial because of creative language usage, the highly contextualized and informal nature of microblog posts, and the limited length of this form of communication. We propose a solution to the problem of determining what a microblog post is about through semantic linking: we add semantics to posts by automatically identifying concepts that are semantically related to it and generating links to the corresponding Wikipedia articles. The identified concepts can subsequently be used for, e.g., social media mining, thereby reducing the need for manual inspection and selection. Using a purpose-built test collection of tweets, we show that recently proposed approaches for semantic linking do not perform well, mainly due to the idiosyncratic nature of microblog posts. We propose a novel method based on machine learning with a set of innovative features and show that it is able to achieve significant improvements over all other methods, especially in terms of precision.</p>
<p>【Keywords】:
microblogs; semantic linking; wikipedia</p>
<h3 id="59. Exploring social influence via posterior effect of word-of-mouth recommendations.">59. Exploring social influence via posterior effect of word-of-mouth recommendations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124365">Paper Link</a>】    【Pages】:573-582</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Junming">Junming Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Huawei">Huawei Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Tao">Tao Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Xiaolong">Xiaolong Jin</a></p>
<p>【Abstract】:
Word-of-mouth has proven an effective strategy for promoting products through social relations. Particularly, existing studies have convincingly demonstrated that word-of-mouth recommendations can boost users' prior expectation and hence encourage them to adopt a certain innovation, such as buying a book or watching a movie. However, less attention has been paid to studying the posterior effect of word-of-mouth recommendations, i.e., whether or not word-of-mouth recommendations can influence users' posterior evaluation on the products or services recommended to them, the answer to which is critical to estimating user satisfaction when proposing a word-of-mouth marketing strategy. In order to fill this gap, in this paper we empirically study the above issue and verify that word-of-mouth recommendations are strongly associated with users' posterior evaluation. Through elaborately designed statistical hypothesis tests we prove the causality that word-of-mouth recommendations directly prompt the posterior evaluation of receivers. Finally, we propose a method for investigating users' social influence, namely, their ability to affect followers' posterior evaluation via word-of-mouth recommendations, by examining the number of their followers and their sensitivity of discovering good items. The experimental results on real datasets show that our method can successfully identify 78% influential friends with strong social influence.</p>
<p>【Keywords】:
posterior evaluation; social influence; word-of-mouth recommendation</p>
<h3 id="60. Find me opinion sources in blogosphere: a unified framework for opinionated blog feed retrieval.">60. Find me opinion sources in blogosphere: a unified framework for opinionated blog feed retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124366">Paper Link</a>】    【Pages】:583-592</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Xueke">Xueke Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tan:Songbo">Songbo Tan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Yue">Yue Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Zheng">Zheng Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a></p>
<p>【Abstract】:
This paper aims to find blog feeds having a principal inclination towards making opinionated comments on the given topic, so that we can subscribe to them to track influential and interesting opinions in the blogosphere. One major challenge is assigning topic-related opinion scores to blog feeds, which is embodied in two aspects. Firstly, we should identify whether the blog feed has a principal on-topic opinionated inclination. This inclination should be collectively revealed by all posts of the feed. We should fully consider evidences from all the posts of the feed to identify salient information among many posts of the feed. Secondly, we should capture topic-related opinions in the blog feed while ignoring irrelevant opinions. In this paper, we propose a unified framework for opinionated blog feed retrieval, which combines topic relevance and opinion scores with a generative model. Furthermore, we propose a language modeling approach to estimating opinion scores that is seamlessly integrated into the framework, where two language models, Topic-specific Opinion Model (TOM) and Topic-biased Feed Model (TFM), work collectively to reflect whether the blog feed shows a principal on-topic opinionated inclination. To estimate TFM, we propose a topic-biased random walk to exploit both content and structural information to capture topic-biased salient information in the feed. As for TOM estimation, we propose to use a generative mixture model with prior guidance to effectively capture topic-specific opinion expressing language usage. The conducted experiments in the context of the TREC 2009-2010 Blog Track show the effectiveness of our proposed approaches.</p>
<p>【Keywords】:
mixture model; opinionated blog feed retrieval; topic-biased random walk; topic-related opinionatedness</p>
<h3 id="61. Understanding cyclic trends in social choices.">61. Understanding cyclic trends in social choices.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124367">Paper Link</a>】    【Pages】:593-602</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sarma:Anish_Das">Anish Das Sarma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gollapudi:Sreenivas">Sreenivas Gollapudi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Panigrahy:Rina">Rina Panigrahy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Li">Li Zhang</a></p>
<p>【Abstract】:
Motivated by trends in popularity of products, we present a formal model for studying trends in our choice of products in terms of three parameters: (1) their innate utility; (2) individual boredom associated with repeated usage of an item; and (3) social influences associated with the preferences from other people. Different from previous work, in this paper we introduce boredom to explain the cyclic pattern in individual and social choices. We formally model boredom and show that a rational individual would make cyclic choices when considering the boredom factor. Furthermore, we extend the model to social choices by showing that a society that votes for a particular style or product can be viewed as a single individual cycling through different choices. We adopt a natural model of utility an individual derives from using an item, i.e., the utility of an item gets discounted by its repeated use and increases when the item is not used. We address the problem of optimally choosing items for usage, so as to maximize overall user satisfaction over a period of time. First we show that the simple greedy heuristic of always choosing the item with the maximum current composite utility can be arbitrarily worse than the optimal. Second, we prove that even with just a single individual, determining the optimal strategy for choosing items is NP-hard. Third, we show that a simple modification to the greedy algorithm is a provably close approximation to the optimal strategy. Finally, we present an experimental study over real-world data collected from query logs to compare our algorithms.</p>
<p>【Keywords】:
cyclic trends; fashion trends; social choice</p>
<h3 id="62. Maximizing product adoption in social networks.">62. Maximizing product adoption in social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124368">Paper Link</a>】    【Pages】:603-612</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bhagat:Smriti">Smriti Bhagat</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goyal_0002:Amit">Amit Goyal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lakshmanan:Laks_V=_S=">Laks V. S. Lakshmanan</a></p>
<p>【Abstract】:
One of the key objectives of viral marketing is to identify a small set of users in a social network, who when convinced to adopt a product will influence others in the network leading to a large number of adoptions in an expected sense. The seminal work of Kempe et al. [13] approaches this as the problem of influence maximization. This and other previous papers tacitly assume that a user who is influenced (or, informed) about a product necessarily adopts the product and encourages her friends to adopt it. However, an influenced user may not adopt the product herself, and yet form an opinion based on the experiences of her friends, and share this opinion with others. Furthermore, a user who adopts the product may not like the product and hence not encourage her friends to adopt it to the same extent as another user who adopted and liked the product. This is independent of the extent to which those friends are influenced by her. Previous works do not account for these phenomena. We argue that it is important to distinguish product adoption from influence. We propose a model that factors in a user's experience (or projected experience) with a product. We adapt the classical Linear Threshold (LT) propagation model by defining an objective function that explicitly captures product adoption, as opposed to influence. We show that under our model, adoption maximization is NP-hard and the objective function is monotone and submodular, thus admitting an approximation algorithm. We perform experiments on three real popular social networks and show that our model is able to distinguish between influence and adoption, and predict product adoption much more accurately than approaches based on the classical LT model.</p>
<p>【Keywords】:
influence; product adoption; viral marketing</p>
<h3 id="63. Answers, not links: extracting tips from yahoo! answers to address how-to web queries.">63. Answers, not links: extracting tips from yahoo! answers to address how-to web queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124369">Paper Link</a>】    【Pages】:613-622</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Ingmar">Ingmar Weber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ukkonen:Antti">Antti Ukkonen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gionis:Aristides">Aristides Gionis</a></p>
<p>【Abstract】:
We investigate the problem of mining "tips" from Yahoo! Answers and displaying those tips in response to related web queries. Here, a "tip" is a short, concrete and self-contained bit of non-obvious advice such as "To zest a lime if you don't have a zester : use a cheese grater." First, we estimate the volume of web queries with "how-to" intent, which could be potentially addressed by a tip. Second, we analyze how to detect such queries automatically without solely relying on literal "how to *" patterns. Third, we describe how to derive potential tips automatically from Yahoo! Answers, and we develop machine-learning techniques to remove low-quality tips. Finally, we discuss how to match web queries with "how-to" intent to tips. We evaluate both the quality of these direct displays as well as the size of the query volume that can be addressed by serving tips.</p>
<p>【Keywords】:
direct display; how-to queries; tips; web search</p>
<h3 id="64. A straw shows which way the wind blows: ranking potentially popular items from early votes.">64. A straw shows which way the wind blows: ranking potentially popular items from early votes.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124370">Paper Link</a>】    【Pages】:623-632</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Peifeng">Peifeng Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Ping">Ping Luo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Min">Min Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a></p>
<p>【Abstract】:
Prediction of popular items in online content sharing systems has recently attracted a lot of attention due to the tremendous need of users and its commercial values. Different from previous works that make prediction by fitting a popularity growth model, we tackle this problem by exploiting the latent conforming and maverick personalities of those who vote to assess the quality of on-line items. We argue that the former personality prompts a user to cast her vote conforming to the majority of the service community while on the contrary the later personality makes her vote different from the community. We thus propose a Conformer-Maverick (CM) model to simulate the voting process and use it to rank top-k potentially popular items based on the early votes they received. Through an extensive experimental evaluation, we validate our ideas and find that our proposed CM model achieves better performance than baseline solutions, especially for smaller k.</p>
<p>【Keywords】:
conformer; generative model; maverick; popular ranking</p>
<h3 id="65. A large-scale sentiment analysis for Yahoo! answers.">65. A large-scale sentiment analysis for Yahoo! answers.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124371">Paper Link</a>】    【Pages】:633-642</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kucuktunc:Onur">Onur Kucuktunc</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weber:Ingmar">Ingmar Weber</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Ferhatosmanoglu:Hakan">Hakan Ferhatosmanoglu</a></p>
<p>【Abstract】:
Sentiment extraction from online web documents has recently been an active research topic due to its potential use in commercial applications. By sentiment analysis, we refer to the problem of assigning a quantitative positive/negative mood to a short bit of text. Most studies in this area are limited to the identification of sentiments and do not investigate the interplay between sentiments and other factors. In this work, we use a sentiment extraction tool to investigate the influence of factors such as gender, age, education level, the topic at hand, or even the time of the day on sentiments in the context of a large online question answering site. We start our analysis by looking at direct correlations, e.g., we observe more positive sentiments on weekends, very neutral ones in the Science &amp; Mathematics topic, a trend for younger people to express stronger sentiments, or people in military bases to ask the most neutral questions. We then extend this basic analysis by investigating how properties of the (asker, answerer) pair affect the sentiment present in the answer. Among other things, we observe a dependence on the pairing of some inferred attributes estimated by a user's ZIP code. We also show that the best answers differ in their sentiments from other answers, e.g., in the Business &amp; Finance topic, best answers tend to have a more neutral sentiment than other answers. Finally, we report results for the task of predicting the attitude that a question will provoke in answers. We believe that understanding factors influencing the mood of users is not only interesting from a sociological point of view, but also has applications in advertising, recommendation, and search.</p>
<p>【Keywords】:
attitude; collaborative question answering; prediction; sentiment analysis; sentimentality</p>
<h2 id="Spotlight on mining    1">Spotlight on mining    1</h2>
<h3 id="66. What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities.">66. What's in a hashtag?: content based prediction of the spread of ideas in microblogging communities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124320">Paper Link</a>】    【Pages】:643-652</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tsur:Oren">Oren Tsur</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rappoport:Ari">Ari Rappoport</a></p>
<p>【Abstract】:
Current social media research mainly focuses on temporal trends of the information flow and on the topology of the social graph that facilitates the propagation of information. In this paper we study the effect of the content of the idea on the information propagation. We present an efficient hybrid approach based on a linear regression for predicting the spread of an idea in a given time frame. We show that a combination of content features with temporal and topological features minimizes prediction error. Our algorithm is evaluated on Twitter hashtags extracted from a dataset of more than 400 million tweets. We analyze the contribution and the limitations of the various feature types to the spread of information, demonstrating that content aspects can be used as strong predictors thus should not be disregarded. We also study the dependencies between global features such as graph topology and content features.</p>
<p>【Keywords】:
hashtags; information diffusion; microblogging; social media; twitter</p>
<h2 id="Spotlight on social    5">Spotlight on social    5</h2>
<h3 id="67. Tips, dones and todos: uncovering user profiles in foursquare.">67. Tips, dones and todos: uncovering user profiles in foursquare.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124372">Paper Link</a>】    【Pages】:653-662</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vasconcelos:Marisa_A=">Marisa A. Vasconcelos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ricci:Saulo_M=_R=">Saulo M. R. Ricci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Almeida:Jussara_M=">Jussara M. Almeida</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Benevenuto:Fabr=iacute=cio">Fabrício Benevenuto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Almeida:Virg=iacute=lio_A=_F=">Virgílio A. F. Almeida</a></p>
<p>【Abstract】:
Online Location Based Social Networks (LBSNs), which combine social network features with geographic information sharing, are becoming increasingly popular. One such application is Foursquare, which doubled its user population in less than six months. Among other features, Foursquare allows users to leave tips (i.e., reviews or recommendations) at specific venues as well as to give feedback on previously posted tips by adding them to their to-do lists or marking them as done. In this paper, we analyze how Foursquare users exploit these three features - tips, dones and to-dos - uncovering different behavior profiles. Our study reveals the existence of very active and influential users, some of which are famous businesses and brands, that seem engaged in posting tips at a large variety of venues while also receiving a great amount of user feedback on them. We also provide evidence of spamming, showing the existence of users that post tips whose contents are unrelated to the nature or domain of the venue where the tips were left.</p>
<p>【Keywords】:
location based social networks; spamming; user behavior characterization</p>
<h3 id="68. When will it happen?: relationship prediction in heterogeneous information networks.">68. When will it happen?: relationship prediction in heterogeneous information networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124373">Paper Link</a>】    【Pages】:663-672</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Yizhou">Yizhou Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aggarwal:Charu_C=">Charu C. Aggarwal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chawla:Nitesh_V=">Nitesh V. Chawla</a></p>
<p>【Abstract】:
Link prediction, i.e., predicting links or interactions between objects in a network, is an important task in network analysis. Although the problem has attracted much attention recently, there are several challenges that have not been addressed so far. First, most existing studies focus only on link prediction in homogeneous networks, where all objects and links belong to the same type. However, in the real world, heterogeneous networks that consist of multi-typed objects and relationships are ubiquitous. Second, most current studies only concern the problem of whether a link will appear in the future but seldom pay attention to the problem of when it will happen. In this paper, we address both issues and study the problem of predicting when a certain relationship will happen in the scenario of heterogeneous networks. First, we extend the link prediction problem to the relationship prediction problem, by systematically defining both the target relation and the topological features, using a meta path-based approach. Then, we directly model the distribution of relationship building time with the use of the extracted topological features. The experiments on citation relationship prediction between authors on the DBLP network demonstrate the effectiveness of our methodology.</p>
<p>【Keywords】:
heterogeneous information networks; relationship prediction</p>
<h3 id="69. The life and death of online groups: predicting group growth and longevity.">69. The life and death of online groups: predicting group growth and longevity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124374">Paper Link</a>】    【Pages】:673-682</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kairam:Sanjay_Ram">Sanjay Ram Kairam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dan_J=">Dan J. Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leskovec:Jure">Jure Leskovec</a></p>
<p>【Abstract】:
We pose a fundamental question in understanding how to identify and design successful communities: What factors predict whether a community will grow and survive in the long term? Social scientists have addressed this question extensively by analyzing offline groups which endeavor to attract new members, such as social movements, finding that new individuals are influenced strongly by their ties to members of the group. As a result, prior work on the growth of communities has treated growth primarily as a diffusion processes, leading to findings about group evolution which can be difficult to explain. The proliferation of online social networks and communities, however, has created new opportunities to study, at a large scale and with very fine resolution, the mechanisms which lead to the formation, growth, and demise of online groups. In this paper, we analyze data from several thousand online social networks built on the Ning platform with the goal of understanding the factors contributing to the growth and longevity of groups within these networks. Specifically, we investigate the role that two types of growth (growth through diffusion and growth by other means) play during a group's formative stages from the perspectives of both the individual member and the group. Applying these insights to a population of groups of different ages and sizes, we build a model to classify groups which will grow rapidly over the short-term and long-term. Our model achieves over 79% accuracy in predicting group growth over the following two months and over 78% accuracy in predictions over the following two years. We utilize a similar approach to predict which groups will die within a year. The results of our combined analysis provide insight into how both early non-diffusion growth and a complex set of network constraints appear to contribute to the initial and continued growth and success of groups within social networks. Finally we discuss implications of this work for the design, maintenance, and analysis of online communities.</p>
<p>【Keywords】:
group formation; information diffusion; online communities; social networks</p>
<h3 id="70. Evaluating search in personal social media collections.">70. Evaluating search in personal social media collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124375">Paper Link</a>】    【Pages】:683-692</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Chia=Jung">Chia-Jung Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Jinyoung">Jinyoung Kim</a></p>
<p>【Abstract】:
The prevalence of social media applications is generating potentially large personal archives of posts, tweets, and other communications. The existence of these archives creates a need for search tools, which can be seen as an extension of current desktop search services. Little is currently known about the best search techniques for personal archives of social data, because of the difficulty of creating test collections. In this paper, we describe how test collections for personal social data can be created by using games to collect queries. We then compare a range of retrieval models that exploit the semi-structured nature of social data. Our results show that a mixture of language models with field distribution estimation can be effective for this type of data, with certain fields, such as the name of the poster, being particularly important. We also analyze the properties of the queries that were generated by users with two versions of the games.</p>
<p>【Keywords】:
desktop search; facebook; personal social media collections; search evaluation; semi-structured document; twitter</p>
<h3 id="71. Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization.">71. Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124376">Paper Link</a>】    【Pages】:693-702</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Saha:Ankan">Ankan Saha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sindhwani:Vikas">Vikas Sindhwani</a></p>
<p>【Abstract】:
As massive repositories of real-time human commentary, social media platforms have arguably evolved far beyond passive facilitation of online social interactions. Rapid analysis of information content in online social media streams (news articles, blogs,tweets etc.) is the need of the hour as it allows business and government bodies to understand public opinion about products and policies. In most of these settings, data points appear as a stream of high dimensional feature vectors. Guided by real-world industrial deployment scenarios, we revisit the problem of online learning of topics from streaming social media content. On one hand, the topics need to be dynamically adapted to the statistics of incoming datapoints, and on the other hand, early detection of rising new trends is important in many applications. We propose an online nonnegative matrix factorizations framework to capture the evolution and emergence of themes in unstructured text under a novel temporal regularization framework. We develop scalable optimization algorithms for our framework, propose a new set of evaluation metrics, and report promising empirical results on traditional TDT tasks as well as streaming Twitter data. Our system is able to rapidly capture emerging themes, track existing topics over time while maintaining temporal consistency and continuity in user views, and can be explicitly configured to bound the amount of information being presented to the user.</p>
<p>【Keywords】:
dictionary learning; nmf; time series analysis; topic models</p>
<h2 id="Social II    5">Social II    5</h2>
<h3 id="72. Effects of user similarity in social media.">72. Effects of user similarity in social media.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124378">Paper Link</a>】    【Pages】:703-712</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Anderson:Ashton">Ashton Anderson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huttenlocher:Daniel_P=">Daniel P. Huttenlocher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kleinberg:Jon_M=">Jon M. Kleinberg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leskovec:Jure">Jure Leskovec</a></p>
<p>【Abstract】:
There are many settings in which users of a social media application provide evaluations of one another. In a variety of domains, mechanisms for evaluation allow one user to say whether he or she trusts another user, or likes the content they produced, or wants to confer special levels of authority or responsibility on them. Earlier work has studied how the relative status between two users - that is, their comparative levels of status in the group - affects the types of evaluations that one user gives to another. Here we study how similarity in the characteristics of two users can affect the evaluation one user provides of another. We analyze this issue under a range of natural similarity measures, showing how the interaction of similarity and status can produce strong effects. Among other consequences, we find that evaluations are less status-driven when users are more similar to each other; and we use effects based on similarity to provide a plausible mechanism for a complex phenomenon observed in studies of user evaluation, that evaluations are particularly low among users of roughly equal status. Our work has natural applications to the prediction of evaluation outcomes based on user characteristics, and the use of similarity information makes possible a novel application that we introduce here - to estimate the chance of a favorable overall evaluation from a group knowing only the attributes of the group's members, but not their expressed opinions.</p>
<p>【Keywords】:
ballot-blind prediction; status; user similarity; user-to-user evaluations</p>
<h3 id="73. How user behavior is related to social affinity.">73. How user behavior is related to social affinity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124379">Paper Link</a>】    【Pages】:713-722</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Panigrahy:Rina">Rina Panigrahy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Najork:Marc">Marc Najork</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Yinglian">Yinglian Xie</a></p>
<p>【Abstract】:
Previous research has suggested that people who are in the same social circle exhibit similar behaviors and tastes. The rise of social networks gives us insights into the social circles of web users, and recommendation services (including search engines, advertisement engines, and collaborative filtering engines) provide a motivation to adapt recommendations to the interests of the audience. An important primitive for supporting these applications is the ability to quantify how connected two users are in a social network. The shortest-path distance between a pair of users is an obvious candidate measure. This paper introduces a new measure of "affinity" in social networks that takes into account not only the distance between two users, but also the number of edge-disjoint paths between them, i.e. the "robustness" of their connection. Our measure is based on a sketch-based approach, and affinity queries can be answered extremely efficiently (at the expense of a one-time offline sketch computation). We compare this affinity measure against the "approximate shortest-path distance", a sketch-based distance measure with similar efficiency characteristics. Our empirical study is based on a Hotmail email exchange graph combined with demographic information and Bing query history, and a Twitter mention-graph together with the text of the underlying tweets. We found that users who are close to each other - either in terms of distance or affinity - have a higher similarity in terms of demographics, queries, and tweets.</p>
<p>【Keywords】:
affinity; distance; influence; sketching; social networks</p>
<h3 id="74. Finding your friends and following them to where you are.">74. Finding your friends and following them to where you are.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124380">Paper Link</a>】    【Pages】:723-732</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sadilek:Adam">Adam Sadilek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kautz:Henry_A=">Henry A. Kautz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bigham:Jeffrey_P=">Jeffrey P. Bigham</a></p>
<p>【Abstract】:
Location plays an essential role in our lives, bridging our online and offline worlds. This paper explores the interplay between people's location, interactions, and their social ties within a large real-world dataset. We present and evaluate Flap, a system that solves two intimately related tasks: link and location prediction in online social networks. For link prediction, Flap infers social ties by considering patterns in friendship formation, the content of people's messages, and user location. We show that while each component is a weak predictor of friendship alone, combining them results in a strong model, accurately identifying the majority of friendships. For location prediction, Flap implements a scalable probabilistic model of human mobility, where we treat users with known GPS positions as noisy sensors of the location of their friends. We explore supervised and unsupervised learning scenarios, and focus on the efficiency of both learning and inference. We evaluate Flap on a large sample of highly active users from two distinct geographical areas and show that it (1) reconstructs the entire friendship graph with high accuracy even when no edges are given; and (2) infers people's fine-grained location, even when they keep their data private and we can only access the location of their friends. Our models significantly outperform current comparable approaches to either task.</p>
<p>【Keywords】:
graphical models; link prediction; location modeling; machine learning; social networks; visualization</p>
<h3 id="75. How to win friends and influence people, truthfully: influence maximization mechanisms for social networks.">75. How to win friends and influence people, truthfully: influence maximization mechanisms for social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124381">Paper Link</a>】    【Pages】:733-742</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Singer:Yaron">Yaron Singer</a></p>
<p>【Abstract】:
Throughout the past decade there has been extensive research on algorithmic and data mining techniques for solving the problem of influence maximization in social networks: if one can incentivize a subset of individuals to become early adopters of a new technology, which subset should be selected so that the word-of-mouth effect in the social network is maximized? Despite the progress in modeling and techniques, the incomplete information aspect of the problem has been largely overlooked. While data can often provide the network structure and influence patterns may be observable, the inherent cost individuals have to become early adopters is difficult to extract. In this paper we introduce mechanisms that elicit individuals' costs while providing desirable approximation guarantees in some of the most well-studied models of social network influence. We follow the mechanism design framework which advocates for allocation and payment schemes that incentivize individuals to report their true information. We also performed experiments using the Mechanical Turk platform and social network data to provide evidence of the framework's effectiveness in practice.</p>
<p>【Keywords】:
influence maximization; mechanism design; social networks; viral marketing</p>
<h3 id="76. Inferring social ties across heterogenous networks.">76. Inferring social ties across heterogenous networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124382">Paper Link</a>】    【Pages】:743-752</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lou:Tiancheng">Tiancheng Lou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kleinberg:Jon_M=">Jon M. Kleinberg</a></p>
<p>【Abstract】:
It is well known that different types of social ties have essentially different influence on people. However, users in online social networks rarely categorize their contacts into "family", "colleagues", or "classmates". While a bulk of research has focused on inferring particular types of relationships in a specific social network, few publications systematically study the generalization of the problem of inferring social ties over multiple heterogeneous networks. In this work, we develop a framework for classifying the type of social relationships by learning across heterogeneous networks. The framework incorporates social theories into a factor graph model, which effectively improves the accuracy of inferring the type of social relationships in a target network by borrowing knowledge from a different source network. Our empirical study on five different genres of networks validates the effectiveness of the proposed framework. For example, by leveraging information from a coauthor network with labeled advisor-advisee relationships, the proposed framework is able to obtain an F1-score of 90% (8-28% improvements over alternative methods) for inferring manager-subordinate relationships in an enterprise email network.</p>
<p>【Keywords】:
inferring social ties; predictive model; social influence analysis; social network</p>
<h2 id="Keynote address    1">Keynote address    1</h2>
<h3 id="77. The secret life of social links.">77. The secret life of social links.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124384">Paper Link</a>】    【Pages】:753-754</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mason:Hilary">Hilary Mason</a></p>
<p>【Abstract】:
The social web is a messy place! At bitly, we see hundreds of millions of shares and clicks per day--clicks that contain all sorts of wonderful content from lolcats to spacecraft launches. I'll discuss our philosophy, tools, and techniques for looking at the data, and new research opportunities that weren't possible before.</p>
<p>【Keywords】:
social content; social web</p>
<h2 id="Doctoral consortium    4">Doctoral consortium    4</h2>
<h3 id="78. Exploration and discovery of user-generated content in large information spaces.">78. Exploration and discovery of user-generated content in large information spaces.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124386">Paper Link</a>】    【Pages】:755-756</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chiarandini:Luca">Luca Chiarandini</a></p>
<p>【Abstract】:
The accumulation of large collections of social media data poses new challenges for the design of exploratory experiences, such as when a user browses through a collection to discover content (e.g. exploring photo collections, network of friends, etc). Cardinality and characteristics of the set, together with volatility of the information, resulting from fast and continuous creation, deletion and updating of entries, trigger novel research questions. In this context, we plan to investigate and contribute to the data analysis, and user interface design of exploratory experiences. The proposed approach is an iterative process where analysis and design phases are performed in cycles. The long-term vision is to understand the underlying reasoning in order to be able to automatically replicate it.</p>
<p>【Keywords】:
data mining; human computer interaction; social media</p>
<h3 id="79. Computational advertising: leveraging user interaction & contextual factors for improved ad relevance & targeting.">79. Computational advertising: leveraging user interaction &amp; contextual factors for improved ad relevance &amp; targeting.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124387">Paper Link</a>】    【Pages】:757-758</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dave:Kushal_S=">Kushal S. Dave</a></p>
<p>【Abstract】:
Computational advertising refers to finding the most relevant ads matching a particular context on the web. The core problem attacked in computational advertising CA is of the match making between the ads and the context. My research work aims at leveraging various user interaction, ad and advertiser related information and contextual information for improving the relevance, ranking and targeting of ads. The research work focuses on the identification of various factors that contribute in retrieving and ranking the most relevant set of ads that match best with the context. Specifically, information associated with the user, publisher and advertiser is leveraged for this purpose.</p>
<p>【Keywords】:
computational advertising; contextual advertising; sponsored search; viral marketing</p>
<h3 id="80. The early bird gets the buzz: detecting anomalies and emerging trends in information networks.">80. The early bird gets the buzz: detecting anomalies and emerging trends in information networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124388">Paper Link</a>】    【Pages】:759-760</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Thompson:Brian">Brian Thompson</a></p>
<p>【Abstract】:
In this work we propose a novel approach to anomaly detection in streaming communication data. We first build a stochastic model for the system based on temporal communication patterns across each edge, which we call the REWARDS (REneWal theory Approach for Real-time Data Streams) model. We then define a measure of anomaly for an arbitrary subgraph based on the likelihood of its recent activity given past behavior. Finally, we develop an algorithm to efficiently identify subgraphs with the most anomalous activity. Although our work has until now focused on the cybersecurity domain, the model we present is more broadly applicable to information retrieval in data streams and information networks.</p>
<p>【Keywords】:
anomaly detection; communication networks; graph algorithms; information networks; renewal theory; streaming time series models; time-evolving graphs</p>
<h3 id="81. Characterizing and harnessing peer-production of information in social tagging systems.">81. Characterizing and harnessing peer-production of information in social tagging systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124389">Paper Link</a>】    【Pages】:761-762</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Santos=Neto:Elizeu">Elizeu Santos-Neto</a></p>
<p>【Abstract】:
Assessing the value of individual users' contributions in peer-production systems is paramount to the design of mechanisms that support collaboration and improve users' experience. For instance, to incentivize contributions, file sharing systems based on the BitTorrent protocol equate value with volume of contributed content and use a prioritization mechanism to reward users who contribute more. This approach and similar techniques used in resource sharing systems rely on the fact that the physical resources shared among users are easily quantifiable. In contrast, information-sharing systems, like social tagging systems, lack the notion of a physical resource unit (e.g., content size, bandwidth) that facilitates the task of evaluating user contributions. For this reason, the issue of estimating the value of user contributions in information sharing systems remains largely unexplored. This paper outlines a research project to tackle the problem of assessing the value of contributions in social tagging systems.</p>
<p>【Keywords】:
information value; peer-production; social; systems; tagging</p>
<h2 id="Tutorials    3">Tutorials    3</h2>
<h3 id="82. Mining, searching and exploiting collaboratively generated content on the web.">82. Mining, searching and exploiting collaboratively generated content on the web.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124391">Paper Link</a>】    【Pages】:763-764</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gabrilovich:Evgeniy">Evgeniy Gabrilovich</a></p>
<p>【Abstract】:
Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content. The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation. However, the quality and comprehensiveness of collaboratively created content vary significantly, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial. The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.</p>
<p>【Keywords】:
collaboratively-generated content</p>
<h3 id="83. Collaborative information seeking: understanding users, systems, and content.">83. Collaborative information seeking: understanding users, systems, and content.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124392">Paper Link</a>】    【Pages】:765-766</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shah:Chirag">Chirag Shah</a></p>
<p>【Abstract】:
The course will introduce the student to theories, methodologies, and tools that focus on information retrieval/seeking in collaboration. The student will have an opportunity to learn about the social aspect of IR with a focus on collaborative information seeking (CIS) situations, systems, and evaluation techniques. Traditionally, IR is considered an individual pursuit, and not surprisingly, the majority of tools, techniques, and models developed for addressing information need, retrieval, and usage have focused on single users. The assumption of information seekers being independent and IR problem being individual has been challenged often in the recent past. This course will introduce such works to the students, with an emphasis on understanding models and systems that support collaborative search or browsing. In addition, the course will provide samples of data collected through several experiments to demonstrate various mining and analysis techniques. Specifically, the course will (1) outline the research and latest developments in the field of collaborative IR, (2) list the challenges for designing and evaluating collaborative IR systems, and (3) show how traditional single user IR models and systems could be mapped to those for CIS. This will be achieved through introduction to appropriate literature, algorithms and interfaces that facilitate CIS, and methodologies for studying and evaluating them. Thus, the course will offer a balance between theoretical and practical elements of CIS.</p>
<p>【Keywords】:
collaborative information seeking</p>
<h3 id="84. Machine learning for query-document matching in search.">84. Machine learning for query-document matching in search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124393">Paper Link</a>】    【Pages】:767-768</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu_0001:Jun">Jun Xu</a></p>
<p>【Abstract】:
In web search, relevance is one of the most important factors to meet users' satisfaction, and the success of a web search engine heavily depends on its performance on relevance. It has been observed that many hard cases in search relevance are due to term mismatch between query and documnt (e.g., query 'ny times' does not match well with document only containing 'new york times'), and thus it is not exaggerated to say that dealing with mismatch between query and document is one of the most critical research problems in web search. Recently researchers have spent significant effort to address the grand challenge. The major approach is to conduct more query and document understanding, and perform better matching between enriched query and document representations. With the availability of large amount of log data and advanced machine learning techniques, this becomes more feasible and significant progress has been made recently. In this tutorial, we will give a systematic and detailed presentation on newly developed machine learning technologies for query document matching in search. We will focus on the fundamental problems, as well as the novel solutions for query document matching at word form level, word sense level, topic level, and structure level. We will talk about novel technologies about query spelling error correction [3, 13], query rewriting [1, 4, 6, 7], query classification [2], topic modeling of documents [5, 9], query document matching [8, 10, 11, 12], and query document-title translation. The ideas and solutions introduced in this tutorial may motivate industrial practitioners to turn the research fruits into product reality. The summary of the state-of-the-art methods and the discussions on the technical issues in this tutorial may stimulate academic researchers to find new research directions and solutions. Matching between query and document is not limited to search, and similar problems can be observed at online advertisement, recommendation system, and other applications, as matching between objects from two spaces. The technologies we introduce can be generalized into more general machine learning techniques, which we call learning to match.</p>
<p>【Keywords】:
machine learning; query-document matching; web search</p>
<h2 id="Workshops    2">Workshops    2</h2>
<h3 id="85. 2nd international workshop on diversity in document retrieval (DDR 2012).">85. 2nd international workshop on diversity in document retrieval (DDR 2012).</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124395">Paper Link</a>】    【Pages】:769-770</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0012:Jun">Jun Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clarke:Charles_L=_A=">Charles L. A. Clarke</a></p>
<p>【Abstract】:
When an ambiguous query is received, a sensible approach is for the information retrieval (IR) system to diversify the results retrieved for this query, in the hope that at least one of the interpretations of the query intent will satisfy the user. Diversity is an increasingly important topic, of interest to both academic researchers (such as participants in the TREC Web and Blog track diversity tasks, or the NTCIR INTENT task), as well as to search engines professionals. In the 2nd edition of the Diversity in Document Retrieval workshop (DDR 2012), we solicited submissions both on approaches and models for diversity, the evaluation of diverse search results, and on applications of diverse search results. This workshop builds upon a successful 1st edition of DDR which was held at ECIR 2011 in Dublin, Ireland.</p>
<p>【Keywords】:
diversity; information retrieval; search</p>
<h3 id="86. WSCD 2012: workshop on web search click data 2012.">86. WSCD 2012: workshop on web search click data 2012.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2124295.2124396">Paper Link</a>】    【Pages】:771-772</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Craswell:Nick">Nick Craswell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dupret:Georges">Georges Dupret</a></p>
<p>【Abstract】:
WSCD2012 is the second workshop on Web Search Click Data, following WSCD2009. It is a forum for new research relating to Web search usage logs and for discussing desirable properties of publicly released search log datasets. Research relating to search logs has been hampered by the limited availability of click datasets. This workshop comes with a new click dataset based on click logs and an accompanying challenge to predict the relevance of documents based on clicks.</p>
<p>【Keywords】:
click data; web search</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
