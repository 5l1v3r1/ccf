 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://rawcdn.githack.com/huntercmd/blog/master/config/css/light.css">
<script src="https://rawcdn.githack.com/huntercmd/blog/d9beff1/config/css/skin.js"></script>
<script src="https://rawcdn.githack.com/huntercmd/blog/master/config/css/classie.js"></script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#56th ACL 2018:Melbourne, Australia - Volume 1: Long Papers">56th ACL 2018:Melbourne, Australia - Volume 1: Long Papers</a><ul>
<li><a href="#Paper Num: 256 || Session Num: 0">Paper Num: 256 || Session Num: 0</a><ul>
<li><a href="#1. Probabilistic FastText for Multi-Sense Word Embeddings.">1. Probabilistic FastText for Multi-Sense Word Embeddings.</a></li>
<li><a href="#2. A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors.">2. A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors.</a></li>
<li><a href="#3. Unsupervised Learning of Distributional Relation Vectors.">3. Unsupervised Learning of Distributional Relation Vectors.</a></li>
<li><a href="#4. Explicit Retrofitting of Distributional Word Vectors.">4. Explicit Retrofitting of Distributional Word Vectors.</a></li>
<li><a href="#5. Unsupervised Neural Machine Translation with Weight Sharing.">5. Unsupervised Neural Machine Translation with Weight Sharing.</a></li>
<li><a href="#6. Triangular Architecture for Rare Language Translation.">6. Triangular Architecture for Rare Language Translation.</a></li>
<li><a href="#7. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.">7. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.</a></li>
<li><a href="#8. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.">8. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.</a></li>
<li><a href="#9. Ultra-Fine Entity Typing.">9. Ultra-Fine Entity Typing.</a></li>
<li><a href="#10. Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking.">10. Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking.</a></li>
<li><a href="#11. Improving Knowledge Graph Embedding Using Simple Constraints.">11. Improving Knowledge Graph Embedding Using Simple Constraints.</a></li>
<li><a href="#12. Towards Understanding the Geometry of Knowledge Graph Embeddings.">12. Towards Understanding the Geometry of Knowledge Graph Embeddings.</a></li>
<li><a href="#13. A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.">13. A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.</a></li>
<li><a href="#14. Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks.">14. Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks.</a></li>
<li><a href="#15. Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization.">15. Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization.</a></li>
<li><a href="#16. Simple and Effective Text Simplification Using Semantic and Neural Methods.">16. Simple and Effective Text Simplification Using Semantic and Neural Methods.</a></li>
<li><a href="#17. Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20, 000 English Words.">17. Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20, 000 English Words.</a></li>
<li><a href="#18. Comprehensive Supersense Disambiguation of English Prepositions and Possessives.">18. Comprehensive Supersense Disambiguation of English Prepositions and Possessives.</a></li>
<li><a href="#19. A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature.">19. A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature.</a></li>
<li><a href="#20. Efficient Online Scalar Annotation with Bounded Support.">20. Efficient Online Scalar Annotation with Bounded Support.</a></li>
<li><a href="#21. Neural Argument Generation Augmented with Externally Retrieved Evidence.">21. Neural Argument Generation Augmented with Externally Retrieved Evidence.</a></li>
<li><a href="#22. A Stylometric Inquiry into Hyperpartisan and Fake News.">22. A Stylometric Inquiry into Hyperpartisan and Fake News.</a></li>
<li><a href="#23. Retrieval of the Best Counterargument without Prior Topic Knowledge.">23. Retrieval of the Best Counterargument without Prior Topic Knowledge.</a></li>
<li><a href="#24. LinkNBed: Multi-Graph Representation Learning with Entity Linkage.">24. LinkNBed: Multi-Graph Representation Learning with Entity Linkage.</a></li>
<li><a href="#25. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures.">25. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures.</a></li>
<li><a href="#26. Graph-to-Sequence Learning using Gated Graph Neural Networks.">26. Graph-to-Sequence Learning using Gated Graph Neural Networks.</a></li>
<li><a href="#27. Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context.">27. Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context.</a></li>
<li><a href="#28. Bridging CNNs, RNNs, and Weighted Finite-State Machines.">28. Bridging CNNs, RNNs, and Weighted Finite-State Machines.</a></li>
<li><a href="#29. Zero-shot Learning of Classifiers from Natural Language Quantification.">29. Zero-shot Learning of Classifiers from Natural Language Quantification.</a></li>
<li><a href="#30. Sentence-State LSTM for Text Representation.">30. Sentence-State LSTM for Text Representation.</a></li>
<li><a href="#31. Universal Language Model Fine-tuning for Text Classification.">31. Universal Language Model Fine-tuning for Text Classification.</a></li>
<li><a href="#32. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.">32. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.</a></li>
<li><a href="#33. Improving Text-to-SQL Evaluation Methodology.">33. Improving Text-to-SQL Evaluation Methodology.</a></li>
<li><a href="#34. Semantic Parsing with Syntax- and Table-Aware SQL Generation.">34. Semantic Parsing with Syntax- and Table-Aware SQL Generation.</a></li>
<li><a href="#35. Multitask Parsing Across Semantic Representations.">35. Multitask Parsing Across Semantic Representations.</a></li>
<li><a href="#36. Character-Level Models versus Morphology in Semantic Role Labeling.">36. Character-Level Models versus Morphology in Semantic Role Labeling.</a></li>
<li><a href="#37. AMR Parsing as Graph Prediction with Latent Alignment.">37. AMR Parsing as Graph Prediction with Latent Alignment.</a></li>
<li><a href="#38. Accurate SHRG-Based Semantic Parsing.">38. Accurate SHRG-Based Semantic Parsing.</a></li>
<li><a href="#39. Using Intermediate Representations to Solve Math Word Problems.">39. Using Intermediate Representations to Solve Math Word Problems.</a></li>
<li><a href="#40. Discourse Representation Structure Parsing.">40. Discourse Representation Structure Parsing.</a></li>
<li><a href="#41. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms.">41. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms.</a></li>
<li><a href="#42. ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations.">42. ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations.</a></li>
<li><a href="#43. Event2Mind: Commonsense Inference on Events, Intents, and Reactions.">43. Event2Mind: Commonsense Inference on Events, Intents, and Reactions.</a></li>
<li><a href="#44. Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis.">44. Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis.</a></li>
<li><a href="#45. Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures.">45. Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures.</a></li>
<li><a href="#46. DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction.">46. DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction.</a></li>
<li><a href="#47. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.">47. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.</a></li>
<li><a href="#48. Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection.">48. Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection.</a></li>
<li><a href="#49. Context-Aware Neural Model for Temporal Information Extraction.">49. Context-Aware Neural Model for Temporal Information Extraction.</a></li>
<li><a href="#50. Temporal Event Knowledge Acquisition via Identifying Narratives.">50. Temporal Event Knowledge Acquisition via Identifying Narratives.</a></li>
<li><a href="#51. Textual Deconvolution Saliency (TDS">51. Textual Deconvolution Saliency (TDS) : a deep tool box for linguistic analysis.</a> : a deep tool box for linguistic analysis.)</li>
<li><a href="#52. Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach.">52. Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach.</a></li>
<li><a href="#53. Deep Reinforcement Learning for Chinese Zero Pronoun Resolution.">53. Deep Reinforcement Learning for Chinese Zero Pronoun Resolution.</a></li>
<li><a href="#54. Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis.">54. Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis.</a></li>
<li><a href="#55. Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars.">55. Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars.</a></li>
<li><a href="#56. Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power.">56. Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power.</a></li>
<li><a href="#57. TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation.">57. TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation.</a></li>
<li><a href="#58. Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays.">58. Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays.</a></li>
<li><a href="#59. Inherent Biases in Reference-based Evaluation for Grammatical Error Correction.">59. Inherent Biases in Reference-based Evaluation for Grammatical Error Correction.</a></li>
<li><a href="#60. The price of debiasing automatic metrics in natural language evalaution.">60. The price of debiasing automatic metrics in natural language evalaution.</a></li>
<li><a href="#61. Neural Document Summarization by Jointly Learning to Score and Select Sentences.">61. Neural Document Summarization by Jointly Learning to Score and Select Sentences.</a></li>
<li><a href="#62. Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization.">62. Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization.</a></li>
<li><a href="#63. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting.">63. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting.</a></li>
<li><a href="#64. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation.">64. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation.</a></li>
<li><a href="#65. Modeling and Prediction of Online Product Review Helpfulness: A Survey.">65. Modeling and Prediction of Online Product Review Helpfulness: A Survey.</a></li>
<li><a href="#66. Mining Cross-Cultural Differences and Similarities in Social Media.">66. Mining Cross-Cultural Differences and Similarities in Social Media.</a></li>
<li><a href="#67. Classification of Moral Foundations in Microblog Political Discourse.">67. Classification of Moral Foundations in Microblog Political Discourse.</a></li>
<li><a href="#68. Coarse-to-Fine Decoding for Neural Semantic Parsing.">68. Coarse-to-Fine Decoding for Neural Semantic Parsing.</a></li>
<li><a href="#69. Confidence Modeling for Neural Semantic Parsing.">69. Confidence Modeling for Neural Semantic Parsing.</a></li>
<li><a href="#70. StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing.">70. StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing.</a></li>
<li><a href="#71. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing.">71. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing.</a></li>
<li><a href="#72. On the Limitations of Unsupervised Bilingual Dictionary Induction.">72. On the Limitations of Unsupervised Bilingual Dictionary Induction.</a></li>
<li><a href="#73. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.">73. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.</a></li>
<li><a href="#74. A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling.">74. A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling.</a></li>
<li><a href="#75. Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable.">75. Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable.</a></li>
<li><a href="#76. Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge.">76. Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge.</a></li>
<li><a href="#77. Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds.">77. Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds.</a></li>
<li><a href="#78. Simple and Effective Multi-Paragraph Reading Comprehension.">78. Simple and Effective Multi-Paragraph Reading Comprehension.</a></li>
<li><a href="#79. Semantically Equivalent Adversarial Rules for Debugging NLP models.">79. Semantically Equivalent Adversarial Rules for Debugging NLP models.</a></li>
<li><a href="#80. Style Transfer Through Back-Translation.">80. Style Transfer Through Back-Translation.</a></li>
<li><a href="#81. Generating Fine-Grained Open Vocabulary Entity Type Descriptions.">81. Generating Fine-Grained Open Vocabulary Entity Type Descriptions.</a></li>
<li><a href="#82. Hierarchical Neural Story Generation.">82. Hierarchical Neural Story Generation.</a></li>
<li><a href="#83. No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling.">83. No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling.</a></li>
<li><a href="#84. Bridging Languages through Images with Deep Partial Canonical Correlation Analysis.">84. Bridging Languages through Images with Deep Partial Canonical Correlation Analysis.</a></li>
<li><a href="#85. Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.">85. Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.</a></li>
<li><a href="#86. What Action Causes This? Towards Naive Physical Action-Effect Prediction.">86. What Action Causes This? Towards Naive Physical Action-Effect Prediction.</a></li>
<li><a href="#87. Transformation Networks for Target-Oriented Sentiment Classification.">87. Transformation Networks for Target-Oriented Sentiment Classification.</a></li>
<li><a href="#88. Target-Sensitive Memory Networks for Aspect Sentiment Classification.">88. Target-Sensitive Memory Networks for Aspect Sentiment Classification.</a></li>
<li><a href="#89. Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification.">89. Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification.</a></li>
<li><a href="#90. Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach.">90. Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach.</a></li>
<li><a href="#91. Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference.">91. Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference.</a></li>
<li><a href="#92. Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module.">92. Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module.</a></li>
<li><a href="#93. Reasoning with Sarcasm by Reading In-Between.">93. Reasoning with Sarcasm by Reading In-Between.</a></li>
<li><a href="#94. Adversarial Contrastive Estimation.">94. Adversarial Contrastive Estimation.</a></li>
<li><a href="#95. Adaptive Scaling for Sparse Detection in Information Extraction.">95. Adaptive Scaling for Sparse Detection in Information Extraction.</a></li>
<li><a href="#96. Strong Baselines for Neural Semi-Supervised Learning under Domain Shift.">96. Strong Baselines for Neural Semi-Supervised Learning under Domain Shift.</a></li>
<li><a href="#97. Fluency Boost Learning and Inference for Neural Grammatical Error Correction.">97. Fluency Boost Learning and Inference for Neural Grammatical Error Correction.</a></li>
<li><a href="#98. A Neural Architecture for Automated ICD Coding.">98. A Neural Architecture for Automated ICD Coding.</a></li>
<li><a href="#99. Domain Adaptation with Adversarial Training and Graph Embeddings.">99. Domain Adaptation with Adversarial Training and Graph Embeddings.</a></li>
<li><a href="#100. TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.">100. TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.</a></li>
<li><a href="#101. Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.">101. Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.</a></li>
<li><a href="#102. Learning to Control the Specificity in Neural Response Generation.">102. Learning to Control the Specificity in Neural Response Generation.</a></li>
<li><a href="#103. Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network.">103. Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network.</a></li>
<li><a href="#104. MojiTalk: Generating Emotional Responses at Scale.">104. MojiTalk: Generating Emotional Responses at Scale.</a></li>
<li><a href="#105. Taylor's law for Human Linguistic Sequences.">105. Taylor's law for Human Linguistic Sequences.</a></li>
<li><a href="#106. A Framework for Representing Language Acquisition in a Population Setting.">106. A Framework for Representing Language Acquisition in a Population Setting.</a></li>
<li><a href="#107. Prefix Lexicalization of Synchronous CFGs using Synchronous TAG.">107. Prefix Lexicalization of Synchronous CFGs using Synchronous TAG.</a></li>
<li><a href="#108. Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.">108. Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.</a></li>
<li><a href="#109. Gaussian Mixture Latent Vector Grammars.">109. Gaussian Mixture Latent Vector Grammars.</a></li>
<li><a href="#110. Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples.">110. Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples.</a></li>
<li><a href="#111. Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations.">111. Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations.</a></li>
<li><a href="#112. Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings.">112. Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings.</a></li>
<li><a href="#113. Word Embedding and WordNet Based Metaphor Identification and Interpretation.">113. Word Embedding and WordNet Based Metaphor Identification and Interpretation.</a></li>
<li><a href="#114. Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings.">114. Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings.</a></li>
<li><a href="#115. A Stochastic Decoder for Neural Machine Translation.">115. A Stochastic Decoder for Neural Machine Translation.</a></li>
<li><a href="#116. Forest-Based Neural Machine Translation.">116. Forest-Based Neural Machine Translation.</a></li>
<li><a href="#117. Context-Aware Neural Machine Translation Learns Anaphora Resolution.">117. Context-Aware Neural Machine Translation Learns Anaphora Resolution.</a></li>
<li><a href="#118. Document Context Neural Machine Translation with Memory Networks.">118. Document Context Neural Machine Translation with Memory Networks.</a></li>
<li><a href="#119. Which Melbourne? Augmenting Geocoding with Maps.">119. Which Melbourne? Augmenting Geocoding with Maps.</a></li>
<li><a href="#120. Learning Prototypical Goal Activities for Locations.">120. Learning Prototypical Goal Activities for Locations.</a></li>
<li><a href="#121. Guess Me if You Can: Acronym Disambiguation for Enterprises.">121. Guess Me if You Can: Acronym Disambiguation for Enterprises.</a></li>
<li><a href="#122. A Multi-Axis Annotation Scheme for Event Temporal Relations.">122. A Multi-Axis Annotation Scheme for Event Temporal Relations.</a></li>
<li><a href="#123. Exemplar Encoder-Decoder for Neural Conversation Generation.">123. Exemplar Encoder-Decoder for Neural Conversation Generation.</a></li>
<li><a href="#124. DialSQL: Dialogue Based Structured Query Generation.">124. DialSQL: Dialogue Based Structured Query Generation.</a></li>
<li><a href="#125. Conversations Gone Awry: Detecting Early Signs of Conversational Failure.">125. Conversations Gone Awry: Detecting Early Signs of Conversational Failure.</a></li>
<li><a href="#126. Are BLEU and Meaning Representation in Opposition?">126. Are BLEU and Meaning Representation in Opposition?</a></li>
<li><a href="#127. Automatic Metric Validation for Grammatical Error Correction.">127. Automatic Metric Validation for Grammatical Error Correction.</a></li>
<li><a href="#128. The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing.">128. The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing.</a></li>
<li><a href="#129. Distilling Knowledge for Search-based Structured Prediction.">129. Distilling Knowledge for Search-based Structured Prediction.</a></li>
<li><a href="#130. Stack-Pointer Networks for Dependency Parsing.">130. Stack-Pointer Networks for Dependency Parsing.</a></li>
<li><a href="#131. Twitter Universal Dependency Parsing for African-American and Mainstream American English.">131. Twitter Universal Dependency Parsing for African-American and Mainstream American English.</a></li>
<li><a href="#132. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better.">132. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better.</a></li>
<li><a href="#133. Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures.">133. Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures.</a></li>
<li><a href="#134. An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking.">134. An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking.</a></li>
<li><a href="#135. Global-Locally Self-Attentive Encoder for Dialogue State Tracking.">135. Global-Locally Self-Attentive Encoder for Dialogue State Tracking.</a></li>
<li><a href="#136. Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems.">136. Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems.</a></li>
<li><a href="#137. Tailored Sequence to Sequence Models to Different Conversation Scenarios.">137. Tailored Sequence to Sequence Models to Different Conversation Scenarios.</a></li>
<li><a href="#138. Knowledge Diffusion for Neural Dialogue Generation.">138. Knowledge Diffusion for Neural Dialogue Generation.</a></li>
<li><a href="#139. Generating Informative Responses with Controlled Sentence Function.">139. Generating Informative Responses with Controlled Sentence Function.</a></li>
<li><a href="#140. Sentiment Adaptive End-to-End Dialog Systems.">140. Sentiment Adaptive End-to-End Dialog Systems.</a></li>
<li><a href="#141. Embedding Learning Through Multilingual Concept Induction.">141. Embedding Learning Through Multilingual Concept Induction.</a></li>
<li><a href="#142. Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP.">142. Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP.</a></li>
<li><a href="#143. Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data.">143. Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data.</a></li>
<li><a href="#144. Chinese NER Using Lattice LSTM.">144. Chinese NER Using Lattice LSTM.</a></li>
<li><a href="#145. Nugget Proposal Networks for Chinese Event Detection.">145. Nugget Proposal Networks for Chinese Event Detection.</a></li>
<li><a href="#146. Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation.">146. Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation.</a></li>
<li><a href="#147. Discovering Implicit Knowledge with Unary Relations.">147. Discovering Implicit Knowledge with Unary Relations.</a></li>
<li><a href="#148. Improving Entity Linking by Modeling Latent Relations between Mentions.">148. Improving Entity Linking by Modeling Latent Relations between Mentions.</a></li>
<li><a href="#149. Dating Documents using Graph Convolution Networks.">149. Dating Documents using Graph Convolution Networks.</a></li>
<li><a href="#150. A Graph-to-Sequence Model for AMR-to-Text Generation.">150. A Graph-to-Sequence Model for AMR-to-Text Generation.</a></li>
<li><a href="#151. GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data.">151. GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data.</a></li>
<li><a href="#152. Learning to Write with Cooperative Discriminators.">152. Learning to Write with Cooperative Discriminators.</a></li>
<li><a href="#153. A Neural Approach to Pun Generation.">153. A Neural Approach to Pun Generation.</a></li>
<li><a href="#154. Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data.">154. Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data.</a></li>
<li><a href="#155. From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction.">155. From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction.</a></li>
<li><a href="#156. DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.">156. DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.</a></li>
<li><a href="#157. Stochastic Answer Networks for Machine Reading Comprehension.">157. Stochastic Answer Networks for Machine Reading Comprehension.</a></li>
<li><a href="#158. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering.">158. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering.</a></li>
<li><a href="#159. Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension.">159. Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension.</a></li>
<li><a href="#160. Efficient and Robust Question Answering from Minimal Context over Documents.">160. Efficient and Robust Question Answering from Minimal Context over Documents.</a></li>
<li><a href="#161. Denoising Distantly Supervised Open-Domain Question Answering.">161. Denoising Distantly Supervised Open-Domain Question Answering.</a></li>
<li><a href="#162. Question Condensing Networks for Answer Selection in Community Question Answering.">162. Question Condensing Networks for Answer Selection in Community Question Answering.</a></li>
<li><a href="#163. Towards Robust Neural Machine Translation.">163. Towards Robust Neural Machine Translation.</a></li>
<li><a href="#164. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.">164. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.</a></li>
<li><a href="#165. Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning.">165. Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning.</a></li>
<li><a href="#166. Accelerating Neural Transformer via an Average Attention Network.">166. Accelerating Neural Transformer via an Average Attention Network.</a></li>
<li><a href="#167. How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.">167. How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.</a></li>
<li><a href="#168. Weakly Supervised Semantic Parsing with Abstract Examples.">168. Weakly Supervised Semantic Parsing with Abstract Examples.</a></li>
<li><a href="#169. Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback.">169. Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback.</a></li>
<li><a href="#170. AMR dependency parsing with a typed semantic algebra.">170. AMR dependency parsing with a typed semantic algebra.</a></li>
<li><a href="#171. Sequence-to-sequence Models for Cache Transition Systems.">171. Sequence-to-sequence Models for Cache Transition Systems.</a></li>
<li><a href="#172. Batch IS NOT Heavy: Learning Word Representations From All Samples.">172. Batch IS NOT Heavy: Learning Word Representations From All Samples.</a></li>
<li><a href="#173. Backpropagating through Structured Argmax using a SPIGOT.">173. Backpropagating through Structured Argmax using a SPIGOT.</a></li>
<li><a href="#174. Learning How to Actively Learn: A Deep Imitation Learning Approach.">174. Learning How to Actively Learn: A Deep Imitation Learning Approach.</a></li>
<li><a href="#175. Training Classifiers with Natural Language Explanations.">175. Training Classifiers with Natural Language Explanations.</a></li>
<li><a href="#176. Did the Model Understand the Question?">176. Did the Model Understand the Question?</a></li>
<li><a href="#177. Harvesting Paragraph-level Question-Answer Pairs from Wikipedia.">177. Harvesting Paragraph-level Question-Answer Pairs from Wikipedia.</a></li>
<li><a href="#178. Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.">178. Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.</a></li>
<li><a href="#179. Language Generation via DAG Transduction.">179. Language Generation via DAG Transduction.</a></li>
<li><a href="#180. A Distributional and Orthographic Aggregation Model for English Derivational Morphology.">180. A Distributional and Orthographic Aggregation Model for English Derivational Morphology.</a></li>
<li><a href="#181. Deep-speare: A joint neural model of poetic language, meter and rhyme.">181. Deep-speare: A joint neural model of poetic language, meter and rhyme.</a></li>
<li><a href="#182. NeuralREG: An end-to-end approach to referring expression generation.">182. NeuralREG: An end-to-end approach to referring expression generation.</a></li>
<li><a href="#183. Stock Movement Prediction from Tweets and Historical Prices.">183. Stock Movement Prediction from Tweets and Historical Prices.</a></li>
<li><a href="#184. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.">184. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.</a></li>
<li><a href="#185. Visual Attention Model for Name Tagging in Multimodal Social Media.">185. Visual Attention Model for Name Tagging in Multimodal Social Media.</a></li>
<li><a href="#186. Multimodal Named Entity Disambiguation for Noisy Social Media Posts.">186. Multimodal Named Entity Disambiguation for Noisy Social Media Posts.</a></li>
<li><a href="#187. Semi-supervised User Geolocation via Graph Convolutional Networks.">187. Semi-supervised User Geolocation via Graph Convolutional Networks.</a></li>
<li><a href="#188. Document Modeling with External Attention for Sentence Extraction.">188. Document Modeling with External Attention for Sentence Extraction.</a></li>
<li><a href="#189. Neural Models for Documents with Metadata.">189. Neural Models for Documents with Metadata.</a></li>
<li><a href="#190. NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing.">190. NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing.</a></li>
<li><a href="#191. Large-Scale QA-SRL Parsing.">191. Large-Scale QA-SRL Parsing.</a></li>
<li><a href="#192. Syntax for Semantic Role Labeling, To Be, Or Not To Be.">192. Syntax for Semantic Role Labeling, To Be, Or Not To Be.</a></li>
<li><a href="#193. Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation.">193. Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation.</a></li>
<li><a href="#194. Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.">194. Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.</a></li>
<li><a href="#195. Token-level and sequence-level loss smoothing for RNN language models.">195. Token-level and sequence-level loss smoothing for RNN language models.</a></li>
<li><a href="#196. Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers.">196. Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers.</a></li>
<li><a href="#197. To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness.">197. To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness.</a></li>
<li><a href="#198. What you can cram into a single \$&!#* vector: Probing sentence embeddings for linguistic properties.">198. What you can cram into a single \$&amp;!#* vector: Probing sentence embeddings for linguistic properties.</a></li>
<li><a href="#199. Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning.">199. Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning.</a></li>
<li><a href="#200. Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.">200. Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.</a></li>
<li><a href="#201. Zero-Shot Transfer Learning for Event Extraction.">201. Zero-Shot Transfer Learning for Event Extraction.</a></li>
<li><a href="#202. Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction.">202. Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction.</a></li>
<li><a href="#203. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning.">203. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning.</a></li>
<li><a href="#204. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.">204. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.</a></li>
<li><a href="#205. Personalizing Dialogue Agents: I have a dog, do you have pets too?">205. Personalizing Dialogue Agents: I have a dog, do you have pets too?</a></li>
<li><a href="#206. Efficient Large-Scale Neural Domain Classification with Personalized Attention.">206. Efficient Large-Scale Neural Domain Classification with Personalized Attention.</a></li>
<li><a href="#207. Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment.">207. Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment.</a></li>
<li><a href="#208. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.">208. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.</a></li>
<li><a href="#209. Efficient Low-rank Multimodal Fusion With Modality-Specific Factors.">209. Efficient Low-rank Multimodal Fusion With Modality-Specific Factors.</a></li>
<li><a href="#210. Discourse Coherence: Concurrent Explicit and Implicit Relations.">210. Discourse Coherence: Concurrent Explicit and Implicit Relations.</a></li>
<li><a href="#211. A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text.">211. A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text.</a></li>
<li><a href="#212. Joint Reasoning for Temporal and Causal Relations.">212. Joint Reasoning for Temporal and Causal Relations.</a></li>
<li><a href="#213. Modeling Naive Psychology of Characters in Simple Commonsense Stories.">213. Modeling Naive Psychology of Characters in Simple Commonsense Stories.</a></li>
<li><a href="#214. A Deep Relevance Model for Zero-Shot Document Filtering.">214. A Deep Relevance Model for Zero-Shot Document Filtering.</a></li>
<li><a href="#215. Disconnected Recurrent Neural Networks for Text Categorization.">215. Disconnected Recurrent Neural Networks for Text Categorization.</a></li>
<li><a href="#216. Joint Embedding of Words and Labels for Text Classification.">216. Joint Embedding of Words and Labels for Text Classification.</a></li>
<li><a href="#217. Neural Sparse Topical Coding.">217. Neural Sparse Topical Coding.</a></li>
<li><a href="#218. Document Similarity for Texts of Varying Lengths via Hidden Topics.">218. Document Similarity for Texts of Varying Lengths via Hidden Topics.</a></li>
<li><a href="#219. Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour.">219. Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour.</a></li>
<li><a href="#220. Multi-Input Attention for Unsupervised OCR Correction.">220. Multi-Input Attention for Unsupervised OCR Correction.</a></li>
<li><a href="#221. Building Language Models for Text with Named Entities.">221. Building Language Models for Text with Named Entities.</a></li>
<li><a href="#222. hyperdoc2vec: Distributed Representations of Hypertext Documents.">222. hyperdoc2vec: Distributed Representations of Hypertext Documents.</a></li>
<li><a href="#223. Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval.">223. Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval.</a></li>
<li><a href="#224. Neural Natural Language Inference Models Enhanced with External Knowledge.">224. Neural Natural Language Inference Models Enhanced with External Knowledge.</a></li>
<li><a href="#225. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples.">225. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples.</a></li>
<li><a href="#226. Subword-level Word Vector Representations for Korean.">226. Subword-level Word Vector Representations for Korean.</a></li>
<li><a href="#227. Incorporating Chinese Characters of Words for Lexical Sememe Prediction.">227. Incorporating Chinese Characters of Words for Lexical Sememe Prediction.</a></li>
<li><a href="#228. SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment.">228. SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment.</a></li>
<li><a href="#229. End-to-End Reinforcement Learning for Automatic Taxonomy Induction.">229. End-to-End Reinforcement Learning for Automatic Taxonomy Induction.</a></li>
<li><a href="#230. Incorporating Glosses into Neural Word Sense Disambiguation.">230. Incorporating Glosses into Neural Word Sense Disambiguation.</a></li>
<li><a href="#231. Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages.">231. Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages.</a></li>
<li><a href="#232. Learning Domain-Sensitive and Sentiment-Aware Word Embeddings.">232. Learning Domain-Sensitive and Sentiment-Aware Word Embeddings.</a></li>
<li><a href="#233. Cross-Domain Sentiment Classification with Target Domain Specific Information.">233. Cross-Domain Sentiment Classification with Target Domain Specific Information.</a></li>
<li><a href="#234. Aspect Based Sentiment Analysis with Gated Convolutional Networks.">234. Aspect Based Sentiment Analysis with Gated Convolutional Networks.</a></li>
<li><a href="#235. A Helping Hand: Transfer Learning for Deep Sentiment Analysis.">235. A Helping Hand: Transfer Learning for Deep Sentiment Analysis.</a></li>
<li><a href="#236. Cold-Start Aware User and Product Attention for Sentiment Classification.">236. Cold-Start Aware User and Product Attention for Sentiment Classification.</a></li>
<li><a href="#237. Modeling Deliberative Argumentation Strategies on Wikipedia.">237. Modeling Deliberative Argumentation Strategies on Wikipedia.</a></li>
<li><a href="#238. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.">238. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.</a></li>
<li><a href="#239. Learning Translations via Images with a Massively Multilingual Image Dataset.">239. Learning Translations via Images with a Massively Multilingual Image Dataset.</a></li>
<li><a href="#240. On the Automatic Generation of Medical Imaging Reports.">240. On the Automatic Generation of Medical Imaging Reports.</a></li>
<li><a href="#241. Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.">241. Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.</a></li>
<li><a href="#242. Think Visually: Question Answering through Virtual Imagery.">242. Think Visually: Question Answering through Virtual Imagery.</a></li>
<li><a href="#243. Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game.">243. Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game.</a></li>
<li><a href="#244. A Purely End-to-End System for Multi-speaker Speech Recognition.">244. A Purely End-to-End System for Multi-speaker Speech Recognition.</a></li>
<li><a href="#245. A Structured Variational Autoencoder for Contextual Morphological Inflection.">245. A Structured Variational Autoencoder for Contextual Morphological Inflection.</a></li>
<li><a href="#246. Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings.">246. Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings.</a></li>
<li><a href="#247. Neural Factor Graph Models for Cross-lingual Morphological Tagging.">247. Neural Factor Graph Models for Cross-lingual Morphological Tagging.</a></li>
<li><a href="#248. Global Transition-based Non-projective Dependency Parsing.">248. Global Transition-based Non-projective Dependency Parsing.</a></li>
<li><a href="#249. Constituency Parsing with a Self-Attentive Encoder.">249. Constituency Parsing with a Self-Attentive Encoder.</a></li>
<li><a href="#250. Pre- and In-Parsing Models for Neural Empty Category Detection.">250. Pre- and In-Parsing Models for Neural Empty Category Detection.</a></li>
<li><a href="#251. Composing Finite State Transducers on GPUs.">251. Composing Finite State Transducers on GPUs.</a></li>
<li><a href="#252. Supervised Treebank Conversion: Data and Approaches.">252. Supervised Treebank Conversion: Data and Approaches.</a></li>
<li><a href="#253. Object-oriented Neural Programming (OONP">253. Object-oriented Neural Programming (OONP) for Document Understanding.</a> for Document Understanding.)</li>
<li><a href="#254. Finding syntax in human encephalography with beam search.">254. Finding syntax in human encephalography with beam search.</a></li>
<li><a href="#255. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information.">255. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information.</a></li>
<li><a href="#256. Let's do it "again": A First Computational Approach to Detecting Adverbial Presupposition Triggers.">256. Let's do it "again": A First Computational Approach to Detecting Adverbial Presupposition Triggers.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="56th ACL 2018:Melbourne, Australia - Volume 1: Long Papers">56th ACL 2018:Melbourne, Australia - Volume 1: Long Papers</h1>
<p><a href="https://www.aclweb.org/anthology/volumes/P18-1/">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers.</a> Association for Computational Linguistics
【<a href="https://dblp.uni-trier.de/db/conf/acl/acl2018-1.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 256 || Session Num: 0">Paper Num: 256 || Session Num: 0</h2>
<h3 id="1. Probabilistic FastText for Multi-Sense Word Embeddings.">1. Probabilistic FastText for Multi-Sense Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1001/">Paper Link</a>】    【Pages】:1-11</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Athiwaratkun:Ben">Ben Athiwaratkun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wilson:Andrew_Gordon">Andrew Gordon Wilson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Anandkumar:Anima">Anima Anandkumar</a></p>
<p>【Abstract】:
We introduce Probabilistic FastText, a new model for word embeddings that can capture multiple word senses, sub-word structure, and uncertainty information. In particular, we represent each word with a Gaussian mixture density, where the mean of a mixture component is given by the sum of n-grams. This representation allows the model to share the “strength” across sub-word structures (e.g. Latin roots), producing accurate representations of rare, misspelt, or even unseen words. Moreover, each component of the mixture can capture a different word sense. Probabilistic FastText outperforms both FastText, which has no probabilistic model, and dictionary-level probabilistic embeddings, which do not incorporate subword structures, on several word-similarity benchmarks, including English RareWord and foreign language datasets. We also achieve state-of-art performance on benchmarks that measure ability to discern different meanings. Thus, our model is the first to achieve best of both the worlds: multi-sense representations while having enriched semantics on rare words.</p>
<p>【Keywords】:</p>
<h3 id="2. A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors.">2. A La Carte Embedding: Cheap but Effective Induction of Semantic Feature Vectors.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1002/">Paper Link</a>】    【Pages】:12-22</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khodak:Mikhail">Mikhail Khodak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Saunshi:Nikunj">Nikunj Saunshi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Yingyu">Yingyu Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Tengyu">Tengyu Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stewart:Brandon">Brandon Stewart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Arora:Sanjeev">Sanjeev Arora</a></p>
<p>【Abstract】:
Motivations like domain adaptation, transfer learning, and feature learning have fueled interest in inducing embeddings for rare or unseen words, n-grams, synsets, and other textual features. This paper introduces a la carte embedding, a simple and general alternative to the usual word2vec-based approaches for building such representations that is based upon recent theoretical results for GloVe-like embeddings. Our method relies mainly on a linear transformation that is efficiently learnable using pretrained word vectors and linear regression. This transform is applicable on the fly in the future when a new text feature or rare word is encountered, even if only a single usage example is available. We introduce a new dataset showing how the a la carte method requires fewer examples of words in context to learn high-quality embeddings and we obtain state-of-the-art results on a nonce task and some unsupervised document classification tasks.</p>
<p>【Keywords】:</p>
<h3 id="3. Unsupervised Learning of Distributional Relation Vectors.">3. Unsupervised Learning of Distributional Relation Vectors.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1003/">Paper Link</a>】    【Pages】:23-33</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jameel:Shoaib">Shoaib Jameel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bouraoui:Zied">Zied Bouraoui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schockaert:Steven">Steven Schockaert</a></p>
<p>【Abstract】:
Word embedding models such as GloVe rely on co-occurrence statistics to learn vector representations of word meaning. While we may similarly expect that co-occurrence statistics can be used to capture rich information about the relationships between different words, existing approaches for modeling such relationships are based on manipulating pre-trained word vectors. In this paper, we introduce a novel method which directly learns relation vectors from co-occurrence statistics. To this end, we first introduce a variant of GloVe, in which there is an explicit connection between word vectors and PMI weighted co-occurrence vectors. We then show how relation vectors can be naturally embedded into the resulting vector space.</p>
<p>【Keywords】:</p>
<h3 id="4. Explicit Retrofitting of Distributional Word Vectors.">4. Explicit Retrofitting of Distributional Word Vectors.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1004/">Paper Link</a>】    【Pages】:34-45</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Glavas:Goran">Goran Glavas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a></p>
<p>【Abstract】:
Semantic specialization of distributional word vectors, referred to as retrofitting, is a process of fine-tuning word vectors using external lexical knowledge in order to better embed some semantic relation. Existing retrofitting models integrate linguistic constraints directly into learning objectives and, consequently, specialize only the vectors of words from the constraints. In this work, in contrast, we transform external lexico-semantic relations into training examples which we use to learn an explicit retrofitting model (ER). The ER model allows us to learn a global specialization function and specialize the vectors of words unobserved in the training data as well. We report large gains over original distributional vector spaces in (1) intrinsic word similarity evaluation and on (2) two downstream tasks − lexical simplification and dialog state tracking. Finally, we also successfully specialize vector spaces of new languages (i.e., unseen in the training data) by coupling ER with shared multilingual distributional vector spaces.</p>
<p>【Keywords】:</p>
<h3 id="5. Unsupervised Neural Machine Translation with Weight Sharing.">5. Unsupervised Neural Machine Translation with Weight Sharing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1005/">Paper Link</a>】    【Pages】:46-55</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0007:Zhen">Zhen Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen_0048:Wei">Wei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0023:Feng">Feng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0002:Bo">Bo Xu</a></p>
<p>【Abstract】:
Unsupervised neural machine translation (NMT) is a recently proposed approach for machine translation which aims to train the model without using any labeled data. The models proposed for unsupervised NMT often use only one shared encoder to map the pairs of sentences from different languages to a shared-latent space, which is weak in keeping the unique and internal characteristics of each language, such as the style, terminology, and sentence structure. To address this issue, we introduce an extension by utilizing two independent encoders but sharing some partial weights which are responsible for extracting high-level representations of the input sentences. Besides, two different generative adversarial networks (GANs), namely the local GAN and global GAN, are proposed to enhance the cross-language translation. With this new approach, we achieve significant improvements on English-German, English-French and Chinese-to-English translation tasks.</p>
<p>【Keywords】:</p>
<h3 id="6. Triangular Architecture for Rare Language Translation.">6. Triangular Architecture for Rare Language Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1006/">Paper Link</a>】    【Pages】:56-65</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Shuo">Shuo Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenhu">Wenhu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Shujie">Shujie Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Mu">Mu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma_0001:Shuai">Shuai Ma</a></p>
<p>【Abstract】:
Neural Machine Translation (NMT) performs poor on the low-resource language pair (X,Z), especially when Z is a rare language. By introducing another rich language Y, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data (Y,Z) (may be small) and (X,Y) (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, Z is taken as the intermediate latent variable, and translation models of Z are jointly optimized with an unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of (X,Y). Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.</p>
<p>【Keywords】:</p>
<h3 id="7. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.">7. Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1007/">Paper Link</a>】    【Pages】:66-75</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kudo:Taku">Taku Kudo</a></p>
<p>【Abstract】:
Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.</p>
<p>【Keywords】:</p>
<h3 id="8. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.">8. The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1008/">Paper Link</a>】    【Pages】:76-86</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Mia_Xu">Mia Xu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Firat:Orhan">Orhan Firat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bapna:Ankur">Ankur Bapna</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Johnson:Melvin">Melvin Johnson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Macherey:Wolfgang">Wolfgang Macherey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Foster:George">George Foster</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jones:Llion">Llion Jones</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schuster:Mike">Mike Schuster</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shazeer:Noam">Noam Shazeer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Parmar:Niki">Niki Parmar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vaswani:Ashish">Ashish Vaswani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Uszkoreit:Jakob">Jakob Uszkoreit</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kaiser:Lukasz">Lukasz Kaiser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Zhifeng">Zhifeng Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Yonghui">Yonghui Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hughes:Macduff">Macduff Hughes</a></p>
<p>【Abstract】:
The past year has witnessed rapid advances in sequence-to-sequence (seq2seq) modeling for Machine Translation (MT). The classic RNN-based approaches to MT were first out-performed by the convolutional seq2seq model, which was then out-performed by the more recent Transformer model. Each of these new approaches consists of a fundamental architecture accompanied by a set of modeling and training techniques that are in principle applicable to other seq2seq architectures. In this paper, we tease apart the new architectures and their accompanying techniques in two ways. First, we identify several key modeling and training techniques, and apply them to the RNN architecture, yielding a new RNMT+ model that outperforms all of the three fundamental architectures on the benchmark WMT’14 English to French and English to German tasks. Second, we analyze the properties of each fundamental seq2seq architecture and devise new hybrid architectures intended to combine their strengths. Our hybrid models obtain further improvements, outperforming the RNMT+ model on both benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="9. Ultra-Fine Entity Typing.">9. Ultra-Fine Entity Typing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1009/">Paper Link</a>】    【Pages】:87-96</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Eunsol">Eunsol Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Levy:Omer">Omer Levy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
We introduce a new entity typing task: given a sentence with an entity mention, the goal is to predict a set of free-form phrases (e.g. skyscraper, songwriter, or criminal) that describe appropriate types for the target entity. This formulation allows us to use a new type of distant supervision at large scale: head words, which indicate the type of the noun phrases they appear in. We show that these ultra-fine types can be crowd-sourced, and introduce new evaluation sets that are much more diverse and fine-grained than existing benchmarks. We present a model that can predict ultra-fine types, and is trained using a multitask objective that pools our new head-word supervision with prior supervision from entity linking. Experimental results demonstrate that our model is effective in predicting entity types at varying granularity; it achieves state of the art performance on an existing fine-grained entity typing benchmark, and sets baselines for our newly-introduced datasets.</p>
<p>【Keywords】:</p>
<h3 id="10. Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking.">10. Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1010/">Paper Link</a>】    【Pages】:97-109</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Murty:Shikhar">Shikhar Murty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verga:Patrick">Patrick Verga</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vilnis:Luke">Luke Vilnis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radovanovic:Irena">Irena Radovanovic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>【Abstract】:
Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.</p>
<p>【Keywords】:</p>
<h3 id="11. Improving Knowledge Graph Embedding Using Simple Constraints.">11. Improving Knowledge Graph Embedding Using Simple Constraints.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1011/">Paper Link</a>】    【Pages】:110-121</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Boyang">Boyang Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0002:Quan">Quan Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0004:Bin">Bin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo_0001:Li">Li Guo</a></p>
<p>【Abstract】:
Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at <a href="https://github.com/iieir-km/ComplEx-NNE_AER">https://github.com/iieir-km/ComplEx-NNE_AER</a>.</p>
<p>【Keywords】:</p>
<h3 id="12. Towards Understanding the Geometry of Knowledge Graph Embeddings.">12. Towards Understanding the Geometry of Knowledge Graph Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1012/">Paper Link</a>】    【Pages】:122-131</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chandrahas:">Chandrahas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Aditya">Aditya Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Knowledge Graph (KG) embedding has emerged as a very active area of research over the last few years, resulting in the development of several embedding methods. These KG embedding methods represent KG entities and relations as vectors in a high-dimensional space. Despite this popularity and effectiveness of KG embeddings in various tasks (e.g., link prediction), geometric understanding of such embeddings (i.e., arrangement of entity and relation vectors in vector space) is unexplored – we fill this gap in the paper. We initiate a study to analyze the geometry of KG embeddings and correlate it with task performance and other hyperparameters. To the best of our knowledge, this is the first study of its kind. Through extensive experiments on real-world datasets, we discover several insights. For example, we find that there are sharp differences between the geometry of embeddings learnt by different classes of KG embeddings methods. We hope that this initial study will inspire other follow-up research on this important but unexplored problem.</p>
<p>【Keywords】:</p>
<h3 id="13. A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.">13. A Unified Model for Extractive and Abstractive Summarization using Inconsistency Loss.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1013/">Paper Link</a>】    【Pages】:132-141</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hsu:Wan_Ting">Wan Ting Hsu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chieh=Kai">Chieh-Kai Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Ming=Ying">Ming-Ying Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Min:Kerui">Kerui Min</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Jing">Jing Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Min">Min Sun</a></p>
<p>【Abstract】:
We propose a unified model combining the strength of extractive and abstractive summarization. On the one hand, a simple extractive model can obtain sentence-level attention with high ROUGE scores but less readable. On the other hand, a more complicated abstractive model can obtain word-level dynamic attention to generate a more readable paragraph. In our model, sentence-level attention is used to modulate the word-level attention such that words in less attended sentences are less likely to be generated. Moreover, a novel inconsistency loss function is introduced to penalize the inconsistency between two levels of attentions. By end-to-end training our model with the inconsistency loss and original losses of extractive and abstractive models, we achieve state-of-the-art ROUGE scores while being the most informative and readable summarization on the CNN/Daily Mail dataset in a solid human evaluation.</p>
<p>【Keywords】:</p>
<h3 id="14. Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks.">14. Extractive Summarization with SWAP-NET: Sentences and Words from Alternating Pointer Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1014/">Paper Link</a>】    【Pages】:142-151</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jadhav:Aishwarya">Aishwarya Jadhav</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rajan:Vaibhav">Vaibhav Rajan</a></p>
<p>【Abstract】:
We present a new neural sequence-to-sequence model for extractive summarization called SWAP-NET (Sentences and Words from Alternating Pointer Networks). Extractive summaries comprising a salient subset of input sentences, often also contain important key words. Guided by this principle, we design SWAP-NET that models the interaction of key words and salient sentences using a new two-level pointer network based architecture. SWAP-NET identifies both salient sentences and key words in an input document, and then combines them to form the extractive summary. Experiments on large scale benchmark corpora demonstrate the efficacy of SWAP-NET that outperforms state-of-the-art extractive summarizers.</p>
<p>【Keywords】:</p>
<h3 id="15. Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization.">15. Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1015/">Paper Link</a>】    【Pages】:152-161</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Ziqiang">Ziqiang Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li_0002:Wenjie">Wenjie Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a></p>
<p>【Abstract】:
Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries.</p>
<p>【Keywords】:</p>
<h3 id="16. Simple and Effective Text Simplification Using Semantic and Neural Methods.">16. Simple and Effective Text Simplification Using Semantic and Neural Methods.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1016/">Paper Link</a>】    【Pages】:162-173</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sulem:Elior">Elior Sulem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rappoport:Ari">Ari Rappoport</a></p>
<p>【Abstract】:
Sentence splitting is a major simplification operator. Here we present a simple and efficient splitting algorithm based on an automatic semantic parser. After splitting, the text is amenable for further fine-tuned simplification operations. In particular, we show that neural Machine Translation can be effectively used in this situation. Previous application of Machine Translation for simplification suffers from a considerable disadvantage in that they are over-conservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the state-of-the-art in combined lexical and structural simplification.</p>
<p>【Keywords】:</p>
<h3 id="17. Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20, 000 English Words.">17. Obtaining Reliable Human Ratings of Valence, Arousal, and Dominance for 20, 000 English Words.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1017/">Paper Link</a>】    【Pages】:174-184</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mohammad:Saif">Saif Mohammad</a></p>
<p>【Abstract】:
Words play a central role in language and thought. Factor analysis studies have shown that the primary dimensions of meaning are valence, arousal, and dominance (VAD). We present the NRC VAD Lexicon, which has human ratings of valence, arousal, and dominance for more than 20,000 English words. We use Best–Worst Scaling to obtain fine-grained scores and address issues of annotation consistency that plague traditional rating scale methods of annotation. We show that the ratings obtained are vastly more reliable than those in existing lexicons. We also show that there exist statistically significant differences in the shared understanding of valence, arousal, and dominance across demographic variables such as age, gender, and personality.</p>
<p>【Keywords】:</p>
<h3 id="18. Comprehensive Supersense Disambiguation of English Prepositions and Possessives.">18. Comprehensive Supersense Disambiguation of English Prepositions and Possessives.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1018/">Paper Link</a>】    【Pages】:185-196</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schneider:Nathan">Nathan Schneider</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwang:Jena_D=">Jena D. Hwang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Srikumar:Vivek">Vivek Srikumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prange:Jakob">Jakob Prange</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blodgett:Austin">Austin Blodgett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moeller:Sarah_R=">Sarah R. Moeller</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stern:Aviram">Aviram Stern</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bitan:Adi">Adi Bitan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a></p>
<p>【Abstract】:
Semantic relations are often signaled with prepositional or possessive marking—but extreme polysemy bedevils their analysis and automatic interpretation. We introduce a new annotation scheme, corpus, and task for the disambiguation of prepositions and possessives in English. Unlike previous approaches, our annotations are comprehensive with respect to types and tokens of these markers; use broadly applicable supersense classes rather than fine-grained dictionary definitions; unite prepositions and possessives under the same class inventory; and distinguish between a marker’s lexical contribution and the role it marks in the context of a predicate or scene. Strong interannotator agreement rates, as well as encouraging disambiguation results with established supervised methods, speak to the viability of the scheme and task.</p>
<p>【Keywords】:</p>
<h3 id="19. A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature.">19. A Corpus with Multi-Level Annotations of Patients, Interventions and Outcomes to Support Language Processing for Medical Literature.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1019/">Paper Link</a>】    【Pages】:197-207</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nye:Benjamin">Benjamin Nye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junyi_Jessy">Junyi Jessy Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patel:Roma">Roma Patel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yinfei">Yinfei Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marshall:Iain_James">Iain James Marshall</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nenkova:Ani">Ani Nenkova</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wallace:Byron_C=">Byron C. Wallace</a></p>
<p>【Abstract】:
We present a corpus of 5,000 richly annotated abstracts of medical articles describing clinical randomized controlled trials. Annotations include demarcations of text spans that describe the Patient population enrolled, the Interventions studied and to what they were Compared, and the Outcomes measured (the ‘PICO’ elements). These spans are further annotated at a more granular level, e.g., individual interventions within them are marked and mapped onto a structured medical vocabulary. We acquired annotations from a diverse set of workers with varying levels of expertise and cost. We describe our data collection process and the corpus itself in detail. We then outline a set of challenging NLP tasks that would aid searching of the medical literature and the practice of evidence-based medicine.</p>
<p>【Keywords】:</p>
<h3 id="20. Efficient Online Scalar Annotation with Bounded Support.">20. Efficient Online Scalar Annotation with Bounded Support.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1020/">Paper Link</a>】    【Pages】:208-218</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sakaguchi:Keisuke">Keisuke Sakaguchi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Durme:Benjamin_Van">Benjamin Van Durme</a></p>
<p>【Abstract】:
We describe a novel method for efficiently eliciting scalar annotations for dataset construction and system quality estimation by human judgments. We contrast direct assessment (annotators assign scores to items directly), online pairwise ranking aggregation (scores derive from annotator comparison of items), and a hybrid approach (EASL: Efficient Annotation of Scalar Labels) proposed here. Our proposal leads to increased correlation with ground truth, at far greater annotator efficiency, suggesting this strategy as an improved mechanism for dataset creation and manual system evaluation.</p>
<p>【Keywords】:</p>
<h3 id="21. Neural Argument Generation Augmented with Externally Retrieved Evidence.">21. Neural Argument Generation Augmented with Externally Retrieved Evidence.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1021/">Paper Link</a>】    【Pages】:219-230</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hua:Xinyu">Xinyu Hua</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0008:Lu">Lu Wang</a></p>
<p>【Abstract】:
High quality arguments are essential elements for human reasoning and decision-making processes. However, effective argument construction is a challenging task for both human and machines. In this work, we study a novel task on automatically generating arguments of a different stance for a given statement. We propose an encoder-decoder style neural network-based argument generation model enriched with externally retrieved evidence from Wikipedia. Our model first generates a set of talking point phrases as intermediate representation, followed by a separate decoder producing the final argument based on both input and the keyphrases. Experiments on a large-scale dataset collected from Reddit show that our model constructs arguments with more topic-relevant content than popular sequence-to-sequence generation models according to automatic evaluation and human assessments.</p>
<p>【Keywords】:</p>
<h3 id="22. A Stylometric Inquiry into Hyperpartisan and Fake News.">22. A Stylometric Inquiry into Hyperpartisan and Fake News.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1022/">Paper Link</a>】    【Pages】:231-240</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Potthast:Martin">Martin Potthast</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiesel:Johannes">Johannes Kiesel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reinartz:Kevin">Kevin Reinartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bevendorff:Janek">Janek Bevendorff</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>【Abstract】:
We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.</p>
<p>【Keywords】:</p>
<h3 id="23. Retrieval of the Best Counterargument without Prior Topic Knowledge.">23. Retrieval of the Best Counterargument without Prior Topic Knowledge.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1023/">Paper Link</a>】    【Pages】:241-251</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wachsmuth:Henning">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Syed:Shahbaz">Shahbaz Syed</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>【Abstract】:
Given any argument on any controversial topic, how to counter it? This question implies the challenging retrieval task of finding the best counterargument. Since prior knowledge of a topic cannot be expected in general, we hypothesize the best counterargument to invoke the same aspects as the argument while having the opposite stance. To operationalize our hypothesis, we simultaneously model the similarity and dissimilarity of pairs of arguments, based on the words and embeddings of the arguments’ premises and conclusions. A salient property of our model is its independence from the topic at hand, i.e., it applies to arbitrary arguments. We evaluate different model variations on millions of argument pairs derived from the web portal idebate.org. Systematic ranking experiments suggest that our hypothesis is true for many arguments: For 7.6 candidates with opposing stance on average, we rank the best counterargument highest with 60% accuracy. Even among all 2801 test set pairs as candidates, we still find the best one about every third time.</p>
<p>【Keywords】:</p>
<h3 id="24. LinkNBed: Multi-Graph Representation Learning with Entity Linkage.">24. LinkNBed: Multi-Graph Representation Learning with Entity Linkage.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1024/">Paper Link</a>】    【Pages】:252-262</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Trivedi:Rakshit">Rakshit Trivedi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sisman:Bunyamin">Bunyamin Sisman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Xin_Luna">Xin Luna Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Faloutsos:Christos">Christos Faloutsos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Jun">Jun Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zha:Hongyuan">Hongyuan Zha</a></p>
<p>【Abstract】:
Knowledge graphs have emerged as an important model for studying complex multi-relational data. This has given rise to the construction of numerous large scale but incomplete knowledge graphs encoding information extracted from various resources. An effective and scalable approach to jointly learn over multiple graphs and eventually construct a unified graph is a crucial next step for the success of knowledge-based inference for many downstream applications. To this end, we propose LinkNBed, a deep relational learning framework that learns entity and relationship representations across multiple graphs. We identify entity linkage across graphs as a vital component to achieve our goal. We design a novel objective that leverage entity linkage and build an efficient multi-task training procedure. Experiments on link prediction and entity linkage demonstrate substantial improvements over the state-of-the-art relational learning approaches.</p>
<p>【Keywords】:</p>
<h3 id="25. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures.">25. Probabilistic Embedding of Knowledge Graphs with Box Lattice Measures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1025/">Paper Link</a>】    【Pages】:263-272</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vilnis:Luke">Luke Vilnis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiang">Xiang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Murty:Shikhar">Shikhar Murty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McCallum:Andrew">Andrew McCallum</a></p>
<p>【Abstract】:
Embedding methods which enforce a partial order or lattice structure over the concept space, such as Order Embeddings (OE), are a natural way to model transitive relational data (e.g. entailment graphs). However, OE learns a deterministic knowledge base, limiting expressiveness of queries and the ability to use uncertainty for both prediction and learning (e.g. learning from expectations). Probabilistic extensions of OE have provided the ability to somewhat calibrate these denotational probabilities while retaining the consistency and inductive bias of ordered models, but lack the ability to model the negative correlations found in real-world knowledge. In this work we show that a broad class of models that assign probability measures to OE can never capture negative correlation, which motivates our construction of a novel box lattice and accompanying probability measure to capture anti-correlation and even disjoint concepts, while still providing the benefits of probabilistic modeling, such as the ability to perform rich joint and conditional queries over arbitrary sets of concepts, and both learning from and predicting calibrated uncertainty. We show improvements over previous approaches in modeling the Flickr and WordNet entailment graphs, and investigate the power of the model.</p>
<p>【Keywords】:</p>
<h3 id="26. Graph-to-Sequence Learning using Gated Graph Neural Networks.">26. Graph-to-Sequence Learning using Gated Graph Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1026/">Paper Link</a>】    【Pages】:273-283</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Beck:Daniel">Daniel Beck</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>【Abstract】:
Many NLP applications can be framed as a graph-to-sequence learning problem. Previous work proposing neural architectures on graph-to-sequence obtained promising results compared to grammar-based approaches but still rely on linearisation heuristics and/or standard recurrent networks to achieve the best performance. In this work propose a new model that encodes the full structural information contained in the graph. Our architecture couples the recently proposed Gated Graph Neural Networks with an input transformation that allows nodes and edges to have their own hidden representations, while tackling the parameter explosion problem present in previous work. Experimental results shows that our model outperforms strong baselines in generation from AMR graphs and syntax-based neural machine translation.</p>
<p>【Keywords】:</p>
<h3 id="27. Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context.">27. Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1027/">Paper Link</a>】    【Pages】:284-294</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khandelwal:Urvashi">Urvashi Khandelwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:He">He He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi_0003:Peng">Peng Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jurafsky:Dan">Dan Jurafsky</a></p>
<p>【Abstract】:
We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.</p>
<p>【Keywords】:</p>
<h3 id="28. Bridging CNNs, RNNs, and Weighted Finite-State Machines.">28. Bridging CNNs, RNNs, and Weighted Finite-State Machines.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1028/">Paper Link</a>】    【Pages】:295-305</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schwartz:Roy">Roy Schwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thomson:Sam">Sam Thomson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
Recurrent and convolutional neural networks comprise two distinct families of models that have proven to be useful for encoding natural language utterances. In this paper we present SoPa, a new model that aims to bridge these two approaches. SoPa combines neural representation learning with weighted finite-state automata (WFSAs) to learn a soft version of traditional surface patterns. We show that SoPa is an extension of a one-layer CNN, and that such CNNs are equivalent to a restricted version of SoPa, and accordingly, to a restricted form of WFSA. Empirically, on three text classification tasks, SoPa is comparable or better than both a BiLSTM (RNN) baseline and a CNN baseline, and is particularly useful in small data settings.</p>
<p>【Keywords】:</p>
<h3 id="29. Zero-shot Learning of Classifiers from Natural Language Quantification.">29. Zero-shot Learning of Classifiers from Natural Language Quantification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1029/">Paper Link</a>】    【Pages】:306-316</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Srivastava:Shashank">Shashank Srivastava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Labutov:Igor">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mitchell:Tom_M=">Tom M. Mitchell</a></p>
<p>【Abstract】:
Humans can efficiently learn new concepts using language. We present a framework through which a set of explanations of a concept can be used to learn a classifier without access to any labeled examples. We use semantic parsing to map explanations to probabilistic assertions grounded in latent class labels and observed attributes of unlabeled data, and leverage the differential semantics of linguistic quantifiers (e.g., ‘usually’ vs ‘always’) to drive model training. Experiments on three domains show that the learned classifiers outperform previous approaches for learning with limited data, and are comparable with fully supervised classifiers trained from a small number of labeled examples.</p>
<p>【Keywords】:</p>
<h3 id="30. Sentence-State LSTM for Text Representation.">30. Sentence-State LSTM for Text Representation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1030/">Paper Link</a>】    【Pages】:317-327</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qi">Qi Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Linfeng">Linfeng Song</a></p>
<p>【Abstract】:
Bi-directional LSTMs are a powerful tool for text representation. On the other hand, they have been shown to suffer various limitations due to their sequential nature. We investigate an alternative LSTM structure for encoding text, which consists of a parallel state for each word. Recurrent steps are used to perform local and global information exchange between words simultaneously, rather than incremental reading of a sequence of words. Results on various classification and sequence labelling benchmarks show that the proposed model has strong representation power, giving highly competitive performances compared to stacked BiLSTM models with similar parameter numbers.</p>
<p>【Keywords】:</p>
<h3 id="31. Universal Language Model Fine-tuning for Text Classification.">31. Universal Language Model Fine-tuning for Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1031/">Paper Link</a>】    【Pages】:328-339</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Howard:Jeremy">Jeremy Howard</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruder:Sebastian">Sebastian Ruder</a></p>
<p>【Abstract】:
Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.</p>
<p>【Keywords】:</p>
<h3 id="32. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.">32. Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1032/">Paper Link</a>】    【Pages】:340-350</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/P=ouml=rner:Nina">Nina Pörner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth_0001:Benjamin">Benjamin Roth</a></p>
<p>【Abstract】:
The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.</p>
<p>【Keywords】:</p>
<h3 id="33. Improving Text-to-SQL Evaluation Methodology.">33. Improving Text-to-SQL Evaluation Methodology.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1033/">Paper Link</a>】    【Pages】:351-360</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Finegan=Dollak:Catherine">Catherine Finegan-Dollak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kummerfeld:Jonathan_K=">Jonathan K. Kummerfeld</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Li">Li Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ramanathan:Karthik">Karthik Ramanathan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sadasivam:Sesh">Sesh Sadasivam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0037:Rui">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radev:Dragomir_R=">Dragomir R. Radev</a></p>
<p>【Abstract】:
To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.</p>
<p>【Keywords】:</p>
<h3 id="34. Semantic Parsing with Syntax- and Table-Aware SQL Generation.">34. Semantic Parsing with Syntax- and Table-Aware SQL Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1034/">Paper Link</a>】    【Pages】:361-372</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Yibo">Yibo Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tang:Duyu">Duyu Tang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Nan">Nan Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Jianshu">Jianshu Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Guihong">Guihong Cao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Xiaocheng">Xiaocheng Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin_0001:Bing">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>【Abstract】:
We present a generative model to map natural language questions into SQL queries. Existing neural network based approaches typically generate a SQL query word-by-word, however, a large portion of the generated results is incorrect or not executable due to the mismatch between question words and table contents. Our approach addresses this problem by considering the structure of table and the syntax of SQL language. The quality of the generated SQL query is significantly improved through (1) learning to replicate content from column names, cells or SQL keywords; and (2) improving the generation of WHERE clause by leveraging the column-cell relation. Experiments are conducted on WikiSQL, a recently released dataset with the largest question- SQL pairs. Our approach significantly improves the state-of-the-art execution accuracy from 69.0% to 74.4%.</p>
<p>【Keywords】:</p>
<h3 id="35. Multitask Parsing Across Semantic Representations.">35. Multitask Parsing Across Semantic Representations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1035/">Paper Link</a>】    【Pages】:373-385</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hershcovich:Daniel">Daniel Hershcovich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rappoport:Ari">Ari Rappoport</a></p>
<p>【Abstract】:
The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others. In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.</p>
<p>【Keywords】:</p>
<h3 id="36. Character-Level Models versus Morphology in Semantic Role Labeling.">36. Character-Level Models versus Morphology in Semantic Role Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1036/">Paper Link</a>】    【Pages】:386-396</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sahin:G=ouml=zde_G=uuml=l">Gözde Gül Sahin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Steedman:Mark">Mark Steedman</a></p>
<p>【Abstract】:
Character-level models have become a popular approach specially for their accessibility and ability to handle unseen data. However, little is known on their ability to reveal the underlying morphological structure of a word, which is a crucial skill for high-level semantic analysis tasks, such as semantic role labeling (SRL). In this work, we train various types of SRL models that use word, character and morphology level information and analyze how performance of characters compare to words and morphology for several languages. We conduct an in-depth error analysis for each morphological typology and analyze the strengths and limitations of character-level models that relate to out-of-domain data, training data size, long range dependencies and model complexity. Our exhaustive analyses shed light on important characteristics of character-level models and their semantic capability.</p>
<p>【Keywords】:</p>
<h3 id="37. AMR Parsing as Graph Prediction with Latent Alignment.">37. AMR Parsing as Graph Prediction with Latent Alignment.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1037/">Paper Link</a>】    【Pages】:397-407</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Chunchuan">Chunchuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a></p>
<p>【Abstract】:
Abstract meaning representations (AMRs) are broad-coverage sentence-level semantic representations. AMRs represent sentences as rooted labeled directed acyclic graphs. AMR parsing is challenging partly due to the lack of annotated alignments between nodes in the graphs and words in the corresponding sentences. We introduce a neural parser which treats alignments as latent variables within a joint probabilistic model of concepts, relations and alignments. As exact inference requires marginalizing over alignments and is infeasible, we use the variational autoencoding framework and a continuous relaxation of the discrete alignments. We show that joint modeling is preferable to using a pipeline of align and parse. The parser achieves the best reported results on the standard benchmark (74.4% on LDC2016E25).</p>
<p>【Keywords】:</p>
<h3 id="38. Accurate SHRG-Based Semantic Parsing.">38. Accurate SHRG-Based Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1038/">Paper Link</a>】    【Pages】:408-418</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yufei">Yufei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Weiwei">Weiwei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>【Abstract】:
We demonstrate that an SHRG-based parser can produce semantic graphs much more accurately than previously shown, by relating synchronous production rules to the syntacto-semantic composition process. Our parser achieves an accuracy of 90.35 for EDS (89.51 for DMRS) in terms of elementary dependency match, which is a 4.87 (5.45) point improvement over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations.</p>
<p>【Keywords】:</p>
<h3 id="39. Using Intermediate Representations to Solve Math Word Problems.">39. Using Intermediate Representations to Solve Math Word Problems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1039/">Paper Link</a>】    【Pages】:419-428</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Danqing">Danqing Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Jin=Ge">Jin-Ge Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Qingyu">Qingyu Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Jian">Jian Yin</a></p>
<p>【Abstract】:
To solve math word problems, previous statistical approaches attempt at learning a direct mapping from a problem description to its corresponding equation system. However, such mappings do not include the information of a few higher-order operations that cannot be explicitly represented in equations but are required to solve the problem. The gap between natural language and equations makes it difficult for a learned model to generalize from limited data. In this work we present an intermediate meaning representation scheme that tries to reduce this gap. We use a sequence-to-sequence model with a novel attention regularization term to generate the intermediate forms, then execute them to obtain the final answers. Since the intermediate forms are latent, we propose an iterative labeling framework for learning by leveraging supervision signals from both equations and answers. Our experiments show using intermediate forms outperforms directly predicting equations.</p>
<p>【Keywords】:</p>
<h3 id="40. Discourse Representation Structure Parsing.">40. Discourse Representation Structure Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1040/">Paper Link</a>】    【Pages】:429-439</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jiangming">Jiangming Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
We introduce an open-domain neural semantic parser which generates formal meaning representations in the style of Discourse Representation Theory (DRT; Kamp and Reyle 1993). We propose a method which transforms Discourse Representation Structures (DRSs) to trees and develop a structure-aware model which decomposes the decoding process into three stages: basic DRS structure prediction, condition prediction (i.e., predicates and relations), and referent prediction (i.e., variables). Experimental results on the Groningen Meaning Bank (GMB) show that our model outperforms competitive baselines by a wide margin.</p>
<p>【Keywords】:</p>
<h3 id="41. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms.">41. Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1041/">Paper Link</a>】    【Pages】:440-450</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Dinghan">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guoyin">Guoyin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenlin">Wenlin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Min:Martin_Renqiang">Martin Renqiang Min</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Qinliang">Qinliang Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yizhe">Yizhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chunyuan">Chunyuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henao:Ricardo">Ricardo Henao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging.</p>
<p>【Keywords】:</p>
<h3 id="42. ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations.">42. ParaNMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1042/">Paper Link</a>】    【Pages】:451-462</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wieting:John">John Wieting</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gimpel:Kevin">Kevin Gimpel</a></p>
<p>【Abstract】:
We describe ParaNMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following Wieting et al. (2017). Our hope is that ParaNMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use ParaNMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation.</p>
<p>【Keywords】:</p>
<h3 id="43. Event2Mind: Commonsense Inference on Events, Intents, and Reactions.">43. Event2Mind: Commonsense Inference on Events, Intents, and Reactions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1043/">Paper Link</a>】    【Pages】:463-473</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rashkin:Hannah">Hannah Rashkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sap:Maarten">Maarten Sap</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Allaway:Emily">Emily Allaway</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>【Abstract】:
We investigate a new commonsense inference task: given an event described in a short free-form text (“X drinks coffee in the morning”), a system reasons about the likely intents (“X wants to stay awake”) and reactions (“X feels alert”) of the event’s participants. To support this study, we construct a new crowdsourced corpus of 25,000 event phrases covering a diverse range of everyday events and situations. We report baseline performance on this task, demonstrating that neural encoder-decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants. In addition, we demonstrate how commonsense inference on people’s intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts.</p>
<p>【Keywords】:</p>
<h3 id="44. Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis.">44. Neural Adversarial Training for Semi-supervised Japanese Predicate-argument Structure Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1044/">Paper Link</a>】    【Pages】:474-484</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kurita:Shuhei">Shuhei Kurita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kawahara:Daisuke">Daisuke Kawahara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurohashi:Sadao">Sadao Kurohashi</a></p>
<p>【Abstract】:
Japanese predicate-argument structure (PAS) analysis involves zero anaphora resolution, which is notoriously difficult. To improve the performance of Japanese PAS analysis, it is straightforward to increase the size of corpora annotated with PAS. However, since it is prohibitively expensive, it is promising to take advantage of a large amount of raw corpora. In this paper, we propose a novel Japanese PAS analysis model based on semi-supervised adversarial training with a raw corpus. In our experiments, our model outperforms existing state-of-the-art models for Japanese PAS analysis.</p>
<p>【Keywords】:</p>
<h3 id="45. Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures.">45. Improving Event Coreference Resolution by Modeling Correlations between Event Coreference Chains and Document Topic Structures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1045/">Paper Link</a>】    【Pages】:485-495</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choubey:Prafulla_Kumar">Prafulla Kumar Choubey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Ruihong">Ruihong Huang</a></p>
<p>【Abstract】:
This paper proposes a novel approach for event coreference resolution that models correlations between event coreference chains and document topical structures through an Integer Linear Programming formulation. We explicitly model correlations between the main event chains of a document with topic transition sentences, inter-coreference chain correlations, event mention distributional characteristics and sub-event structure, and use them with scores obtained from a local coreference relation classifier for jointly resolving multiple event chains in a document. Our experiments across KBP 2016 and 2017 datasets suggest that each of the structures contribute to improving event coreference resolution performance.</p>
<p>【Keywords】:</p>
<h3 id="46. DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction.">46. DSGAN: Generative Adversarial Training for Distant Supervision Relation Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1046/">Paper Link</a>】    【Pages】:496-505</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Pengda">Pengda Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Weiran">Weiran Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Distant supervision can effectively label data for relation extraction, but suffers from the noise labeling problem. Recent works mainly perform soft bag-level noise reduction strategies to find the relatively better samples in a sentence bag, which is suboptimal compared with making a hard decision of false positive samples in sentence level. In this paper, we introduce an adversarial learning framework, which we named DSGAN, to learn a sentence-level true-positive generator. Inspired by Generative Adversarial Networks, we regard the positive samples generated by the generator as the negative samples to train the discriminator. The optimal generator is obtained until the discrimination ability of the discriminator has the greatest decline. We adopt the generator to filter distant supervision training dataset and redistribute the false positive instances into the negative set, in which way to provide a cleaned dataset for relation classification. The experimental results show that the proposed strategy significantly improves the performance of distant supervision relation extraction comparing to state-of-the-art systems.</p>
<p>【Keywords】:</p>
<h3 id="47. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.">47. Extracting Relational Facts by an End-to-End Neural Model with Copy Mechanism.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1047/">Paper Link</a>】    【Pages】:506-514</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Xiangrong">Xiangrong Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Daojian">Daojian Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Shizhu">Shizhu He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Kang">Kang Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>【Abstract】:
The relational facts in sentences are often complicated. Different relational triplets may have overlaps in a sentence. We divided the sentences into three types according to triplet overlap degree, including Normal, EntityPairOverlap and SingleEntiyOverlap. Existing methods mainly focus on Normal class and fail to extract relational triplets precisely. In this paper, we propose an end-to-end model based on sequence-to-sequence learning with copy mechanism, which can jointly extract relational facts from sentences of any of these classes. We adopt two different strategies in decoding process: employing only one united decoder or applying multiple separated decoders. We test our models in two public datasets and our model outperform the baseline method significantly.</p>
<p>【Keywords】:</p>
<h3 id="48. Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection.">48. Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1048/">Paper Link</a>】    【Pages】:515-526</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hong:Yu">Yu Hong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Wenxuan">Wenxuan Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Jingli">Jingli Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Qiaoming">Qiaoming Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Guodong">Guodong Zhou</a></p>
<p>【Abstract】:
Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable.</p>
<p>【Keywords】:</p>
<h3 id="49. Context-Aware Neural Model for Temporal Information Extraction.">49. Context-Aware Neural Model for Temporal Information Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1049/">Paper Link</a>】    【Pages】:527-536</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Yuanliang">Yuanliang Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Rumshisky:Anna">Anna Rumshisky</a></p>
<p>【Abstract】:
We propose a context-aware neural network model for temporal information extraction. This model has a uniform architecture for event-event, event-timex and timex-timex pairs. A Global Context Layer (GCL), inspired by Neural Turing Machine (NTM), stores processed temporal relations in narrative order, and retrieves them for use when relevant entities come in. Relations are then classified in context. The GCL model has long-term memory and attention mechanisms to resolve irregular long-distance dependencies that regular RNNs such as LSTM cannot recognize. It does not require any new input features, while outperforming the existing models in literature. To our knowledge it is also the first model to use NTM-like architecture to process the information from global context in discourse-scale natural text processing. We are going to release the source code in the future.</p>
<p>【Keywords】:</p>
<h3 id="50. Temporal Event Knowledge Acquisition via Identifying Narratives.">50. Temporal Event Knowledge Acquisition via Identifying Narratives.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1050/">Paper Link</a>】    【Pages】:537-547</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yao:Wenlin">Wenlin Yao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Ruihong">Ruihong Huang</a></p>
<p>【Abstract】:
Inspired by the double temporality characteristic of narrative texts, we propose a novel approach for acquiring rich temporal “before/after” event knowledge across sentences in narrative stories. The double temporality states that a narrative story often describes a sequence of events following the chronological order and therefore, the temporal order of events matches with their textual order. We explored narratology principles and built a weakly supervised approach that identifies 287k narrative paragraphs from three large corpora. We then extracted rich temporal event knowledge from these narrative paragraphs. Such event knowledge is shown useful to improve temporal relation classification and outperforms several recent neural network models on the narrative cloze task.</p>
<p>【Keywords】:</p>
<h3 id="51. Textual Deconvolution Saliency (TDS) : a deep tool box for linguistic analysis.">51. Textual Deconvolution Saliency (TDS) : a deep tool box for linguistic analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1051/">Paper Link</a>】    【Pages】:548-557</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vanni:Laurent">Laurent Vanni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ducoffe:M=eacute=lanie">Mélanie Ducoffe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aguilar:Carlos">Carlos Aguilar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Precioso:Fr=eacute=d=eacute=ric">Frédéric Precioso</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mayaffre:Damon">Damon Mayaffre</a></p>
<p>【Abstract】:
In this paper, we propose a new strategy, called Text Deconvolution Saliency (TDS), to visualize linguistic information detected by a CNN for text classification. We extend Deconvolution Networks to text in order to present a new perspective on text analysis to the linguistic community. We empirically demonstrated the efficiency of our Text Deconvolution Saliency on corpora from three different languages: English, French, and Latin. For every tested dataset, our Text Deconvolution Saliency automatically encodes complex linguistic patterns based on co-occurrences and possibly on grammatical and syntax analysis.</p>
<p>【Keywords】:</p>
<h3 id="52. Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach.">52. Coherence Modeling of Asynchronous Conversations: A Neural Entity Grid Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1052/">Paper Link</a>】    【Pages】:558-568</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mohiuddin:Tasnim">Tasnim Mohiuddin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joty:Shafiq_R=">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nguyen:Dat_Tien">Dat Tien Nguyen</a></p>
<p>【Abstract】:
We propose a novel coherence model for written asynchronous conversations (e.g., forums, emails), and show its applications in coherence assessment and thread reconstruction tasks. We conduct our research in two steps. First, we propose improvements to the recently proposed neural entity grid model by lexicalizing its entity transitions. Then, we extend the model to asynchronous conversations by incorporating the underlying conversational structure in the entity grid representation and feature computation. Our model achieves state of the art results on standard coherence assessment tasks in monologue and conversations outperforming existing models. We also demonstrate its effectiveness in reconstructing thread structures.</p>
<p>【Keywords】:</p>
<h3 id="53. Deep Reinforcement Learning for Chinese Zero Pronoun Resolution.">53. Deep Reinforcement Learning for Chinese Zero Pronoun Resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1053/">Paper Link</a>】    【Pages】:569-578</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Qingyu">Qingyu Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0030:Yu">Yu Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Weinan">Weinan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Recent neural network models for Chinese zero pronoun resolution gain great performance by capturing semantic information for zero pronouns and candidate antecedents, but tend to be short-sighted, operating solely by making local decisions. They typically predict coreference links between the zero pronoun and one single candidate antecedent at a time while ignoring their influence on future decisions. Ideally, modeling useful information of preceding potential antecedents is crucial for classifying later zero pronoun-candidate antecedent pairs, a need which leads traditional models of zero pronoun resolution to draw on reinforcement learning. In this paper, we show how to integrate these goals, applying deep reinforcement learning to deal with the task. With the help of the reinforcement learning agent, our system learns the policy of selecting antecedents in a sequential manner, where useful information provided by earlier predicted antecedents could be utilized for making later coreference decisions. Experimental results on OntoNotes 5.0 show that our approach substantially outperforms the state-of-the-art methods under three experimental settings.</p>
<p>【Keywords】:</p>
<h3 id="54. Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis.">54. Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1054/">Paper Link</a>】    【Pages】:579-589</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shibata:Tomohide">Tomohide Shibata</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kurohashi:Sadao">Sadao Kurohashi</a></p>
<p>【Abstract】:
Predicate argument structure analysis is a task of identifying structured events. To improve this field, we need to identify a salient entity, which cannot be identified without performing coreference resolution and predicate argument structure analysis simultaneously. This paper presents an entity-centric joint model for Japanese coreference resolution and predicate argument structure analysis. Each entity is assigned an embedding, and when the result of both analyses refers to an entity, the entity embedding is updated. The analyses take the entity embedding into consideration to access the global information of entities. Our experimental results demonstrate the proposed method can improve the performance of the inter-sentential zero anaphora resolution drastically, which is a notoriously difficult task in predicate argument structure analysis.</p>
<p>【Keywords】:</p>
<h3 id="55. Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars.">55. Constraining MGbank: Agreement, L-Selection and Supertagging in Minimalist Grammars.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1055/">Paper Link</a>】    【Pages】:590-600</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Torr:John">John Torr</a></p>
<p>【Abstract】:
This paper reports on two strategies that have been implemented for improving the efficiency and precision of wide-coverage Minimalist Grammar (MG) parsing. The first extends the formalism presented in Torr and Stabler (2016) with a mechanism for enforcing fine-grained selectional restrictions and agreements. The second is a method for factoring computationally costly null heads out from bottom-up MG parsing; this has the additional benefit of rendering the formalism fully compatible for the first time with highly efficient Markovian supertaggers. These techniques aided in the task of generating MGbank, the first wide-coverage corpus of Minimalist Grammar derivation trees.</p>
<p>【Keywords】:</p>
<h3 id="56. Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power.">56. Not that much power: Linguistic alignment is influenced more by low-level linguistic features rather than social power.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1056/">Paper Link</a>】    【Pages】:601-610</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cole:Jeremy_R=">Jeremy R. Cole</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reitter:David">David Reitter</a></p>
<p>【Abstract】:
Linguistic alignment between dialogue partners has been claimed to be affected by their relative social power. A common finding has been that interlocutors of higher power tend to receive more alignment than those of lower power. However, these studies overlook some low-level linguistic features that can also affect alignment, which casts doubts on these findings. This work characterizes the effect of power on alignment with logistic regression models in two datasets, finding that the effect vanishes or is reversed after controlling for low-level features such as utterance length. Thus, linguistic alignment is explained better by low-level features than by social power. We argue that a wider range of factors, especially cognitive factors, need to be taken into account for future studies on observational data when social factors of language use are in question.</p>
<p>【Keywords】:</p>
<h3 id="57. TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation.">57. TutorialBank: A Manually-Collected Corpus for Prerequisite Chains, Survey Extraction and Resource Recommendation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1057/">Paper Link</a>】    【Pages】:611-620</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fabbri:Alexander_R=">Alexander R. Fabbri</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Irene">Irene Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Trairatvorakul:Prawat">Prawat Trairatvorakul</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Yijiao">Yijiao He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Ting:Wei_Tai">Wei Tai Ting</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tung:Robert">Robert Tung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Westerfield:Caitlin">Caitlin Westerfield</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Radev:Dragomir_R=">Dragomir R. Radev</a></p>
<p>【Abstract】:
The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest research, students as well as educators and researchers must constantly sift through multiple sources to find valuable, relevant information. To address this situation, we introduce TutorialBank, a new, publicly available dataset which aims to facilitate NLP education and research. We have manually collected and categorized over 5,600 resources on NLP as well as the related fields of Artificial Intelligence (AI), Machine Learning (ML) and Information Retrieval (IR). Our dataset is notably the largest manually-picked corpus of resources intended for NLP education which does not include only academic papers. Additionally, we have created both a search engine and a command-line tool for the resources and have annotated the corpus to include lists of research topics, relevant resources for each topic, prerequisite relations among topics, relevant sub-parts of individual resources, among other annotations. We are releasing the dataset and present several avenues for further research.</p>
<p>【Keywords】:</p>
<h3 id="58. Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays.">58. Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1058/">Paper Link</a>】    【Pages】:621-631</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Carlile:Winston">Winston Carlile</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gurrapadi:Nishant">Nishant Gurrapadi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Ke:Zixuan">Zixuan Ke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Vincent">Vincent Ng</a></p>
<p>【Abstract】:
While argument persuasiveness is one of the most important dimensions of argumentative essay quality, it is relatively little studied in automated essay scoring research. Progress on scoring argument persuasiveness is hindered in part by the scarcity of annotated corpora. We present the first corpus of essays that are simultaneously annotated with argument components, argument persuasiveness scores, and attributes of argument components that impact an argument’s persuasiveness. This corpus could trigger the development of novel computational models concerning argument persuasiveness that provide useful feedback to students on why their arguments are (un)persuasive in addition to how persuasive they are.</p>
<p>【Keywords】:</p>
<h3 id="59. Inherent Biases in Reference-based Evaluation for Grammatical Error Correction.">59. Inherent Biases in Reference-based Evaluation for Grammatical Error Correction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1059/">Paper Link</a>】    【Pages】:632-642</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choshen:Leshem">Leshem Choshen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a></p>
<p>【Abstract】:
The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims.</p>
<p>【Keywords】:</p>
<h3 id="60. The price of debiasing automatic metrics in natural language evalaution.">60. The price of debiasing automatic metrics in natural language evalaution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1060/">Paper Link</a>】    【Pages】:643-653</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chaganty:Arun_Tejasvi">Arun Tejasvi Chaganty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mussmann:Stephen">Stephen Mussmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a></p>
<p>【Abstract】:
For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an unbiased estimator with lower cost than human evaluation alone. In practice, however, we obtain only a 7-13% cost reduction on evaluating summarization and open-response question answering systems. We then prove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks—the automatic metric and the prompt shown to human evaluators—both of which need to be improved to obtain greater cost savings.</p>
<p>【Keywords】:</p>
<h3 id="61. Neural Document Summarization by Jointly Learning to Score and Select Sentences.">61. Neural Document Summarization by Jointly Learning to Score and Select Sentences.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1061/">Paper Link</a>】    【Pages】:654-663</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Qingyu">Qingyu Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0002:Nan">Nan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Shaohan">Shaohan Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Tiejun">Tiejun Zhao</a></p>
<p>【Abstract】:
Sentence scoring and sentence selection are two main steps in extractive document summarization systems. However, previous works treat them as two separated subtasks. In this paper, we present a novel end-to-end neural network framework for extractive document summarization by jointly learning to score and select sentences. It first reads the document sentences with a hierarchical encoder to obtain the representation of sentences. Then it builds the output summary by extracting sentences one by one. Different from previous methods, our approach integrates the selection strategy into the scoring model, which directly predicts the relative importance given previously selected sentences. Experiments on the CNN/Daily Mail dataset show that the proposed framework significantly outperforms the state-of-the-art extractive summarization models.</p>
<p>【Keywords】:</p>
<h3 id="62. Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization.">62. Unsupervised Abstractive Meeting Summarization with Multi-Sentence Compression and Budgeted Submodular Maximization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1062/">Paper Link</a>】    【Pages】:664-674</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shang:Guokan">Guokan Shang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Wensi">Wensi Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zekun">Zekun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tixier:Antoine_J==P=">Antoine J.-P. Tixier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meladianos:Polykarpos">Polykarpos Meladianos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vazirgiannis:Michalis">Michalis Vazirgiannis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lorr=eacute=:Jean=Pierre">Jean-Pierre Lorré</a></p>
<p>【Abstract】:
We introduce a novel graph-based framework for abstractive meeting speech summarization that is fully unsupervised and does not rely on any annotations. Our work combines the strengths of multiple recent approaches while addressing their weaknesses. Moreover, we leverage recent advances in word embeddings and graph degeneracy applied to NLP to take exterior semantic knowledge into account, and to design custom diversity and informativeness measures. Experiments on the AMI and ICSI corpus show that our system improves on the state-of-the-art. Code and data are publicly available, and our system can be interactively tested.</p>
<p>【Keywords】:</p>
<h3 id="63. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting.">63. Fast Abstractive Summarization with Reinforce-Selected Sentence Rewriting.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1063/">Paper Link</a>】    【Pages】:675-686</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yen=Chun">Yen-Chun Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
Inspired by how humans summarize long documents, we propose an accurate and fast summarization model that first selects salient sentences and then rewrites them abstractively (i.e., compresses and paraphrases) to generate a concise overall summary. We use a novel sentence-level policy gradient method to bridge the non-differentiable computation between these two neural networks in a hierarchical way, while maintaining language fluency. Empirically, we achieve the new state-of-the-art on all metrics (including human evaluation) on the CNN/Daily Mail dataset, as well as significantly higher abstractiveness scores. Moreover, by first operating at the sentence-level and then the word-level, we enable parallel decoding of our neural generative model that results in substantially faster (10-20x) inference speed as well as 4x faster training convergence than previous long-paragraph encoder-decoder models. We also demonstrate the generalization of our model on the test-only DUC-2002 dataset, where we achieve higher scores than a state-of-the-art model.</p>
<p>【Keywords】:</p>
<h3 id="64. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation.">64. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1064/">Paper Link</a>】    【Pages】:687-697</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Han">Han Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pasunuru:Ramakanth">Ramakanth Pasunuru</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bansal:Mohit">Mohit Bansal</a></p>
<p>【Abstract】:
An accurate abstractive summary of a document should contain all its salient information and should be logically entailed by the input document. We improve these important aspects of abstractive summarization via multi-task learning with the auxiliary tasks of question generation and entailment generation, where the former teaches the summarization model how to look for salient questioning-worthy details, and the latter teaches the model how to rewrite a summary which is a directed-logical subset of the input document. We also propose novel multi-task architectures with high-level (semantic) layer-specific sharing across multiple encoder and decoder layers of the three tasks, as well as soft-sharing mechanisms (and show performance ablations and analysis examples of each contribution). Overall, we achieve statistically significant improvements over the state-of-the-art on both the CNN/DailyMail and Gigaword datasets, as well as on the DUC-2002 transfer setup. We also present several quantitative and qualitative analysis studies of our model’s learned saliency and entailment skills.</p>
<p>【Keywords】:</p>
<h3 id="65. Modeling and Prediction of Online Product Review Helpfulness: A Survey.">65. Modeling and Prediction of Online Product Review Helpfulness: A Survey.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1065/">Paper Link</a>】    【Pages】:698-708</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Diaz:Gerardo_Ocampo">Gerardo Ocampo Diaz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Ng:Vincent">Vincent Ng</a></p>
<p>【Abstract】:
As the amount of free-form user-generated reviews in e-commerce websites continues to increase, there is an increasing need for automatic mechanisms that sift through the vast amounts of user reviews and identify quality content. Review helpfulness modeling is a task which studies the mechanisms that affect review helpfulness and attempts to accurately predict it. This paper provides an overview of the most relevant work in helpfulness prediction and understanding in the past decade, discusses the insights gained from said work, and provides guidelines for future research.</p>
<p>【Keywords】:</p>
<h3 id="66. Mining Cross-Cultural Differences and Similarities in Social Media.">66. Mining Cross-Cultural Differences and Similarities in Social Media.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1066/">Paper Link</a>】    【Pages】:709-719</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Bill_Y=">Bill Y. Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Frank_F=">Frank F. Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Kenny_Q=">Kenny Q. Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwang:Seung=won">Seung-won Hwang</a></p>
<p>【Abstract】:
Cross-cultural differences and similarities are common in cross-lingual natural language understanding, especially for research in social media. For instance, people of distinct cultures often hold different opinions on a single named entity. Also, understanding slang terms across languages requires knowledge of cross-cultural similarities. In this paper, we study the problem of computing such cross-cultural differences and similarities. We present a lightweight yet effective approach, and evaluate it on two novel tasks: 1) mining cross-cultural differences of named entities and 2) finding similar terms for slang across languages. Experimental results show that our framework substantially outperforms a number of baseline methods on both tasks. The framework could be useful for machine translation applications and research in computational social science.</p>
<p>【Keywords】:</p>
<h3 id="67. Classification of Moral Foundations in Microblog Political Discourse.">67. Classification of Moral Foundations in Microblog Political Discourse.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1067/">Paper Link</a>】    【Pages】:720-730</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Johnson:Kristen">Kristen Johnson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goldwasser:Dan">Dan Goldwasser</a></p>
<p>【Abstract】:
Previous works in computer science, as well as political and social science, have shown correlation in text between political ideologies and the moral foundations expressed within that text. Additional work has shown that policy frames, which are used by politicians to bias the public towards their stance on an issue, are also correlated with political ideology. Based on these associations, this work takes a first step towards modeling both the language and how politicians frame issues on Twitter, in order to predict the moral foundations that are used by politicians to express their stances on issues. The contributions of this work includes a dataset annotated for the moral foundations, annotation guidelines, and probabilistic graphical models which show the usefulness of jointly modeling abstract political slogans, as opposed to the unigrams of previous works, with policy frames for the prediction of the morality underlying political tweets.</p>
<p>【Keywords】:</p>
<h3 id="68. Coarse-to-Fine Decoding for Neural Semantic Parsing.">68. Coarse-to-Fine Decoding for Neural Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1068/">Paper Link</a>】    【Pages】:731-742</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong_0004:Li">Li Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
Semantic parsing aims at mapping natural language utterances into structured meaning representations. In this work, we propose a structure-aware neural architecture which decomposes the semantic parsing process into two stages. Given an input utterance, we first generate a rough sketch of its meaning, where low-level information (such as variable names and arguments) is glossed over. Then, we fill in missing details by taking into account the natural language input and the sketch itself. Experimental results on four datasets characteristic of different domains and meaning representations show that our approach consistently improves performance, achieving competitive results despite the use of relatively simple decoders.</p>
<p>【Keywords】:</p>
<h3 id="69. Confidence Modeling for Neural Semantic Parsing.">69. Confidence Modeling for Neural Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1069/">Paper Link</a>】    【Pages】:743-753</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong_0004:Li">Li Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Quirk:Chris">Chris Quirk</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a></p>
<p>【Abstract】:
In this work we focus on confidence modeling for neural semantic parsers which are built upon sequence-to-sequence models. We outline three major causes of uncertainty, and design various metrics to quantify these factors. These metrics are then used to estimate confidence scores that indicate whether model predictions are likely to be correct. Beyond confidence estimation, we identify which parts of the input contribute to uncertain predictions allowing users to interpret their model, and verify or refine its input. Experimental results show that our confidence model significantly outperforms a widely used method that relies on posterior probability, and improves the quality of interpretation compared to simply relying on attention scores.</p>
<p>【Keywords】:</p>
<h3 id="70. StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing.">70. StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1070/">Paper Link</a>】    【Pages】:754-765</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Pengcheng">Pengcheng Yin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Chunting">Chunting Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Junxian">Junxian He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.</p>
<p>【Keywords】:</p>
<h3 id="71. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing.">71. Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1071/">Paper Link</a>】    【Pages】:766-777</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Bo">Bo Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a></p>
<p>【Abstract】:
This paper proposes a neural semantic parsing approach – Sequence-to-Action, which models semantic parsing as an end-to-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on Overnight dataset and gets competitive performance on Geo and Atis datasets.</p>
<p>【Keywords】:</p>
<h3 id="72. On the Limitations of Unsupervised Bilingual Dictionary Induction.">72. On the Limitations of Unsupervised Bilingual Dictionary Induction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1072/">Paper Link</a>】    【Pages】:778-788</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/S=oslash=gaard:Anders">Anders Søgaard</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ruder:Sebastian">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a></p>
<p>【Abstract】:
Unsupervised machine translation - i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora - seems impossible, but nevertheless, Lample et al. (2017) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised cross-lingual word embedding technique for bilingual dictionary induction (Conneau et al., 2017), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.</p>
<p>【Keywords】:</p>
<h3 id="73. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.">73. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1073/">Paper Link</a>】    【Pages】:789-798</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Artetxe:Mikel">Mikel Artetxe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Labaka:Gorka">Gorka Labaka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agirre:Eneko">Eneko Agirre</a></p>
<p>【Abstract】:
Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at <a href="https://github.com/artetxem/vecmap">https://github.com/artetxem/vecmap</a>.</p>
<p>【Keywords】:</p>
<h3 id="74. A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling.">74. A Multi-lingual Multi-task Architecture for Low-resource Sequence Labeling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1074/">Paper Link</a>】    【Pages】:799-809</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Ying">Ying Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Shengqi">Shengqi Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stoyanov:Veselin">Veselin Stoyanov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a></p>
<p>【Abstract】:
We propose a multi-lingual multi-task architecture to develop supervised models with a minimal amount of labeled data for sequence labeling. In this new architecture, we combine various transfer models using two layers of parameter sharing. On the first layer, we construct the basis of the architecture to provide universal word representation and feature extraction capability for all models. On the second level, we adopt different parameter sharing strategies for different transfer schemes. This architecture proves to be particularly effective for low-resource settings, when there are less than 200 training sentences for the target task. Using Name Tagging as a target task, our approach achieved 4.3%-50.5% absolute F-score gains compared to the mono-lingual single-task baseline model.</p>
<p>【Keywords】:</p>
<h3 id="75. Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable.">75. Two Methods for Domain Adaptation of Bilingual Tasks: Delightfully Simple and Broadly Applicable.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1075/">Paper Link</a>】    【Pages】:810-820</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hangya:Viktor">Viktor Hangya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Braune:Fabienne">Fabienne Braune</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fraser:Alexander_M=">Alexander M. Fraser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
Bilingual tasks, such as bilingual lexicon induction and cross-lingual classification, are crucial for overcoming data sparsity in the target language. Resources required for such tasks are often out-of-domain, thus domain adaptation is an important problem here. We make two contributions. First, we test a delightfully simple method for domain adaptation of bilingual word embeddings. We evaluate these embeddings on two bilingual tasks involving different domains: cross-lingual twitter sentiment classification and medical bilingual lexicon induction. Second, we tailor a broadly applicable semi-supervised classification method from computer vision to these tasks. We show that this method also helps in low-resource setups. Using both methods together we achieve large improvements over our baselines, by using only additional unlabeled data.</p>
<p>【Keywords】:</p>
<h3 id="76. Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge.">76. Knowledgeable Reader: Enhancing Cloze-Style Reading Comprehension with External Commonsense Knowledge.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1076/">Paper Link</a>】    【Pages】:821-832</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mihaylov:Todor">Todor Mihaylov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Frank:Anette">Anette Frank</a></p>
<p>【Abstract】:
We introduce a neural reading comprehension model that integrates external commonsense knowledge, encoded as a key-value memory, in a cloze-style setting. Instead of relying only on document-to-question interaction or discrete features as in prior work, our model attends to relevant external knowledge and combines this knowledge with the context representation before inferring the answer. This allows the model to attract and imply knowledge from an external knowledge source that is not explicitly stated in the text, but that is relevant for inferring the answer. Our model improves results over a very strong baseline on a hard Common Nouns dataset, making it a strong competitor of much more complex models. By including knowledge explicitly, our model can also provide evidence about the background knowledge used in the RC process.</p>
<p>【Keywords】:</p>
<h3 id="77. Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds.">77. Multi-Relational Question Answering from Narratives: Machine Reading and Reasoning in Simulated Worlds.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1077/">Paper Link</a>】    【Pages】:833-844</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Labutov:Igor">Igor Labutov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Bishan">Bishan Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Prakash:Anusha">Anusha Prakash</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Azaria:Amos">Amos Azaria</a></p>
<p>【Abstract】:
Question Answering (QA), as a research field, has primarily focused on either knowledge bases (KBs) or free text as a source of knowledge. These two sources have historically shaped the kinds of questions that are asked over these sources, and the methods developed to answer them. In this work, we look towards a practical use-case of QA over user-instructed knowledge that uniquely combines elements of both structured QA over knowledge bases, and unstructured QA over narrative, introducing the task of multi-relational QA over personal narrative. As a first step towards this goal, we make three key contributions: (i) we generate and release TextWorldsQA, a set of five diverse datasets, where each dataset contains dynamic narrative that describes entities and relations in a simulated world, paired with variably compositional questions over that knowledge, (ii) we perform a thorough evaluation and analysis of several state-of-the-art QA models and their variants at this task, and (iii) we release a lightweight Python-based framework we call TextWorlds for easily generating arbitrary additional worlds and narrative, with the goal of allowing the community to create and share a growing collection of diverse worlds as a test-bed for this task.</p>
<p>【Keywords】:</p>
<h3 id="78. Simple and Effective Multi-Paragraph Reading Comprehension.">78. Simple and Effective Multi-Paragraph Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1078/">Paper Link</a>】    【Pages】:845-855</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Christopher">Christopher Clark</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gardner_0001:Matt">Matt Gardner</a></p>
<p>【Abstract】:
We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.</p>
<p>【Keywords】:</p>
<h3 id="79. Semantically Equivalent Adversarial Rules for Debugging NLP models.">79. Semantically Equivalent Adversarial Rules for Debugging NLP models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1079/">Paper Link</a>】    【Pages】:856-865</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ribeiro:Marco_T=uacute=lio">Marco Túlio Ribeiro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Singh_0001:Sameer">Sameer Singh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guestrin:Carlos">Carlos Guestrin</a></p>
<p>【Abstract】:
Complex machine learning models for NLP are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries (SEAs) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules (SEARs) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and flexibility of SEAs and SEARs by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual question-answering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that SEARs induce four times as many mistakes as the bugs discovered by human experts. SEARs are also actionable: retraining models using data augmentation significantly reduces bugs, while maintaining accuracy.</p>
<p>【Keywords】:</p>
<h3 id="80. Style Transfer Through Back-Translation.">80. Style Transfer Through Back-Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1080/">Paper Link</a>】    【Pages】:866-876</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Prabhumoye:Shrimai">Shrimai Prabhumoye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tsvetkov:Yulia">Yulia Tsvetkov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Salakhutdinov:Ruslan">Ruslan Salakhutdinov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Black:Alan_W=">Alan W. Black</a></p>
<p>【Abstract】:
Style transfer is the task of rephrasing the text to contain specific stylistic properties without changing the intent or affect within the context. This paper introduces a new method for automatic style transfer. We first learn a latent representation of the input sentence which is grounded in a language translation model in order to better preserve the meaning of the sentence while reducing stylistic properties. Then adversarial generation techniques are used to make the output match the desired style. We evaluate this technique on three different style transformations: sentiment, gender and political slant. Compared to two state-of-the-art style transfer modeling techniques we show improvements both in automatic evaluation of style transfer and in manual evaluation of meaning preservation and fluency.</p>
<p>【Keywords】:</p>
<h3 id="81. Generating Fine-Grained Open Vocabulary Entity Type Descriptions.">81. Generating Fine-Grained Open Vocabulary Entity Type Descriptions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1081/">Paper Link</a>】    【Pages】:877-888</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bhowmik:Rajarshi">Rajarshi Bhowmik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Melo:Gerard_de">Gerard de Melo</a></p>
<p>【Abstract】:
While large-scale knowledge graphs provide vast amounts of structured facts about entities, a short textual description can often be useful to succinctly characterize an entity and its type. Unfortunately, many knowledge graphs entities lack such textual descriptions. In this paper, we introduce a dynamic memory-based network that generates a short open vocabulary description of an entity by jointly leveraging induced fact embeddings as well as the dynamic context of the generated sequence of words. We demonstrate the ability of our architecture to discern relevant information for more accurate generation of type description by pitting the system against several strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="82. Hierarchical Neural Story Generation.">82. Hierarchical Neural Story Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1082/">Paper Link</a>】    【Pages】:889-898</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Angela">Angela Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lewis:Mike">Mike Lewis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dauphin:Yann_N=">Yann N. Dauphin</a></p>
<p>【Abstract】:
We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.</p>
<p>【Keywords】:</p>
<h3 id="83. No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling.">83. No Metrics Are Perfect: Adversarial Reward Learning for Visual Storytelling.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1083/">Paper Link</a>】    【Pages】:899-909</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0061:Xin">Xin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Wenhu">Wenhu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yuan=Fang">Yuan-Fang Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Though impressive results have been achieved in visual captioning, the task of generating abstract stories from photo streams is still a little-tapped problem. Different from captions, stories have more expressive language styles and contain many imaginary concepts that do not appear in the images. Thus it poses challenges to behavioral cloning algorithms. Furthermore, due to the limitations of automatic metrics on evaluating story quality, reinforcement learning methods with hand-crafted rewards also face difficulties in gaining an overall performance boost. Therefore, we propose an Adversarial REward Learning (AREL) framework to learn an implicit reward function from human demonstrations, and then optimize policy search with the learned reward function. Though automatic evaluation indicates slight performance boost over state-of-the-art (SOTA) methods in cloning expert behaviors, human evaluation shows that our approach achieves significant improvement in generating more human-like stories than SOTA systems.</p>
<p>【Keywords】:</p>
<h3 id="84. Bridging Languages through Images with Deep Partial Canonical Correlation Analysis.">84. Bridging Languages through Images with Deep Partial Canonical Correlation Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1084/">Paper Link</a>】    【Pages】:910-921</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rotman:Guy">Guy Rotman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a></p>
<p>【Abstract】:
We present a deep neural network that leverages images to improve bilingual text embeddings. Relying on bilingual image tags and descriptions, our approach conditions text embedding induction on the shared visual information for both languages, producing highly correlated bilingual embeddings. In particular, we propose a novel model based on Partial Canonical Correlation Analysis (PCCA). While the original PCCA finds linear projections of two views in order to maximize their canonical correlation conditioned on a shared third variable, we introduce a non-linear Deep PCCA (DPCCA) model, and develop a new stochastic iterative algorithm for its optimization. We evaluate PCCA and DPCCA on multilingual word similarity and cross-lingual image description retrieval. Our models outperform a large variety of previous methods, despite not having access to any visual signal during test time inference.</p>
<p>【Keywords】:</p>
<h3 id="85. Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.">85. Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1085/">Paper Link</a>】    【Pages】:922-933</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kiros:Jamie_Ryan">Jamie Ryan Kiros</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chan:William">William Chan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hinton:Geoffrey_E=">Geoffrey E. Hinton</a></p>
<p>【Abstract】:
We introduce Picturebook, a large-scale lookup operation to ground language via ‘snapshots’ of our physical world accessed through image search. For each word in a vocabulary, we extract the top-k images from Google image search and feed the images through a convolutional network to extract a word embedding. We introduce a multimodal gating function to fuse our Picturebook embeddings with other word representations. We also introduce Inverse Picturebook, a mechanism to map a Picturebook embedding back into words. We experiment and report results across a wide range of tasks: word similarity, natural language inference, semantic relatedness, sentiment/topic classification, image-sentence ranking and machine translation. We also show that gate activations corresponding to Picturebook embeddings are highly correlated to human judgments of concreteness ratings.</p>
<p>【Keywords】:</p>
<h3 id="86. What Action Causes This? Towards Naive Physical Action-Effect Prediction.">86. What Action Causes This? Towards Naive Physical Action-Effect Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1086/">Paper Link</a>】    【Pages】:934-945</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Qiaozi">Qiaozi Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Shaohua">Shaohua Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chai:Joyce_Yue">Joyce Yue Chai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vanderwende:Lucy">Lucy Vanderwende</a></p>
<p>【Abstract】:
Despite recent advances in knowledge representation, automated reasoning, and machine learning, artificial agents still lack the ability to understand basic action-effect relations regarding the physical world, for example, the action of cutting a cucumber most likely leads to the state where the cucumber is broken apart into smaller pieces. If artificial agents (e.g., robots) ever become our partners in joint tasks, it is critical to empower them with such action-effect understanding so that they can reason about the state of the world and plan for actions. Towards this goal, this paper introduces a new task on naive physical action-effect prediction, which addresses the relations between concrete actions (expressed in the form of verb-noun pairs) and their effects on the state of the physical world as depicted by images. We collected a dataset for this task and developed an approach that harnesses web image data through distant supervision to facilitate learning for action-effect prediction. Our empirical results have shown that web data can be used to complement a small number of seed examples (e.g., three examples for each action) for model learning. This opens up possibilities for agents to learn physical action-effect relations for tasks at hand through communication with humans with a few examples.</p>
<p>【Keywords】:</p>
<h3 id="87. Transformation Networks for Target-Oriented Sentiment Classification.">87. Transformation Networks for Target-Oriented Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1087/">Paper Link</a>】    【Pages】:946-956</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xin">Xin Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Bei">Bei Shi</a></p>
<p>【Abstract】:
Target-oriented sentiment classification aims at classifying sentiment polarities over individual opinion targets in a sentence. RNN with attention seems a good fit for the characteristics of this task, and indeed it achieves the state-of-the-art performance. After re-examining the drawbacks of attention mechanism and the obstacles that block CNN to perform well in this classification task, we propose a new model that achieves new state-of-the-art results on a few benchmarks. Instead of attention, our model employs a CNN layer to extract salient features from the transformed word representations originated from a bi-directional RNN layer. Between the two layers, we propose a component which first generates target-specific representations of words in the sentence, and then incorporates a mechanism for preserving the original contextual information from the RNN layer.</p>
<p>【Keywords】:</p>
<h3 id="88. Target-Sensitive Memory Networks for Aspect Sentiment Classification.">88. Target-Sensitive Memory Networks for Aspect Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1088/">Paper Link</a>】    【Pages】:957-967</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Shuai">Shuai Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mazumder:Sahisnu">Sahisnu Mazumder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Bing">Bing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Mianwei">Mianwei Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a></p>
<p>【Abstract】:
Aspect sentiment classification (ASC) is a fundamental task in sentiment analysis. Given an aspect/target and a sentence, the task classifies the sentiment polarity expressed on the target in the sentence. Memory networks (MNs) have been used for this task recently and have achieved state-of-the-art results. In MNs, attention mechanism plays a crucial role in detecting the sentiment context for the given target. However, we found an important problem with the current MNs in performing the ASC task. Simply improving the attention mechanism will not solve it. The problem is referred to as target-sensitive sentiment, which means that the sentiment polarity of the (detected) context is dependent on the given target and it cannot be inferred from the context alone. To tackle this problem, we propose the target-sensitive memory networks (TMNs). Several alternative techniques are designed for the implementation of TMNs and their effectiveness is experimentally evaluated.</p>
<p>【Keywords】:</p>
<h3 id="89. Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification.">89. Identifying Transferable Information Across Domains for Cross-domain Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1089/">Paper Link</a>】    【Pages】:968-978</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Raksha">Raksha Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dandapat:Sandipan">Sandipan Dandapat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhatt:Himanshu_Sharad">Himanshu Sharad Bhatt</a></p>
<p>【Abstract】:
Getting manually labeled data in each domain is always an expensive and a time consuming task. Cross-domain sentiment analysis has emerged as a demanding concept where a labeled source domain facilitates a sentiment classifier for an unlabeled target domain. However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another domain. Owing to these differences, cross-domain sentiment classification is still a challenging task. In this paper, we propose that words that do not change their polarity and significance represent the transferable (usable) information across domains for cross-domain sentiment classification. We present a novel approach based on χ2 test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains. Furthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.</p>
<p>【Keywords】:</p>
<h3 id="90. Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach.">90. Unpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1090/">Paper Link</a>】    【Pages】:979-988</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Jingjing">Jingjing Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zeng:Qi">Qi Zeng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiaodong">Xiaodong Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xuancheng">Xuancheng Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a></p>
<p>【Abstract】:
The goal of sentiment-to-sentiment “translation” is to change the underlying sentiment of a sentence while keeping its content. The main challenge is the lack of parallel data. To solve this problem, we propose a cycled reinforcement learning method that enables training on unpaired data by collaboration between a neutralization module and an emotionalization module. We evaluate our approach on two review datasets, Yelp and Amazon. Experimental results show that our approach significantly outperforms the state-of-the-art systems. Especially, the proposed method substantially improves the content preservation performance. The BLEU score is improved from 1.64 to 22.46 and from 0.56 to 14.06 on the two datasets, respectively.</p>
<p>【Keywords】:</p>
<h3 id="91. Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference.">91. Discourse Marker Augmented Network with Reinforcement Learning for Natural Language Inference.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1091/">Paper Link</a>】    【Pages】:989-999</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pan:Boyuan">Boyuan Pan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Yazheng">Yazheng Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Zhou">Zhou Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhuang:Yueting">Yueting Zhuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cai:Deng">Deng Cai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Xiaofei">Xiaofei He</a></p>
<p>【Abstract】:
Natural Language Inference (NLI), also known as Recognizing Textual Entailment (RTE), is one of the most important problems in natural language processing. It requires to infer the logical relationship between two given sentences. While current approaches mostly focus on the interaction architectures of the sentences, in this paper, we propose to transfer knowledge from some important discourse markers to augment the quality of the NLI model. We observe that people usually use some discourse markers such as “so” or “but” to represent the logical relationship between two sentences. These words potentially have deep connections with the meanings of the sentences, thus can be utilized to help improve the representations of them. Moreover, we use reinforcement learning to optimize a new objective function with a reward defined by the property of the NLI datasets to make full use of the labels information. Experiments show that our method achieves the state-of-the-art performance on several large-scale datasets.</p>
<p>【Keywords】:</p>
<h3 id="92. Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module.">92. Working Memory Networks: Augmenting Memory Networks with a Relational Reasoning Module.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1092/">Paper Link</a>】    【Pages】:1000-1009</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pavez:Juan">Juan Pavez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Allende:H=eacute=ctor">Héctor Allende</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Allende=Cid:H=eacute=ctor">Héctor Allende-Cid</a></p>
<p>【Abstract】:
During the last years, there has been a lot of interest in achieving some kind of complex reasoning using deep neural networks. To do that, models like Memory Networks (MemNNs) have combined external memory storages and attention mechanisms. These architectures, however, lack of more complex reasoning mechanisms that could allow, for instance, relational reasoning. Relation Networks (RNs), on the other hand, have shown outstanding results in relational reasoning tasks. Unfortunately, their computational cost grows quadratically with the number of memories, something prohibitive for larger problems. To solve these issues, we introduce the Working Memory Network, a MemNN architecture with a novel working memory storage and reasoning module. Our model retains the relational reasoning abilities of the RN while reducing its computational complexity from quadratic to linear. We tested our model on the text QA dataset bAbI and the visual QA dataset NLVR. In the jointly trained bAbI-10k, we set a new state-of-the-art, achieving a mean error of less than 0.5%. Moreover, a simple ensemble of two of our models solves all 20 tasks in the joint version of the benchmark.</p>
<p>【Keywords】:</p>
<h3 id="93. Reasoning with Sarcasm by Reading In-Between.">93. Reasoning with Sarcasm by Reading In-Between.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1093/">Paper Link</a>】    【Pages】:1010-1020</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tay:Yi">Yi Tay</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luu:Anh_Tuan">Anh Tuan Luu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Siu_Cheung">Siu Cheung Hui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jian">Jian Su</a></p>
<p>【Abstract】:
Sarcasm is a sophisticated speech act which commonly manifests on social communities such as Twitter and Reddit. The prevalence of sarcasm on the social web is highly disruptive to opinion mining systems due to not only its tendency of polarity flipping but also usage of figurative language. Sarcasm commonly manifests with a contrastive theme either between positive-negative sentiments or between literal-figurative scenarios. In this paper, we revisit the notion of modeling contrast in order to reason with sarcasm. More specifically, we propose an attention-based neural model that looks in-between instead of across, enabling it to explicitly model contrast and incongruity. We conduct extensive experiments on six benchmark datasets from Twitter, Reddit and the Internet Argument Corpus. Our proposed model not only achieves state-of-the-art performance on all datasets but also enjoys improved interpretability.</p>
<p>【Keywords】:</p>
<h3 id="94. Adversarial Contrastive Estimation.">94. Adversarial Contrastive Estimation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1094/">Paper Link</a>】    【Pages】:1021-1032</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bose:Avishek_Joey">Avishek Joey Bose</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Ling:Huan">Huan Ling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cao:Yanshuai">Yanshuai Cao</a></p>
<p>【Abstract】:
Learning by contrasting positive and negative samples is a general strategy adopted by many methods. Noise contrastive estimation (NCE) for word embeddings and translating embeddings for knowledge graphs are examples in NLP employing this approach. In this work, we view contrastive learning as an abstraction of all such methods and augment the negative sampler into a mixture distribution containing an adversarially learned sampler. The resulting adaptive sampler finds harder negative examples, which forces the main model to learn a better representation of the data. We evaluate our proposal on learning word embeddings, order embeddings and knowledge graph embeddings and observe both faster convergence and improved results on multiple metrics.</p>
<p>【Keywords】:</p>
<h3 id="95. Adaptive Scaling for Sparse Detection in Information Extraction.">95. Adaptive Scaling for Sparse Detection in Information Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1095/">Paper Link</a>】    【Pages】:1033-1043</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Hongyu">Hongyu Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0001:Yaojie">Yaojie Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a></p>
<p>【Abstract】:
This paper focuses on detection tasks in information extraction, where positive instances are sparsely distributed and models are usually evaluated using F-measure on positive classes. These characteristics often result in deficient performance of neural network based detection models. In this paper, we propose adaptive scaling, an algorithm which can handle the positive sparsity problem and directly optimize over F-measure via dynamic cost-sensitive learning. To this end, we borrow the idea of marginal utility from economics and propose a theoretical framework for instance importance measuring without introducing any additional hyper-parameters. Experiments show that our algorithm leads to a more effective and stable training of neural network based detection models.</p>
<p>【Keywords】:</p>
<h3 id="96. Strong Baselines for Neural Semi-Supervised Learning under Domain Shift.">96. Strong Baselines for Neural Semi-Supervised Learning under Domain Shift.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1096/">Paper Link</a>】    【Pages】:1044-1054</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Ruder:Sebastian">Sebastian Ruder</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Plank:Barbara">Barbara Plank</a></p>
<p>【Abstract】:
Novel neural models have been proposed in recent years for learning under domain shift. Most models, however, only evaluate on a single task, on proprietary datasets, or compare to weak baselines, which makes comparison of models difficult. In this paper, we re-evaluate classic general-purpose bootstrapping approaches in the context of neural networks under domain shifts vs. recent neural approaches and propose a novel multi-task tri-training method that reduces the time and space complexity of classic tri-training. Extensive experiments on two benchmarks for part-of-speech tagging and sentiment analysis are negative: while our novel method establishes a new state-of-the-art for sentiment analysis, it does not fare consistently the best. More importantly, we arrive at the somewhat surprising conclusion that classic tri-training, with some additions, outperforms the state-of-the-art for NLP. Hence classic approaches constitute an important and strong baseline.</p>
<p>【Keywords】:</p>
<h3 id="97. Fluency Boost Learning and Inference for Neural Grammatical Error Correction.">97. Fluency Boost Learning and Inference for Neural Grammatical Error Correction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1097/">Paper Link</a>】    【Pages】:1055-1065</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Ge:Tao">Tao Ge</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou_0001:Ming">Ming Zhou</a></p>
<p>【Abstract】:
Most of the neural sequence-to-sequence (seq2seq) models for grammatical error correction (GEC) have two limitations: (1) a seq2seq model may not be well generalized with only limited error-corrected data; (2) a seq2seq model may fail to completely correct a sentence with multiple errors through normal seq2seq inference. We attempt to address these limitations by proposing a fluency boost learning and inference mechanism. Fluency boosting learning generates fluency-boost sentence pairs during training, enabling the error correction model to learn how to improve a sentence’s fluency from more instances, while fluency boosting inference allows the model to correct a sentence incrementally with multiple inference steps until the sentence’s fluency stops increasing. Experiments show our approaches improve the performance of seq2seq models for GEC, achieving state-of-the-art results on both CoNLL-2014 and JFLEG benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="98. A Neural Architecture for Automated ICD Coding.">98. A Neural Architecture for Automated ICD Coding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1098/">Paper Link</a>】    【Pages】:1066-1076</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Pengtao">Pengtao Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Haoran">Haoran Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Ming">Ming Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
The International Classification of Diseases (ICD) provides a hierarchy of diagnostic codes for classifying diseases. Medical coding – which assigns a subset of ICD codes to a patient visit – is a mandatory process that is crucial for patient care and billing. Manual coding is time-consuming, expensive, and error prone. In this paper, we build a neural architecture for automated coding. It takes the diagnosis descriptions (DDs) of a patient as inputs and selects the most relevant ICD codes. This architecture contains four major ingredients: (1) tree-of-sequences LSTM encoding of code descriptions (CDs), (2) adversarial learning for reconciling the different writing styles of DDs and CDs, (3) isotonic constraints for incorporating the importance order among the assigned codes, and (4) attentional matching for performing many-to-one and one-to-many mappings from DDs to CDs. We demonstrate the effectiveness of the proposed methods on a clinical datasets with 59K patient visits.</p>
<p>【Keywords】:</p>
<h3 id="99. Domain Adaptation with Adversarial Training and Graph Embeddings.">99. Domain Adaptation with Adversarial Training and Graph Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1099/">Paper Link</a>】    【Pages】:1077-1087</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Alam:Firoj">Firoj Alam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joty:Shafiq_R=">Shafiq R. Joty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Imran_0002:Muhammad">Muhammad Imran</a></p>
<p>【Abstract】:
The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data. However, obtaining labeled data is a big challenge in many real-world problems. In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains. In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake). For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event. We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework. Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.</p>
<p>【Keywords】:</p>
<h3 id="100. TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.">100. TDNN: A Two-stage Deep Neural Network for Prompt-independent Automated Essay Scoring.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1100/">Paper Link</a>】    【Pages】:1088-1097</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Cancan">Cancan Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hui:Kai">Kai Hui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a></p>
<p>【Abstract】:
Existing automated essay scoring (AES) models rely on rated essays for the target prompt as training data. Despite their successes in prompt-dependent AES, how to effectively predict essay ratings under a prompt-independent setting remains a challenge, where the rated essays for the target prompt are not available. To close this gap, a two-stage deep neural network (TDNN) is proposed. In particular, in the first stage, using the rated essays for non-target prompts as the training data, a shallow model is learned to select essays with an extreme quality for the target prompt, serving as pseudo training data; in the second stage, an end-to-end hybrid deep model is proposed to learn a prompt-dependent rating model consuming the pseudo training data from the first step. Evaluation of the proposed TDNN on the standard ASAP dataset demonstrates a promising improvement for the prompt-independent AES task.</p>
<p>【Keywords】:</p>
<h3 id="101. Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.">101. Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1101/">Paper Link</a>】    【Pages】:1098-1107</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Tiancheng">Tiancheng Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Kyusong">Kyusong Lee</a> ; <a href="https://dblp.uni-trier.de/pers/hd/e/Esk=eacute=nazi:Maxine">Maxine Eskénazi</a></p>
<p>【Abstract】:
The encoder-decoder dialog model is one of the most prominent methods used to build dialog systems in complex domains. Yet it is limited because it cannot output interpretable actions as in traditional systems, which hinders humans from understanding its generation process. We present an unsupervised discrete sentence representation learning method that can integrate with any existing encoder-decoder dialog models for interpretable response generation. Building upon variational autoencoders (VAEs), we present two novel models, DI-VAE and DI-VST that improve VAEs and can discover interpretable semantics via either auto encoding or context predicting. Our methods have been validated on real-world dialog datasets to discover semantic representations and enhance encoder-decoder models with interpretable generation.</p>
<p>【Keywords】:</p>
<h3 id="102. Learning to Control the Specificity in Neural Response Generation.">102. Learning to Control the Specificity in Neural Response Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1102/">Paper Link</a>】    【Pages】:1108-1117</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Ruqing">Ruqing Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fan:Yixing">Yixing Fan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Yanyan">Yanyan Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0001:Jun">Jun Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
In conversation, a general response (e.g., “I don’t know”) could correspond to a large variety of input utterances. Previous generative conversational models usually employ a single model to learn the relationship between different utterance-response pairs, thus tend to favor general and trivial responses which appear frequently. To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels. We describe two ways to acquire distant labels for the specificity control variable in learning. Empirical studies show that our model can significantly outperform the state-of-the-art response generation models under both automatic and human evaluations.</p>
<p>【Keywords】:</p>
<h3 id="103. Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network.">103. Multi-Turn Response Selection for Chatbots with Deep Attention Matching Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1103/">Paper Link</a>】    【Pages】:1118-1127</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Xiangyang">Xiangyang Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Lu">Lu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Daxiang">Daxiang Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yi">Yi Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Ying">Ying Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wayne_Xin">Wayne Xin Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Dianhai">Dianhai Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0003:Hua">Hua Wu</a></p>
<p>【Abstract】:
Human generates responses relying on semantic and functional dependencies, including coreference relation, among dialogue elements and their context. In this paper, we investigate matching a response with its multi-turn context using dependency information based entirely on attention. Our solution is inspired by the recently proposed Transformer in machine translation (Vaswani et al., 2017) and we extend the attention mechanism in two ways. First, we construct representations of text segments at different granularities solely with stacked self-attention. Second, we try to extract the truly matched segment pairs with attention across the context and response. We jointly introduce those two kinds of attention in one uniform neural network. Experiments on two large-scale multi-turn response selection tasks show that our proposed model significantly outperforms the state-of-the-art models.</p>
<p>【Keywords】:</p>
<h3 id="104. MojiTalk: Generating Emotional Responses at Scale.">104. MojiTalk: Generating Emotional Responses at Scale.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1104/">Paper Link</a>】    【Pages】:1128-1137</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Xianda">Xianda Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Generating emotional language is a key step towards building empathetic natural language processing agents. However, a major challenge for this line of research is the lack of large-scale labeled training data, and previous studies are limited to only small sets of human annotated sentiment labels. Additionally, explicitly controlling the emotion and sentiment of generated text is also difficult. In this paper, we take a more radical approach: we exploit the idea of leveraging Twitter data that are naturally labeled with emojis. We collect a large corpus of Twitter conversations that include emojis in the response and assume the emojis convey the underlying emotions of the sentence. We investigate several conditional variational autoencoders training on these conversations, which allow us to use emojis to control the emotion of the generated text. Experimentally, we show in our quantitative and qualitative analyses that the proposed models can successfully generate high-quality abstractive conversation responses in accordance with designated emotions.</p>
<p>【Keywords】:</p>
<h3 id="105. Taylor's law for Human Linguistic Sequences.">105. Taylor's law for Human Linguistic Sequences.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1105/">Paper Link</a>】    【Pages】:1138-1148</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kobayashi:Tatsuru">Tatsuru Kobayashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tanaka=Ishii:Kumiko">Kumiko Tanaka-Ishii</a></p>
<p>【Abstract】:
Taylor’s law describes the fluctuation characteristics underlying a system in which the variance of an event within a time span grows by a power law with respect to the mean. Although Taylor’s law has been applied in many natural and social systems, its application for language has been scarce. This article describes a new way to quantify Taylor’s law in natural language and conducts Taylor analysis of over 1100 texts across 14 languages. We found that the Taylor exponents of natural language written texts exhibit almost the same value. The exponent was also compared for other language-related data, such as the child-directed speech, music, and programming languages. The results show how the Taylor exponent serves to quantify the fundamental structural complexity underlying linguistic time series. The article also shows the applicability of these findings in evaluating language models.</p>
<p>【Keywords】:</p>
<h3 id="106. A Framework for Representing Language Acquisition in a Population Setting.">106. A Framework for Representing Language Acquisition in a Population Setting.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1106/">Paper Link</a>】    【Pages】:1149-1159</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kodner:Jordan">Jordan Kodner</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Falco:Christopher_Cerezo">Christopher Cerezo Falco</a></p>
<p>【Abstract】:
Language variation and change are driven both by individuals’ internal cognitive processes and by the social structures through which language propagates. A wide range of computational frameworks have been proposed to connect these drivers. We compare the strengths and weaknesses of existing approaches and propose a new analytic framework which combines previous network models’ ability to capture realistic social structure with practically and more elegant computational properties. The framework privileges the process of language acquisition and embeds learners in a social network but is modular so that population structure can be combined with different acquisition models. We demonstrate two applications for the framework: a test of practical concerns that arise when modeling acquisition in a population setting and an application of the framework to recent work on phonological mergers in progress.</p>
<p>【Keywords】:</p>
<h3 id="107. Prefix Lexicalization of Synchronous CFGs using Synchronous TAG.">107. Prefix Lexicalization of Synchronous CFGs using Synchronous TAG.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1107/">Paper Link</a>】    【Pages】:1160-1170</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Born:Logan">Logan Born</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarkar:Anoop">Anoop Sarkar</a></p>
<p>【Abstract】:
We show that an epsilon-free, chain-free synchronous context-free grammar (SCFG) can be converted into a weakly equivalent synchronous tree-adjoining grammar (STAG) which is prefix lexicalized. This transformation at most doubles the grammar’s rank and cubes its size, but we show that in practice the size increase is only quadratic. Our results extend Greibach normal form from CFGs to SCFGs and prove new formal properties about SCFG, a formalism with many applications in natural language processing.</p>
<p>【Keywords】:</p>
<h3 id="108. Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.">108. Straight to the Tree: Constituency Parsing with Neural Syntactic Distance.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1108/">Paper Link</a>】    【Pages】:1171-1180</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Yikang">Yikang Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Zhouhan">Zhouhan Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jacob:Athul_Paul">Athul Paul Jacob</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sordoni:Alessandro">Alessandro Sordoni</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Courville:Aaron_C=">Aaron C. Courville</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bengio:Yoshua">Yoshua Bengio</a></p>
<p>【Abstract】:
In this work, we propose a novel constituency parsing scheme. The model first predicts a real-valued scalar, named syntactic distance, for each split position in the sentence. The topology of grammar tree is then determined by the values of syntactic distances. Compared to traditional shift-reduce parsing schemes, our approach is free from the potentially disastrous compounding error. It is also easier to parallelize and much faster. Our model achieves the state-of-the-art single model F1 score of 92.1 on PTB and 86.4 on CTB dataset, which surpasses the previous single model results by a large margin.</p>
<p>【Keywords】:</p>
<h3 id="109. Gaussian Mixture Latent Vector Grammars.">109. Gaussian Mixture Latent Vector Grammars.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1109/">Paper Link</a>】    【Pages】:1181-1189</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Yanpeng">Yanpeng Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Liwen">Liwen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Kewei">Kewei Tu</a></p>
<p>【Abstract】:
We introduce Latent Vector Grammars (LVeGs), a new framework that extends latent variable grammars such that each nonterminal symbol is associated with a continuous vector space representing the set of (infinitely many) subtypes of the nonterminal. We show that previous models such as latent variable grammars and compositional vector grammars can be interpreted as special cases of LVeGs. We then present Gaussian Mixture LVeGs (GM-LVeGs), a new special case of LVeGs that uses Gaussian mixtures to formulate the weights of production rules over subtypes of nonterminals. A major advantage of using Gaussian mixtures is that the partition function and the expectations of subtype rules can be computed using an extension of the inside-outside algorithm, which enables efficient inference and learning. We apply GM-LVeGs to part-of-speech tagging and constituency parsing and show that GM-LVeGs can achieve competitive accuracies.</p>
<p>【Keywords】:</p>
<h3 id="110. Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples.">110. Extending a Parser to Distant Domains Using a Few Dozen Partially Annotated Examples.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1110/">Paper Link</a>】    【Pages】:1190-1199</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Joshi:Vidur">Vidur Joshi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peters:Matthew_E=">Matthew E. Peters</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hopkins:Mark">Mark Hopkins</a></p>
<p>【Abstract】:
We revisit domain adaptation for parsers in the neural era. First we show that recent advances in word representations greatly diminish the need for domain adaptation when the target domain is syntactically similar to the source domain. As evidence, we train a parser on the Wall Street Journal alone that achieves over 90% F1 on the Brown corpus. For more syntactically distant domains, we provide a simple way to adapt a parser using only dozens of partial annotations. For instance, we increase the percentage of error-free geometry-domain parses in a held-out set from 45% to 73% using approximately five dozen training examples. In the process, we demonstrate a new state-of-the-art single model result on the Wall Street Journal test set of 94.3%. This is an absolute increase of 1.7% over the previous state-of-the-art of 92.6%.</p>
<p>【Keywords】:</p>
<h3 id="111. Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations.">111. Paraphrase to Explicate: Revealing Implicit Noun-Compound Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1111/">Paper Link</a>】    【Pages】:1200-1211</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shwartz:Vered">Vered Shwartz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dagan:Ido">Ido Dagan</a></p>
<p>【Abstract】:
Revealing the implicit semantic relation between the constituents of a noun-compound is important for many NLP applications. It has been addressed in the literature either as a classification task to a set of pre-defined relations or by producing free text paraphrases explicating the relations. Most existing paraphrasing methods lack the ability to generalize, and have a hard time interpreting infrequent or new noun-compounds. We propose a neural model that generalizes better by representing paraphrases in a continuous space, generalizing for both unseen noun-compounds and rare paraphrases. Our model helps improving performance on both the noun-compound paraphrasing and classification tasks.</p>
<p>【Keywords】:</p>
<h3 id="112. Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings.">112. Searching for the X-Factor: Exploring Corpus Subjectivity for Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1112/">Paper Link</a>】    【Pages】:1212-1221</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Tkachenko:Maksim">Maksim Tkachenko</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chia:Chong_Cher">Chong Cher Chia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lauw:Hady_W=">Hady W. Lauw</a></p>
<p>【Abstract】:
We explore the notion of subjectivity, and hypothesize that word embeddings learnt from input corpora of varying levels of subjectivity behave differently on natural language processing tasks such as classifying a sentence by sentiment, subjectivity, or topic. Through systematic comparative analyses, we establish this to be the case indeed. Moreover, based on the discovery of the outsized role that sentiment words play on subjectivity-sensitive tasks such as sentiment classification, we develop a novel word embedding SentiVec which is infused with sentiment information from a lexical resource, and is shown to outperform baselines on such tasks.</p>
<p>【Keywords】:</p>
<h3 id="113. Word Embedding and WordNet Based Metaphor Identification and Interpretation.">113. Word Embedding and WordNet Based Metaphor Identification and Interpretation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1113/">Paper Link</a>】    【Pages】:1222-1231</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mao:Rui">Rui Mao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Chenghua">Chenghua Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guerin:Frank">Frank Guerin</a></p>
<p>【Abstract】:
Metaphoric expressions are widespread in natural language, posing a significant challenge for various natural language processing tasks such as Machine Translation. Current word embedding based metaphor identification models cannot identify the exact metaphorical words within a sentence. In this paper, we propose an unsupervised learning method that identifies and interprets metaphors at word-level without any preprocessing, outperforming strong baselines in the metaphor identification task. Our model extends to interpret the identified metaphors, paraphrasing them into their literal counterparts, so that they can be better translated by machines. We evaluated this with two popular translation systems for English to Chinese, showing that our model improved the systems significantly.</p>
<p>【Keywords】:</p>
<h3 id="114. Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings.">114. Incorporating Latent Meanings of Morphological Compositions to Enhance Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1114/">Paper Link</a>】    【Pages】:1232-1242</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yang">Yang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jiawei">Jiawei Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0011:Wei">Wei Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Liusheng">Liusheng Huang</a></p>
<p>【Abstract】:
Traditional word embedding approaches learn semantic information at word level while ignoring the meaningful internal structures of words like morphemes. Furthermore, existing morphology-based models directly incorporate morphemes to train word embeddings, but still neglect the latent meanings of morphemes. In this paper, we explore to employ the latent meanings of morphological compositions of words to train and enhance word embeddings. Based on this purpose, we propose three Latent Meaning Models (LMMs), named LMM-A, LMM-S and LMM-M respectively, which adopt different strategies to incorporate the latent meanings of morphemes during the training process. Experiments on word similarity, syntactic analogy and text classification are conducted to validate the feasibility of our models. The results demonstrate that our models outperform the baselines on five word similarity datasets. On Wordsim-353 and RG-65 datasets, our models nearly achieve 5% and 7% gains over the classic CBOW model, respectively. For the syntactic analogy and text classification tasks, our models also surpass all the baselines including a morphology-based model.</p>
<p>【Keywords】:</p>
<h3 id="115. A Stochastic Decoder for Neural Machine Translation.">115. A Stochastic Decoder for Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1115/">Paper Link</a>】    【Pages】:1243-1252</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Schulz:Philip">Philip Schulz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aziz:Wilker">Wilker Aziz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a></p>
<p>【Abstract】:
The process of translation is ambiguous, in that there are typically many valid translations for a given sentence. This gives rise to significant variation in parallel corpora, however, most current models of machine translation do not account for this variation, instead treating the problem as a deterministic process. To this end, we present a deep generative model of machine translation which incorporates a chain of latent variables, in order to account for local lexical and syntactic variation in parallel corpora. We provide an in-depth analysis of the pitfalls encountered in variational inference for training deep generative models. Experiments on several different language pairs demonstrate that the model consistently improves over strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="116. Forest-Based Neural Machine Translation.">116. Forest-Based Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1116/">Paper Link</a>】    【Pages】:1253-1263</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Chunpeng">Chunpeng Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tamura:Akihiro">Akihiro Tamura</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Utiyama:Masao">Masao Utiyama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Tiejun">Tiejun Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sumita:Eiichiro">Eiichiro Sumita</a></p>
<p>【Abstract】:
Tree-based neural machine translation (NMT) approaches, although achieved impressive performance, suffer from a major drawback: they only use the 1-best parse tree to direct the translation, which potentially introduces translation mistakes due to parsing errors. For statistical machine translation (SMT), forest-based methods have been proven to be effective for solving this problem, while for NMT this kind of approach has not been attempted. This paper proposes a forest-based NMT method that translates a linearized packed forest under a simple sequence-to-sequence framework (i.e., a forest-to-sequence NMT model). The BLEU score of the proposed method is higher than that of the sequence-to-sequence NMT, tree-based NMT, and forest-based SMT systems.</p>
<p>【Keywords】:</p>
<h3 id="117. Context-Aware Neural Machine Translation Learns Anaphora Resolution.">117. Context-Aware Neural Machine Translation Learns Anaphora Resolution.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1117/">Paper Link</a>】    【Pages】:1264-1274</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Voita:Elena">Elena Voita</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Serdyukov:Pavel">Pavel Serdyukov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sennrich:Rico">Rico Sennrich</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a></p>
<p>【Abstract】:
Standard machine translation systems process sentences in isolation and hence ignore extra-sentential information, even though extended context can both prevent mistakes in ambiguous cases and improve translation coherence. We introduce a context-aware neural machine translation model designed in such way that the flow of information from the extended context to the translation model can be controlled and analyzed. We experiment with an English-Russian subtitles dataset, and observe that much of what is captured by our model deals with improving pronoun translation. We measure correspondences between induced attention distributions and coreference relations and observe that the model implicitly captures anaphora. It is consistent with gains for sentences where pronouns need to be gendered in translation. Beside improvements in anaphoric cases, the model also improves in overall BLEU, both over its context-agnostic version (+0.7) and over simple concatenation of the context and source sentences (+0.6).</p>
<p>【Keywords】:</p>
<h3 id="118. Document Context Neural Machine Translation with Memory Networks.">118. Document Context Neural Machine Translation with Memory Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1118/">Paper Link</a>】    【Pages】:1275-1284</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Maruf:Sameen">Sameen Maruf</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a></p>
<p>【Abstract】:
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model end-to-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.</p>
<p>【Keywords】:</p>
<h3 id="119. Which Melbourne? Augmenting Geocoding with Maps.">119. Which Melbourne? Augmenting Geocoding with Maps.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1119/">Paper Link</a>】    【Pages】:1285-1296</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gritta:Milan">Milan Gritta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pilehvar:Mohammad_Taher">Mohammad Taher Pilehvar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Collier:Nigel">Nigel Collier</a></p>
<p>【Abstract】:
The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing.</p>
<p>【Keywords】:</p>
<h3 id="120. Learning Prototypical Goal Activities for Locations.">120. Learning Prototypical Goal Activities for Locations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1120/">Paper Link</a>】    【Pages】:1297-1307</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Tianyu">Tianyu Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riloff:Ellen">Ellen Riloff</a></p>
<p>【Abstract】:
People go to different places to engage in activities that reflect their goals. For example, people go to restaurants to eat, libraries to study, and churches to pray. We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act). Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning. First, we extract activities and locations that co-occur in goal-oriented syntactic patterns. Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data. We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.</p>
<p>【Keywords】:</p>
<h3 id="121. Guess Me if You Can: Acronym Disambiguation for Enterprises.">121. Guess Me if You Can: Acronym Disambiguation for Enterprises.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1121/">Paper Link</a>】    【Pages】:1308-1317</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Yang">Yang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Bo">Bo Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fuxman:Ariel">Ariel Fuxman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tao:Fangbo">Fangbo Tao</a></p>
<p>【Abstract】:
Acronyms are abbreviations formed from the initial components of words or phrases. In enterprises, people often use acronyms to make communications more efficient. However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity. To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context. Acronym disambiguation for enterprises is challenging for several reasons. First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings. Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises. Finally, the system should be generic to work for any enterprise. In this work we propose an end-to-end framework to tackle all these challenges. The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output. Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples. Therefore, our proposed framework can be deployed to any enterprise to support high-quality acronym disambiguation. Experimental results on real world data justified the effectiveness of our system.</p>
<p>【Keywords】:</p>
<h3 id="122. A Multi-Axis Annotation Scheme for Event Temporal Relations.">122. A Multi-Axis Annotation Scheme for Event Temporal Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1122/">Paper Link</a>】    【Pages】:1318-1328</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Ning:Qiang">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0034:Hao">Hao Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Existing temporal relation (TempRel) annotation schemes often have low inter-annotator agreements (IAA) even between experts, suggesting that the current annotation task needs a better definition. This paper proposes a new multi-axis modeling to better capture the temporal structure of events. In addition, we identify that event end-points are a major source of confusion in annotation, so we also propose to annotate TempRels based on start-points only. A pilot expert annotation effort using the proposed scheme shows significant improvement in IAA from the conventional 60’s to 80’s (Cohen’s Kappa). This better-defined annotation scheme further enables the use of crowdsourcing to alleviate the labor intensity for each annotator. We hope that this work can foster more interesting studies towards event understanding.</p>
<p>【Keywords】:</p>
<h3 id="123. Exemplar Encoder-Decoder for Neural Conversation Generation.">123. Exemplar Encoder-Decoder for Neural Conversation Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1123/">Paper Link</a>】    【Pages】:1329-1338</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pandey_0001:Gaurav">Gaurav Pandey</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Contractor:Danish">Danish Contractor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Vineet">Vineet Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Joshi:Sachindra">Sachindra Joshi</a></p>
<p>【Abstract】:
In this paper we present the Exemplar Encoder-Decoder network (EED), a novel conversation model that learns to utilize similar examples from training data to generate responses. Similar conversation examples (context-response pairs) from training data are retrieved using a traditional TF-IDF based retrieval model and the corresponding responses are used by our decoder to generate the ground truth response. The contribution of each retrieved response is weighed by the similarity of corresponding context with the input context. As a result, our model learns to assign higher similarity scores to those retrieved contexts whose responses are crucial for generating the final response. We present detailed experiments on two large data sets and we find that our method out-performs state of the art sequence to sequence generative models on several recently proposed evaluation metrics.</p>
<p>【Keywords】:</p>
<h3 id="124. DialSQL: Dialogue Based Structured Query Generation.">124. DialSQL: Dialogue Based Structured Query Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1124/">Paper Link</a>】    【Pages】:1339-1349</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gur:Izzeddin">Izzeddin Gur</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yavuz:Semih">Semih Yavuz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su_0001:Yu">Yu Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Xifeng">Xifeng Yan</a></p>
<p>【Abstract】:
The recent advance in deep learning and semantic parsing has significantly improved the translation accuracy of natural language questions to structured queries. However, further improvement of the existing approaches turns out to be quite challenging. Rather than solely relying on algorithmic innovations, in this work, we introduce DialSQL, a dialogue-based structured query generation framework that leverages human intelligence to boost the performance of existing algorithms via user interaction. DialSQL is capable of identifying potential errors in a generated SQL query and asking users for validation via simple multi-choice questions. User feedback is then leveraged to revise the query. We design a generic simulator to bootstrap synthetic training dialogues and evaluate the performance of DialSQL on the WikiSQL dataset. Using SQLNet as a black box query generation tool, DialSQL improves its performance from 61.3% to 69.0% using only 2.4 validation questions per dialogue.</p>
<p>【Keywords】:</p>
<h3 id="125. Conversations Gone Awry: Detecting Early Signs of Conversational Failure.">125. Conversations Gone Awry: Detecting Early Signs of Conversational Failure.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1125/">Paper Link</a>】    【Pages】:1350-1361</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Justine">Justine Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Jonathan_P=">Jonathan P. Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Danescu=Niculescu=Mizil:Cristian">Cristian Danescu-Niculescu-Mizil</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dixon:Lucas">Lucas Dixon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hua:Yiqing">Yiqing Hua</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taraborelli:Dario">Dario Taraborelli</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thain:Nithum">Nithum Thain</a></p>
<p>【Abstract】:
One of the main challenges online social systems face is the prevalence of antisocial behavior, such as harassment and personal attacks. In this work, we introduce the task of predicting from the very start of a conversation whether it will get out of hand. As opposed to detecting undesirable behavior after the fact, this task aims to enable early, actionable prediction at a time when the conversation might still be salvaged. To this end, we develop a framework for capturing pragmatic devices—such as politeness strategies and rhetorical prompts—used to start a conversation, and analyze their relation to its future trajectory. Applying this framework in a controlled setting, we demonstrate the feasibility of detecting early warning signs of antisocial behavior in online discussions.</p>
<p>【Keywords】:</p>
<h3 id="126. Are BLEU and Meaning Representation in Opposition?">126. Are BLEU and Meaning Representation in Opposition?</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1126/">Paper Link</a>】    【Pages】:1362-1371</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/C=iacute=fka:Ondrej">Ondrej Cífka</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bojar:Ondrej">Ondrej Bojar</a></p>
<p>【Abstract】:
One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation (NMT) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive NMT architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.</p>
<p>【Keywords】:</p>
<h3 id="127. Automatic Metric Validation for Grammatical Error Correction.">127. Automatic Metric Validation for Grammatical Error Correction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1127/">Paper Link</a>】    【Pages】:1372-1382</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Choshen:Leshem">Leshem Choshen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Abend:Omri">Omri Abend</a></p>
<p>【Abstract】:
Metric validation in Grammatical Error Correction (GEC) is currently done by observing the correlation between human and metric-induced rankings. However, such correlation studies are costly, methodologically troublesome, and suffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties in the existing methodology. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that some types of valid edits are consistently penalized by existing metrics.</p>
<p>【Keywords】:</p>
<h3 id="128. The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing.">128. The Hitchhiker's Guide to Testing Statistical Significance in Natural Language Processing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1128/">Paper Link</a>】    【Pages】:1383-1392</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dror:Rotem">Rotem Dror</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baumer:Gili">Gili Baumer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shlomov:Segev">Segev Shlomov</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a></p>
<p>【Abstract】:
Statistical significance testing is a standard statistical tool designed to ensure that experimental results are not coincidental. In this opinion/ theoretical paper we discuss the role of statistical significance testing in Natural Language Processing (NLP) research. We establish the fundamental concepts of significance testing and discuss the specific aspects of NLP tasks, experimental setups and evaluation measures that affect the choice of significance tests in NLP research. Based on this discussion we propose a simple practical protocol for statistical significance test selection in NLP setups and accompany this protocol with a brief survey of the most relevant tests. We then survey recent empirical papers published in ACL and TACL during 2017 and show that while our community assigns great value to experimental results, statistical significance testing is often ignored or misused. We conclude with a brief discussion of open issues that should be properly addressed so that this important tool can be applied. in NLP research in a statistically sound manner.</p>
<p>【Keywords】:</p>
<h3 id="129. Distilling Knowledge for Search-based Structured Prediction.">129. Distilling Knowledge for Search-based Structured Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1129/">Paper Link</a>】    【Pages】:1393-1402</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Yijia">Yijia Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Che:Wanxiang">Wanxiang Che</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Huaipeng">Huaipeng Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qin_0001:Bing">Bing Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Ting">Ting Liu</a></p>
<p>【Abstract】:
Many natural language processing tasks can be modeled into structured prediction and solved as a search problem. In this paper, we distill an ensemble of multiple models trained with different initialization into a single model. In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration. Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.</p>
<p>【Keywords】:</p>
<h3 id="130. Stack-Pointer Networks for Dependency Parsing.">130. Stack-Pointer Networks for Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1130/">Paper Link</a>】    【Pages】:1403-1414</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Xuezhe">Xuezhe Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Zecong">Zecong Hu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jingzhou">Jingzhou Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Nanyun">Nanyun Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>【Abstract】:
We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them</p>
<p>【Keywords】:</p>
<h3 id="131. Twitter Universal Dependency Parsing for African-American and Mainstream American English.">131. Twitter Universal Dependency Parsing for African-American and Mainstream American English.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1131/">Paper Link</a>】    【Pages】:1415-1425</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Blodgett:Su_Lin">Su Lin Blodgett</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Johnny">Johnny Wei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/O=Connor:Brendan_T=">Brendan T. O&apos;Connor</a></p>
<p>【Abstract】:
Due to the presence of both Twitter-specific conventions and non-standard and dialectal language, Twitter presents a significant parsing challenge to current dependency parsing tools. We broaden English dependency parsing to handle social media English, particularly social media African-American English (AAE), by developing and annotating a new dataset of 500 tweets, 250 of which are in AAE, within the Universal Dependencies 2.0 framework. We describe our standards for handling Twitter- and AAE-specific features and evaluate a variety of cross-domain strategies for improving parsing with no, or very little, in-domain labeled data, including a new data synthesis approach. We analyze these methods’ impact on performance disparities between AAE and Mainstream American English tweets, and assess parsing accuracy for specific AAE lexical and syntactic features. Our annotated data and a parsing model are available at: <a href="http://slanglab.cs.umass.edu/TwitterAAE/">http://slanglab.cs.umass.edu/TwitterAAE/</a>.</p>
<p>【Keywords】:</p>
<h3 id="132. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better.">132. LSTMs Can Learn Syntax-Sensitive Dependencies Well, But Modeling Structure Makes Them Better.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1132/">Paper Link</a>】    【Pages】:1426-1436</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kuncoro:Adhiguna">Adhiguna Kuncoro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dyer:Chris">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hale:John">John Hale</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yogatama:Dani">Dani Yogatama</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Clark:Stephen">Stephen Clark</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Blunsom:Phil">Phil Blunsom</a></p>
<p>【Abstract】:
Language exhibits hierarchical structure, but recent work using a subject-verb agreement diagnostic argued that state-of-the-art language models, LSTMs, fail to learn long-range syntax sensitive dependencies. Using the same diagnostic, we show that, in fact, LSTMs do succeed in learning such dependencies—provided they have enough capacity. We then explore whether models that have access to explicit syntactic information learn agreement more effectively, and how the way in which this structural information is incorporated into the model impacts performance. We find that the mere presence of syntactic information does not improve accuracy, but when model architecture is determined by syntax, number agreement is improved. Further, we find that the choice of how syntactic structure is built affects how well number agreement is learned: top-down construction outperforms left-corner and bottom-up variants in capturing non-local structural dependencies.</p>
<p>【Keywords】:</p>
<h3 id="133. Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures.">133. Sequicity: Simplifying Task-oriented Dialogue Systems with Single Sequence-to-Sequence Architectures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1133/">Paper Link</a>】    【Pages】:1437-1447</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lei:Wenqiang">Wenqiang Lei</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Xisen">Xisen Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kan:Min=Yen">Min-Yen Kan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Zhaochun">Zhaochun Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0001:Xiangnan">Xiangnan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Dawei">Dawei Yin</a></p>
<p>【Abstract】:
Existing solutions to task-oriented dialogue systems follow pipeline designs which introduces architectural complexity and fragility. We propose a novel, holistic, extendable framework based on a single sequence-to-sequence (seq2seq) model which can be optimized with supervised or reinforcement learning. A key contribution is that we design text spans named belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a seq2seq way. Based on this, we propose a simplistic Two Stage CopyNet instantiation which emonstrates good scalability: significantly reducing model complexity in terms of number of parameters and training time by a magnitude. It significantly outperforms state-of-the-art pipeline-based methods on large datasets and retains a satisfactory entity match rate on out-of-vocabulary (OOV) cases where pipeline-designed competitors totally fail.</p>
<p>【Keywords】:</p>
<h3 id="134. An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking.">134. An End-to-end Approach for Handling Unknown Slot Values in Dialogue State Tracking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1134/">Paper Link</a>】    【Pages】:1448-1457</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Puyang">Puyang Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hu:Qi">Qi Hu</a></p>
<p>【Abstract】:
We highlight a practical yet rarely discussed problem in dialogue state tracking (DST), namely handling unknown slot values. Previous approaches generally assume predefined candidate lists and thus are not designed to output unknown values, especially when the spoken language understanding (SLU) module is absent as in many end-to-end (E2E) systems. We describe in this paper an E2E architecture based on the pointer network (PtrNet) that can effectively extract unknown slot values while still obtains state-of-the-art accuracy on the standard DSTC2 benchmark. We also provide extensive empirical evidence to show that tracking unknown values can be challenging and our approach can bring significant improvement with the help of an effective feature dropout technique.</p>
<p>【Keywords】:</p>
<h3 id="135. Global-Locally Self-Attentive Encoder for Dialogue State Tracking.">135. Global-Locally Self-Attentive Encoder for Dialogue State Tracking.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1135/">Paper Link</a>】    【Pages】:1458-1467</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Victor">Victor Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Caiming">Caiming Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Socher:Richard">Richard Socher</a></p>
<p>【Abstract】:
Dialogue state tracking, which estimates user goals and requests given the dialogue context, is an essential part of task-oriented dialogue systems. In this paper, we propose the Global-Locally Self-Attentive Dialogue State Tracker (GLAD), which learns representations of the user utterance and previous system actions with global-local modules. Our model uses global modules to shares parameters between estimators for different types (called slots) of dialogue states, and uses local modules to learn slot-specific features. We show that this significantly improves tracking of rare states. GLAD obtains 88.3% joint goal accuracy and 96.4% request accuracy on the WoZ state tracking task, outperforming prior work by 3.9% and 4.8%. On the DSTC2 task, our model obtains 74.7% joint goal accuracy and 97.3% request accuracy, outperforming prior work by 1.3% and 0.8%</p>
<p>【Keywords】:</p>
<h3 id="136. Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems.">136. Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1136/">Paper Link</a>】    【Pages】:1468-1478</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Madotto:Andrea">Andrea Madotto</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Chien=Sheng">Chien-Sheng Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fung:Pascale">Pascale Fung</a></p>
<p>【Abstract】:
End-to-end task-oriented dialog systems usually suffer from the challenge of incorporating knowledge bases. In this paper, we propose a novel yet simple end-to-end differentiable model called memory-to-sequence (Mem2Seq) to address this issue. Mem2Seq is the first neural generative model that combines the multi-hop attention over memories with the idea of pointer network. We empirically show how Mem2Seq controls each generation step, and how its multi-hop attention mechanism helps in learning correlations between memories. In addition, our model is quite general without complicated task-specific designs. As a result, we show that Mem2Seq can be trained faster and attain the state-of-the-art performance on three different task-oriented dialog datasets.</p>
<p>【Keywords】:</p>
<h3 id="137. Tailored Sequence to Sequence Models to Different Conversation Scenarios.">137. Tailored Sequence to Sequence Models to Different Conversation Scenarios.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1137/">Paper Link</a>】    【Pages】:1479-1488</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Hainan">Hainan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lan:Yanyan">Yanyan Lan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu_0001:Jun">Jun Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
Sequence to sequence (Seq2Seq) models have been widely used for response generation in the area of conversation. However, the requirements for different conversation scenarios are distinct. For example, customer service requires the generated responses to be specific and accurate, while chatbot prefers diverse responses so as to attract different users. The current Seq2Seq model fails to meet these diverse requirements, by using a general average likelihood as the optimization criteria. As a result, it usually generates safe and commonplace responses, such as ‘I don’t know’. In this paper, we propose two tailored optimization criteria for Seq2Seq to different conversation scenarios, i.e., the maximum generated likelihood for specific-requirement scenario, and the conditional value-at-risk for diverse-requirement scenario. Experimental results on the Ubuntu dialogue corpus (Ubuntu service scenario) and Chinese Weibo dataset (social chatbot scenario) show that our proposed models not only satisfies diverse requirements for different scenarios, but also yields better performances against traditional Seq2Seq models in terms of both metric-based and human evaluations.</p>
<p>【Keywords】:</p>
<h3 id="138. Knowledge Diffusion for Neural Dialogue Generation.">138. Knowledge Diffusion for Neural Dialogue Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1138/">Paper Link</a>】    【Pages】:1489-1498</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Shuman">Shuman Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hongshen">Hongshen Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Zhaochun">Zhaochun Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng_0004:Yang">Yang Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Qun">Qun Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yin:Dawei">Dawei Yin</a></p>
<p>【Abstract】:
End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset prove that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly.</p>
<p>【Keywords】:</p>
<h3 id="139. Generating Informative Responses with Controlled Sentence Function.">139. Generating Informative Responses with Controlled Sentence Function.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1139/">Paper Link</a>】    【Pages】:1499-1508</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Ke:Pei">Pei Ke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Guan:Jian">Jian Guan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Minlie">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu_0001:Xiaoyan">Xiaoyan Zhu</a></p>
<p>【Abstract】:
Sentence function is a significant factor to achieve the purpose of the speaker, which, however, has not been touched in large-scale conversation generation so far. In this paper, we present a model to generate informative responses with controlled sentence function. Our model utilizes a continuous latent variable to capture various word patterns that realize the expected sentence function, and introduces a type controller to deal with the compatibility of controlling sentence function and generating informative content. Conditioned on the latent variable, the type controller determines the type (i.e., function-related, topic, and ordinary word) of a word to be generated at each decoding position. Experiments show that our model outperforms state-of-the-art baselines, and it has the ability to generate responses with both controlled sentence function and informative content.</p>
<p>【Keywords】:</p>
<h3 id="140. Sentiment Adaptive End-to-End Dialog Systems.">140. Sentiment Adaptive End-to-End Dialog Systems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1140/">Paper Link</a>】    【Pages】:1509-1519</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Weiyan">Weiyan Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhou">Zhou Yu</a></p>
<p>【Abstract】:
End-to-end learning framework is useful for building dialog systems for its simplicity in training and efficiency in model updating. However, current end-to-end approaches only consider user semantic inputs in learning and under-utilize other user information. Therefore, we propose to include user sentiment obtained through multimodal information (acoustic, dialogic and textual), in the end-to-end learning framework to make systems more user-adaptive and effective. We incorporated user sentiment information in both supervised and reinforcement learning settings. In both settings, adding sentiment information reduced the dialog length and improved the task success rate on a bus information search task. This work is the first attempt to incorporate multimodal user information in the adaptive end-to-end dialog system training framework and attained state-of-the-art performance.</p>
<p>【Keywords】:</p>
<h3 id="141. Embedding Learning Through Multilingual Concept Induction.">141. Embedding Learning Through Multilingual Concept Induction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1141/">Paper Link</a>】    【Pages】:1520-1530</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dufter:Philipp">Philipp Dufter</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Mengjie">Mengjie Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schmitt:Martin">Martin Schmitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fraser:Alexander_M=">Alexander M. Fraser</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
We present a new method for estimating vector space representations of words: embedding learning by concept induction. We test this method on a highly parallel corpus and learn semantic representations of words in 1259 different languages in a single common space. An extensive experimental evaluation on crosslingual word similarity and sentiment analysis indicates that concept-based multilingual embedding learning performs better than previous approaches.</p>
<p>【Keywords】:</p>
<h3 id="142. Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP.">142. Isomorphic Transfer of Syntactic Structures in Cross-Lingual NLP.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1142/">Paper Link</a>】    【Pages】:1531-1542</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Ponti:Edoardo_Maria">Edoardo Maria Ponti</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Reichart:Roi">Roi Reichart</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Korhonen:Anna">Anna Korhonen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Vulic:Ivan">Ivan Vulic</a></p>
<p>【Abstract】:
The transfer or share of knowledge between languages is a potential solution to resource scarcity in NLP. However, the effectiveness of cross-lingual transfer can be challenged by variation in syntactic structures. Frameworks such as Universal Dependencies (UD) are designed to be cross-lingually consistent, but even in carefully designed resources trees representing equivalent sentences may not always overlap. In this paper, we measure cross-lingual syntactic variation, or anisomorphism, in the UD treebank collection, considering both morphological and structural properties. We show that reducing the level of anisomorphism yields consistent gains in cross-lingual transfer tasks. We introduce a source language selection procedure that facilitates effective cross-lingual parser transfer, and propose a typologically driven method for syntactic tree processing which reduces anisomorphism. Our results show the effectiveness of this method for both machine translation and cross-lingual sentence similarity, demonstrating the importance of syntactic structure compatibility for boosting cross-lingual transfer in NLP.</p>
<p>【Keywords】:</p>
<h3 id="143. Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data.">143. Language Modeling for Code-Mixing: The Role of Linguistic Theory based Synthetic Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1143/">Paper Link</a>】    【Pages】:1543-1553</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Pratapa:Adithya">Adithya Pratapa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhat:Gayatri">Gayatri Bhat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choudhury:Monojit">Monojit Choudhury</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sitaram:Sunayana">Sunayana Sitaram</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dandapat:Sandipan">Sandipan Dandapat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bali:Kalika">Kalika Bali</a></p>
<p>【Abstract】:
Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language. We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory. We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model. We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.</p>
<p>【Keywords】:</p>
<h3 id="144. Chinese NER Using Lattice LSTM.">144. Chinese NER Using Lattice LSTM.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1144/">Paper Link</a>】    【Pages】:1554-1564</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang_0039:Jie">Jie Yang</a></p>
<p>【Abstract】:
We investigate a lattice-structured LSTM model for Chinese NER, which encodes a sequence of input characters as well as all potential words that match a lexicon. Compared with character-based methods, our model explicitly leverages word and word sequence information. Compared with word-based methods, lattice LSTM does not suffer from segmentation errors. Gated recurrent cells allow our model to choose the most relevant characters and words from a sentence for better NER results. Experiments on various datasets show that lattice LSTM outperforms both word-based and character-based LSTM baselines, achieving the best results.</p>
<p>【Keywords】:</p>
<h3 id="145. Nugget Proposal Networks for Chinese Event Detection.">145. Nugget Proposal Networks for Chinese Event Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1145/">Paper Link</a>】    【Pages】:1565-1574</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Hongyu">Hongyu Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lu_0001:Yaojie">Yaojie Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a></p>
<p>【Abstract】:
Neural network based models commonly regard event detection as a word-wise classification task, which suffer from the mismatch problem between words and event triggers, especially in languages without natural word delimiters such as Chinese. In this paper, we propose Nugget Proposal Networks (NPNs), which can solve the word-trigger mismatch problem by directly proposing entire trigger nuggets centered at each character regardless of word boundaries. Specifically, NPNs perform event detection in a character-wise paradigm, where a hybrid representation for each character is first learned to capture both structural and semantic information from both characters and words. Then based on learned representations, trigger nuggets are proposed and categorized by exploiting character compositional structures of Chinese event triggers. Experiments on both ACE2005 and TAC KBP 2017 datasets show that NPNs significantly outperform the state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="146. Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation.">146. Higher-order Relation Schema Induction using Tensor Factorization with Back-off and Aggregation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1146/">Paper Link</a>】    【Pages】:1575-1584</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Nimishakavi:Madhav">Madhav Nimishakavi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Manish">Manish Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Relation Schema Induction (RSI) is the problem of identifying type signatures of arguments of relations from unlabeled text. Most of the previous work in this area have focused only on binary RSI, i.e., inducing only the subject and object type signatures per relation. However, in practice, many relations are high-order, i.e., they have more than two arguments and inducing type signatures of all arguments is necessary. For example, in the sports domain, inducing a schema win(WinningPlayer, OpponentPlayer, Tournament, Location) is more informative than inducing just win(WinningPlayer, OpponentPlayer). We refer to this problem as Higher-order Relation Schema Induction (HRSI). In this paper, we propose Tensor Factorization with Back-off and Aggregation (TFBA), a novel framework for the HRSI problem. To the best of our knowledge, this is the first attempt at inducing higher-order relation schemata from unlabeled text. Using the experimental analysis on three real world datasets we show how TFBA helps in dealing with sparsity and induce higher-order schemata.</p>
<p>【Keywords】:</p>
<h3 id="147. Discovering Implicit Knowledge with Unary Relations.">147. Discovering Implicit Knowledge with Unary Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1147/">Paper Link</a>】    【Pages】:1585-1594</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Glass:Michael_R=">Michael R. Glass</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gliozzo:Alfio">Alfio Gliozzo</a></p>
<p>【Abstract】:
State-of-the-art relation extraction approaches are only able to recognize relationships between mentions of entity arguments stated explicitly in the text and typically localized to the same sentence. However, the vast majority of relations are either implicit or not sententially localized. This is a major problem for Knowledge Base Population, severely limiting recall. In this paper we propose a new methodology to identify relations between two entities, consisting of detecting a very large number of unary relations, and using them to infer missing entities. We describe a deep learning architecture able to learn thousands of such relations very efficiently by using a common deep learning based representation. Our approach largely outperforms state of the art relation extraction technology on a newly introduced web scale knowledge base population benchmark, that we release to the research community.</p>
<p>【Keywords】:</p>
<h3 id="148. Improving Entity Linking by Modeling Latent Relations between Mentions.">148. Improving Entity Linking by Modeling Latent Relations between Mentions.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1148/">Paper Link</a>】    【Pages】:1595-1604</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Le:Phong">Phong Le</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Titov:Ivan">Ivan Titov</a></p>
<p>【Abstract】:
Entity linking involves aligning textual mentions of named entities to their corresponding entries in a knowledge base. Entity linking systems often exploit relations between textual mentions in a document (e.g., coreference) to decide if the linking decisions are compatible. Unlike previous approaches, which relied on supervised systems or heuristics to predict these relations, we treat relations as latent variables in our neural entity-linking model. We induce the relations without any supervision while optimizing the entity-linking system in an end-to-end fashion. Our multi-relational model achieves the best reported scores on the standard benchmark (AIDA-CoNLL) and substantially outperforms its relation-agnostic version. Its training also converges much faster, suggesting that the injected structural bias helps to explain regularities in the training data.</p>
<p>【Keywords】:</p>
<h3 id="149. Dating Documents using Graph Convolution Networks.">149. Dating Documents using Graph Convolution Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1149/">Paper Link</a>】    【Pages】:1605-1615</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/v/Vashishth:Shikhar">Shikhar Vashishth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dasgupta:Shib_Sankar">Shib Sankar Dasgupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ray:Swayambhu_Nath">Swayambhu Nath Ray</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Talukdar:Partha_P=">Partha P. Talukdar</a></p>
<p>【Abstract】:
Document date is essential for many important tasks, such as document retrieval, summarization, event detection, etc. While existing approaches for these tasks assume accurate knowledge of the document date, this is not always available, especially for arbitrary documents from the Web. Document Dating is a challenging problem which requires inference over the temporal structure of the document. Prior document dating systems have largely relied on handcrafted features while ignoring such document-internal structures. In this paper, we propose NeuralDater, a Graph Convolutional Network (GCN) based document dating approach which jointly exploits syntactic and temporal graph structures of document in a principled way. To the best of our knowledge, this is the first application of deep learning for the problem of document dating. Through extensive experiments on real-world datasets, we find that NeuralDater significantly outperforms state-of-the-art baseline by 19% absolute (45% relative) accuracy points.</p>
<p>【Keywords】:</p>
<h3 id="150. A Graph-to-Sequence Model for AMR-to-Text Generation.">150. A Graph-to-Sequence Model for AMR-to-Text Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1150/">Paper Link</a>】    【Pages】:1616-1626</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Song:Linfeng">Linfeng Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yue">Yue Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhiguo">Zhiguo Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gildea:Daniel">Daniel Gildea</a></p>
<p>【Abstract】:
The problem of AMR-to-text generation is to recover a text representing the same meaning as an input AMR graph. The current state-of-the-art method uses a sequence-to-sequence model, leveraging LSTM for encoding a linearized AMR structure. Although being able to model non-local semantic information, a sequence LSTM can lose information from the AMR graph structure, and thus facing challenges with large-graphs, which result in long sequences. We introduce a neural graph-to-sequence model, using a novel LSTM structure for directly encoding graph-level semantics. On a standard benchmark, our model shows superior results to existing methods in the literature.</p>
<p>【Keywords】:</p>
<h3 id="151. GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data.">151. GTR-LSTM: A Triple Encoder for Sentence Generation from RDF Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1151/">Paper Link</a>】    【Pages】:1627-1637</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Trisedya:Bayu_Distiawan">Bayu Distiawan Trisedya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/q/Qi_0001:Jianzhong">Jianzhong Qi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0003:Rui">Rui Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0011:Wei">Wei Wang</a></p>
<p>【Abstract】:
A knowledge base is a large repository of facts that are mainly represented as RDF triples, each of which consists of a subject, a predicate (relationship), and an object. The RDF triple representation offers a simple interface for applications to access the facts. However, this representation is not in a natural language form, which is difficult for humans to understand. We address this problem by proposing a system to translate a set of RDF triples into natural sentences based on an encoder-decoder framework. To preserve as much information from RDF triples as possible, we propose a novel graph-based triple encoder. The proposed encoder encodes not only the elements of the triples but also the relationships both within a triple and between the triples. Experimental results show that the proposed encoder achieves a consistent improvement over the baseline models by up to 17.6%, 6.0%, and 16.4% in three common metrics BLEU, METEOR, and TER, respectively.</p>
<p>【Keywords】:</p>
<h3 id="152. Learning to Write with Cooperative Discriminators.">152. Learning to Write with Cooperative Discriminators.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1152/">Paper Link</a>】    【Pages】:1638-1649</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Holtzman:Ari">Ari Holtzman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buys:Jan">Jan Buys</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Forbes:Maxwell">Maxwell Forbes</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bosselut:Antoine">Antoine Bosselut</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Golub:David">David Golub</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>【Abstract】:
Despite their local fluency, long-form text generated from RNNs is often generic, repetitive, and even self-contradictory. We propose a unified learning framework that collectively addresses all the above issues by composing a committee of discriminators that can guide a base RNN generator towards more globally coherent generations. More concretely, discriminators each specialize in a different principle of communication, such as Grice’s maxims, and are collectively combined with the base RNN generator through a composite decoding objective. Human evaluation demonstrates that text generated by our model is preferred over that of baselines by a large margin, significantly enhancing the overall coherence, style, and information of the generations.</p>
<p>【Keywords】:</p>
<h3 id="153. A Neural Approach to Pun Generation.">153. A Neural Approach to Pun Generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1153/">Paper Link</a>】    【Pages】:1650-1660</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Zhiwei">Zhiwei Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Jiwei">Jiwei Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>【Abstract】:
Automatic pun generation is an interesting and challenging text generation task. Previous efforts rely on templates or laboriously manually annotated pun datasets, which heavily constrains the quality and diversity of generated puns. Since sequence-to-sequence models provide an effective technique for text generation, it is promising to investigate these models on the pun generation task. In this paper, we propose neural network models for homographic pun generation, and they can generate puns without requiring any pun data for training. We first train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality.</p>
<p>【Keywords】:</p>
<h3 id="154. Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data.">154. Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1154/">Paper Link</a>】    【Pages】:1661-1671</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jhamtani:Harsh">Harsh Jhamtani</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gangal:Varun">Varun Gangal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berg=Kirkpatrick:Taylor">Taylor Berg-Kirkpatrick</a></p>
<p>【Abstract】:
This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.</p>
<p>【Keywords】:</p>
<h3 id="155. From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction.">155. From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1155/">Paper Link</a>】    【Pages】:1672-1682</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dai:Zihang">Zihang Dai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Qizhe">Qizhe Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>【Abstract】:
In this work, we study the credit assignment problem in reward augmented maximum likelihood (RAML) learning, and establish a theoretical equivalence between the token-level counterpart of RAML and the entropy regularized reinforcement learning. Inspired by the connection, we propose two sequence prediction algorithms, one extending RAML with fine-grained credit assignment and the other improving Actor-Critic with a systematic entropy regularization. On two benchmark datasets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction.</p>
<p>【Keywords】:</p>
<h3 id="156. DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.">156. DuoRC: Towards Complex Language Understanding with Paraphrased Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1156/">Paper Link</a>】    【Pages】:1683-1693</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Saha:Amrita">Amrita Saha</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Aralikatte:Rahul">Rahul Aralikatte</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khapra:Mitesh_M=">Mitesh M. Khapra</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sankaranarayanan:Karthik">Karthik Sankaranarayanan</a></p>
<p>【Abstract】:
We propose DuoRC, a novel dataset for Reading Comprehension (RC) that motivates several new challenges for neural approaches in language understanding beyond those offered by existing RC datasets. DuoRC contains 186,089 unique question-answer pairs created from a collection of 7680 pairs of movie plots where each pair in the collection reflects two versions of the same movie - one from Wikipedia and the other from IMDb - written by two different authors. We asked crowdsourced workers to create questions from one version of the plot and a different set of workers to extract or synthesize answers from the other version. This unique characteristic of DuoRC where questions and answers are created from different versions of a document narrating the same underlying story, ensures by design, that there is very little lexical overlap between the questions created from one version and the segments containing the answer in the other version. Further, since the two versions have different levels of plot detail, narration style, vocabulary, etc., answering questions from the second version requires deeper language understanding and incorporating external background knowledge. Additionally, the narrative style of passages arising from movie plots (as opposed to typical descriptive passages in existing datasets) exhibits the need to perform complex reasoning over events across multiple sentences. Indeed, we observe that state-of-the-art neural RC models which have achieved near human performance on the SQuAD dataset, even when coupled with traditional NLP techniques to address the challenges presented in DuoRC exhibit very poor performance (F1 score of 37.42% on DuoRC v/s 86% on SQuAD dataset). This opens up several interesting research avenues wherein DuoRC could complement other RC datasets to explore novel neural approaches for studying language understanding.</p>
<p>【Keywords】:</p>
<h3 id="157. Stochastic Answer Networks for Machine Reading Comprehension.">157. Stochastic Answer Networks for Machine Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1157/">Paper Link</a>】    【Pages】:1694-1704</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xiaodong">Xiaodong Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Yelong">Yelong Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duh:Kevin">Kevin Duh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a></p>
<p>【Abstract】:
We propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. Compared to previous work such as ReasoNet which used reinforcement learning to determine the number of steps, the unique feature is the use of a kind of stochastic prediction dropout on the answer module (final layer) of the neural network during the training. We show that this simple trick improves robustness and achieves results competitive to the state-of-the-art on the Stanford Question Answering Dataset (SQuAD), the Adversarial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO).</p>
<p>【Keywords】:</p>
<h3 id="158. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering.">158. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1158/">Paper Link</a>】    【Pages】:1705-1714</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wei">Wei Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Chen">Chen Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Ming">Ming Yan</a></p>
<p>【Abstract】:
This paper describes a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In the proposed method, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. Specifically, it first encode the question and paragraph with fine-grained language embeddings, to better capture the respective representations at semantic level. Then it proposes a multi-granularity fusion approach to fully fuse information from both global and attended representations. Finally, it introduces a hierarchical attention network to focuses on the answer span progressively with multi-level soft-alignment. Extensive experiments on the large-scale SQuAD, TriviaQA dataset validate the effectiveness of the proposed method. At the time of writing the paper, our model achieves state-of-the-art on the both SQuAD and TriviaQA Wiki leaderboard as well as two adversarial SQuAD datasets.</p>
<p>【Keywords】:</p>
<h3 id="159. Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension.">159. Joint Training of Candidate Extraction and Answer Selection for Reading Comprehension.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1159/">Paper Link</a>】    【Pages】:1715-1724</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Zhen">Zhen Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jiachen">Jiachen Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiao:Xinyan">Xinyan Xiao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Yajuan">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Tian">Tian Wu</a></p>
<p>【Abstract】:
While sophisticated neural-based techniques have been developed in reading comprehension, most approaches model the answer in an independent manner, ignoring its relations with other answer candidates. This problem can be even worse in open-domain scenarios, where candidates from multiple passages should be combined to answer a single question. In this paper, we formulate reading comprehension as an extract-then-select two-stage procedure. We first extract answer candidates from passages, then select the final answer by combining information from all the candidates. Furthermore, we regard candidate extraction as a latent variable and train the two-stage process jointly with reinforcement learning. As a result, our approach has improved the state-of-the-art performance significantly on two challenging open-domain reading comprehension datasets. Further analysis demonstrates the effectiveness of our model components, especially the information fusion of all the candidates and the joint training of the extract-then-select procedure.</p>
<p>【Keywords】:</p>
<h3 id="160. Efficient and Robust Question Answering from Minimal Context over Documents.">160. Efficient and Robust Question Answering from Minimal Context over Documents.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1160/">Paper Link</a>】    【Pages】:1725-1735</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Min:Sewon">Sewon Min</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhong:Victor">Victor Zhong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Socher:Richard">Richard Socher</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Caiming">Caiming Xiong</a></p>
<p>【Abstract】:
Neural models for question answering (QA) over documents have achieved significant performance improvements. Although effective, these models do not scale to large corpora due to their complex modeling of interactions between the document and the question. Moreover, recent work has shown that such models are sensitive to adversarial inputs. In this paper, we study the minimal context required to answer the question, and find that most questions in existing datasets can be answered with a small set of sentences. Inspired by this observation, we propose a simple sentence selector to select the minimal set of sentences to feed into the QA model. Our overall system achieves significant reductions in training (up to 15 times) and inference times (up to 13 times), with accuracy comparable to or better than the state-of-the-art on SQuAD, NewsQA, TriviaQA and SQuAD-Open. Furthermore, our experimental results and analyses show that our approach is more robust to adversarial inputs.</p>
<p>【Keywords】:</p>
<h3 id="161. Denoising Distantly Supervised Open-Domain Question Answering.">161. Denoising Distantly Supervised Open-Domain Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1161/">Paper Link</a>】    【Pages】:1736-1745</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Yankai">Yankai Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Haozhe">Haozhe Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a></p>
<p>【Abstract】:
Distantly supervised open-domain question answering (DS-QA) aims to find answers in collections of unlabeled text. Existing DS-QA models usually retrieve related paragraphs from a large-scale corpus and apply reading comprehension technique to extract answers from the most relevant paragraph. They ignore the rich information contained in other paragraphs. Moreover, distant supervision data inevitably accompanies with the wrong labeling problem, and these noisy data will substantially degrade the performance of DS-QA. To address these issues, we propose a novel DS-QA model which employs a paragraph selector to filter out those noisy paragraphs and a paragraph reader to extract the correct answer from those denoised paragraphs. Experimental results on real-world datasets show that our model can capture useful information from noisy data and achieve significant improvements on DS-QA as compared to all baselines.</p>
<p>【Keywords】:</p>
<h3 id="162. Question Condensing Networks for Answer Selection in Community Question Answering.">162. Question Condensing Networks for Answer Selection in Community Question Answering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1162/">Paper Link</a>】    【Pages】:1746-1755</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wu:Wei">Wei Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun_0001:Xu">Xu Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Houfeng">Houfeng Wang</a></p>
<p>【Abstract】:
Answer selection is an important subtask of community question answering (CQA). In a real-world CQA forum, a question is often represented as two parts: a subject that summarizes the main points of the question, and a body that elaborates on the subject in detail. Previous researches on answer selection usually ignored the difference between these two parts and concatenated them as the question representation. In this paper, we propose the Question Condensing Networks (QCN) to make use of the subject-body relationship of community questions. In our model, the question subject is the primary part of the question representation, and the question body information is aggregated based on similarity and disparity with the question subject. Experimental results show that QCN outperforms all existing models on two CQA datasets.</p>
<p>【Keywords】:</p>
<h3 id="163. Towards Robust Neural Machine Translation.">163. Towards Robust Neural Machine Translation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1163/">Paper Link</a>】    【Pages】:1756-1766</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cheng:Yong">Yong Cheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tu:Zhaopeng">Zhaopeng Tu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Meng:Fandong">Fandong Meng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhai:Junjie">Junjie Zhai</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0005:Yang">Yang Liu</a></p>
<p>【Abstract】:
Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.</p>
<p>【Keywords】:</p>
<h3 id="164. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.">164. Attention Focusing for Neural Machine Translation by Bridging Source and Target Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1164/">Paper Link</a>】    【Pages】:1767-1776</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kuang:Shaohui">Shaohui Kuang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Junhui">Junhui Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Branco:Ant=oacute=nio">António Branco</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Weihua">Weihua Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a></p>
<p>【Abstract】:
In neural machine translation, a source sequence of words is encoded into a vector from which a target sequence is generated in the decoding phase. Differently from statistical machine translation, the associations between source words and their possible target counterparts are not explicitly stored. Source and target words are at the two ends of a long information processing procedure, mediated by hidden states at both the source encoding and the target decoding phases. This makes it possible that a source word is incorrectly translated into a target word that is not any of its admissible equivalent counterparts in the target language. In this paper, we seek to somewhat shorten the distance between source and target words in that procedure, and thus strengthen their association, by means of a method we term bridging source and target word embeddings. We experiment with three strategies: (1) a source-side bridging model, where source word embeddings are moved one step closer to the output target sequence; (2) a target-side bridging model, which explores the more relevant source word embeddings for the prediction of the target sequence; and (3) a direct bridging model, which directly connects source and target word embeddings seeking to minimize errors in the translation of ones by the others. Experiments and analysis presented in this paper demonstrate that the proposed bridging models are able to significantly improve quality of both sentence translation, in general, and alignment and translation of individual source words with target words, in particular.</p>
<p>【Keywords】:</p>
<h3 id="165. Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning.">165. Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1165/">Paper Link</a>】    【Pages】:1777-1788</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kreutzer:Julia">Julia Kreutzer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Uyheng:Joshua">Joshua Uyheng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riezler:Stefan">Stefan Riezler</a></p>
<p>【Abstract】:
We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator α-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.</p>
<p>【Keywords】:</p>
<h3 id="166. Accelerating Neural Transformer via an Average Attention Network.">166. Accelerating Neural Transformer via an Average Attention Network.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1166/">Paper Link</a>】    【Pages】:1789-1798</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0002:Biao">Biao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Deyi">Deyi Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Jinsong">Jinsong Su</a></p>
<p>【Abstract】:
With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.</p>
<p>【Keywords】:</p>
<h3 id="167. How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.">167. How Much Attention Do You Need? A Granular Analysis of Neural Machine Translation Architectures.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1167/">Paper Link</a>】    【Pages】:1799-1808</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Domhan:Tobias">Tobias Domhan</a></p>
<p>【Abstract】:
With recent advances in network architectures for Neural Machine Translation (NMT) recurrent models have effectively been replaced by either convolutional or self-attentional approaches, such as in the Transformer. While the main innovation of the Transformer architecture is its use of self-attentional layers, there are several other aspects, such as attention with multiple heads and the use of many attention layers, that distinguish the model from previous baselines. In this work we take a fine-grained look at the different architectures for NMT. We introduce an Architecture Definition Language (ADL) allowing for a flexible combination of common building blocks. Making use of this language we show in experiments that one can bring recurrent and convolutional models very close to the Transformer performance by borrowing concepts from the Transformer architecture, but not using self-attention. Additionally, we find that self-attention is much more important on the encoder side than on the decoder side, where it can be replaced by a RNN or CNN without a loss in performance in most settings. Surprisingly, even a model without any target side self-attention performs well.</p>
<p>【Keywords】:</p>
<h3 id="168. Weakly Supervised Semantic Parsing with Abstract Examples.">168. Weakly Supervised Semantic Parsing with Abstract Examples.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1168/">Paper Link</a>】    【Pages】:1809-1819</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goldman:Omer">Omer Goldman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Latcinnik:Veronica">Veronica Latcinnik</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nave:Ehud">Ehud Nave</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Globerson:Amir">Amir Globerson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Berant:Jonathan">Jonathan Berant</a></p>
<p>【Abstract】:
Training semantic parsers from weak supervision (denotations) rather than strong supervision (programs) complicates training in two ways. First, a large search space of potential programs needs to be explored at training time to find a correct program. Second, spurious programs that accidentally lead to a correct denotation add noise to training. In this work we propose that in closed worlds with clear semantic types, one can substantially alleviate these problems by utilizing an abstract representation, where tokens in both the language utterance and program are lifted to an abstract form. We show that these abstractions can be defined with a handful of lexical rules and that they result in sharing between different examples that alleviates the difficulties in training. To test our approach, we develop the first semantic parser for CNLVR, a challenging visual reasoning dataset, where the search space is large and overcoming spuriousness is critical, because denotations are either TRUE or FALSE, and thus random programs are likely to lead to a correct denotation. Our method substantially improves performance, and reaches 82.5% accuracy, a 14.7% absolute accuracy improvement compared to the best reported accuracy so far.</p>
<p>【Keywords】:</p>
<h3 id="169. Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback.">169. Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1169/">Paper Link</a>】    【Pages】:1820-1830</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lawrence:Carolin">Carolin Lawrence</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riezler:Stefan">Stefan Riezler</a></p>
<p>【Abstract】:
Counterfactual learning from human bandit feedback describes a scenario where user feedback on the quality of outputs of a historic system is logged and used to improve a target system. We show how to apply this learning framework to neural semantic parsing. From a machine learning perspective, the key challenge lies in a proper reweighting of the estimator so as to avoid known degeneracies in counterfactual learning, while still being applicable to stochastic gradient optimization. To conduct experiments with human users, we devise an easy-to-use interface to collect human feedback on semantic parses. Our work is the first to show that semantic parsers can be improved significantly by counterfactual learning from logged human feedback data.</p>
<p>【Keywords】:</p>
<h3 id="170. AMR dependency parsing with a typed semantic algebra.">170. AMR dependency parsing with a typed semantic algebra.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1170/">Paper Link</a>】    【Pages】:1831-1841</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Groschwitz:Jonas">Jonas Groschwitz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lindemann:Matthias">Matthias Lindemann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fowlie:Meaghan">Meaghan Fowlie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Johnson:Mark">Mark Johnson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Koller:Alexander">Alexander Koller</a></p>
<p>【Abstract】:
We present a semantic parser for Abstract Meaning Representations which learns to parse strings into tree representations of the compositional structure of an AMR graph. This allows us to use standard neural techniques for supertagging and dependency tree parsing, constrained by a linguistically principled type system. We present two approximative decoding algorithms, which achieve state-of-the-art accuracy and outperform strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="171. Sequence-to-sequence Models for Cache Transition Systems.">171. Sequence-to-sequence Models for Cache Transition Systems.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1171/">Paper Link</a>】    【Pages】:1842-1852</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Xiaochang">Xiaochang Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Linfeng">Linfeng Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gildea:Daniel">Daniel Gildea</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Satta:Giorgio">Giorgio Satta</a></p>
<p>【Abstract】:
In this paper, we present a sequence-to-sequence based approach for mapping natural language sentences to AMR semantic graphs. We transform the sequence to graph mapping problem to a word sequence to transition action sequence problem using a special transition system called a cache transition system. To address the sparsity issue of neural AMR parsing, we feed feature embeddings from the transition state to provide relevant local information for each decoder state. We present a monotonic hard attention model for the transition framework to handle the strictly left-to-right alignment between each transition state and the current buffer input focus. We evaluate our neural transition model on the AMR parsing task, and our parser outperforms other sequence-to-sequence approaches and achieves competitive results in comparison with the best-performing models.</p>
<p>【Keywords】:</p>
<h3 id="172. Batch IS NOT Heavy: Learning Word Representations From All Samples.">172. Batch IS NOT Heavy: Learning Word Representations From All Samples.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1172/">Paper Link</a>】    【Pages】:1853-1862</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xin:Xin">Xin Xin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yuan:Fajie">Fajie Yuan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0001:Xiangnan">Xiangnan He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
Stochastic Gradient Descent (SGD) with negative sampling is the most prevalent approach to learn word representations. However, it is known that sampling methods are biased especially when the sampling distribution deviates from the true data distribution. Besides, SGD suffers from dramatic fluctuation due to the one-sample learning scheme. In this work, we propose AllVec that uses batch gradient learning to generate word representations from all training samples. Remarkably, the time complexity of AllVec remains at the same level as SGD, being determined by the number of positive samples rather than all samples. We evaluate AllVec on several benchmark tasks. Experiments show that AllVec outperforms sampling-based SGD methods with comparable efficiency, especially for small training corpora.</p>
<p>【Keywords】:</p>
<h3 id="173. Backpropagating through Structured Argmax using a SPIGOT.">173. Backpropagating through Structured Argmax using a SPIGOT.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1173/">Paper Link</a>】    【Pages】:1863-1873</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Hao">Hao Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Thomson:Sam">Sam Thomson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
We introduce structured projection of intermediate gradients (SPIGOT), a new method for backpropagating through neural networks that include hard-decision structured predictions (e.g., parsing) in intermediate layers. SPIGOT requires no marginal inference, unlike structured attention networks and reinforcement learning-inspired solutions. Like so-called straight-through estimators, SPIGOT defines gradient-like quantities associated with intermediate nondifferentiable operations, allowing backpropagation before and after them; SPIGOT’s proxy aims to ensure that, after a parameter update, the intermediate structure will remain well-formed. We experiment on two structured NLP pipelines: syntactic-then-semantic dependency parsing, and semantic parsing followed by sentiment classification. We show that training with SPIGOT leads to a larger improvement on the downstream task than a modularly-trained pipeline, the straight-through estimator, and structured attention, reaching a new state of the art on semantic dependency parsing.</p>
<p>【Keywords】:</p>
<h3 id="174. Learning How to Actively Learn: A Deep Imitation Learning Approach.">174. Learning How to Actively Learn: A Deep Imitation Learning Approach.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1174/">Paper Link</a>】    【Pages】:1874-1883</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Ming">Ming Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Buntine:Wray_L=">Wray L. Buntine</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Haffari:Gholamreza">Gholamreza Haffari</a></p>
<p>【Abstract】:
Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary. We introduce a method that learns an AL “policy” using “imitation learning” (IL). Our IL-based approach makes use of an efficient and effective “algorithmic expert”, which provides the policy learner with good actions in the encountered AL situations. The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints. We evaluate our method on two different tasks: text classification and named entity recognition. Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.</p>
<p>【Keywords】:</p>
<h3 id="175. Training Classifiers with Natural Language Explanations.">175. Training Classifiers with Natural Language Explanations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1175/">Paper Link</a>】    【Pages】:1884-1895</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hancock:Braden">Braden Hancock</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Varma:Paroma">Paroma Varma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Stephanie">Stephanie Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bringmann:Martin">Martin Bringmann</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Percy">Percy Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/R=eacute=:Christopher">Christopher Ré</a></p>
<p>【Abstract】:
Training accurate classifiers requires many labels, but each label provides only limited information (one bit for binary classification). In this work, we propose BabbleLabble, a framework for training classifiers in which an annotator provides a natural language explanation for each labeling decision. A semantic parser converts these explanations into programmatic labeling functions that generate noisy labels for an arbitrary amount of unlabeled data, which is used to train a classifier. On three relation extraction tasks, we find that users are able to train classifiers with comparable F1 scores from 5-100 faster by providing explanations instead of just labels. Furthermore, given the inherent imperfection of labeling functions, we find that a simple rule-based semantic parser suffices.</p>
<p>【Keywords】:</p>
<h3 id="176. Did the Model Understand the Question?">176. Did the Model Understand the Question?</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1176/">Paper Link</a>】    【Pages】:1896-1906</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mudrakarta:Pramod_Kaushik">Pramod Kaushik Mudrakarta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Taly:Ankur">Ankur Taly</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sundararajan:Mukund">Mukund Sundararajan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dhamdhere:Kedar">Kedar Dhamdhere</a></p>
<p>【Abstract】:
We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of “attribution” (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from 61.1% to 19%, and that of a tabular question answering model from 33.5% to 3.3%. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.</p>
<p>【Keywords】:</p>
<h3 id="177. Harvesting Paragraph-level Question-Answer Pairs from Wikipedia.">177. Harvesting Paragraph-level Question-Answer Pairs from Wikipedia.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1177/">Paper Link</a>】    【Pages】:1907-1917</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Du:Xinya">Xinya Du</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cardie:Claire">Claire Cardie</a></p>
<p>【Abstract】:
We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence. We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism. As compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-the-art. We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top ranking Wikipedia articles and create a corpus of over one million question-answer pairs. We provide qualitative analysis for the this large-scale generated corpus from Wikipedia.</p>
<p>【Keywords】:</p>
<h3 id="178. Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.">178. Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1178/">Paper Link</a>】    【Pages】:1918-1927</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yizhong">Yizhong Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0023:Kai">Kai Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0022:Jing">Jing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He_0014:Wei">Wei He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lyu:Yajuan">Yajuan Lyu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0003:Hua">Hua Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sujian">Sujian Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Haifeng">Haifeng Wang</a></p>
<p>【Abstract】:
Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings.</p>
<p>【Keywords】:</p>
<h3 id="179. Language Generation via DAG Transduction.">179. Language Generation via DAG Transduction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1179/">Paper Link</a>】    【Pages】:1928-1937</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/y/Ye:Yajie">Yajie Ye</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Weiwei">Weiwei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>【Abstract】:
A DAG automaton is a formal device for manipulating graphs. By augmenting a DAG automaton with transduction rules, a DAG transducer has potential applications in fundamental NLP tasks. In this paper, we propose a novel DAG transducer to perform graph-to-program transformation. The target structure of our transducer is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string. Our transducer is designed especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design.</p>
<p>【Keywords】:</p>
<h3 id="180. A Distributional and Orthographic Aggregation Model for English Derivational Morphology.">180. A Distributional and Orthographic Aggregation Model for English Derivational Morphology.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1180/">Paper Link</a>】    【Pages】:1938-1947</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Deutsch:Daniel">Daniel Deutsch</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hewitt:John">John Hewitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Modeling derivational morphology to generate words with particular semantics is useful in many text generation tasks, such as machine translation or abstractive question answering. In this work, we tackle the task of derived word generation. That is, we attempt to generate the word “runner” for “someone who runs.” We identify two key problems in generating derived words from root words and transformations. We contribute a novel aggregation model of derived word generation that learns derivational transformations both as orthographic functions using sequence-to-sequence models and as functions in distributional word embedding space. The model then learns to choose between the hypothesis of each system. We also present two ways of incorporating corpus information into derived word generation.</p>
<p>【Keywords】:</p>
<h3 id="181. Deep-speare: A joint neural model of poetic language, meter and rhyme.">181. Deep-speare: A joint neural model of poetic language, meter and rhyme.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1181/">Paper Link</a>】    【Pages】:1948-1958</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lau:Jey_Han">Jey Han Lau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brooke:Julian">Julian Brooke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hammond:Adam">Adam Hammond</a></p>
<p>【Abstract】:
In this paper, we propose a joint architecture that captures language, rhyme and meter for sonnet modelling. We assess the quality of generated poems using crowd and expert judgements. The stress and rhyme models perform very well, as generated poems are largely indistinguishable from human-written poems. Expert evaluation, however, reveals that a vanilla language model captures meter implicitly, and that machine-generated poems still underperform in terms of readability and emotion. Our research shows the importance expert evaluation for poetry generation, and that future research should look beyond rhyme/meter and focus on poetic language.</p>
<p>【Keywords】:</p>
<h3 id="182. NeuralREG: An end-to-end approach to referring expression generation.">182. NeuralREG: An end-to-end approach to referring expression generation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1182/">Paper Link</a>】    【Pages】:1959-1969</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/Ferreira:Thiago_Castro">Thiago Castro Ferreira</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Moussallem:Diego">Diego Moussallem</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/K=aacute=d=aacute=r:=Aacute=kos">Ákos Kádár</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wubben:Sander">Sander Wubben</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Krahmer:Emiel">Emiel Krahmer</a></p>
<p>【Abstract】:
Traditionally, Referring Expression Generation (REG) models first decide on the form and then on the content of references to discourse entities in text, typically relying on features such as salience and grammatical function. In this paper, we present a new approach (NeuralREG), relying on deep neural networks, which makes decisions about form and content in one go without explicit feature extraction. Using a delexicalized version of the WebNLG corpus, we show that the neural model substantially improves over two strong baselines.</p>
<p>【Keywords】:</p>
<h3 id="183. Stock Movement Prediction from Tweets and Historical Prices.">183. Stock Movement Prediction from Tweets and Historical Prices.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1183/">Paper Link</a>】    【Pages】:1970-1979</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Yumo">Yumo Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a></p>
<p>【Abstract】:
Stock movement prediction is a challenging problem: the market is highly stochastic, and we make temporally-dependent predictions from chaotic data. We treat these three complexities and present a novel deep generative model jointly exploiting text and price signals for this task. Unlike the case with discriminative or topic modeling, our model introduces recurrent, continuous latent variables for a better treatment of stochasticity, and uses neural variational inference to address the intractable posterior inference. We also provide a hybrid objective with temporal auxiliary to flexibly capture predictive dependencies. We demonstrate the state-of-the-art performance of our proposed model on a new stock movement prediction dataset which we collected.</p>
<p>【Keywords】:</p>
<h3 id="184. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.">184. Rumor Detection on Twitter with Tree-structured Recursive Neural Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1184/">Paper Link</a>】    【Pages】:1980-1989</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Ma:Jing">Jing Ma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao_0001:Wei">Wei Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Kam=Fai">Kam-Fai Wong</a></p>
<p>【Abstract】:
Automatic rumor detection is technically very challenging. In this work, we try to learn discriminative features from tweets content by following their non-sequential propagation structure and generate more powerful representations for identifying different type of rumors. We propose two recursive neural models based on a bottom-up and a top-down tree-structured neural networks for rumor representation learning and classification, which naturally conform to the propagation layout of tweets. Results on two public Twitter datasets demonstrate that our recursive neural models 1) achieve much better performance than state-of-the-art approaches; 2) demonstrate superior capacity on detecting rumors at very early stage.</p>
<p>【Keywords】:</p>
<h3 id="185. Visual Attention Model for Name Tagging in Multimodal Social Media.">185. Visual Attention Model for Name Tagging in Multimodal Social Media.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1185/">Paper Link</a>】    【Pages】:1990-1999</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Di">Di Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neves:Leonardo">Leonardo Neves</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carvalho:Vitor">Vitor Carvalho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Ning">Ning Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a></p>
<p>【Abstract】:
Everyday billions of multimodal posts containing both images and text are shared in social media sites such as Snapchat, Twitter or Instagram. This combination of image and text in a single message allows for more creative and expressive forms of communication, and has become increasingly common in such sites. This new paradigm brings new challenges for natural language understanding, as the textual component tends to be shorter, more informal, and often is only understood if combined with the visual context. In this paper, we explore the task of name tagging in multimodal social media posts. We start by creating two new multimodal datasets: the first based on Twitter posts and the second based on Snapchat captions (exclusively submitted to public and crowd-sourced stories). We then propose a novel model architecture based on Visual Attention that not only provides deeper visual understanding on the decisions of the model, but also significantly outperforms other state-of-the-art baseline methods for this task.</p>
<p>【Keywords】:</p>
<h3 id="186. Multimodal Named Entity Disambiguation for Noisy Social Media Posts.">186. Multimodal Named Entity Disambiguation for Noisy Social Media Posts.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1186/">Paper Link</a>】    【Pages】:2000-2008</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Moon:Seungwhan">Seungwhan Moon</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neves:Leonardo">Leonardo Neves</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carvalho:Vitor">Vitor Carvalho</a></p>
<p>【Abstract】:
We introduce the new Multimodal Named Entity Disambiguation (MNED) task for multimodal social media posts such as Snapchat or Instagram captions, which are composed of short captions with accompanying images. Social media posts bring significant challenges for disambiguation tasks because 1) ambiguity not only comes from polysemous entities, but also from inconsistent or incomplete notations, 2) very limited context is provided with surrounding words, and 3) there are many emerging entities often unseen during training. To this end, we build a new dataset called SnapCaptionsKB, a collection of Snapchat image captions submitted to public and crowd-sourced stories, with named entity mentions fully annotated and linked to entities in an external knowledge base. We then build a deep zeroshot multimodal network for MNED that 1) extracts contexts from both text and image, and 2) predicts correct entity in the knowledge graph embeddings space, allowing for zeroshot disambiguation of entities unseen in training set as well. The proposed model significantly outperforms the state-of-the-art text-only NED models, showing efficacy and potentials of the MNED task.</p>
<p>【Keywords】:</p>
<h3 id="187. Semi-supervised User Geolocation via Graph Convolutional Networks.">187. Semi-supervised User Geolocation via Graph Convolutional Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1187/">Paper Link</a>】    【Pages】:2009-2019</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rahimi_0001:Afshin">Afshin Rahimi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohn:Trevor">Trevor Cohn</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baldwin:Timothy">Timothy Baldwin</a></p>
<p>【Abstract】:
Social media user geolocation is vital to many applications such as event detection. In this paper, we propose GCN, a multiview geolocation model based on Graph Convolutional Networks, that uses both text and network context. We compare GCN to the state-of-the-art, and to two baselines we propose, and show that our model achieves or is competitive with the state-of-the-art over three benchmark geolocation datasets when sufficient supervision is available. We also evaluate GCN under a minimal supervision scenario, and show it outperforms baselines. We find that highway network gates are essential for controlling the amount of useful neighbourhood expansion in GCN.</p>
<p>【Keywords】:</p>
<h3 id="188. Document Modeling with External Attention for Sentence Extraction.">188. Document Modeling with External Attention for Sentence Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1188/">Paper Link</a>】    【Pages】:2020-2030</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Narayan:Shashi">Shashi Narayan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cardenas:Ronald">Ronald Cardenas</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Papasarantopoulos:Nikos">Nikos Papasarantopoulos</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cohen:Shay_B=">Shay B. Cohen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lapata:Mirella">Mirella Lapata</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Jiangsheng">Jiangsheng Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a></p>
<p>【Abstract】:
Document modeling is essential to a variety of natural language understanding tasks. We propose to use external information to improve document modeling for problems that can be framed as sentence extraction. We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information. We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question). We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.</p>
<p>【Keywords】:</p>
<h3 id="189. Neural Models for Documents with Metadata.">189. Neural Models for Documents with Metadata.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1189/">Paper Link</a>】    【Pages】:2031-2040</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Card:Dallas">Dallas Card</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tan:Chenhao">Chenhao Tan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:Noah_A=">Noah A. Smith</a></p>
<p>【Abstract】:
Most real-world document collections involve various types of metadata, such as author, source, and date, and yet the most commonly-used approaches to modeling text corpora ignore this information. While specialized models have been developed for particular applications, few are widely used in practice, as customization typically requires derivation of a custom inference algorithm. In this paper, we build on recent advances in variational inference methods and propose a general neural framework, based on topic models, to enable flexible incorporation of metadata and allow for rapid exploration of alternative models. Our approach achieves strong performance, with a manageable tradeoff between perplexity, coherence, and sparsity. Finally, we demonstrate the potential of our framework through an exploration of a corpus of articles about US immigration.</p>
<p>【Keywords】:</p>
<h3 id="190. NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing.">190. NASH: Toward End-to-End Neural Architecture for Generative Semantic Hashing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1190/">Paper Link</a>】    【Pages】:2041-2050</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Dinghan">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Su:Qinliang">Qinliang Su</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chapfuwa:Paidamoyo">Paidamoyo Chapfuwa</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenlin">Wenlin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guoyin">Guoyin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henao:Ricardo">Ricardo Henao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
Semantic hashing has become a powerful paradigm for fast similarity search in many information retrieval systems. While fairly successful, previous techniques generally require two-stage training, and the binary constraints are handled ad-hoc. In this paper, we present an end-to-end Neural Architecture for Semantic Hashing (NASH), where the binary hashing codes are treated as Bernoulli latent variables. A neural variational inference framework is proposed for training, where gradients are directly backpropagated through the discrete latent variable to optimize the hash function. We also draw the connections between proposed method and rate-distortion theory, which provides a theoretical foundation for the effectiveness of our framework. Experimental results on three public datasets demonstrate that our method significantly outperforms several state-of-the-art models on both unsupervised and supervised scenarios.</p>
<p>【Keywords】:</p>
<h3 id="191. Large-Scale QA-SRL Parsing.">191. Large-Scale QA-SRL Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1191/">Paper Link</a>】    【Pages】:2051-2060</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/f/FitzGerald:Nicholas">Nicholas FitzGerald</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Michael:Julian">Julian Michael</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/He:Luheng">Luheng He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zettlemoyer:Luke">Luke Zettlemoyer</a></p>
<p>【Abstract】:
We present a new large-scale corpus of Question-Answer driven Semantic Role Labeling (QA-SRL) annotations, and the first high-quality QA-SRL parser. Our corpus, QA-SRL Bank 2.0, consists of over 250,000 question-answer pairs for over 64,000 sentences across 3 domains and was gathered with a new crowd-sourcing scheme that we show has high precision and good recall at modest cost. We also present neural models for two QA-SRL subtasks: detecting argument spans for a predicate and generating questions to label the semantic relationship. The best models achieve question accuracy of 82.6% and span-level accuracy of 77.6% (under human evaluation) on the full pipelined QA-SRL prediction task. They can also, as we show, be used to gather additional annotations at low cost.</p>
<p>【Keywords】:</p>
<h3 id="192. Syntax for Semantic Role Labeling, To Be, Or Not To Be.">192. Syntax for Semantic Role Labeling, To Be, Or Not To Be.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1192/">Paper Link</a>】    【Pages】:2061-2071</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/He:Shexia">Shexia He</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zuchao">Zuchao Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Hai">Hai Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bai:Hongxiao">Hongxiao Bai</a></p>
<p>【Abstract】:
Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence. Previous studies have shown syntactic information has a remarkable contribution to SRL performance. However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone. This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework. We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information. Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.</p>
<p>【Keywords】:</p>
<h3 id="193. Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation.">193. Situated Mapping of Sequential Instructions to Actions with Single-step Reward Observation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1193/">Paper Link</a>】    【Pages】:2072-2082</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Suhr:Alane">Alane Suhr</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Artzi:Yoav">Yoav Artzi</a></p>
<p>【Abstract】:
We propose a learning approach for mapping context-dependent sequential instructions to actions. We address the problem of discourse and state dependencies with an attention-based model that considers both the history of the interaction and the state of the world. To train from start and goal states without access to demonstrations, we propose SESTRA, a learning algorithm that takes advantage of single-step reward observations and immediate expected reward maximization. We evaluate on the SCONE domains, and show absolute accuracy improvements of 9.8%-25.3% across the domains over approaches that use high-level logical representations.</p>
<p>【Keywords】:</p>
<h3 id="194. Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.">194. Marrying Up Regular Expressions with Neural Networks: A Case Study for Spoken Language Understanding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1194/">Paper Link</a>】    【Pages】:2083-2093</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Bingfeng">Bingfeng Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yansong">Yansong Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0001:Zheng">Zheng Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Songfang">Songfang Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan_0001:Rui">Rui Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Dongyan">Dongyan Zhao</a></p>
<p>【Abstract】:
The success of many natural language processing (NLP) tasks is bound by the number and quality of annotated data, but there is often a shortage of such training data. In this paper, we ask the question: “Can we combine a neural network (NN) with regular expressions (RE) to improve supervised learning for NLP?”. In answer, we develop novel methods to exploit the rich expressiveness of REs at different levels within a NN, showing that the combination significantly enhances the learning effectiveness when a small number of training examples are available. We evaluate our approach by applying it to spoken language understanding for intent detection and slot filling. Experimental results show that our approach is highly effective in exploiting the available training data, giving a clear boost to the RE-unaware NN.</p>
<p>【Keywords】:</p>
<h3 id="195. Token-level and sequence-level loss smoothing for RNN language models.">195. Token-level and sequence-level loss smoothing for RNN language models.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1195/">Paper Link</a>】    【Pages】:2094-2103</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/e/Elbayad:Maha">Maha Elbayad</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Besacier:Laurent">Laurent Besacier</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Verbeek:Jakob">Jakob Verbeek</a></p>
<p>【Abstract】:
Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from ’exposure bias’: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.</p>
<p>【Keywords】:</p>
<h3 id="196. Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers.">196. Numeracy for Language Models: Evaluating and Improving their Ability to Predict Numbers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1196/">Paper Link</a>】    【Pages】:2104-2115</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Spithourakis:Georgios_P=">Georgios P. Spithourakis</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riedel_0001:Sebastian">Sebastian Riedel</a></p>
<p>【Abstract】:
Numeracy is the ability to understand and work with numbers. It is a necessary skill for composing and understanding documents in clinical, scientific, and other technical domains. In this paper, we explore different strategies for modelling numerals with language models, such as memorisation and digit-by-digit composition, and propose a novel neural architecture that uses a continuous probability density function to model numerals from an open vocabulary. Our evaluation on clinical and scientific datasets shows that using hierarchical models to distinguish numerals from words improves a perplexity metric on the subset of numerals by 2 and 4 orders of magnitude, respectively, over non-hierarchical models. A combination of strategies can further improve perplexity. Our continuous probability density function model reduces mean absolute percentage errors by 18% and 54% in comparison to the second best strategy for each dataset, respectively.</p>
<p>【Keywords】:</p>
<h3 id="197. To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness.">197. To Attend or not to Attend: A Case Study on Syntactic Structures for Semantic Relatedness.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1197/">Paper Link</a>】    【Pages】:2116-2125</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gupta:Amulya">Amulya Gupta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Zhu">Zhu Zhang</a></p>
<p>【Abstract】:
With the recent success of Recurrent Neural Networks (RNNs) in Machine Translation (MT), attention mechanisms have become increasingly popular. The purpose of this paper is two-fold; firstly, we propose a novel attention model on Tree Long Short-Term Memory Networks (Tree-LSTMs), a tree-structured generalization of standard LSTM. Secondly, we study the interaction between attention and syntactic structures, by experimenting with three LSTM variants: bidirectional-LSTMs, Constituency Tree-LSTMs, and Dependency Tree-LSTMs. Our models are evaluated on two semantic relatedness tasks: semantic relatedness scoring for sentence pairs (SemEval 2012, Task 6 and SemEval 2014, Task 1) and paraphrase detection for question pairs (Quora, 2017).</p>
<p>【Keywords】:</p>
<h3 id="198. What you can cram into a single \$&!#* vector: Probing sentence embeddings for linguistic properties.">198. What you can cram into a single \$&amp;!#* vector: Probing sentence embeddings for linguistic properties.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1198/">Paper Link</a>】    【Pages】:2126-2136</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Conneau:Alexis">Alexis Conneau</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kruszewski:Germ=aacute=n">Germán Kruszewski</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lample:Guillaume">Guillaume Lample</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Barrault:Lo=iuml=c">Loïc Barrault</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baroni:Marco">Marco Baroni</a></p>
<p>【Abstract】:
Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.</p>
<p>【Keywords】:</p>
<h3 id="199. Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning.">199. Robust Distant Supervision Relation Extraction via Deep Reinforcement Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1199/">Paper Link</a>】    【Pages】:2137-2147</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/q/Qin:Pengda">Pengda Qin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Weiran">Weiran Xu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:William_Yang">William Yang Wang</a></p>
<p>【Abstract】:
Distant supervision has become the standard method for relation extraction. However, even though it is an efficient method, it does not come at no cost—The resulted distantly-supervised training samples are often very noisy. To combat the noise, most of the recent state-of-the-art approaches focus on selecting one-best sentence or calculating soft attention weights over the set of the sentences of one specific entity pair. However, these methods are suboptimal, and the false positive problem is still a key stumbling bottleneck for the performance. We argue that those incorrectly-labeled candidate sentences must be treated with a hard decision, rather than being dealt with soft attention weights. To do this, our paper describes a radical solution—We explore a deep reinforcement learning strategy to generate the false-positive indicator, where we automatically recognize false positives for each relation type without any supervised information. Unlike the removal operation in the previous studies, we redistribute them into the negative examples. The experimental results show that the proposed strategy significantly improves the performance of distant supervision comparing to state-of-the-art systems.</p>
<p>【Keywords】:</p>
<h3 id="200. Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.">200. Interpretable and Compositional Relation Learning by Joint Training with an Autoencoder.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1200/">Paper Link</a>】    【Pages】:2148-2159</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/t/Takahashi:Ryo">Ryo Takahashi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Ran">Ran Tian</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inui:Kentaro">Kentaro Inui</a></p>
<p>【Abstract】:
Embedding models for entities and relations are extremely useful for recovering missing facts in a knowledge base. Intuitively, a relation can be modeled by a matrix mapping entity vectors. However, relations reside on low dimension sub-manifolds in the parameter space of arbitrary matrices – for one reason, composition of two relations M1, M2 may match a third M3 (e.g. composition of relations currency_of_country and country_of_film usually matches currency_of_film_budget), which imposes compositional constraints to be satisfied by the parameters (i.e. M1*M2=M3). In this paper we investigate a dimension reduction technique by training relations jointly with an autoencoder, which is expected to better capture compositional constraints. We achieve state-of-the-art on Knowledge Base Completion tasks with strongly improved Mean Rank, and show that joint training with an autoencoder leads to interpretable sparse codings of relations, helps discovering compositional constraints and benefits from compositional training. Our source code is released at github.com/tianran/glimvec.</p>
<p>【Keywords】:</p>
<h3 id="201. Zero-Shot Transfer Learning for Event Extraction.">201. Zero-Shot Transfer Learning for Event Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1201/">Paper Link</a>】    【Pages】:2160-2170</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Lifu">Lifu Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Kyunghyun">Kyunghyun Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dagan:Ido">Ido Dagan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Riedel:Sebastian">Sebastian Riedel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/v/Voss:Clare_R=">Clare R. Voss</a></p>
<p>【Abstract】:
Most previous supervised event extraction methods have relied on features derived from manual annotations, and thus cannot be applied to new event types without extra annotation effort. We take a fresh look at event extraction and model it as a generic grounding problem: mapping each event mention to a specific type in a target event ontology. We design a transferable architecture of structural and compositional neural networks to jointly represent and map event mentions and types into a shared semantic space. Based on this new framework, we can select, for each event mention, the event type which is semantically closest in this space as its type. By leveraging manual annotations available for a small set of existing event types, our framework can be applied to new unseen event types without additional manual annotations. When tested on 23 unseen event types, our zero-shot framework, without manual annotations, achieved performance comparable to a supervised model trained from 3,000 sentences annotated with 500 event mentions.</p>
<p>【Keywords】:</p>
<h3 id="202. Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction.">202. Recursive Neural Structural Correspondence Network for Cross-domain Aspect and Opinion Co-Extraction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1202/">Paper Link</a>】    【Pages】:2171-2181</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenya">Wenya Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pan:Sinno_Jialin">Sinno Jialin Pan</a></p>
<p>【Abstract】:
Fine-grained opinion analysis aims to extract aspect and opinion terms from each sentence for opinion summarization. Supervised learning methods have proven to be effective for this task. However, in many domains, the lack of labeled data hinders the learning of a precise extraction model. In this case, unsupervised domain adaptation methods are desired to transfer knowledge from the source domain to any unlabeled target domain. In this paper, we develop a novel recursive neural network that could reduce domain shift effectively in word level through syntactic relations. We treat these relations as invariant “pivot information” across domains to build structural correspondences and generate an auxiliary task to predict the relation between any two adjacent words in the dependency tree. In the end, we demonstrate state-of-the-art results on three benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="203. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning.">203. Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy Learning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1203/">Paper Link</a>】    【Pages】:2182-2192</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Baolin">Baolin Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xiujun">Xiujun Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Jingjing">Jingjing Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wong:Kam=Fai">Kam-Fai Wong</a></p>
<p>【Abstract】:
Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings.</p>
<p>【Keywords】:</p>
<h3 id="204. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.">204. Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1204/">Paper Link</a>】    【Pages】:2193-2203</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Yansen">Yansen Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Chenyi">Chenyi Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Minlie">Minlie Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Nie:Liqiang">Liqiang Nie</a></p>
<p>【Abstract】:
Asking good questions in open-domain conversational systems is quite significant but rather untouched. This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics. We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words. Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence. We devise two typed decoders (soft typed decoder and hard typed decoder) in which a type distribution over the three types is estimated and the type distribution is used to modulate the final generation distribution. Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions.</p>
<p>【Keywords】:</p>
<h3 id="205. Personalizing Dialogue Agents: I have a dog, do you have pets too?">205. Personalizing Dialogue Agents: I have a dog, do you have pets too?</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1205/">Paper Link</a>】    【Pages】:2204-2213</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Saizheng">Saizheng Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dinan:Emily">Emily Dinan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/u/Urbanek:Jack">Jack Urbanek</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Szlam:Arthur">Arthur Szlam</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kiela:Douwe">Douwe Kiela</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Weston:Jason">Jason Weston</a></p>
<p>【Abstract】:
Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.</p>
<p>【Keywords】:</p>
<h3 id="206. Efficient Large-Scale Neural Domain Classification with Personalized Attention.">206. Efficient Large-Scale Neural Domain Classification with Personalized Attention.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1206/">Paper Link</a>】    【Pages】:2214-2224</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Young=Bum">Young-Bum Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Dongchan">Dongchan Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kumar:Anjishnu">Anjishnu Kumar</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sarikaya:Ruhi">Ruhi Sarikaya</a></p>
<p>【Abstract】:
In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs). This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment built-in first party domains to rapidly increase domain coverage and overall IPDA capabilities. We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently. Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining. We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency. We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.</p>
<p>【Keywords】:</p>
<h3 id="207. Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment.">207. Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1207/">Paper Link</a>】    【Pages】:2225-2235</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Yue">Yue Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yang:Kangning">Kangning Yang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Shiyu">Shiyu Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Shuhong">Shuhong Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Xinyu">Xinyu Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Marsic:Ivan">Ivan Marsic</a></p>
<p>【Abstract】:
Multimodal affective computing, learning to recognize and interpret human affect and subjective information from multiple data sources, is still a challenge because: (i) it is hard to extract informative features to represent human affects from heterogeneous inputs; (ii) current fusion strategies only fuse different modalities at abstract levels, ignoring time-dependent interactions between modalities. Addressing such issues, we introduce a hierarchical multimodal architecture with attention and word-level fusion to classify utterance-level sentiment and emotion from text and audio data. Our introduced model outperforms state-of-the-art approaches on published datasets, and we demonstrate that our model is able to visualize and interpret synchronized attention over modalities.</p>
<p>【Keywords】:</p>
<h3 id="208. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.">208. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1208/">Paper Link</a>】    【Pages】:2236-2246</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zadeh_0001:Amir">Amir Zadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Paul_Pu">Paul Pu Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Poria:Soujanya">Soujanya Poria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cambria:Erik">Erik Cambria</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morency:Louis=Philippe">Louis-Philippe Morency</a></p>
<p>【Abstract】:
Analyzing human multimodal language is an emerging area of research in NLP. Intrinsically this language is multimodal (heterogeneous), sequential and asynchronous; it consists of the language (words), visual (expressions) and acoustic (paralinguistic) modalities all in the form of asynchronous coordinated sequences. From a resource perspective, there is a genuine need for large scale datasets that allow for in-depth studies of this form of language. In this paper we introduce CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI), the largest dataset of sentiment analysis and emotion recognition to date. Using data from CMU-MOSEI and a novel multimodal fusion technique called the Dynamic Fusion Graph (DFG), we conduct experimentation to exploit how modalities interact with each other in human multimodal language. Unlike previously proposed fusion techniques, DFG is highly interpretable and achieves competative performance when compared to the previous state of the art.</p>
<p>【Keywords】:</p>
<h3 id="209. Efficient Low-rank Multimodal Fusion With Modality-Specific Factors.">209. Efficient Low-rank Multimodal Fusion With Modality-Specific Factors.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1209/">Paper Link</a>】    【Pages】:2247-2256</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Zhun">Zhun Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Ying">Ying Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lakshminarasimhan:Varun_Bharadhwaj">Varun Bharadhwaj Lakshminarasimhan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liang:Paul_Pu">Paul Pu Liang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zadeh_0001:Amir">Amir Zadeh</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Morency:Louis=Philippe">Louis-Philippe Morency</a></p>
<p>【Abstract】:
Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion. The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation. Previous research in this field has exploited the expressiveness of tensors for multimodal representation. However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor. In this paper, we propose the Low-rank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency. We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition. Our model achieves competitive results on all these tasks while drastically reducing computational complexity. Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.</p>
<p>【Keywords】:</p>
<h3 id="210. Discourse Coherence: Concurrent Explicit and Implicit Relations.">210. Discourse Coherence: Concurrent Explicit and Implicit Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1210/">Paper Link</a>】    【Pages】:2257-2267</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rohde:Hannah">Hannah Rohde</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Johnson:Alexander">Alexander Johnson</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Schneider:Nathan">Nathan Schneider</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Webber:Bonnie_L=">Bonnie L. Webber</a></p>
<p>【Abstract】:
Theories of discourse coherence posit relations between discourse segments as a key feature of coherent text. Our prior work suggests that multiple discourse relations can be simultaneously operative between two segments for reasons not predicted by the literature. Here we test how this joint presence can lead participants to endorse seemingly divergent conjunctions (e.g., BUT and SO) to express the link they see between two segments. These apparent divergences are not symptomatic of participant naivety or bias, but arise reliably from the concurrent availability of multiple relations between segments – some available through explicit signals and some via inference. We believe that these new results can both inform future progress in theoretical work on discourse coherence and lead to higher levels of performance in discourse parsing.</p>
<p>【Keywords】:</p>
<h3 id="211. A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text.">211. A Spatial Model for Extracting and Visualizing Latent Discourse Structure in Text.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1211/">Paper Link</a>】    【Pages】:2268-2277</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Srivastava:Shashank">Shashank Srivastava</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jojic:Nebojsa">Nebojsa Jojic</a></p>
<p>【Abstract】:
We present a generative probabilistic model of documents as sequences of sentences, and show that inference in it can lead to extraction of long-range latent discourse structure from a collection of documents. The approach is based on embedding sequences of sentences from longer texts into a 2- or 3-D spatial grids, in which one or two coordinates model smooth topic transitions, while the third captures the sequential nature of the modeled text. A significant advantage of our approach is that the learned models are naturally visualizable and interpretable, as semantic similarity and sequential structure are modeled along orthogonal directions in the grid. We show that the method is effective in capturing discourse structures in narrative text across multiple genres, including biographies, stories, and newswire reports. In particular, our method outperforms or is competitive with state-of-the-art generative approaches on tasks such as predicting the outcome of a story, and sentence ordering.</p>
<p>【Keywords】:</p>
<h3 id="212. Joint Reasoning for Temporal and Causal Relations.">212. Joint Reasoning for Temporal and Causal Relations.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1212/">Paper Link</a>】    【Pages】:2278-2288</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/n/Ning:Qiang">Qiang Ning</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Zhili">Zhili Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wu_0034:Hao">Hao Wu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roth:Dan">Dan Roth</a></p>
<p>【Abstract】:
Understanding temporal and causal relations between events is a fundamental natural language understanding task. Because a cause must occur earlier than its effect, temporal and causal relations are closely related and one relation often dictates the value of the other. However, limited attention has been paid to studying these two relations jointly. This paper presents a joint inference framework for them using constrained conditional models (CCMs). Specifically, we formulate the joint problem as an integer linear programming (ILP) problem, enforcing constraints that are inherent in the nature of time and causality. We show that the joint inference framework results in statistically significant improvement in the extraction of both temporal and causal relations from text.</p>
<p>【Keywords】:</p>
<h3 id="213. Modeling Naive Psychology of Characters in Simple Commonsense Stories.">213. Modeling Naive Psychology of Characters in Simple Commonsense Stories.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1213/">Paper Link</a>】    【Pages】:2289-2299</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rashkin:Hannah">Hannah Rashkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bosselut:Antoine">Antoine Bosselut</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sap:Maarten">Maarten Sap</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Knight:Kevin">Kevin Knight</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Choi:Yejin">Yejin Choi</a></p>
<p>【Abstract】:
Understanding a narrative requires reading between the lines and reasoning about the unspoken but obvious implications about events and people’s mental states — a capability that is trivial for humans but remarkably hard for machines. To facilitate research addressing this challenge, we introduce a new annotation framework to explain naive psychology of story characters as fully-specified chains of mental states with respect to motivations and emotional reactions. Our work presents a new large-scale dataset with rich low-level annotations and establishes baseline performance on several new tasks, suggesting avenues for future research.</p>
<p>【Keywords】:</p>
<h3 id="214. A Deep Relevance Model for Zero-Shot Document Filtering.">214. A Deep Relevance Model for Zero-Shot Document Filtering.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1214/">Paper Link</a>】    【Pages】:2300-2310</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chenliang">Chenliang Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhou:Wei">Wei Zhou</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Ji:Feng">Feng Ji</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Duan:Yu">Yu Duan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Haiqing">Haiqing Chen</a></p>
<p>【Abstract】:
In the era of big data, focused analysis for diverse topics with a short response time becomes an urgent demand. As a fundamental task, information filtering therefore becomes a critical necessity. In this paper, we propose a novel deep relevance model for zero-shot document filtering, named DAZER. DAZER estimates the relevance between a document and a category by taking a small set of seed words relevant to the category. With pre-trained word embeddings from a large external corpus, DAZER is devised to extract the relevance signals by modeling the hidden feature interactions in the word embedding space. The relevance signals are extracted through a gated convolutional process. The gate mechanism controls which convolution filters output the relevance signals in a category dependent manner. Experiments on two document collections of two different tasks (i.e., topic categorization and sentiment analysis) demonstrate that DAZER significantly outperforms the existing alternative solutions, including the state-of-the-art deep relevance ranking models.</p>
<p>【Keywords】:</p>
<h3 id="215. Disconnected Recurrent Neural Networks for Text Categorization.">215. Disconnected Recurrent Neural Networks for Text Categorization.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1215/">Paper Link</a>】    【Pages】:2311-2320</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Baoxin">Baoxin Wang</a></p>
<p>【Abstract】:
Recurrent neural network (RNN) has achieved remarkable performance in text categorization. RNN can model the entire sequence and capture long-term dependencies, but it does not do well in extracting key patterns. In contrast, convolutional neural network (CNN) is good at extracting local and position-invariant features. In this paper, we present a novel model named disconnected recurrent neural network (DRNN), which incorporates position-invariance into RNN. By limiting the distance of information flow in RNN, the hidden state at each time step is restricted to represent words near the current position. The proposed model makes great improvements over RNN and CNN models and achieves the best performance on several benchmark datasets for text categorization.</p>
<p>【Keywords】:</p>
<h3 id="216. Joint Embedding of Words and Labels for Text Classification.">216. Joint Embedding of Words and Labels for Text Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1216/">Paper Link</a>】    【Pages】:2321-2331</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Guoyin">Guoyin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Chunyuan">Chunyuan Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Wenlin">Wenlin Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yizhe">Yizhe Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Dinghan">Dinghan Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Xinyuan">Xinyuan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Henao:Ricardo">Ricardo Henao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Carin:Lawrence">Lawrence Carin</a></p>
<p>【Abstract】:
Word embeddings are effective intermediate representations for capturing semantic regularities between words, when learning the representations of text sequences. We propose to view text classification as a label-word joint embedding problem: each label is embedded in the same space with the word vectors. We introduce an attention framework that measures the compatibility of embeddings between text sequences and labels. The attention is learned on a training set of labeled samples to ensure that, given a text sequence, the relevant words are weighted higher than the irrelevant ones. Our method maintains the interpretability of word embeddings, and enjoys a built-in ability to leverage alternative sources of information, in addition to input text sequences. Extensive results on the several large text datasets show that the proposed framework outperforms the state-of-the-art methods by a large margin, in terms of both accuracy and speed.</p>
<p>【Keywords】:</p>
<h3 id="217. Neural Sparse Topical Coding.">217. Neural Sparse Topical Coding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1217/">Paper Link</a>】    【Pages】:2332-2340</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Min">Min Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Qianqian">Qianqian Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang_0002:Hua">Hua Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Yanchun">Yanchun Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Xiuzhen">Xiuzhen Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Jimin">Jimin Huang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/t/Tian:Gang">Gang Tian</a></p>
<p>【Abstract】:
Topic models with sparsity enhancement have been proven to be effective at learning discriminative and coherent latent topics of short texts, which is critical to many scientific and engineering applications. However, the extensions of these models require carefully tailored graphical models and re-deduced inference algorithms, limiting their variations and applications. We propose a novel sparsity-enhanced topic model, Neural Sparse Topical Coding (NSTC) base on a sparsity-enhanced topic model called Sparse Topical Coding (STC). It focuses on replacing the complex inference process with the back propagation, which makes the model easy to explore extensions. Moreover, the external semantic information of words in word embeddings is incorporated to improve the representation of short texts. To illustrate the flexibility offered by the neural network based framework, we present three extensions base on NSTC without re-deduced inference algorithms. Experiments on Web Snippet and 20Newsgroups datasets demonstrate that our models outperform existing methods.</p>
<p>【Keywords】:</p>
<h3 id="218. Document Similarity for Texts of Varying Lengths via Hidden Topics.">218. Document Similarity for Texts of Varying Lengths via Hidden Topics.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1218/">Paper Link</a>】    【Pages】:2341-2351</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Gong:Hongyu">Hongyu Gong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sakakini:Tarek">Tarek Sakakini</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhat:Suma">Suma Bhat</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Jinjun">Jinjun Xiong</a></p>
<p>【Abstract】:
Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.</p>
<p>【Keywords】:</p>
<h3 id="219. Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour.">219. Eyes are the Windows to the Soul: Predicting the Rating of Text Quality Using Gaze Behaviour.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1219/">Paper Link</a>】    【Pages】:2352-2362</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mathias:Sandeep">Sandeep Mathias</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kanojia:Diptesh">Diptesh Kanojia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Patel:Kevin">Kevin Patel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Agrawal:Samarth">Samarth Agrawal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Pushpak">Pushpak Bhattacharyya</a></p>
<p>【Abstract】:
Predicting a reader’s rating of text quality is a challenging task that involves estimating different subjective aspects of the text, like structure, clarity, etc. Such subjective aspects are better handled using cognitive information. One such source of cognitive information is gaze behaviour. In this paper, we show that gaze behaviour does indeed help in effectively predicting the rating of text quality. To do this, we first we model text quality as a function of three properties - organization, coherence and cohesion. Then, we demonstrate how capturing gaze behaviour helps in predicting each of these properties, and hence the overall quality, by reporting improvements obtained by adding gaze features to traditional textual features for score prediction. We also hypothesize that if a reader has fully understood the text, the corresponding gaze behaviour would give a better indication of the assigned rating, as opposed to partial understanding. Our experiments validate this hypothesis by showing greater agreement between the given rating and the predicted rating when the reader has a full understanding of the text.</p>
<p>【Keywords】:</p>
<h3 id="220. Multi-Input Attention for Unsupervised OCR Correction.">220. Multi-Input Attention for Unsupervised OCR Correction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1220/">Paper Link</a>】    【Pages】:2363-2372</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Rui">Rui Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Smith:David">David Smith</a></p>
<p>【Abstract】:
We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods.</p>
<p>【Keywords】:</p>
<h3 id="221. Building Language Models for Text with Named Entities.">221. Building Language Models for Text with Named Entities.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1221/">Paper Link</a>】    【Pages】:2373-2383</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Parvez:Md=_Rizwan">Md. Rizwan Parvez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chakraborty:Saikat">Saikat Chakraborty</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ray:Baishakhi">Baishakhi Ray</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Kai=Wei">Kai-Wei Chang</a></p>
<p>【Abstract】:
Text in many domains involves a significant amount of named entities. Predicting the entity names is often challenging for a language model as they appear less frequent on the training corpus. In this paper, we propose a novel and effective approach to building a language model which can learn the entity names by leveraging their entity type information. We also introduce two benchmark datasets based on recipes and Java programming codes, on which we evaluate the proposed model. Experimental results show that our model achieves 52.2% better perplexity in recipe generation and 22.06% on code generation than state-of-the-art language models.</p>
<p>【Keywords】:</p>
<h3 id="222. hyperdoc2vec: Distributed Representations of Hypertext Documents.">222. hyperdoc2vec: Distributed Representations of Hypertext Documents.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1222/">Paper Link</a>】    【Pages】:2384-2394</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Han:Jialong">Jialong Han</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Song:Yan">Yan Song</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Wayne_Xin">Wayne Xin Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Shuming">Shuming Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Haisong">Haisong Zhang</a></p>
<p>【Abstract】:
Hypertext documents, such as web pages and academic papers, are of great importance in delivering information in our daily life. Although being effective on plain documents, conventional text embedding methods suffer from information loss if directly adapted to hyper-documents. In this paper, we propose a general embedding approach for hyper-documents, namely, hyperdoc2vec, along with four criteria characterizing necessary information that hyper-document embedding models should preserve. Systematic comparisons are conducted between hyperdoc2vec and several competitors on two tasks, i.e., paper classification and citation recommendation, in the academic paper domain. Analyses and experiments both validate the superiority of hyperdoc2vec to other models w.r.t. the four criteria.</p>
<p>【Keywords】:</p>
<h3 id="223. Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval.">223. Entity-Duet Neural Ranking: Understanding the Role of Knowledge Graph Semantics in Neural Information Retrieval.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1223/">Paper Link</a>】    【Pages】:2395-2405</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Zheng=Hao">Zheng-Hao Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xiong:Chenyan">Chenyan Xiong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a></p>
<p>【Abstract】:
This paper presents the Entity-Duet Neural Ranking Model (EDRM), which introduces knowledge graphs to neural search systems. EDRM represents queries and documents by their words and entity annotations. The semantics from knowledge graphs are integrated in the distributed representations of their entities, while the ranking is conducted by interaction-based neural ranking networks. The two components are learned end-to-end, making EDRM a natural combination of entity-oriented search and neural information retrieval. Our experiments on a commercial search log demonstrate the effectiveness of EDRM. Our analyses reveal that knowledge graph semantics significantly improve the generalization ability of neural ranking models.</p>
<p>【Keywords】:</p>
<h3 id="224. Neural Natural Language Inference Models Enhanced with External Knowledge.">224. Neural Natural Language Inference Models Enhanced with External Knowledge.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1224/">Paper Link</a>】    【Pages】:2406-2417</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Qian">Qian Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Xiaodan">Xiaodan Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Ling:Zhen=Hua">Zhen-Hua Ling</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Inkpen:Diana">Diana Inkpen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wei:Si">Si Wei</a></p>
<p>【Abstract】:
Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which have shown to achieve the state-of-the-art performance. Although there exist relatively large annotated data, can machines learn all knowledge needed to perform natural language inference (NLI) from these data? If not, how can neural-network-based NLI models benefit from external knowledge and how to build NLI models to leverage it? In this paper, we enrich the state-of-the-art neural natural language inference models with external knowledge. We demonstrate that the proposed models improve neural NLI models to achieve the state-of-the-art performance on the SNLI and MultiNLI datasets.</p>
<p>【Keywords】:</p>
<h3 id="225. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples.">225. AdvEntuRe: Adversarial Training for Textual Entailment with Knowledge-Guided Examples.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1225/">Paper Link</a>】    【Pages】:2418-2428</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kang:Dongyeop">Dongyeop Kang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Khot:Tushar">Tushar Khot</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sabharwal:Ashish">Ashish Sabharwal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hovy:Eduard_H=">Eduard H. Hovy</a></p>
<p>【Abstract】:
We consider the problem of learning textual entailment models with limited supervision (5K-10K training examples), and present two complementary approaches for it. First, we propose knowledge-guided adversarial example generators for incorporating large lexical resources in entailment models via only a handful of rule templates. Second, to make the entailment model—a discriminator—more robust, we propose the first GAN-style approach for training it using a natural language example generator that iteratively adjusts to the discriminator’s weaknesses. We demonstrate effectiveness using two entailment datasets, where the proposed methods increase accuracy by 4.7% on SciTail and by 2.8% on a 1% sub-sample of SNLI. Notably, even a single hand-written rule, negate, improves the accuracy of negation examples in SNLI by 6.1%.</p>
<p>【Keywords】:</p>
<h3 id="226. Subword-level Word Vector Representations for Korean.">226. Subword-level Word Vector Representations for Korean.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1226/">Paper Link</a>】    【Pages】:2429-2438</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Park:Sungjoon">Sungjoon Park</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Byun:Jeongmin">Jeongmin Byun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Baek:Sion">Sion Baek</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cho:Yongseok">Yongseok Cho</a> ; <a href="https://dblp.uni-trier.de/pers/hd/o/Oh:Alice">Alice Oh</a></p>
<p>【Abstract】:
Research on distributed word representations is focused on widely-used languages such as English. Although the same methods can be used for other languages, language-specific knowledge can enhance the accuracy and richness of word vector representations. In this paper, we look at improving distributed word representations for Korean using knowledge about the unique linguistic structure of Korean. Specifically, we decompose Korean words into the jamo-level, beyond the character-level, allowing a systematic use of subword information. To evaluate the vectors, we develop Korean test sets for word similarity and analogy and make them publicly available. The results show that our simple method outperforms word2vec and character-level Skip-Grams on semantic and syntactic similarity and analogy tasks and contributes positively toward downstream NLP tasks such as sentiment analysis.</p>
<p>【Keywords】:</p>
<h3 id="227. Incorporating Chinese Characters of Words for Lexical Sememe Prediction.">227. Incorporating Chinese Characters of Words for Lexical Sememe Prediction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1227/">Paper Link</a>】    【Pages】:2439-2449</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jin:Huiming">Huiming Jin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhu:Hao">Hao Zhu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu_0001:Zhiyuan">Zhiyuan Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Ruobing">Ruobing Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Maosong">Maosong Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Fen">Fen Lin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lin:Leyu">Leyu Lin</a></p>
<p>【Abstract】:
Sememes are minimum semantic units of concepts in human languages, such that each word sense is composed of one or multiple sememes. Words are usually manually annotated with their sememes by linguists, and form linguistic common-sense knowledge bases widely used in various NLP tasks. Recently, the lexical sememe prediction task has been introduced. It consists of automatically recommending sememes for words, which is expected to improve annotation efficiency and consistency. However, existing methods of lexical sememe prediction typically rely on the external context of words to represent the meaning, which usually fails to deal with low-frequency and out-of-vocabulary words. To address this issue for Chinese, we propose a novel framework to take advantage of both internal character information and external context information of words. We experiment on HowNet, a Chinese sememe knowledge base, and demonstrate that our framework outperforms state-of-the-art baselines by a large margin, and maintains a robust performance even for low-frequency words.</p>
<p>【Keywords】:</p>
<h3 id="228. SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment.">228. SemAxis: A Lightweight Framework to Characterize Domain-Specific Word Semantics Beyond Sentiment.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1228/">Paper Link</a>】    【Pages】:2450-2461</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/An:Jisun">Jisun An</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kwak:Haewoon">Haewoon Kwak</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Ahn:Yong=Yeol">Yong-Yeol Ahn</a></p>
<p>【Abstract】:
Because word semantics can substantially change across communities and contexts, capturing domain-specific word semantics is an important challenge. Here, we propose SemAxis, a simple yet powerful framework to characterize word semantics using many semantic axes in word-vector spaces beyond sentiment. We demonstrate that SemAxis can capture nuanced semantic representations in multiple online communities. We also show that, when the sentiment axis is examined, SemAxis outperforms the state-of-the-art approaches in building domain-specific sentiment lexicons.</p>
<p>【Keywords】:</p>
<h3 id="229. End-to-End Reinforcement Learning for Automatic Taxonomy Induction.">229. End-to-End Reinforcement Learning for Automatic Taxonomy Induction.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1229/">Paper Link</a>】    【Pages】:2462-2472</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Mao:Yuning">Yuning Mao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Ren:Xiang">Xiang Ren</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shen:Jiaming">Jiaming Shen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gu:Xiaotao">Xiaotao Gu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Han_0001:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
We present a novel end-to-end reinforcement learning approach to automatic taxonomy induction from a set of terms. While prior methods treat the problem as a two-phase task (i.e.,, detecting hypernymy pairs followed by organizing these pairs into a tree-structured hierarchy), we argue that such two-phase methods may suffer from error propagation, and cannot effectively optimize metrics that capture the holistic structure of a taxonomy. In our approach, the representations of term pairs are learned using multiple sources of information and used to determine which term to select and where to place it on the taxonomy via a policy network. All components are trained in an end-to-end manner with cumulative rewards, measured by a holistic tree metric over the training taxonomies. Experiments on two public datasets of different domains show that our approach outperforms prior state-of-the-art taxonomy induction methods up to 19.6% on ancestor F1.</p>
<p>【Keywords】:</p>
<h3 id="230. Incorporating Glosses into Neural Word Sense Disambiguation.">230. Incorporating Glosses into Neural Word Sense Disambiguation.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1230/">Paper Link</a>】    【Pages】:2473-2482</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Luo:Fuli">Fuli Luo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Tianyu">Tianyu Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xia:Qiaolin">Qiaolin Xia</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chang:Baobao">Baobao Chang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sui:Zhifang">Zhifang Sui</a></p>
<p>【Abstract】:
Word Sense Disambiguation (WSD) aims to identify the correct meaning of polysemous words in the particular context. Lexical resources like WordNet which are proved to be of great help for WSD in the knowledge-based methods. However, previous neural networks for WSD always rely on massive labeled data (context), ignoring lexical resources like glosses (sense definitions). In this paper, we integrate the context and glosses of the target word into a unified framework in order to make full use of both labeled data and lexical knowledge. Therefore, we propose GAS: a gloss-augmented WSD neural network which jointly encodes the context and glosses of the target word. GAS models the semantic relationship between the context and the gloss in an improved memory network framework, which breaks the barriers of the previous supervised methods and knowledge-based methods. We further extend the original gloss of word sense via its semantic relations in WordNet to enrich the gloss information. The experimental results show that our model outperforms the state-of-the-art systems on several English all-words WSD datasets.</p>
<p>【Keywords】:</p>
<h3 id="231. Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages.">231. Bilingual Sentiment Embeddings: Joint Projection of Sentiment Across Languages.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1231/">Paper Link</a>】    【Pages】:2483-2493</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Barnes:Jeremy">Jeremy Barnes</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klinger:Roman">Roman Klinger</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Walde:Sabine_Schulte_im">Sabine Schulte im Walde</a></p>
<p>【Abstract】:
Sentiment analysis in low-resource languages suffers from a lack of annotated corpora to estimate high-performing models. Machine translation and bilingual word embeddings provide some relief through cross-lingual sentiment approaches. However, they either require large amounts of parallel data or do not sufficiently capture sentiment information. We introduce Bilingual Sentiment Embeddings (BLSE), which jointly represent sentiment information in a source and target language. This model only requires a small bilingual lexicon, a source-language corpus annotated for sentiment, and monolingual word embeddings for each language. We perform experiments on three language combinations (Spanish, Catalan, Basque) for sentence-level cross-lingual sentiment classification and find that our model significantly outperforms state-of-the-art methods on four out of six experimental setups, as well as capturing complementary information to machine translation. Our analysis of the resulting embedding space provides evidence that it represents sentiment information in the resource-poor target language without any annotated data in that language.</p>
<p>【Keywords】:</p>
<h3 id="232. Learning Domain-Sensitive and Sentiment-Aware Word Embeddings.">232. Learning Domain-Sensitive and Sentiment-Aware Word Embeddings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1232/">Paper Link</a>】    【Pages】:2494-2504</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Bei">Bei Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Fu:Zihao">Zihao Fu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Bing:Lidong">Lidong Bing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lam:Wai">Wai Lam</a></p>
<p>【Abstract】:
Word embeddings have been widely used in sentiment classification because of their efficacy for semantic representations of words. Given reviews from different domains, some existing methods for word embeddings exploit sentiment information, but they cannot produce domain-sensitive embeddings. On the other hand, some other existing methods can generate domain-sensitive word embeddings, but they cannot distinguish words with similar contexts but opposite sentiment polarity. We propose a new method for learning domain-sensitive and sentiment-aware embeddings that simultaneously capture the information of sentiment semantics and domain sensitivity of individual words. Our method can automatically determine and produce domain-common embeddings and domain-specific embeddings. The differentiation of domain-common and domain-specific words enables the advantage of data augmentation of common semantics from multiple domains and capture the varied semantics of specific words from different domains at the same time. Experimental results show that our model provides an effective way to learn domain-sensitive and sentiment-aware word embeddings which benefit sentiment classification at both sentence level and lexicon term level.</p>
<p>【Keywords】:</p>
<h3 id="233. Cross-Domain Sentiment Classification with Target Domain Specific Information.">233. Cross-Domain Sentiment Classification with Target Domain Specific Information.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1233/">Paper Link</a>】    【Pages】:2505-2513</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/p/Peng:Minlong">Minlong Peng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Qi">Qi Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Yu=Gang">Yu-Gang Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Huang:Xuanjing">Xuanjing Huang</a></p>
<p>【Abstract】:
The task of adopting a model with good performance to a target domain that is different from the source domain used for training has received considerable attention in sentiment analysis. Most existing approaches mainly focus on learning representations that are domain-invariant in both the source and target domains. Few of them pay attention to domain-specific information, which should also be informative. In this work, we propose a method to simultaneously extract domain specific and invariant representations and train a classifier on each of the representation, respectively. And we introduce a few target domain labeled data for learning domain-specific information. To effectively utilize the target domain labeled data, we train the domain invariant representation based classifier with both the source and target domain labeled data and train the domain-specific representation based classifier with only the target domain labeled data. These two classifiers then boost each other in a co-training style. Extensive sentiment analysis experiments demonstrated that the proposed method could achieve better performance than state-of-the-art methods.</p>
<p>【Keywords】:</p>
<h3 id="234. Aspect Based Sentiment Analysis with Gated Convolutional Networks.">234. Aspect Based Sentiment Analysis with Gated Convolutional Networks.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1234/">Paper Link</a>】    【Pages】:2514-2523</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/x/Xue:Wei">Wei Xue</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a></p>
<p>【Abstract】:
Aspect based sentiment analysis (ABSA) can provide more detailed information than general sentiment analysis, because it aims to predict the sentiment polarities of the given aspects or entities in text. We summarize previous approaches into two subtasks: aspect-category sentiment analysis (ACSA) and aspect-term sentiment analysis (ATSA). Most previous approaches employ long short-term memory and attention mechanisms to predict the sentiment polarity of the concerned targets, which are often complicated and need more training time. We propose a model based on convolutional neural networks and gating mechanisms, which is more accurate and efficient. First, the novel Gated Tanh-ReLU Units can selectively output the sentiment features according to the given aspect or entity. The architecture is much simpler than attention layer used in the existing models. Second, the computations of our model could be easily parallelized during training, because convolutional layers do not have time dependency as in LSTM layers, and gating units also work independently. The experiments on SemEval datasets demonstrate the efficiency and effectiveness of our models.</p>
<p>【Keywords】:</p>
<h3 id="235. A Helping Hand: Transfer Learning for Deep Sentiment Analysis.">235. A Helping Hand: Transfer Learning for Deep Sentiment Analysis.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1235/">Paper Link</a>】    【Pages】:2524-2534</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/d/Dong:Xin">Xin Dong</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Melo:Gerard_de">Gerard de Melo</a></p>
<p>【Abstract】:
Deep convolutional neural networks excel at sentiment polarity classification, but tend to require substantial amounts of training data, which moreover differs quite significantly between domains. In this work, we present an approach to feed generic cues into the training process of such networks, leading to better generalization abilities given limited training data. We propose to induce sentiment embeddings via supervision on extrinsic data, which are then fed into the model via a dedicated memory-based component. We observe significant gains in effectiveness on a range of different datasets in seven different languages.</p>
<p>【Keywords】:</p>
<h3 id="236. Cold-Start Aware User and Product Attention for Sentiment Classification.">236. Cold-Start Aware User and Product Attention for Sentiment Classification.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1236/">Paper Link</a>】    【Pages】:2535-2544</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Amplayo:Reinald_Kim">Reinald Kim Amplayo</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kim:Jihyeok">Jihyeok Kim</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sung:Sua">Sua Sung</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hwang:Seung=won">Seung-won Hwang</a></p>
<p>【Abstract】:
The use of user/product information in sentiment analysis is important, especially for cold-start users/products, whose number of reviews are very limited. However, current models do not deal with the cold-start problem which is typical in review websites. In this paper, we present Hybrid Contextualized Sentiment Classifier (HCSC), which contains two modules: (1) a fast word encoder that returns word vectors embedded with short and long range dependency features; and (2) Cold-Start Aware Attention (CSAA), an attention mechanism that considers the existence of cold-start problem when attentively pooling the encoded word vectors. HCSC introduces shared vectors that are constructed from similar users/products, and are used when the original distinct vectors do not have sufficient information (i.e. cold-start). This is decided by a frequency-guided selective gate vector. Our experiments show that in terms of RMSE, HCSC performs significantly better when compared with on famous datasets, despite having less complexity, and thus can be trained much faster. More importantly, our model performs significantly better than previous models when the training data is sparse and has cold-start problems.</p>
<p>【Keywords】:</p>
<h3 id="237. Modeling Deliberative Argumentation Strategies on Wikipedia.">237. Modeling Deliberative Argumentation Strategies on Wikipedia.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1237/">Paper Link</a>】    【Pages】:2545-2555</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Khatib:Khalid_Al">Khalid Al Khatib</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wachsmuth:Henning">Henning Wachsmuth</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lang:Kevin">Kevin Lang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Herpel:Jakob">Jakob Herpel</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hagen:Matthias">Matthias Hagen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Stein_0001:Benno">Benno Stein</a></p>
<p>【Abstract】:
This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a model for deliberative discussions and we illustrate its operationalization. Previous models have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our model statistically from several types of metadata that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.</p>
<p>【Keywords】:</p>
<h3 id="238. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.">238. Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1238/">Paper Link</a>】    【Pages】:2556-2565</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Sharma:Piyush">Piyush Sharma</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Ding:Nan">Nan Ding</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Goodman:Sebastian">Sebastian Goodman</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Soricut:Radu">Radu Soricut</a></p>
<p>【Abstract】:
We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.</p>
<p>【Keywords】:</p>
<h3 id="239. Learning Translations via Images with a Massively Multilingual Image Dataset.">239. Learning Translations via Images with a Massively Multilingual Image Dataset.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1239/">Paper Link</a>】    【Pages】:2566-2576</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hewitt:John">John Hewitt</a> ; <a href="https://dblp.uni-trier.de/pers/hd/i/Ippolito:Daphne">Daphne Ippolito</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Callahan:Brendan">Brendan Callahan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kriz:Reno">Reno Kriz</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wijaya:Derry_Tanti">Derry Tanti Wijaya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Callison=Burch:Chris">Chris Callison-Burch</a></p>
<p>【Abstract】:
We conduct the most comprehensive study to date into translating words via images. To facilitate research on the task, we introduce a large-scale multilingual corpus of images, each labeled with the word it represents. Past datasets have been limited to only a few high-resource languages and unrealistically easy translation settings. In contrast, we have collected by far the largest available dataset for this task, with images for approximately 10,000 words in each of 100 languages. We run experiments on a dozen high resource languages and 20 low resources languages, demonstrating the effect of word concreteness and part-of-speech on translation quality. %We find that while image features work best for concrete nouns, they are sometimes effective on other parts of speech. To improve image-based translation, we introduce a novel method of predicting word concreteness from images, which improves on a previous state-of-the-art unsupervised technique. This allows us to predict when image-based translation may be effective, enabling consistent improvements to a state-of-the-art text-based word translation system. Our code and the Massively Multilingual Image Dataset (MMID) are available at <a href="http://multilingual-images.org/">http://multilingual-images.org/</a>.</p>
<p>【Keywords】:</p>
<h3 id="240. On the Automatic Generation of Medical Imaging Reports.">240. On the Automatic Generation of Medical Imaging Reports.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1240/">Paper Link</a>】    【Pages】:2577-2586</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jing:Baoyu">Baoyu Jing</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xie:Pengtao">Pengtao Xie</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
Medical imaging is widely used in clinical practice for diagnosis and treatment. Report-writing can be error-prone for unexperienced physicians, and time-consuming and tedious for experienced physicians. To address these issues, we study the automatic generation of medical imaging reports. This task presents several challenges. First, a complete report contains multiple heterogeneous forms of information, including findings and tags. Second, abnormal regions in medical images are difficult to identify. Third, the reports are typically long, containing multiple sentences. To cope with these challenges, we (1) build a multi-task learning framework which jointly performs the prediction of tags and the generation of paragraphs, (2) propose a co-attention mechanism to localize regions containing abnormalities and generate narrations for them, (3) develop a hierarchical LSTM model to generate long paragraphs. We demonstrate the effectiveness of the proposed methods on two publicly available dataset.</p>
<p>【Keywords】:</p>
<h3 id="241. Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.">241. Attacking Visual Language Grounding with Adversarial Examples: A Case Study on Neural Image Captioning.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1241/">Paper Link</a>】    【Pages】:2587-2597</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Hongge">Hongge Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Huan">Huan Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Pin=Yu">Pin-Yu Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yi:Jinfeng">Jinfeng Yi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hsieh:Cho=Jui">Cho-Jui Hsieh</a></p>
<p>【Abstract】:
Visual language grounding is widely studied in modern neural image captioning systems, which typically adopts an encoder-decoder framework consisting of two principal components: a convolutional neural network (CNN) for image feature extraction and a recurrent neural network (RNN) for language caption generation. To study the robustness of language grounding to adversarial perturbations in machine vision and perception, we propose Show-and-Fool, a novel algorithm for crafting adversarial examples in neural image captioning. The proposed algorithm provides two evaluation approaches, which check if we can mislead neural image captioning systems to output some randomly chosen captions or keywords. Our extensive experiments show that our algorithm can successfully craft visually-similar adversarial examples with randomly targeted captions or keywords, and the adversarial examples can be made highly transferable to other image captioning systems. Consequently, our approach leads to new robustness implications of neural image captioning and novel insights in visual language grounding.</p>
<p>【Keywords】:</p>
<h3 id="242. Think Visually: Question Answering through Virtual Imagery.">242. Think Visually: Question Answering through Virtual Imagery.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1242/">Paper Link</a>】    【Pages】:2598-2608</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/Goyal:Ankit">Ankit Goyal</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wang:Jian">Jian Wang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Deng:Jia">Jia Deng</a></p>
<p>【Abstract】:
In this paper, we study the problem of geometric reasoning (a form of visual reasoning) in the context of question-answering. We introduce Dynamic Spatial Memory Network (DSMN), a new deep network architecture that specializes in answering questions that admit latent visual representations, and learns to generate and reason over such representations. Further, we propose two synthetic benchmarks, FloorPlanQA and ShapeIntersection, to evaluate the geometric reasoning capability of QA systems. Experimental results validate the effectiveness of our proposed DSMN for visual thinking tasks.</p>
<p>【Keywords】:</p>
<h3 id="243. Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game.">243. Interactive Language Acquisition with One-shot Visual Concept Learning through a Conversational Game.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1243/">Paper Link</a>】    【Pages】:2609-2619</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Haichao">Haichao Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yu:Haonan">Haonan Yu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/x/Xu:Wei">Wei Xu</a></p>
<p>【Abstract】:
Building intelligent agents that can communicate with and learn from humans in natural language is of great value. Supervised language learning is limited by the ability of capturing mainly the statistics of training data, and is hardly adaptive to new scenarios or flexible for acquiring new knowledge without inefficient retraining or catastrophic forgetting. We highlight the perspective that conversational interaction serves as a natural interface both for language learning and for novel knowledge acquisition and propose a joint imitation and reinforcement approach for grounded language learning through an interactive conversational game. The agent trained with this approach is able to actively acquire information by asking questions about novel objects and use the just-learned knowledge in subsequent conversations in a one-shot fashion. Results compared with other methods verified the effectiveness of the proposed approach.</p>
<p>【Keywords】:</p>
<h3 id="244. A Purely End-to-End System for Multi-speaker Speech Recognition.">244. A Purely End-to-End System for Multi-speaker Speech Recognition.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1244/">Paper Link</a>】    【Pages】:2620-2630</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/s/Seki:Hiroshi">Hiroshi Seki</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hori:Takaaki">Takaaki Hori</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Watanabe:Shinji">Shinji Watanabe</a> ; <a href="https://dblp.uni-trier.de/pers/hd/r/Roux:Jonathan_Le">Jonathan Le Roux</a> ; <a href="https://dblp.uni-trier.de/pers/hd/h/Hershey:John_R=">John R. Hershey</a></p>
<p>【Abstract】:
Recently, there has been growing interest in multi-speaker speech recognition, where the utterances of multiple speakers are recognized from their mixture. Promising techniques have been proposed for this task, but earlier works have required additional training data such as isolated source signals or senone alignments for effective learning. In this paper, we propose a new sequence-to-sequence framework to directly decode multiple label sequences from a single speech sequence by unifying source separation and speech recognition functions in an end-to-end manner. We further propose a new objective function to improve the contrast between the hidden vectors to avoid generating similar hypotheses. Experimental results show that the model is directly able to learn a mapping from a speech mixture to multiple label sequences, achieving 83.1% relative improvement compared to a model trained without the proposed objective. Interestingly, the results are comparable to those produced by previous end-to-end works featuring explicit separation and recognition modules.</p>
<p>【Keywords】:</p>
<h3 id="245. A Structured Variational Autoencoder for Contextual Morphological Inflection.">245. A Structured Variational Autoencoder for Contextual Morphological Inflection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1245/">Paper Link</a>】    【Pages】:2631-2641</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/w/Wolf=Sonkin:Lawrence">Lawrence Wolf-Sonkin</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Naradowsky:Jason">Jason Naradowsky</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Mielke:Sebastian_J=">Sebastian J. Mielke</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cotterell:Ryan">Ryan Cotterell</a></p>
<p>【Abstract】:
Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.</p>
<p>【Keywords】:</p>
<h3 id="246. Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings.">246. Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1246/">Paper Link</a>】    【Pages】:2642-2652</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/b/Bohnet:Bernd">Bernd Bohnet</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/McDonald:Ryan_T=">Ryan T. McDonald</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sim=otilde=es:Gon=ccedil=alo">Gonçalo Simões</a> ; <a href="https://dblp.uni-trier.de/pers/hd/a/Andor:Daniel">Daniel Andor</a> ; <a href="https://dblp.uni-trier.de/pers/hd/p/Pitler:Emily">Emily Pitler</a> ; <a href="https://dblp.uni-trier.de/pers/hd/m/Maynez:Joshua">Joshua Maynez</a></p>
<p>【Abstract】:
The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with dynamically and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial character and word-based representations. In particular we show that optimal results are obtained by integrating these context sensitive representations through synchronized training with a meta-model that learns to combine their states.</p>
<p>【Keywords】:</p>
<h3 id="247. Neural Factor Graph Models for Cross-lingual Morphological Tagging.">247. Neural Factor Graph Models for Cross-lingual Morphological Tagging.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1247/">Paper Link</a>】    【Pages】:2653-2663</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/m/Malaviya:Chaitanya">Chaitanya Malaviya</a> ; <a href="https://dblp.uni-trier.de/pers/hd/g/Gormley:Matthew_R=">Matthew R. Gormley</a> ; <a href="https://dblp.uni-trier.de/pers/hd/n/Neubig:Graham">Graham Neubig</a></p>
<p>【Abstract】:
Morphological analysis involves predicting the syntactic traits of a word (e.g. POS: Noun, Case: Acc, Gender: Fem). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.</p>
<p>【Keywords】:</p>
<h3 id="248. Global Transition-based Non-projective Dependency Parsing.">248. Global Transition-based Non-projective Dependency Parsing.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1248/">Paper Link</a>】    【Pages】:2664-2675</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/g/G=oacute=mez=Rodr=iacute=guez:Carlos">Carlos Gómez-Rodríguez</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Shi:Tianze">Tianze Shi</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Lee:Lillian">Lillian Lee</a></p>
<p>【Abstract】:
Shi, Huang, and Lee (2017a) obtained state-of-the-art results for English and Chinese dependency parsing by combining dynamic-programming implementations of transition-based dependency parsers with a minimal set of bidirectional LSTM features. However, their results were limited to projective parsing. In this paper, we extend their approach to support non-projectivity by providing the first practical implementation of the MH₄ algorithm, an O(n4) mildly nonprojective dynamic-programming parser with very high coverage on non-projective treebanks. To make MH₄ compatible with minimal transition-based feature sets, we introduce a transition-based interpretation of it in which parser items are mapped to sequences of transitions. We thus obtain the first implementation of global decoding for non-projective transition-based parsing, and demonstrate empirically that it is effective than its projective counterpart in parsing a number of highly non-projective languages.</p>
<p>【Keywords】:</p>
<h3 id="249. Constituency Parsing with a Self-Attentive Encoder.">249. Constituency Parsing with a Self-Attentive Encoder.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1249/">Paper Link</a>】    【Pages】:2676-2686</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/k/Kitaev:Nikita">Nikita Kitaev</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Klein:Dan">Dan Klein</a></p>
<p>【Abstract】:
We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.</p>
<p>【Keywords】:</p>
<h3 id="250. Pre- and In-Parsing Models for Neural Empty Category Detection.">250. Pre- and In-Parsing Models for Neural Empty Category Detection.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1250/">Paper Link</a>】    【Pages】:2687-2696</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Chen:Yufei">Yufei Chen</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhao:Yuanyuan">Yuanyuan Zhao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Sun:Weiwei">Weiwei Sun</a> ; <a href="https://dblp.uni-trier.de/pers/hd/w/Wan_0001:Xiaojun">Xiaojun Wan</a></p>
<p>【Abstract】:
Motivated by the positive impact of empty category on syntactic parsing, we study neural models for pre- and in-parsing detection of empty category, which has not previously been investigated. We find several non-obvious facts: (a) BiLSTM can capture non-local contextual information which is essential for detecting empty categories, (b) even with a BiLSTM, syntactic information is still able to enhance the detection, and (c) automatic detection of empty categories improves parsing quality for overt words. Our neural ECD models outperform the prior state-of-the-art by significant margins.</p>
<p>【Keywords】:</p>
<h3 id="251. Composing Finite State Transducers on GPUs.">251. Composing Finite State Transducers on GPUs.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1251/">Paper Link</a>】    【Pages】:2697-2705</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/a/Argueta:Arturo">Arturo Argueta</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Chiang_0001:David">David Chiang</a></p>
<p>【Abstract】:
Weighted finite state transducers (FSTs) are frequently used in language processing to handle tasks such as part-of-speech tagging and speech recognition. There has been previous work using multiple CPU cores to accelerate finite state algorithms, but limited attention has been given to parallel graphics processing unit (GPU) implementations. In this paper, we introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6 times over our serial implementation and 4.5 times over OpenFST.</p>
<p>【Keywords】:</p>
<h3 id="252. Supervised Treebank Conversion: Data and Approaches.">252. Supervised Treebank Conversion: Data and Approaches.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1252/">Paper Link</a>】    【Pages】:2706-2716</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/j/Jiang:Xinzhou">Xinzhou Jiang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Zhenghua">Zhenghua Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang:Bo">Bo Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zhang_0005:Min">Min Zhang</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Li:Sheng">Sheng Li</a> ; <a href="https://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Treebank conversion is a straightforward and effective way to exploit various heterogeneous treebanks for boosting parsing performance. However, previous work mainly focuses on unsupervised treebank conversion and has made little progress due to the lack of manually labeled data where each sentence has two syntactic trees complying with two different guidelines at the same time, referred as bi-tree aligned data. In this work, we for the first time propose the task of supervised treebank conversion. First, we manually construct a bi-tree aligned dataset containing over ten thousand sentences. Then, we propose two simple yet effective conversion approaches (pattern embedding and treeLSTM) based on the state-of-the-art deep biaffine parser. Experimental results show that 1) the two conversion approaches achieve comparable conversion accuracy, and 2) treebank conversion is superior to the widely used multi-task learning framework in multi-treebank exploitation and leads to significantly higher parsing accuracy.</p>
<p>【Keywords】:</p>
<h3 id="253. Object-oriented Neural Programming (OONP) for Document Understanding.">253. Object-oriented Neural Programming (OONP) for Document Understanding.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1253/">Paper Link</a>】    【Pages】:2717-2726</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/l/Lu:Zhengdong">Zhengdong Lu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/l/Liu:Xianggen">Xianggen Liu</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cui:Haotian">Haotian Cui</a> ; <a href="https://dblp.uni-trier.de/pers/hd/y/Yan:Yukun">Yukun Yan</a> ; <a href="https://dblp.uni-trier.de/pers/hd/z/Zheng:Daqi">Daqi Zheng</a></p>
<p>【Abstract】:
We propose Object-oriented Neural Programming (OONP), a framework for semantically parsing documents in specific domains. Basically, OONP reads a document and parses it into a predesigned object-oriented data structure that reflects the domain-specific semantics of the document. An OONP parser models semantic parsing as a decision process: a neural net-based Reader sequentially goes through the document, and builds and updates an intermediate ontology during the process to summarize its partial understanding of the text. OONP supports a big variety of forms (both symbolic and differentiable) for representing the state and the document, and a rich family of operations to compose the representation. An OONP parser can be trained with supervision of different forms and strength, including supervised learning (SL), reinforcement learning (RL) and hybrid of the two. Our experiments on both synthetic and real-world document parsing tasks have shown that OONP can learn to handle fairly complicated ontology with training data of modest sizes.</p>
<p>【Keywords】:</p>
<h3 id="254. Finding syntax in human encephalography with beam search.">254. Finding syntax in human encephalography with beam search.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1254/">Paper Link</a>】    【Pages】:2727-2736</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/h/Hale:John">John Hale</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Dyer:Chris">Chris Dyer</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kuncoro:Adhiguna">Adhiguna Kuncoro</a> ; <a href="https://dblp.uni-trier.de/pers/hd/b/Brennan:Jonathan">Jonathan Brennan</a></p>
<p>【Abstract】:
Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.</p>
<p>【Keywords】:</p>
<h3 id="255. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information.">255. Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1255/">Paper Link</a>】    【Pages】:2737-2746</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/r/Rao:Sudha">Sudha Rao</a> ; <a href="https://dblp.uni-trier.de/pers/hd/d/Daum=eacute=_III:Hal">Hal Daumé III</a></p>
<p>【Abstract】:
Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions. In this work, we build a neural network model for the task of ranking clarification questions. Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful. We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster. We create a dataset of clarification questions consisting of 77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser. We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.</p>
<p>【Keywords】:</p>
<h3 id="256. Let's do it "again": A First Computational Approach to Detecting Adverbial Presupposition Triggers.">256. Let's do it "again": A First Computational Approach to Detecting Adverbial Presupposition Triggers.</h3>
<p>【<a href="https://www.aclweb.org/anthology/P18-1256/">Paper Link</a>】    【Pages】:2747-2755</p>
<p>【Authors】:
<a href="https://dblp.uni-trier.de/pers/hd/c/Cianflone:Andre">Andre Cianflone</a> ; <a href="https://dblp.uni-trier.de/pers/hd/f/Feng:Yulan">Yulan Feng</a> ; <a href="https://dblp.uni-trier.de/pers/hd/k/Kabbara:Jad">Jad Kabbara</a> ; <a href="https://dblp.uni-trier.de/pers/hd/c/Cheung:Jackie_Chi_Kit">Jackie Chi Kit Cheung</a></p>
<p>【Abstract】:
We introduce the novel task of predicting adverbial presupposition triggers, which is useful for natural language generation tasks such as summarization and dialogue systems. We introduce two new corpora, derived from the Penn Treebank and the Annotated English Gigaword dataset and investigate the use of a novel attention mechanism tailored to this task. Our attention mechanism augments a baseline recurrent neural network without the need for additional trainable parameters, minimizing the added computational cost of our mechanism. We demonstrate that this model statistically outperforms our baselines.</p>
<p>【Keywords】:</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="https://github.com/huntercmd/ccf"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
