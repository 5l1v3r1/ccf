 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="../config/font-awesome-4.3.0/css/font-awesome.min.css" rel="stylesheet">

<link id="light" rel="stylesheet" type="text/css" href="../config/css/light.css" />  
<link id="dark" rel="stylesheet" type="text/css" href="../config/css/dark.css" disabled/>

<script src="../config/css/classie.js"></script>


<!-- This is for Mathjax -->

<script type="text/javascript"
  src="../config/mathjax2.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["$","$"] ],
			displayMath: [ ['$$','$$'], ["$$","$$"] ],
			processEscapes: true
			},
		TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
		"HTML-CSS": {linebreaks: {automatic: true}},
		SVG: {linebreaks: {automatic: true}}
	});
</script>

<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#34. SIGIR 2011:Beijing, China">34. SIGIR 2011:Beijing, China</a><ul>
<li><a href="#Paper Num: 238 || Session Num: 37">Paper Num: 238 || Session Num: 37</a></li>
<li><a href="#Keynote address 1    1">Keynote address 1    1</a><ul>
<li><a href="#1. Future of the web and search.">1. Future of the web and search.</a></li>
</ul>
</li>
<li><a href="#Keynote address 2    1">Keynote address 2    1</a><ul>
<li><a href="#2. Beyond search: statistical topic models for text analysis.">2. Beyond search: statistical topic models for text analysis.</a></li>
</ul>
</li>
<li><a href="#Users 1    4">Users 1    4</a><ul>
<li><a href="#3. Modeling and analysis of cross-session search tasks.">3. Modeling and analysis of cross-session search tasks.</a></li>
<li><a href="#4. The economics in interactive information retrieval.">4. The economics in interactive information retrieval.</a></li>
<li><a href="#5. Seeding simulated queries with user-study data forpersonal search evaluation.">5. Seeding simulated queries with user-study data forpersonal search evaluation.</a></li>
<li><a href="#6. Understanding re-finding behavior in naturalistic email interaction logs.">6. Understanding re-finding behavior in naturalistic email interaction logs.</a></li>
</ul>
</li>
<li><a href="#Query analysis I    4">Query analysis I    4</a><ul>
<li><a href="#7. People searching for people: analysis of a people search engine log.">7. People searching for people: analysis of a people search engine log.</a></li>
<li><a href="#8. Learning search tasks in queries and web pages via graph regularization.">8. Learning search tasks in queries and web pages via graph regularization.</a></li>
<li><a href="#9. Intentions and attention in exploratory health search.">9. Intentions and attention in exploratory health search.</a></li>
<li><a href="#10. User behavior in zero-recall ecommerce queries.">10. User behavior in zero-recall ecommerce queries.</a></li>
</ul>
</li>
<li><a href="#Learning to rank    4">Learning to rank    4</a><ul>
<li><a href="#11. Bagging gradient-boosted trees for high precision, low variance ranking models.">11. Bagging gradient-boosted trees for high precision, low variance ranking models.</a></li>
<li><a href="#12. Learning to rank for freshness and relevance.">12. Learning to rank for freshness and relevance.</a></li>
<li><a href="#13. A cascade ranking model for efficient ranked retrieval.">13. A cascade ranking model for efficient ranked retrieval.</a></li>
<li><a href="#14. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation.">14. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation.</a></li>
</ul>
</li>
<li><a href="#Personalization    3">Personalization    3</a><ul>
<li><a href="#15. SCENE: a scalable two-stage personalized news recommendation system.">15. SCENE: a scalable two-stage personalized news recommendation system.</a></li>
<li><a href="#16. Inferring and using location metadata to personalize web search.">16. Inferring and using location metadata to personalize web search.</a></li>
<li><a href="#17. Active learning to maximize accuracy vs. effort in interactive information retrieval.">17. Active learning to maximize accuracy vs. effort in interactive information retrieval.</a></li>
</ul>
</li>
<li><a href="#Retrieval models I    3">Retrieval models I    3</a><ul>
<li><a href="#18. CRTER: using cross terms to enhance probabilistic information retrieval.">18. CRTER: using cross terms to enhance probabilistic information retrieval.</a></li>
<li><a href="#19. A boosting approach to improving pseudo-relevance feedback.">19. A boosting approach to improving pseudo-relevance feedback.</a></li>
<li><a href="#20. Enhancing ad-hoc relevance weighting using probability density estimation.">20. Enhancing ad-hoc relevance weighting using probability density estimation.</a></li>
</ul>
</li>
<li><a href="#Social media    3">Social media    3</a><ul>
<li><a href="#21. Who should share what?: item-level social influence prediction for users and posts ranking.">21. Who should share what?: item-level social influence prediction for users and posts ranking.</a></li>
<li><a href="#22. Mining tags using social endorsement networks.">22. Mining tags using social endorsement networks.</a></li>
<li><a href="#23. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking.">23. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking.</a></li>
</ul>
</li>
<li><a href="#Content analysis    4">Content analysis    4</a><ul>
<li><a href="#24. A site oriented method for segmenting web pages.">24. A site oriented method for segmenting web pages.</a></li>
<li><a href="#25. Composite hashing with multiple information sources.">25. Composite hashing with multiple information sources.</a></li>
<li><a href="#26. Detecting outlier sections in us congressional legislation.">26. Detecting outlier sections in us congressional legislation.</a></li>
<li><a href="#27. DOM based content extraction via text density.">27. DOM based content extraction via text density.</a></li>
</ul>
</li>
<li><a href="#Web IR    4">Web IR    4</a><ul>
<li><a href="#28. Social context summarization.">28. Social context summarization.</a></li>
<li><a href="#29. Probabilistic factor models for web site recommendation.">29. Probabilistic factor models for web site recommendation.</a></li>
<li><a href="#30. Efficiently collecting relevance information from clickthroughs for web retrieval system evaluation.">30. Efficiently collecting relevance information from clickthroughs for web retrieval system evaluation.</a></li>
<li><a href="#31. Unsupervised query segmentation using clickthrough for information retrieval.">31. Unsupervised query segmentation using clickthrough for information retrieval.</a></li>
</ul>
</li>
<li><a href="#Collaborative filtering I    4">Collaborative filtering I    4</a><ul>
<li><a href="#32. Collaborative competitive filtering: learning recommender using context of user choice.">32. Collaborative competitive filtering: learning recommender using context of user choice.</a></li>
<li><a href="#33. CLR: a collaborative location recommendation framework based on co-clustering.">33. CLR: a collaborative location recommendation framework based on co-clustering.</a></li>
<li><a href="#34. Functional matrix factorizations for cold-start recommendation.">34. Functional matrix factorizations for cold-start recommendation.</a></li>
<li><a href="#35. Exploiting geographical influence for collaborative point-of-interest recommendation.">35. Exploiting geographical influence for collaborative point-of-interest recommendation.</a></li>
</ul>
</li>
<li><a href="#Users II    4">Users II    4</a><ul>
<li><a href="#36. Why searchers switch: understanding and predicting engine switching rationales.">36. Why searchers switch: understanding and predicting engine switching rationales.</a></li>
<li><a href="#37. Find it if you can: a game for modeling different types of web search success using interaction data.">37. Find it if you can: a game for modeling different types of web search success using interaction data.</a></li>
<li><a href="#38. Measuring improvement in user search performance resulting from optimal search tips.">38. Measuring improvement in user search performance resulting from optimal search tips.</a></li>
<li><a href="#39. ViewSer: enabling large-scale remote user studies of web search examination and interaction.">39. ViewSer: enabling large-scale remote user studies of web search examination and interaction.</a></li>
</ul>
</li>
<li><a href="#Query analysis II    4">Query analysis II    4</a><ul>
<li><a href="#40. CrowdLogging: distributed, private, and anonymous search logging.">40. CrowdLogging: distributed, private, and anonymous search logging.</a></li>
<li><a href="#41. Out of sight, not out of mind: on the effect of social and physical detachment on information need.">41. Out of sight, not out of mind: on the effect of social and physical detachment on information need.</a></li>
<li><a href="#42. Scalable multi-dimensional user intent identification using tree structured distributions.">42. Scalable multi-dimensional user intent identification using tree structured distributions.</a></li>
<li><a href="#43. Social annotation in query expansion: a machine learning approach.">43. Social annotation in query expansion: a machine learning approach.</a></li>
</ul>
</li>
<li><a href="#Communities    4">Communities    4</a><ul>
<li><a href="#44. Predicting web searcher satisfaction with existing community-based answers.">44. Predicting web searcher satisfaction with existing community-based answers.</a></li>
<li><a href="#45. Competition-based user expertise score estimation.">45. Competition-based user expertise score estimation.</a></li>
<li><a href="#46. Learning online discussion structures by conditional random fields.">46. Learning online discussion structures by conditional random fields.</a></li>
<li><a href="#47. Mining topics on participations for community discovery.">47. Mining topics on participations for community discovery.</a></li>
</ul>
</li>
<li><a href="#Classification    3">Classification    3</a><ul>
<li><a href="#48. Authorship classification: a discriminative syntactic tree mining approach.">48. Authorship classification: a discriminative syntactic tree mining approach.</a></li>
<li><a href="#49. On theme location discovery for travelogue services.">49. On theme location discovery for travelogue services.</a></li>
<li><a href="#50. Effective sentiment stream analysis with self-augmenting training and demand-driven projection.">50. Effective sentiment stream analysis with self-augmenting training and demand-driven projection.</a></li>
</ul>
</li>
<li><a href="#Retrieval models II    3">Retrieval models II    3</a><ul>
<li><a href="#51. Hypergeometric language models for republished article finding.">51. Hypergeometric language models for republished article finding.</a></li>
<li><a href="#52. Estimation methods for ranking recent information.">52. Estimation methods for ranking recent information.</a></li>
<li><a href="#53. Query by document via a decomposition-based two-level retrieval approach.">53. Query by document via a decomposition-based two-level retrieval approach.</a></li>
</ul>
</li>
<li><a href="#Image search    3">Image search    3</a><ul>
<li><a href="#54. Integrating hierarchical feature selection and classifier training for multi-label image annotation.">54. Integrating hierarchical feature selection and classifier training for multi-label image annotation.</a></li>
<li><a href="#55. Efficient manifold ranking for image retrieval.">55. Efficient manifold ranking for image retrieval.</a></li>
<li><a href="#56. Mining weakly labeled web facial images for search-based face annotation.">56. Mining weakly labeled web facial images for search-based face annotation.</a></li>
</ul>
</li>
<li><a href="#Indexing    4">Indexing    4</a><ul>
<li><a href="#57. Temporal index sharding for space-time efficiency in archive search.">57. Temporal index sharding for space-time efficiency in archive search.</a></li>
<li><a href="#58. Inverted indexes for phrases and strings.">58. Inverted indexes for phrases and strings.</a></li>
<li><a href="#59. Faster temporal range queries over versioned text.">59. Faster temporal range queries over versioned text.</a></li>
<li><a href="#60. Indexing strategies for graceful degradation of search quality.">60. Indexing strategies for graceful degradation of search quality.</a></li>
</ul>
</li>
<li><a href="#Web queries    4">Web queries    4</a><ul>
<li><a href="#61. Incremental diversification for very large sets: a streaming-based approach.">61. Incremental diversification for very large sets: a streaming-based approach.</a></li>
<li><a href="#62. Intent-aware search result diversification.">62. Intent-aware search result diversification.</a></li>
<li><a href="#63. Parameterized concept weighting in verbose queries.">63. Parameterized concept weighting in verbose queries.</a></li>
<li><a href="#64. UPS: efficient privacy protection in personalized web search.">64. UPS: efficient privacy protection in personalized web search.</a></li>
</ul>
</li>
<li><a href="#Collaborative filtering II    4">Collaborative filtering II    4</a><ul>
<li><a href="#65. Handling data sparsity in collaborative filtering using emotion and semantic based features.">65. Handling data sparsity in collaborative filtering using emotion and semantic based features.</a></li>
<li><a href="#66. Fast context-aware recommendations with factorization machines.">66. Fast context-aware recommendations with factorization machines.</a></li>
<li><a href="#67. Filtering semi-structured documents based on faceted feedback.">67. Filtering semi-structured documents based on faceted feedback.</a></li>
<li><a href="#68. Learning relevance from heterogeneous social network and its application in online targeting.">68. Learning relevance from heterogeneous social network and its application in online targeting.</a></li>
</ul>
</li>
<li><a href="#Latent semantic analysis    3">Latent semantic analysis    3</a><ul>
<li><a href="#69. ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews.">69. ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews.</a></li>
<li><a href="#70. Clickthrough-based latent semantic models for web search.">70. Clickthrough-based latent semantic models for web search.</a></li>
<li><a href="#71. Regularized latent semantic indexing.">71. Regularized latent semantic indexing.</a></li>
</ul>
</li>
<li><a href="#Multimedia IR    3">Multimedia IR    3</a><ul>
<li><a href="#72. Multimedia answering: enriching text QA with media information.">72. Multimedia answering: enriching text QA with media information.</a></li>
<li><a href="#73. Enhancing multi-label music genre classification through ensemble techniques.">73. Enhancing multi-label music genre classification through ensemble techniques.</a></li>
<li><a href="#74. Picasso - to sing, you must close your eyes and draw.">74. Picasso - to sing, you must close your eyes and draw.</a></li>
</ul>
</li>
<li><a href="#Summarization    3">Summarization    3</a><ul>
<li><a href="#75. Enhanced results for web search.">75. Enhanced results for web search.</a></li>
<li><a href="#76. Summarizing the differences in multilingual news.">76. Summarizing the differences in multilingual news.</a></li>
<li><a href="#77. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.">77. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</a></li>
</ul>
</li>
<li><a href="#Vertical & entity search    4">Vertical &amp; entity search    4</a><ul>
<li><a href="#78. Ranking related news predictions.">78. Ranking related news predictions.</a></li>
<li><a href="#79. Collective entity linking in web text: a graph-based method.">79. Collective entity linking in web text: a graph-based method.</a></li>
<li><a href="#80. From one tree to a forest: a unified solution for structured web data extraction.">80. From one tree to a forest: a unified solution for structured web data extraction.</a></li>
<li><a href="#81. Improving local search ranking through external logs.">81. Improving local search ranking through external logs.</a></li>
</ul>
</li>
<li><a href="#Query suggestions    4">Query suggestions    4</a><ul>
<li><a href="#82. Query suggestions in the absence of query logs.">82. Query suggestions in the absence of query logs.</a></li>
<li><a href="#83. Synthesizing high utility suggestions for rare web search queries.">83. Synthesizing high utility suggestions for rare web search queries.</a></li>
<li><a href="#84. Post-ranking query suggestion by diversifying search results.">84. Post-ranking query suggestion by diversifying search results.</a></li>
<li><a href="#85. Automatic boolean query suggestion for professional search.">85. Automatic boolean query suggestion for professional search.</a></li>
</ul>
</li>
<li><a href="#Linguistic analysis    4">Linguistic analysis    4</a><ul>
<li><a href="#86. Improved video categorization from text metadata and user comments.">86. Improved video categorization from text metadata and user comments.</a></li>
<li><a href="#87. Multifaceted toponym recognition for streaming news.">87. Multifaceted toponym recognition for streaming news.</a></li>
<li><a href="#88. Enriching document representation via translation for improved monolingual information retrieval.">88. Enriching document representation via translation for improved monolingual information retrieval.</a></li>
<li><a href="#89. A novel corpus-based stemming algorithm using co-occurrence statistics.">89. A novel corpus-based stemming algorithm using co-occurrence statistics.</a></li>
</ul>
</li>
<li><a href="#Clustering    3">Clustering    3</a><ul>
<li><a href="#90. Document clustering with universum.">90. Document clustering with universum.</a></li>
<li><a href="#91. Identifying points of interest by self-tuning clustering.">91. Identifying points of interest by self-tuning clustering.</a></li>
<li><a href="#92. Cluster-based fusion of retrieved lists.">92. Cluster-based fusion of retrieved lists.</a></li>
</ul>
</li>
<li><a href="#Effectiveness    3">Effectiveness    3</a><ul>
<li><a href="#93. System effectiveness, user models, and user utility: a conceptual framework for investigation.">93. System effectiveness, user models, and user utility: a conceptual framework for investigation.</a></li>
<li><a href="#94. Evaluating the synergic effect of collaboration in information seeking.">94. Evaluating the synergic effect of collaboration in information seeking.</a></li>
<li><a href="#95. Repeatable and reliable search system evaluation using crowdsourcing.">95. Repeatable and reliable search system evaluation using crowdsourcing.</a></li>
</ul>
</li>
<li><a href="#Multilingual IR    3">Multilingual IR    3</a><ul>
<li><a href="#96. Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization.">96. Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization.</a></li>
<li><a href="#97. No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity.">97. No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity.</a></li>
<li><a href="#98. An event-centric model for multilingual document similarity.">98. An event-centric model for multilingual document similarity.</a></li>
</ul>
</li>
<li><a href="#Efficiency    4">Efficiency    4</a><ul>
<li><a href="#99. Posting list intersection on multicore architectures.">99. Posting list intersection on multicore architectures.</a></li>
<li><a href="#100. Timestamp-based result cache invalidation for web search engines.">100. Timestamp-based result cache invalidation for web search engines.</a></li>
<li><a href="#101. Energy-price-driven query processing in multi-center web search engines.">101. Energy-price-driven query processing in multi-center web search engines.</a></li>
<li><a href="#102. Faster top-k document retrieval using block-max indexes.">102. Faster top-k document retrieval using block-max indexes.</a></li>
</ul>
</li>
<li><a href="#Recommender systems    4">Recommender systems    4</a><ul>
<li><a href="#103. Utilizing marginal net utility for recommendation in e-commerce.">103. Utilizing marginal net utility for recommendation in e-commerce.</a></li>
<li><a href="#104. Recommending ephemeral items at web scale.">104. Recommending ephemeral items at web scale.</a></li>
<li><a href="#105. A unified framework for recommendations based on quaternary semantic analysis.">105. A unified framework for recommendations based on quaternary semantic analysis.</a></li>
<li><a href="#106. Associative tag recommendation exploiting multiple textual features.">106. Associative tag recommendation exploiting multiple textual features.</a></li>
</ul>
</li>
<li><a href="#Test collections    4">Test collections    4</a><ul>
<li><a href="#107. Evaluating diversified search results using per-intent graded relevance.">107. Evaluating diversified search results using per-intent graded relevance.</a></li>
<li><a href="#108. Evaluating multi-query sessions.">108. Evaluating multi-query sessions.</a></li>
<li><a href="#109. Quantifying test collection quality based on the consistency of relevance judgements.">109. Quantifying test collection quality based on the consistency of relevance judgements.</a></li>
<li><a href="#110. Pseudo test collections for learning web search ranking functions.">110. Pseudo test collections for learning web search ranking functions.</a></li>
</ul>
</li>
<li><a href="#Posters presentations    90">Posters presentations    90</a><ul>
<li><a href="#111. Parallel learning to rank for information retrieval.">111. Parallel learning to rank for information retrieval.</a></li>
<li><a href="#112. Learning features through feedback for blog distillation.">112. Learning features through feedback for blog distillation.</a></li>
<li><a href="#113. Time-based relevance models.">113. Time-based relevance models.</a></li>
<li><a href="#114. Improved query performance prediction using standard deviation.">114. Improved query performance prediction using standard deviation.</a></li>
<li><a href="#115. Learning to rank using query-level regression.">115. Learning to rank using query-level regression.</a></li>
<li><a href="#116. Diversifying product search results.">116. Diversifying product search results.</a></li>
<li><a href="#117. Ad hoc IR: not much room for improvement.">117. Ad hoc IR: not much room for improvement.</a></li>
<li><a href="#118. Image annotation based on recommendation model.">118. Image annotation based on recommendation model.</a></li>
<li><a href="#119. Utilizing minimal relevance feedback for ad hoc retrieval.">119. Utilizing minimal relevance feedback for ad hoc retrieval.</a></li>
<li><a href="#120. Sense discrimination for physics retrieval.">120. Sense discrimination for physics retrieval.</a></li>
<li><a href="#121. When documents are very long, BM25 fails!">121. When documents are very long, BM25 fails!</a></li>
<li><a href="#122. Location and timeliness of information sources during news events.">122. Location and timeliness of information sources during news events.</a></li>
<li><a href="#123. What deliberately degrading search quality tells us about discount functions.">123. What deliberately degrading search quality tells us about discount functions.</a></li>
<li><a href="#124. Collective topic modeling for heterogeneous networks.">124. Collective topic modeling for heterogeneous networks.</a></li>
<li><a href="#125. Graph-cut based tag enrichment.">125. Graph-cut based tag enrichment.</a></li>
<li><a href="#126. Personalized social query expansion using social bookmarking systems.">126. Personalized social query expansion using social bookmarking systems.</a></li>
<li><a href="#127. What are the real differences of children's and adults' web search.">127. What are the real differences of children's and adults' web search.</a></li>
<li><a href="#128. Cognitive coordinating behaviors in multitasking web search.">128. Cognitive coordinating behaviors in multitasking web search.</a></li>
<li><a href="#129. Optimizing multimodal reranking for web image search.">129. Optimizing multimodal reranking for web image search.</a></li>
<li><a href="#130. Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce.">130. Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce.</a></li>
<li><a href="#131. Tackling class imbalance and data scarcity in literature-based gene function annotation.">131. Tackling class imbalance and data scarcity in literature-based gene function annotation.</a></li>
<li><a href="#132. Bootstrapping subjectivity detection.">132. Bootstrapping subjectivity detection.</a></li>
<li><a href="#133. The effects of choice in routing relevance judgments.">133. The effects of choice in routing relevance judgments.</a></li>
<li><a href="#134. Statistical feature extraction for cross-language web content quality assessment.">134. Statistical feature extraction for cross-language web content quality assessment.</a></li>
<li><a href="#135. Exploiting endorsement information and social influence for item recommendation.">135. Exploiting endorsement information and social influence for item recommendation.</a></li>
<li><a href="#136. Modeling subset distributions for verbose queries.">136. Modeling subset distributions for verbose queries.</a></li>
<li><a href="#137. Domain expert topic familiarity and search behavior.">137. Domain expert topic familiarity and search behavior.</a></li>
<li><a href="#138. Sample selection for dictionary-based corpus compression.">138. Sample selection for dictionary-based corpus compression.</a></li>
<li><a href="#139. Evaluating medical information retrieval.">139. Evaluating medical information retrieval.</a></li>
<li><a href="#140. Region-based landmark discovery by crowdsourcing geo-referenced photos.">140. Region-based landmark discovery by crowdsourcing geo-referenced photos.</a></li>
<li><a href="#141. Towards effective short text deep classification.">141. Towards effective short text deep classification.</a></li>
<li><a href="#142. Temporal latent semantic analysis for collaboratively generated content: preliminary results.">142. Temporal latent semantic analysis for collaboratively generated content: preliminary results.</a></li>
<li><a href="#143. Self-adjusting hybrid recommenders based on social network analysis.">143. Self-adjusting hybrid recommenders based on social network analysis.</a></li>
<li><a href="#144. BlogCast effect on information diffusion in a blogosphere.">144. BlogCast effect on information diffusion in a blogosphere.</a></li>
<li><a href="#145. Product comparison using comparative relations.">145. Product comparison using comparative relations.</a></li>
<li><a href="#146. Collaborative cyberporn filtering with collective intelligence.">146. Collaborative cyberporn filtering with collective intelligence.</a></li>
<li><a href="#147. Do IR models satisfy the TDC retrieval constraint.">147. Do IR models satisfy the TDC retrieval constraint.</a></li>
<li><a href="#148. On diversifying and personalizing web search.">148. On diversifying and personalizing web search.</a></li>
<li><a href="#149. Semantic tag recommendation using concept model.">149. Semantic tag recommendation using concept model.</a></li>
<li><a href="#150. Recommending interesting activity-related local entities.">150. Recommending interesting activity-related local entities.</a></li>
<li><a href="#151. Cross-corpus relevance projection.">151. Cross-corpus relevance projection.</a></li>
<li><a href="#152. Location disambiguation for geo-tagged images.">152. Location disambiguation for geo-tagged images.</a></li>
<li><a href="#153. Towards an indexing method to speed-up music retrieval.">153. Towards an indexing method to speed-up music retrieval.</a></li>
<li><a href="#154. An investigation of decompounding for cross-language patent search.">154. An investigation of decompounding for cross-language patent search.</a></li>
<li><a href="#155. Detecting seasonal queries by time-series analysis.">155. Detecting seasonal queries by time-series analysis.</a></li>
<li><a href="#156. Learning to rank under tight budget constraints.">156. Learning to rank under tight budget constraints.</a></li>
<li><a href="#157. A novel hybrid index structure for efficient text retrieval.">157. A novel hybrid index structure for efficient text retrieval.</a></li>
<li><a href="#158. A weighted curve fitting method for result merging in federated search.">158. A weighted curve fitting method for result merging in federated search.</a></li>
<li><a href="#159. Effect of different docid orderings on dynamic pruning retrieval strategies.">159. Effect of different docid orderings on dynamic pruning retrieval strategies.</a></li>
<li><a href="#160. Time-based query performance predictors.">160. Time-based query performance predictors.</a></li>
<li><a href="#161. Search task difficulty: the expected vs. the reflected.">161. Search task difficulty: the expected vs. the reflected.</a></li>
<li><a href="#162. On the suitability of diversity metrics for learning-to-rank for diversity.">162. On the suitability of diversity metrics for learning-to-rank for diversity.</a></li>
<li><a href="#163. How diverse are web search results?">163. How diverse are web search results?</a></li>
<li><a href="#164. Analysis of an expert search query log.">164. Analysis of an expert search query log.</a></li>
<li><a href="#165. A model for expert finding in social networks.">165. A model for expert finding in social networks.</a></li>
<li><a href="#166. Transductive learning over automatically detected themes for multi-document summarization.">166. Transductive learning over automatically detected themes for multi-document summarization.</a></li>
<li><a href="#167. Rating-based collaborative filtering combined with additional regularization.">167. Rating-based collaborative filtering combined with additional regularization.</a></li>
<li><a href="#168. Words-of-interest selection based on temporal motion coherence for video retrieval.">168. Words-of-interest selection based on temporal motion coherence for video retrieval.</a></li>
<li><a href="#169. Aggregating multiple opinion evidence in proximity-based opinion retrieval.">169. Aggregating multiple opinion evidence in proximity-based opinion retrieval.</a></li>
<li><a href="#170. Enhancing mobile search using web search log data.">170. Enhancing mobile search using web search log data.</a></li>
<li><a href="#171. Award prediction with temporal citation network analysis.">171. Award prediction with temporal citation network analysis.</a></li>
<li><a href="#172. Rating prediction using feature words extracted from customer reviews.">172. Rating prediction using feature words extracted from customer reviews.</a></li>
<li><a href="#173. Ranking tags in resource collections.">173. Ranking tags in resource collections.</a></li>
<li><a href="#174. Identifying similar people in professional social networks with discriminative probabilistic models.">174. Identifying similar people in professional social networks with discriminative probabilistic models.</a></li>
<li><a href="#175. Intent-oriented diversity in recommender systems.">175. Intent-oriented diversity in recommender systems.</a></li>
<li><a href="#176. Disambiguating biomedical acronyms using EMIM.">176. Disambiguating biomedical acronyms using EMIM.</a></li>
<li><a href="#177. Best document selection based on approximate utility optimization.">177. Best document selection based on approximate utility optimization.</a></li>
<li><a href="#178. Forecasting counts of user visits for online display advertising with probabilistic latent class models.">178. Forecasting counts of user visits for online display advertising with probabilistic latent class models.</a></li>
<li><a href="#179. Knowledge effects on document selection in search results pages.">179. Knowledge effects on document selection in search results pages.</a></li>
<li><a href="#180. Learning to rank from a noisy crowd.">180. Learning to rank from a noisy crowd.</a></li>
<li><a href="#181. How to count thumb-ups and thumb-downs?: an information retrieval approach to user-rating based ranking of items.">181. How to count thumb-ups and thumb-downs?: an information retrieval approach to user-rating based ranking of items.</a></li>
<li><a href="#182. Predicting users' domain knowledge from search behaviors.">182. Predicting users' domain knowledge from search behaviors.</a></li>
<li><a href="#183. The interactive PRP for diversifying document rankings.">183. The interactive PRP for diversifying document rankings.</a></li>
<li><a href="#184. Detecting success in mobile search from interaction.">184. Detecting success in mobile search from interaction.</a></li>
<li><a href="#185. Measuring assessor accuracy: a comparison of nist assessors and user study participants.">185. Measuring assessor accuracy: a comparison of nist assessors and user study participants.</a></li>
<li><a href="#186. A bipartite graph based social network splicing method for person name disambiguation.">186. A bipartite graph based social network splicing method for person name disambiguation.</a></li>
<li><a href="#187. Link formation analysis in microblogs.">187. Link formation analysis in microblogs.</a></li>
<li><a href="#188. Evolution of web search results within years.">188. Evolution of web search results within years.</a></li>
<li><a href="#189. Decayed DivRank: capturing relevance, diversity and prestige in information networks.">189. Decayed DivRank: capturing relevance, diversity and prestige in information networks.</a></li>
<li><a href="#190. Multi-objective optimization in learning to rank.">190. Multi-objective optimization in learning to rank.</a></li>
<li><a href="#191. A large-scale study of the effect of training set characteristics over learning-to-rank algorithms.">191. A large-scale study of the effect of training set characteristics over learning-to-rank algorithms.</a></li>
<li><a href="#192. Exploring term temporality for pseudo-relevance feedback.">192. Exploring term temporality for pseudo-relevance feedback.</a></li>
<li><a href="#193. MSSF: a multi-document summarization framework based on submodularity.">193. MSSF: a multi-document summarization framework based on submodularity.</a></li>
<li><a href="#194. SEJoin: an optimized algorithm towards efficient approximate string searches.">194. SEJoin: an optimized algorithm towards efficient approximate string searches.</a></li>
<li><a href="#195. Bag-of-visual-words vs global image descriptors on two-stage multimodal retrieval.">195. Bag-of-visual-words vs global image descriptors on two-stage multimodal retrieval.</a></li>
<li><a href="#196. Query term ranking based on search results overlap.">196. Query term ranking based on search results overlap.</a></li>
<li><a href="#197. Tossing coins to trim long queries.">197. Tossing coins to trim long queries.</a></li>
<li><a href="#198. A comparison of time-aware ranking methods.">198. A comparison of time-aware ranking methods.</a></li>
<li><a href="#199. Learning for graphs with annotated edges.">199. Learning for graphs with annotated edges.</a></li>
<li><a href="#200. Formulating effective questions for community-based question answering.">200. Formulating effective questions for community-based question answering.</a></li>
</ul>
</li>
<li><a href="#Demonstrations    15">Demonstrations    15</a><ul>
<li><a href="#201. ClusteringWiki: personalized and collaborative clustering of search results.">201. ClusteringWiki: personalized and collaborative clustering of search results.</a></li>
<li><a href="#202. OrientSTS: spatio-temporal sequence searching in flickr.">202. OrientSTS: spatio-temporal sequence searching in flickr.</a></li>
<li><a href="#203. A toolkit for knowledge base population.">203. A toolkit for knowledge base population.</a></li>
<li><a href="#204. iMecho: a context-aware desktop search system.">204. iMecho: a context-aware desktop search system.</a></li>
<li><a href="#205. Visualizing and querying semantic social networks.">205. Visualizing and querying semantic social networks.</a></li>
<li><a href="#206. What-you-retrieve-is-what-you-see: a preliminary cyber-physical search engine.">206. What-you-retrieve-is-what-you-see: a preliminary cyber-physical search engine.</a></li>
<li><a href="#207. QuickView: advanced search of tweets.">207. QuickView: advanced search of tweets.</a></li>
<li><a href="#208. Personalized video: leanback online video consumption.">208. Personalized video: leanback online video consumption.</a></li>
<li><a href="#209. GreenMeter: a tool for assessing the quality and recommending tags for web 2.0 applications.">209. GreenMeter: a tool for assessing the quality and recommending tags for web 2.0 applications.</a></li>
<li><a href="#210. JuSe: a picture dictionary query system for children.">210. JuSe: a picture dictionary query system for children.</a></li>
<li><a href="#211. CrowdTracker: enabling community-based real-time web monitoring.">211. CrowdTracker: enabling community-based real-time web monitoring.</a></li>
<li><a href="#212. The Meta-Dex Suite: generating and analyzing indexes and meta-indexes.">212. The Meta-Dex Suite: generating and analyzing indexes and meta-indexes.</a></li>
<li><a href="#213. Tulsa: web search for writing assistance.">213. Tulsa: web search for writing assistance.</a></li>
<li><a href="#214. The TREC files: the (ground">214. The TREC files: the (ground) truth is out there.</a> truth is out there.)</li>
<li><a href="#215. A tool for comparative IR evaluation on component level.">215. A tool for comparative IR evaluation on component level.</a></li>
</ul>
</li>
<li><a href="#Tutorials    7">Tutorials    7</a><ul>
<li><a href="#216. Machine learning for information retrieval.">216. Machine learning for information retrieval.</a></li>
<li><a href="#217. Enhancing web search by mining search and browse logs.">217. Enhancing web search by mining search and browse logs.</a></li>
<li><a href="#218. A new look at old tricks: the fertile roots of current research.">218. A new look at old tricks: the fertile roots of current research.</a></li>
<li><a href="#219. Crowdsourcing for information retrieval: principles, methods, and applications.">219. Crowdsourcing for information retrieval: principles, methods, and applications.</a></li>
<li><a href="#220. Practical online retrieval evaluation.">220. Practical online retrieval evaluation.</a></li>
<li><a href="#221. Web retrieval: the role of users.">221. Web retrieval: the role of users.</a></li>
<li><a href="#222. Information organization and retrieval with collaboratively generated content.">222. Information organization and retrieval with collaboratively generated content.</a></li>
</ul>
</li>
<li><a href="#Doctoral consortium    11">Doctoral consortium    11</a><ul>
<li><a href="#223. Persistence in the ephemeral: utilizing repeat behaviors for multi-session personalized search.">223. Persistence in the ephemeral: utilizing repeat behaviors for multi-session personalized search.</a></li>
<li><a href="#224. Search engines that learn online.">224. Search engines that learn online.</a></li>
<li><a href="#225. Query expansion based on a semantic graph model.">225. Query expansion based on a semantic graph model.</a></li>
<li><a href="#226. Descriptive modelling of text classification and its integration with other IR tasks.">226. Descriptive modelling of text classification and its integration with other IR tasks.</a></li>
<li><a href="#227. Efficient and effective solutions for search engines.">227. Efficient and effective solutions for search engines.</a></li>
<li><a href="#228. Modeling document scores for distributed information retrieval.">228. Modeling document scores for distributed information retrieval.</a></li>
<li><a href="#229. Improving query and result list adaptation in personalized multilingual information retrieval.">229. Improving query and result list adaptation in personalized multilingual information retrieval.</a></li>
<li><a href="#230. Using k-Top retrieved web snippets to date temporalimplicit queries based on web content analysis.">230. Using k-Top retrieved web snippets to date temporalimplicit queries based on web content analysis.</a></li>
<li><a href="#231. Domain-specific information retrieval using rcommenders.">231. Domain-specific information retrieval using rcommenders.</a></li>
<li><a href="#232. Understanding and using contextual information in recommender systems.">232. Understanding and using contextual information in recommender systems.</a></li>
<li><a href="#233. Multidimensional search result diversification: diverse search results for diverse users.">233. Multidimensional search result diversification: diverse search results for diverse users.</a></li>
</ul>
</li>
<li><a href="#Industrial track    5">Industrial track    5</a><ul>
<li><a href="#234. Sensor-aided mobile information management and retrieval.">234. Sensor-aided mobile information management and retrieval.</a></li>
<li><a href="#235. Predicting eBay listing conversion.">235. Predicting eBay listing conversion.</a></li>
<li><a href="#236. A large scale machine learning system for recommending heterogeneous content in social networks.">236. A large scale machine learning system for recommending heterogeneous content in social networks.</a></li>
<li><a href="#237. Review of MSR-Bing web scale speller challenge.">237. Review of MSR-Bing web scale speller challenge.</a></li>
<li><a href="#238. Elsevier SIGIR 2011 application challenge abstract.">238. Elsevier SIGIR 2011 application challenge abstract.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="34. SIGIR 2011:Beijing, China">34. SIGIR 2011:Beijing, China</h1>
<p><a href="">Proceeding of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2011, Beijing, China, July 25-29, 2011.</a> ACM
【<a href="http://dblp.uni-trier.de/db/conf/sigir/sigir2011.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 238 || Session Num: 37">Paper Num: 238 || Session Num: 37</h2>
<h2 id="Keynote address 1    1">Keynote address 1    1</h2>
<h3 id="1. Future of the web and search.">1. Future of the web and search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009918">Paper Link</a>】    【Pages】:1-2</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Qi">Qi Lu</a></p>
<p>【Abstract】:
No one doubts that we have only scratched the surface of what is possible with the Web. The day is coming fast when the Web will become almost a virtual mind reader. Your intent, interests, and needs will be instantly perceived and the information you want will be promptly delivered -- whether you ask for it directly or not -- based on a deep understanding of the meaning of words in your query, knowledge of your preferences and patterns, what others have done before you, your location, and more. In this talk, I will share some of my thoughts about where the Web is heading and how search will be transformed to align to this new Web, laying out some specifics behind Microsoft's vision to empower people with knowledge.</p>
<p>【Keywords】:
web search</p>
<h2 id="Keynote address 2    1">Keynote address 2    1</h2>
<h3 id="2. Beyond search: statistical topic models for text analysis.">2. Beyond search: statistical topic models for text analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009920">Paper Link</a>】    【Pages】:3-4</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
Search is generally a means to the end of finishing a task. While the current search engines are useful to users for finding relevant information, they offer little help to users for further digesting and analyzing the overwhelming found information needed for finishing a complex task. In this talk, I will discuss how statistical topic models can be used to help users analyze and digest the found relevant information and turn search results into actionable knowledge needed to complete a task. I will present several general statistical topic models for extracting and analyzing topics and their patterns in text, and show sample applications of such models in tasks such as opinion integration, comparative summarization, contextual topic trend analysis, and event impact analysis. The talk will conclude with a discussion of novel challenges raised in extending a search engine to an analysis engine that can go beyond search to provide more complete support for users to finish their tasks.</p>
<p>【Keywords】:
statisical topic models; text analysis</p>
<h2 id="Users 1    4">Users 1    4</h2>
<h3 id="3. Modeling and analysis of cross-session search tasks.">3. Modeling and analysis of cross-session search tasks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009922">Paper Link</a>】    【Pages】:5-14</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kotov:Alexander">Alexander Kotov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bennett:Paul_N=">Paul N. Bennett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Teevan:Jaime">Jaime Teevan</a></p>
<p>【Abstract】:
The information needs of search engine users vary in complexity, depending on the task they are trying to accomplish. Some simple needs can be satisfied with a single query, whereas others require a series of queries issued over a longer period of time. While search engines effectively satisfy many simple needs, searchers receive little support when their information needs span session boundaries. In this work, we propose methods for modeling and analyzing user search behavior that extends over multiple search sessions. We focus on two problems: (i) given a user query, identify all of the related queries from previous sessions that the same user has issued, and (ii) given a multi-query task for a user, predict whether the user will return to this task in the future. We model both problems within a classification framework that uses features of individual queries and long-term user search behavior at different granularity. Experimental evaluation of the proposed models for both tasks indicates that it is possible to effectively model and analyze cross-session search behavior. Our findings have implications for improving search for complex information needs and designing search engine features to support cross-session search tasks.</p>
<p>【Keywords】:
cross-session search tasks; machine learning; user behavior</p>
<h3 id="4. The economics in interactive information retrieval.">4. The economics in interactive information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009923">Paper Link</a>】    【Pages】:15-24</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Azzopardi:Leif">Leif Azzopardi</a></p>
<p>【Abstract】:
Searching is inherently an interactive process usually requiring numerous iterations of querying and assessing in order to find the desired amount of relevant information. Essentially, the search process can be viewed as a combination of inputs (queries and assessments) which are used to "produce" output (relevance). Under this view, it is possible to adapt microeconomic theory to analyze and understand the dynamics of Interactive Information Retrieval. In this paper, we approach the search process as an economics problem and conduct extensive simulations on TREC test collections analyzing various combinations of inputs in the "production" of relevance. The analysis reveals that the total Cumulative Gain (output) obtained during the course of a search session is functionally related to querying and assessing (inputs), and this can be characterized mathematically by the Cobbs-Douglas production function. Further analysis using cost models, that are grounded using cognitive load as the cost, reveals which search strategies minimize the cost of interaction for a given level of output. This paper demonstrates how economics can be applied to formally model the search process. This development establishes the theoretical foundations of Interactive Information Retrieval, providing numerous directions for empirical experimentation that are motivated directly from theory.</p>
<p>【Keywords】:
consumer theory; evaluation; production theory; prosumer theory; retrieval strategies; simulatution</p>
<h3 id="5. Seeding simulated queries with user-study data forpersonal search evaluation.">5. Seeding simulated queries with user-study data forpersonal search evaluation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009924">Paper Link</a>】    【Pages】:25-34</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Elsweiler:David">David Elsweiler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Losada:David_E=">David E. Losada</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Toucedo:Jos=eacute=_Carlos">José Carlos Toucedo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fern=aacute=ndez:Ronald_T=">Ronald T. Fernández</a></p>
<p>【Abstract】:
In this paper we perform a lab-based user study (n=21) of email re-finding behaviour, examining how the characteristics of submitted queries change in different situations. A number of logistic regression models are developed on the query data to explore the relationship between user- and contextual- variables and query characteristics including length, field submitted to and use of named entities. We reveal several interesting trends and use the findings to seed a simulated evaluation of various retrieval models. Not only is this an enhancement of existing evaluation methods for Personal Search, but the results show that different models are more effective in different situations, which has implications both for the design of email search tools and for the way algorithms for Personal Search are evaluated.</p>
<p>【Keywords】:
email re-finding; evaluation; personal search; user study</p>
<h3 id="6. Understanding re-finding behavior in naturalistic email interaction logs.">6. Understanding re-finding behavior in naturalistic email interaction logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009925">Paper Link</a>】    【Pages】:35-44</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Elsweiler:David">David Elsweiler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Harvey:Morgan">Morgan Harvey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hacker:Martin">Martin Hacker</a></p>
<p>【Abstract】:
In this paper we present a longitudinal, naturalistic study of email behavior (n=47) and describe our efforts at isolating re-finding behavior in the logs through various qualitative and quantitative analyses. The presented work underlines the methodological challenges faced with this kind of research, but demonstrates that it is possible to isolate re-finding behavior from email interaction logs with reasonable accuracy. Using the approaches developed we uncover interesting aspects of email re-finding behavior that have so far been impossible to study, such as how various features of email-clients are used in re-finding and the difficulties people encounter when using these. We explain how our findings could influence the design of email-clients and outline our thoughts on how future, more in depth analyses, can build on the work presented here to achieve a fuller understanding of email behavior and the support that people need.</p>
<p>【Keywords】:
naturalistic user evaluation; personal information management; query log analysis; re-finding</p>
<h2 id="Query analysis I    4">Query analysis I    4</h2>
<h3 id="7. People searching for people: analysis of a people search engine log.">7. People searching for people: analysis of a people search engine log.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009927">Paper Link</a>】    【Pages】:45-54</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weerkamp:Wouter">Wouter Weerkamp</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Berendsen:Richard">Richard Berendsen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kovachev:Bogomil">Bogomil Kovachev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meij:Edgar">Edgar Meij</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Balog:Krisztian">Krisztian Balog</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
Recent years show an increasing interest in vertical search: searching within a particular type of information. Understanding what people search for in these "verticals" gives direction to research and provides pointers for the search engines themselves. In this paper we analyze the search logs of one particular vertical: people search engines. Based on an extensive analysis of the logs of a search engine geared towards finding people, we propose a classification scheme for people search at three levels: (a) queries, (b) sessions, and (c) users. For queries, we identify three types, (i) event-based high-profile queries (people that become "popular" because of an event happening), (ii) regular high-profile queries (celebrities), and (iii) low-profile queries (other, less-known people). We present experiments on automatic classification of queries. On the session level, we observe five types: (i) family sessions (users looking for relatives), (ii) event sessions (querying the main players of an event), (iii) spotting sessions (trying to "spot" different celebrities online), (iv) polymerous sessions (sessions without a clear relation between queries), and (v) repetitive sessions (query refinement and copying). Finally, for users we identify four types: (i) monitors, (ii) spotters, (iii) followers, and (iv) polymers. Our findings not only offer insight into search behavior in people search engines, but they are also useful to identify future research directions and to provide pointers for search engine improvements.</p>
<p>【Keywords】:
classification; people search; query log analysis</p>
<h3 id="8. Learning search tasks in queries and web pages via graph regularization.">8. Learning search tasks in queries and web pages via graph regularization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009928">Paper Link</a>】    【Pages】:55-64</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Ji:Ming">Ming Ji</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Jun">Jun Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gu:Siyu">Siyu Gu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Xiaofei">Xiaofei He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Wei_Vivian">Wei Vivian Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a></p>
<p>【Abstract】:
As the Internet grows explosively, search engines play a more and more important role for users in effectively accessing online information. Recently, it has been recognized that a query is often triggered by a search task that the user wants to accomplish. Similarly, many web pages are specifically designed to help accomplish a certain task. Therefore, learning hidden tasks behind queries and web pages can help search engines return the most useful web pages to users by task matching. For instance, the search task that triggers query "thinkpad T410 broken" is to maintain a computer, and it is desirable for a search engine to return the Lenovo troubleshooting page on the top of the list. However, existing search engine technologies mainly focus on topic detection or relevance ranking, which are not able to predict the task that triggers a query and the task a web page can accomplish. In this paper, we propose to simultaneously classify queries and web pages into the popular search tasks by exploiting their content together with click-through logs. Specifically, we construct a taskoriented heterogeneous graph among queries and web pages. Each pair of objects in the graph are linked together as long as they potentially share similar search tasks. A novel graph-based regularization algorithm is designed for search task prediction by leveraging the graph. Extensive experiments in real search log data demonstrate the effectiveness of our method over state-of-the-art classifiers, and the search performance can be significantly improved by using the task prediction results as additional information.</p>
<p>【Keywords】:
classification; graph regularization; web search task</p>
<h3 id="9. Intentions and attention in exploratory health search.">9. Intentions and attention in exploratory health search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009929">Paper Link</a>】    【Pages】:65-74</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cartright:Marc=Allen">Marc-Allen Cartright</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Horvitz:Eric">Eric Horvitz</a></p>
<p>【Abstract】:
We study information goals and patterns of attention in explorato-ry search for health information on the Web, reporting results of a large-scale log-based study. We examine search activity associated with the goal of diagnosing illness from symptoms versus more general information-seeking about health and illness. We decom-pose exploratory health search into evidence-based and hypothe-sis-directed information seeking. Evidence-based search centers on the pursuit of details and relevance of signs and symptoms. Hypothesis-directed search includes the pursuit of content on one or more illnesses, including risk factors, treatments, and therapies for illnesses, and on the discrimination among different diseases under the uncertainty that exists in advance of a confirmed diag-nosis. These different goals of exploratory health search are not independent, and transitions can occur between them within or across search sessions. We construct a classifier that identifies medically-related search sessions in log data. Given a set of search sessions flagged as health-related, we show how we can identify different intentions persisting as foci of attention within those sessions. Finally, we discuss how insights about foci dynamics can help us better understand exploratory health search behavior and better support health search on the Web.</p>
<p>【Keywords】:
cyberchondria; diagnosis; health search; medical search</p>
<h3 id="10. User behavior in zero-recall ecommerce queries.">10. User behavior in zero-recall ecommerce queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009930">Paper Link</a>】    【Pages】:75-84</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Gyanit">Gyanit Singh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Parikh:Nish">Nish Parikh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sundaresan:Neel">Neel Sundaresan</a></p>
<p>【Abstract】:
User expectation and experience for web search and eCommerce (product) search are quite different. Product descriptions are concise as compared to typical web documents. User expectation is more specific to find the right product. The difference in the publisher and searcher vocabulary (in case of product search the seller and the buyer vocabulary) combined with the fact that there are fewer products to search over than web documents result in observable numbers of searches that return no results (zero recall searches). In this paper we describe a study of zero recall searches. Our study is focused on eCommerce search and uses data from a leading eCommerce site's user click stream logs. There are 3 main contributions of our study: 1) The cause of zero recall searches; 2) A study of user's reaction and recovery from zero recall; 3) A study of differences in behavior of power users versus novice users to zero recall searches.</p>
<p>【Keywords】:
ecommerce search; user studies; zero recall</p>
<h2 id="Learning to rank    4">Learning to rank    4</h2>
<h3 id="11. Bagging gradient-boosted trees for high precision, low variance ranking models.">11. Bagging gradient-boosted trees for high precision, low variance ranking models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009932">Paper Link</a>】    【Pages】:85-94</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ganjisaffar:Yasser">Yasser Ganjisaffar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caruana:Rich">Rich Caruana</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lopes:Cristina_Videira">Cristina Videira Lopes</a></p>
<p>【Abstract】:
Recent studies have shown that boosting provides excellent predictive performance across a wide variety of tasks. In Learning-to-rank, boosted models such as RankBoost and LambdaMART have been shown to be among the best performing learning methods based on evaluations on public data sets. In this paper, we show how the combination of bagging as a variance reduction technique and boosting as a bias reduction technique can result in very high precision and low variance ranking models. We perform thousands of parameter tuning experiments for LambdaMART to achieve a high precision boosting model. Then we show that a bagged ensemble of such LambdaMART boosted models results in higher accuracy ranking models while also reducing variance as much as 50%. We report our results on three public learning-to-rank data sets using four metrics. Bagged LamdbaMART outperforms all previously reported results on ten of the twelve comparisons, and bagged LambdaMART outperforms non-bagged LambdaMART on all twelve comparisons. For example, wrapping bagging around LambdaMART increases NDCG@1 from 0.4137 to 0.4200 on the MQ2007 data set; the best prior results in the literature for this data set is 0.4134 by RankBoost.</p>
<p>【Keywords】:
bagging; learning-to-rank; tree ensembles</p>
<h3 id="12. Learning to rank for freshness and relevance.">12. Learning to rank for freshness and relevance.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009933">Paper Link</a>】    【Pages】:95-104</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dai:Na">Na Dai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shokouhi:Milad">Milad Shokouhi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davison_0001:Brian_D=">Brian D. Davison</a></p>
<p>【Abstract】:
Freshness of results is important in modern web search. Failing to recognize the temporal aspect of a query can negatively affect the user experience, and make the search engine appear stale. While freshness and relevance can be closely related for some topics (e.g., news queries), they are more independent in others (e.g., time insensitive queries). Therefore, optimizing one criterion does not necessarily improve the other, and can even do harm in some cases. We propose a machine-learning framework for simultaneously optimizing freshness and relevance, in which the trade-off is automatically adaptive to query temporal characteristics. We start by illustrating different temporal characteristics of queries, and the features that can be used for capturing these properties. We then introduce our supervised framework that leverages the temporal profile of queries (inferred from pseudo-feedback documents) along with the other ranking features to improve both freshness and relevance of search results. Our experiments on a large archival web corpus demonstrate the efficacy of our techniques.</p>
<p>【Keywords】:
freshness ranking; query classification; temporal profiles</p>
<h3 id="13. A cascade ranking model for efficient ranked retrieval.">13. A cascade ranking model for efficient ranked retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009934">Paper Link</a>】    【Pages】:105-114</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Lidan">Lidan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy_J=">Jimmy J. Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metzler:Donald">Donald Metzler</a></p>
<p>【Abstract】:
There is a fundamental tradeoff between effectiveness and efficiency when designing retrieval models for large-scale document collections. Effectiveness tends to derive from sophisticated ranking functions, such as those constructed using learning to rank, while efficiency gains tend to arise from improvements in query evaluation and caching strategies. Given their inherently disjoint nature, it is difficult to jointly optimize effectiveness and efficiency in end-to-end systems. To address this problem, we formulate and develop a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both top k ranked effectiveness and retrieval efficiency. The model constructs a cascade of increasingly complex ranking functions that progressively prunes and refines the set of candidate documents to minimize retrieval latency and maximize result set quality. We present a novel boosting algorithm for learning such cascades to directly optimize the tradeoff between effectiveness and efficiency. Experimental results show that our cascades are faster and return higher quality results than comparable ranking models.</p>
<p>【Keywords】:
effectiveness; efficiency; learning to rank</p>
<h3 id="14. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation.">14. Relevant knowledge helps in choosing right teacher: active query selection for ranking adaptation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009935">Paper Link</a>】    【Pages】:115-124</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Peng">Peng Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Wei">Wei Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Aoying">Aoying Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wong:Kam=Fai">Kam-Fai Wong</a></p>
<p>【Abstract】:
Learning to adapt in a new setting is a common challenge to our knowledge and capability. New life would be easier if we actively pursued supervision from the right mentor chosen with our relevant but limited prior knowledge. This variant principle of active learning seems intuitively useful to many domain adaptation problems. In this paper, we substantiate its power for advancing automatic ranking adaptation, which is important in web search since it's prohibitive to gather enough labeled data for every search domain for fully training domain-specific rankers. For the cost-effectiveness, it is expected that only those most informative instances in target domain are collected to annotate while we can still utilize the abundant ranking knowledge in source domain. We propose a unified ranking framework to mutually reinforce the active selection of informative target-domain queries and the appropriate weighting of source training data as related prior knowledge. We select to annotate those target queries whose documents' order most disagrees among the members of a committee built on the mixture of source training data and the already selected target data. Then the replenished labeled set is used to adjust the importance of source queries for enhancing their rank transfer. This procedure iterates until labeling budget exhausts. Based on LETOR3.0 and Yahoo! Learning to Rank Challenge data sets, our approach significantly outperforms the random query annotation commonly used in ranking adaptation and the active rank learner on target-domain data only.</p>
<p>【Keywords】:
active learning; query by committee; ranking adaptation</p>
<h2 id="Personalization    3">Personalization    3</h2>
<h3 id="15. SCENE: a scalable two-stage personalized news recommendation system.">15. SCENE: a scalable two-stage personalized news recommendation system.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009937">Paper Link</a>】    【Pages】:125-134</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dingding">Dingding Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Knox:Daniel">Daniel Knox</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Padmanabhan:Balaji">Balaji Padmanabhan</a></p>
<p>【Abstract】:
Recommending news articles has become a promising research direction as the Internet provides fast access to real-time information from multiple sources around the world. Traditional news recommendation systems strive to adapt their services to individual users by virtue of both user and news content information. However, the latent relationships among different news items, and the special properties of new articles, such as short shelf lives and value of immediacy, render the previous approaches inefficient. In this paper, we propose a scalable two-stage personalized news recommendation approach with a two-level representation, which considers the exclusive characteristics (e.g., news content, access patterns, named entities, popularity and recency) of news items when performing recommendation. Also, a principled framework for news selection based on the intrinsic property of user interest is presented, with a good balance between the novelty and diversity of the recommended result. Extensive empirical experiments on a collection of news articles obtained from various news websites demonstrate the efficacy and efficiency of our approach.</p>
<p>【Keywords】:
multi-level news recommendation; news entity; personalization; submodularity; user profile</p>
<h3 id="16. Inferring and using location metadata to personalize web search.">16. Inferring and using location metadata to personalize web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009938">Paper Link</a>】    【Pages】:135-144</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bennett:Paul_N=">Paul N. Bennett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Radlinski:Filip">Filip Radlinski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yilmaz:Emine">Emine Yilmaz</a></p>
<p>【Abstract】:
Personalization of search results offers the potential for significant improvements in Web search. Among the many observable user attributes, approximate user location is particularly simple for search engines to obtain and allows personalization even for a first-time Web search user. However, acting on user location information is difficult, since few Web documents include an address that can be interpreted as constraining the locations where the document is relevant. Furthermore, many Web documents -- such as local news stories, lottery results, and sports team fan pages -- may not correspond to physical addresses, but the location of the user still plays an important role in document relevance. In this paper, we show how to infer a more general location relevance which uses not only physical location but a more general notion of locations of interest for Web pages. We compute this information using implicit user behavioral data, characterize the most location-centric pages, and show how location information can be incorporated into Web search ranking. Our results show that a substantial fraction of Web search queries can be significantly improved by incorporating location-based features.</p>
<p>【Keywords】:
location metadata; personalization; web search</p>
<h3 id="17. Active learning to maximize accuracy vs. effort in interactive information retrieval.">17. Active learning to maximize accuracy vs. effort in interactive information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009939">Paper Link</a>】    【Pages】:145-154</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tian:Aibo">Aibo Tian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lease:Matthew">Matthew Lease</a></p>
<p>【Abstract】:
We consider an interactive information retrieval task in which the user is interested in finding several to many relevant documents with minimal effort. Given an initial document ranking, user interaction with the system produces relevance feedback (RF) which the system then uses to revise the ranking. This interactive process repeats until the user terminates the search. To maximize accuracy relative to user effort, we propose an active learning strategy. At each iteration, the document whose relevance is maximally uncertain to the system is slotted high into the ranking in order to obtain user feedback for it. Simulated feedback on the Robust04 TREC collection shows our active learning approach dominates several standard RF baselines relative to the amount of feedback provided by the user. Evaluation on Robust04 under noisy feedback and on LETOR collections further demonstrate the effectiveness of active learning, as well as value of negative feedback in this task scenario.</p>
<p>【Keywords】:
active learning; personalized search; relevance feedback</p>
<h2 id="Retrieval models I    3">Retrieval models I    3</h2>
<h3 id="18. CRTER: using cross terms to enhance probabilistic information retrieval.">18. CRTER: using cross terms to enhance probabilistic information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009941">Paper Link</a>】    【Pages】:155-164</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Jiashu">Jiashu Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jimmy_Xiangji">Jimmy Xiangji Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a></p>
<p>【Abstract】:
Term proximity retrieval rewards a document where the matched query terms occur close to each other. Although term proximity is known to be effective in many Information Retrieval (IR) applications, the within-document distribution of each individual query term and how the query terms associate with each other, are not fully considered. In this paper, we introduce a pseudo term, namely Cross Term, to model term proximity for boosting retrieval performance. An occurrence of a query term is assumed to have an impact towards its neighboring text, which gradually weakens with the increase of the distance to the place of occurrence. We use a shape function to characterize such an impact. A Cross Term occurs when two query terms appear close to each other and their impact shape functions have an intersection. We propose a Cross Term Retrieval (CRTER) model that combines the Cross Terms' information with basic probabilistic weighting models to rank the retrieved documents. Extensive experiments on standard TREC collections illustrate the effectiveness of our proposed CRTER model.</p>
<p>【Keywords】:
bm25; cross term; kernel; probabilistic ir; proximity</p>
<h3 id="19. A boosting approach to improving pseudo-relevance feedback.">19. A boosting approach to improving pseudo-relevance feedback.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009942">Paper Link</a>】    【Pages】:165-174</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Yuanhua">Yuanhua Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Wan">Wan Chen</a></p>
<p>【Abstract】:
Pseudo-relevance feedback has proven effective for improving the average retrieval performance. Unfortunately, many experiments have shown that although pseudo-relevance feedback helps many queries, it also often hurts many other queries, limiting its usefulness in real retrieval applications. Thus an important, yet difficult challenge is to improve the overall effectiveness of pseudo-relevance feedback without sacrificing the performance of individual queries too much. In this paper, we propose a novel learning algorithm, FeedbackBoost, based on the boosting framework to improve pseudo-relevance feedback through optimizing the combination of a set of basis feedback algorithms using a loss function defined to directly measure both robustness and effectiveness. FeedbackBoost can potentially accommodate many basis feedback methods as features in the model, making the proposed method a general optimization framework for pseudo-relevance feedback. As an application, we apply FeedbackBoost to improve pseudo feedback based on language models through combining different document weighting strategies. The experiment results demonstrate that FeedbackBoost can achieve better average precision and meanwhile dramatically reduce the number and magnitude of feedback failures as compared to three representative pseudo feedback methods and a standard learning to rank approach for pseudo feedback.</p>
<p>【Keywords】:
boosting; feedbackboost; learning; loss function; optimization; pseudo-relevance feedback; robustness</p>
<h3 id="20. Enhancing ad-hoc relevance weighting using probability density estimation.">20. Enhancing ad-hoc relevance weighting using probability density estimation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009943">Paper Link</a>】    【Pages】:175-184</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Xiaofeng">Xiaofeng Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jimmy_Xiangji">Jimmy Xiangji Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Ben">Ben He</a></p>
<p>【Abstract】:
Classical probabilistic information retrieval (IR) models, e.g. BM25, deal with document length based on a trade-off between the Verbosity hypothesis, which assumes the independence of a document's relevance of its length, and the Scope hypothesis, which assumes the opposite. Despite the effectiveness of the classical probabilistic models, the potential relationship between document length and relevance is not fully explored to improve retrieval performance. In this paper, we conduct an in-depth study of this relationship based on the Scope hypothesis that document length does have its impact on relevance. We study a list of probability density functions and examine which of the density functions fits the best to the actual distribution of the document length. Based on the studied probability density functions, we propose a length-based BM25 relevance weighting model, called BM25L, which incorporates document length as a substantial weighting factor. Extensive experiments conducted on standard TREC collections show that our proposed BM25L markedly outperforms the original BM25 model, even if the latter is optimized.</p>
<p>【Keywords】:
bm25; document length; probabilistic ir</p>
<h2 id="Social media    3">Social media    3</h2>
<h3 id="21. Who should share what?: item-level social influence prediction for users and posts ranking.">21. Who should share what?: item-level social influence prediction for users and posts ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009945">Paper Link</a>】    【Pages】:185-194</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cui:Peng">Peng Cui</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Fei">Fei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Shaowei">Shaowei Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ou:Mingdong">Mingdong Ou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shiqiang">Shiqiang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Lifeng">Lifeng Sun</a></p>
<p>【Abstract】:
People and information are two core dimensions in a social network. People sharing information (such as blogs, news, albums, etc.) is the basic behavior. In this paper, we focus on predicting item-level social influence to answer the question Who should share What, which can be extended into two information retrieval scenarios: (1) Users ranking: given an item, who should share it so that its diffusion range can be maximized in a social network; (2) Web posts ranking: given a user, what should she share to maximize her influence among her friends. We formulate the social influence prediction problem as the estimation of a user-post matrix, in which each entry represents the strength of influence of a user given a web post. We propose a Hybrid Factor Non-Negative Matrix Factorization (HF-NMF) approach for item-level social influence modeling, and devise an efficient projected gradient method to solve the HF-NMF problem. Intensive experiments are conducted and demonstrate the advantages and characteristics of the proposed method.</p>
<p>【Keywords】:
matrix factorization; social influence; user ranking; web post ranking</p>
<h3 id="22. Mining tags using social endorsement networks.">22. Mining tags using social endorsement networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009946">Paper Link</a>】    【Pages】:195-204</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lappas:Theodoros">Theodoros Lappas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Punera:Kunal">Kunal Punera</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sarl=oacute=s:Tam=aacute=s">Tamás Sarlós</a></p>
<p>【Abstract】:
Entities on social systems, such as users on Twitter, and images on Flickr, are at the core of many interesting applications: they can be ranked in search results, recommended to users, or used in contextual advertising. Such applications assume knowledge of an entity's nature and characteristic attributes. An effective way to encode such knowledge is in the form of tags. An untagged entity is practically inaccessible, since it is hard to retrieve or interact with. To address this, some platforms allow users to manually tag entities. However,while such tags can be informative, they can oftentimes be inadequate, trivial, ambiguous, or even plain false. Numerous automated tagging methods have been proposed to address these issues. However,most of them require pre-existing high-quality tags or descriptive texts for every entity that needs to be tagged. In our work, we propose a method based on social endorsements that is free from such constraints. Virtually every major social networking platform allows users to endorse entities that they find appealing. Examples include "following" Twitter users or "favoriting" Flickr photos. These endorsements are abundant and directly capture the preferences of users. In this paper, we pose and solve the problem of using the underlying social endorsement network to extract useful tags for entities in a social system. Our work leverages techniques from topic modeling to capture the interests of users and then uses them to extract relevant and descriptive tags for the entities they endorse. We perform an extensive evaluation of our proposed approach on real large-scale datasets from both Twitter and Flickr, and show that it significantly outperforms meaningful and competitive baselines.</p>
<p>【Keywords】:
endorsement networks; multimodal data mining; tag extraction</p>
<h3 id="23. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking.">23. Crowdsourcing for book search evaluation: impact of hit design on comparative system ranking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009947">Paper Link</a>】    【Pages】:205-214</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kazai:Gabriella">Gabriella Kazai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kamps:Jaap">Jaap Kamps</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koolen:Marijn">Marijn Koolen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Milic=Frayling:Natasa">Natasa Milic-Frayling</a></p>
<p>【Abstract】:
The evaluation of information retrieval (IR) systems over special collections, such as large book repositories, is out of reach of traditional methods that rely upon editorial relevance judgments. Increasingly, the use of crowdsourcing to collect relevance labels has been regarded as a viable alternative that scales with modest costs. However, crowdsourcing suffers from undesirable worker practices and low quality contributions. In this paper we investigate the design and implementation of effective crowdsourcing tasks in the context of book search evaluation. We observe the impact of aspects of the Human Intelligence Task (HIT) design on the quality of relevance labels provided by the crowd. We assess the output in terms of label agreement with a gold standard data set and observe the effect of the crowdsourced relevance judgments on the resulting system rankings. This enables us to observe the effect of crowdsourcing on the entire IR evaluation process. Using the test set and experimental runs from the INEX 2010 Book Track, we find that varying the HIT design, and the pooling and document ordering strategies leads to considerable differences in agreement with the gold set labels. We then observe the impact of the crowdsourced relevance label sets on the relative system rankings using four IR performance metrics. System rankings based on MAP and Bpref remain less affected by different label sets while the Precision@10 and nDCG@10 lead to dramatically different system rankings, especially for labels acquired from HITs with weaker quality controls. Overall, we find that crowdsourcing can be an effective tool for the evaluation of IR systems, provided that care is taken when designing the HITs.</p>
<p>【Keywords】:
book search; crowdsourcing quality; prove it</p>
<h2 id="Content analysis    4">Content analysis    4</h2>
<h3 id="24. A site oriented method for segmenting web pages.">24. A site oriented method for segmenting web pages.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009949">Paper Link</a>】    【Pages】:215-224</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Oliveira:David_Fernandes_de">David Fernandes de Oliveira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moura:Edleno_Silva_de">Edleno Silva de Moura</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silva:Altigran_Soares_da">Altigran Soares da Silva</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ribeiro=Neto:Berthier_A=">Berthier A. Ribeiro-Neto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Ara=uacute=jo:Edisson_Braga">Edisson Braga Araújo</a></p>
<p>【Abstract】:
Information about how to segment a Web page can be used nowadays by applications such as segment aware Web search, classification and link analysis. In this research, we propose a fully automatic method for page segmentation and evaluate its application through experiments with four separate Web sites. While the method may be used in other applications, our main focus in this article is to use it as input to segment aware Web search systems. Our results indicate that the proposed method produces better segmentation results when compared to the best segmentation method we found in literature. Further, when applied as input to a segment aware Web search method, it produces results close to those produced when using a manual page segmentation method.</p>
<p>【Keywords】:
page segmentation; segment class</p>
<h3 id="25. Composite hashing with multiple information sources.">25. Composite hashing with multiple information sources.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009950">Paper Link</a>】    【Pages】:225-234</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0007:Dan">Dan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Fei">Fei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Similarity search applications with a large amount of text and image data demands an efficient and effective solution. One useful strategy is to represent the examples in databases as compact binary codes through semantic hashing, which has attracted much attention due to its fast query/search speed and drastically reduced storage requirement. All of the current semantic hashing methods only deal with the case when each example is represented by one type of features. However, examples are often described from several different information sources in many real world applications. For example, the characteristics of a webpage can be derived from both its content part and its associated links. To address the problem of learning good hashing codes in this scenario, we propose a novel research problem -- Composite Hashing with Multiple Information Sources (CHMIS). The focus of the new research problem is to design an algorithm for incorporating the features from different information sources into the binary hashing codes efficiently and effectively. In particular, we propose an algorithm CHMIS-AW (CHMIS with Adjusted Weights) for learning the codes. The proposed algorithm integrates information from several different sources into the binary hashing codes by adjusting the weights on each individual source for maximizing the coding performance, and enables fast conversion from query examples to their binary hashing codes. Experimental results on five different datasets demonstrate the superior performance of the proposed method against several other state-of-the-art semantic hashing techniques.</p>
<p>【Keywords】:
composite hashing; multiple information sources; semantic hashing</p>
<h3 id="26. Detecting outlier sections in us congressional legislation.">26. Detecting outlier sections in us congressional legislation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009951">Paper Link</a>】    【Pages】:235-244</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Aktolga:Elif">Elif Aktolga</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ros:Irene">Irene Ros</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Assogba:Yannick">Yannick Assogba</a></p>
<p>【Abstract】:
Reading congressional legislation, also known as bills, is often tedious because bills tend to be long and written in complex language. In IBM Many Bills, an interactive web-based visualization of legislation, users of different backgrounds can browse bills and quickly explore parts that are of interest to them. One task users have is to be able to locate sections that don't seem to fit with the overall topic of the bill. In this paper, we present novel techniques to determine which sections within a bill are likely to be outliers by employing approaches from information retrieval. The most promising techniques first detect the most topically relevant parts of a bill by ranking its sections, followed by a comparison between these topically relevant parts and the remaining sections in the bill. To compare sections we use various dissimilarity metrics based on Kullback-Leibler Divergence. The results indicate that these techniques are more successful than a classification based approach. Finally, we analyze how the dissimilarity metrics succeed in discriminating between sections that are strong outliers versus those that are 'milder' outliers.</p>
<p>【Keywords】:
dissimilarity; language modeling; outlier detection</p>
<h3 id="27. DOM based content extraction via text density.">27. DOM based content extraction via text density.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009952">Paper Link</a>】    【Pages】:245-254</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Fei">Fei Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Dandan">Dandan Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liao:Lejian">Lejian Liao</a></p>
<p>【Abstract】:
In addition to the main content, most web pages also contain navigation panels, advertisements and copyright and disclaimer notices. This additional content, which is also known as noise, is typically not related to the main subject and may hamper the performance of web data mining, and hence needs to be removed properly. In this paper, we present Content Extraction via Text Density (CETD) a fast, accurate and general method for extracting content from diverse web pages, and using DOM (Document Object Model) node text density to preserve the original structure. For this purpose, we introduce two concepts to measure the importance of nodes: Text Density and Composite Text Density. In order to extract content intact, we propose a technique called DensitySum to replace Data Smoothing. The approach was evaluated with the CleanEval benchmark and with randomly selected pages from well-known websites, where various web domains and styles are tested. The average F1-scores with our method were 8.79% higher than the best scores among several alternative methods.</p>
<p>【Keywords】:
composite text density; content extraction; densitysum; text density</p>
<h2 id="Web IR    4">Web IR    4</h2>
<h3 id="28. Social context summarization.">28. Social context summarization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009954">Paper Link</a>】    【Pages】:255-264</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Zi">Zi Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Keke">Keke Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0007:Li">Li Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Zhong">Zhong Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Juanzi">Juanzi Li</a></p>
<p>【Abstract】:
We study a novel problem of social context summarization for Web documents. Traditional summarization research has focused on extracting informative sentences from standard documents. With the rapid growth of online social networks, abundant user generated content (e.g., comments) associated with the standard documents is available. Which parts in a document are social users really caring about? How can we generate summaries for standard documents by considering both the informativeness of sentences and interests of social users? This paper explores such an approach by modeling Web documents and social contexts into a unified framework. We propose a dual wing factor graph (DWFG) model, which utilizes the mutual reinforcement between Web documents and their associated social contexts to generate summaries. An efficient algorithm is designed to learn the proposed factor graph model.Experimental results on a Twitter data set validate the effectiveness of the proposed model. By leveraging the social context information, our approach obtains significant improvement (averagely +5.0%-17.3%) over several alternative methods (CRF, SVM, LR, PR, and DocLead) on the performance of summarization.</p>
<p>【Keywords】:
document summarization; factor graph; social context; twitter</p>
<h3 id="29. Probabilistic factor models for web site recommendation.">29. Probabilistic factor models for web site recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009955">Paper Link</a>】    【Pages】:265-274</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Hao">Hao Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Chao">Chao Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/King:Irwin">Irwin King</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lyu:Michael_R=">Michael R. Lyu</a></p>
<p>【Abstract】:
Due to the prevalence of personalization and information filtering applications, modeling users' interests on the Web has become increasingly important during the past few years. In this paper, aiming at providing accurate personalized Web site recommendations for Web users, we propose a novel probabilistic factor model based on dimensionality reduction techniques. We also extend the proposed method to collective probabilistic factor modeling, which further improves model performance by incorporating heterogeneous data sources. The proposed method is general, and can be applied to not only Web site recommendations, but also a wide range of Web applications, including behavioral targeting, sponsored search, etc. The experimental analysis on Web site recommendation shows that our method outperforms other traditional recommendation approaches. Moreover, the complexity analysis indicates that our approach can be applied to very large datasets since it scales linearly with the number of observations.</p>
<p>【Keywords】:
matrix factorization; probabilistic factor modeling; recommender systems; web site recommendation</p>
<h3 id="30. Efficiently collecting relevance information from clickthroughs for web retrieval system evaluation.">30. Efficiently collecting relevance information from clickthroughs for web retrieval system evaluation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009956">Paper Link</a>】    【Pages】:275-284</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/He:Jing">Jing He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Wayne_Xin">Wayne Xin Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shu:Baihan">Baihan Shu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Hongfei">Hongfei Yan</a></p>
<p>【Abstract】:
Various click models have been recently proposed as a principled approach to infer the relevance of documents from the clickthrough data. The inferred document relevance is potentially useful in evaluating the Web retrieval systems. In practice, it generally requires to acquire the accurate evaluation results within minimal users' query submissions. This problem is important for speeding up search engine development and evaluation cycle and acquiring reliable evaluation results on tail queries. In this paper, we propose a reordering framework for efficient evaluation problem in the context of clickthrough based Web retrieval evaluation. The main idea is to move up the documents that contribute more for the evaluation task. In this framework, we propose four intuitions and formulate them as an optimization problem. Both user study and TREC data based experiments validate that the reordering framework results in much fewer query submissions to get accurate evaluation results with only a little harm to the users' utility.</p>
<p>【Keywords】:
click model; clickthrough data; evaluation</p>
<h3 id="31. Unsupervised query segmentation using clickthrough for information retrieval.">31. Unsupervised query segmentation using clickthrough for information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009957">Paper Link</a>】    【Pages】:285-294</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Yanen">Yanen Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Bo=June_Paul">Bo-June Paul Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Kuansan">Kuansan Wang</a></p>
<p>【Abstract】:
Query segmentation is an important task toward understanding queries accurately, which is essential for improving search results. Existing segmentation models either use labeled data to predict the segmentation boundaries, for which the training data is expensive to collect, or employ unsupervised strategy based on a large text corpus, which might be inaccurate because of the lack of relevant information. In this paper, we propose a probabilistic model to exploit clickthrough data for query segmentation, where the model parameters are estimated via an efficient EM algorithm. We further study how to properly interpret the segmentation results and utilize them to improve retrieval accuracy. Specifically, we propose an integrated language model based on the standard bigram language model to exploit the probabilistic structure obtained through query segmentation. Experiment results on two datasets show that our segmentation model outperforms existing segmentation models. Furthermore, extensive experiments on a large retrieval dataset reveals that the results of query segmentation can be leveraged to improve retrieval relevance by using the proposed integrated language model.</p>
<p>【Keywords】:
expectation maximization algorithm; language modeling; qslm; query segmentation</p>
<h2 id="Collaborative filtering I    4">Collaborative filtering I    4</h2>
<h3 id="32. Collaborative competitive filtering: learning recommender using context of user choice.">32. Collaborative competitive filtering: learning recommender using context of user choice.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009959">Paper Link</a>】    【Pages】:295-304</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shuang=Hong">Shuang-Hong Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Long:Bo">Bo Long</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Smola:Alexander_J=">Alexander J. Smola</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Hongyuan">Hongyuan Zha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Zhaohui">Zhaohui Zheng</a></p>
<p>【Abstract】:
While a user's preference is directly reflected in the interactive choice process between her and the recommender, this wealth of information was not fully exploited for learning recommender models. In particular, existing collaborative filtering (CF) approaches take into account only the binary events of user actions but totally disregard the contexts in which users' decisions are made. In this paper, we propose Collaborative Competitive Filtering (CCF), a framework for learning user preferences by modeling the choice process in recommender systems. CCF employs a multiplicative latent factor model to characterize the dyadic utility function. But unlike CF, CCF models the user behavior of choices by encoding a local competition effect. In this way, CCF allows us to leverage dyadic data that was previously lumped together with missing data in existing CF models. We present two formulations and an efficient large scale optimization algorithm. Experiments on three real-world recommendation data sets demonstrate that CCF significantly outperforms standard CF approaches in both offline and online evaluations.</p>
<p>【Keywords】:
behavior model; collaborative-competitive filtering; recommendation systems</p>
<h3 id="33. CLR: a collaborative location recommendation framework based on co-clustering.">33. CLR: a collaborative location recommendation framework based on co-clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009960">Paper Link</a>】    【Pages】:305-314</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Leung:Kenneth_Wai=Ting">Kenneth Wai-Ting Leung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Dik_Lun">Dik Lun Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a></p>
<p>【Abstract】:
GPS data tracked on mobile devices contains rich information about human activities and preferences. In this paper, GPS data is used in location-based services (LBSs) to provide collaborative location recommendations. We observe that most existing LBSs provide location recommendations by clustering the User-Location matrix. Since the User-Location matrix created based on GPS data is huge, there are two major problems with these methods. First, the number of similar locations that need to be considered in computing the recommendations can be numerous. As a result, the identification of truly relevant locations from numerous candidates is challenging. Second, the clustering process on large matrix is time consuming. Thus, when new GPS data arrives, complete re-clustering of the whole matrix is infeasible. To tackle these two problems, we propose the Collaborative Location Recommendation (CLR) framework for location recommendation. By considering activities (i.e., temporal preferences) and different user classes (i.e., Pattern Users, Normal Users, and Travelers) in the recommendation process, CLR is capable of generating more precise and refined recommendations to the users compared to the existing methods. Moreover, CLR employs a dynamic clustering algorithm CADC to cluster the trajectory data into groups of similar users, similar activities and similar locations efficiently by supporting incremental update of the groups when new GPS trajectory data arrives. We evaluate CLR with a real-world GPS dataset, and confirm that the CLR framework provides more accurate location recommendations compared to the existing methods.</p>
<p>【Keywords】:
co-clustering; collaborative filtering; location recommendation</p>
<h3 id="34. Functional matrix factorizations for cold-start recommendation.">34. Functional matrix factorizations for cold-start recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009961">Paper Link</a>】    【Pages】:315-324</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ke">Ke Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shuang=Hong">Shuang-Hong Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Hongyuan">Hongyuan Zha</a></p>
<p>【Abstract】:
A key challenge in recommender system research is how to effectively profile new users, a problem generally known as cold-start recommendation. Recently the idea of progressively querying user responses through an initial interview process has been proposed as a useful new user preference elicitation strategy. In this paper, we present functional matrix factorization (fMF), a novel cold-start recommendation method that solves the problem of initial interview construction within the context of learning user and item profiles. Specifically, fMF constructs a decision tree for the initial interview with each node being an interview question, enabling the recommender to query a user adaptively according to her prior responses. More importantly, we associate latent profiles for each node of the tree --- in effect restricting the latent profiles to be a function of possible answers to the interview questions --- which allows the profiles to be gradually refined through the interview process based on user responses. We develop an iterative optimization algorithm that alternates between decision tree construction and latent profiles extraction as well as a regularization scheme that takes into account of the tree structure. Experimental results on three benchmark recommendation data sets demonstrate that the proposed fMF algorithm significantly outperforms existing methods for cold-start recommendation.</p>
<p>【Keywords】:
cold-start problem; collaborative filtering; decision tree; functional matrix factorization; recommender systems</p>
<h3 id="35. Exploiting geographical influence for collaborative point-of-interest recommendation.">35. Exploiting geographical influence for collaborative point-of-interest recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009962">Paper Link</a>】    【Pages】:325-334</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Mao">Mao Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Peifeng">Peifeng Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Dik_Lun">Dik Lun Lee</a></p>
<p>【Abstract】:
In this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches.</p>
<p>【Keywords】:
collaborative recommendation; geographical influence; location-based social networks</p>
<h2 id="Users II    4">Users II    4</h2>
<h3 id="36. Why searchers switch: understanding and predicting engine switching rationales.">36. Why searchers switch: understanding and predicting engine switching rationales.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009964">Paper Link</a>】    【Pages】:335-344</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Qi">Qi Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yunqiao">Yunqiao Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Anderson:Blake">Blake Anderson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dumais:Susan_T=">Susan T. Dumais</a></p>
<p>【Abstract】:
Search engine switching is the voluntary transition between Web search engines. Engine switching can occur for a number of reasons, including user dissatisfaction with search results, a desire for broader topic coverage or verification, user preferences, or even unintentionally. An improved understanding of switching rationales allows search providers to tailor the search experience according to the different causes. In this paper we study the reasons behind search engine switching within a session. We address the challenge of identifying switching rationales by designing and implementing client-side instrumentation to acquire in-situ feedbacks from users. Using this feedback, we investigate in detail the reasons that users switch engines within a session. We also study the relationship between implicit behavioral signals and the switching causes, and develop and evaluate models to predict the reasons for switching. In addition, we collect editorial judgments of switching rationales by third-party judges and show that we can recover switching causes a posteriori. Our findings provide valuable insights into why users switch search engines in a session and demonstrate the relationship between search behavior and switching motivations. The findings also reveal sufficient behavioral consistency to afford accurate prediction of switching rationale, which can be used to dynamically adapt the search experience and derive more accurate competitive metrics.</p>
<p>【Keywords】:
predicting switching rationales; search engine switching</p>
<h3 id="37. Find it if you can: a game for modeling different types of web search success using interaction data.">37. Find it if you can: a game for modeling different types of web search success using interaction data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009965">Paper Link</a>】    【Pages】:345-354</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Ageev:Mikhail">Mikhail Ageev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Qi">Qi Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lagun:Dmitry">Dmitry Lagun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a></p>
<p>【Abstract】:
A better understanding of strategies and behavior of successful searchers is crucial for improving the experience of all searchers. However, research of search behavior has been struggling with the tension between the relatively small-scale, but controlled lab studies, and the large-scale log-based studies where the searcher intent and many other important factors have to be inferred. We present our solution for performing controlled, yet realistic, scalable, and reproducible studies of searcher behavior. We focus on difficult informational tasks, which tend to frustrate many users of the current web search technology. First, we propose a principled formalization of different types of "success" for informational search, which encapsulate and sharpen previously proposed models. Second, we present a scalable game-like infrastructure for crowdsourcing search behavior studies, specifically targeted towards capturing and evaluating successful search strategies on informational tasks with known intent. Third, we report our analysis of search success using these data, which confirm and extends previous findings. Finally, we demonstrate that our model can predict search success more effectively than the existing state-of-the-art methods, on both our data and on a different set of log data collected from regular search engine sessions. Together, our search success models, the data collection infrastructure, and the associated behavior analysis techniques, significantly advance the study of success in web search.</p>
<p>【Keywords】:
query log analysis; user studies; web search success</p>
<h3 id="38. Measuring improvement in user search performance resulting from optimal search tips.">38. Measuring improvement in user search performance resulting from optimal search tips.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009966">Paper Link</a>】    【Pages】:355-364</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moraveji:Neema">Neema Moraveji</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Russell:Daniel_M=">Daniel M. Russell</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bien:Jacob">Jacob Bien</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mease:David">David Mease</a></p>
<p>【Abstract】:
Web search performance can be improved by either improving the search engine itself or by educating the user to search more efficiently. There is a large amount of literature describing techniques for measuring the former; whereas, improvements resulting from the latter are more difficult to quantify. In this paper we demonstrate an experimental methodology that proves to successfully quantify improvements from user education. The user education in our study is realized in the form of tactical search feature tips that expand user awareness of task-relevant tools and features of the search application. Initially, these tips are presented in an idealized situation: each tip is shown at the same time as the study participants are given a task that is constructed to benefit from the specific tip. However, we also present a follow-up study roughly one week later in which the search tips are no longer presented but the study participants who previously were shown search tips still demonstrate improved search efficiency compared to the control group. This research has implications for search user interface designers and the study of information retrieval systems.</p>
<p>【Keywords】:
assistance; effectiveness measures; efficiency; experimental design; expertise; query reformulation; search interface; suggestions; tactics; tips; user studies</p>
<h3 id="39. ViewSer: enabling large-scale remote user studies of web search examination and interaction.">39. ViewSer: enabling large-scale remote user studies of web search examination and interaction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009967">Paper Link</a>】    【Pages】:365-374</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lagun:Dmitry">Dmitry Lagun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a></p>
<p>【Abstract】:
Web search behaviour studies, including eye-tracking studies of search result examination, have resulted in numerous insights to improve search result quality and presentation. Yet, eye tracking studies have been restricted in scale, due to the expense and the effort required. Furthermore, as the reach of the Web expands, it becomes increasingly important to understand how searchers around the world see and interact with the search results. To address both challenges, we introduce ViewSer, a novel methodology for performing web search examination studies remotely, at scale, and without requiring eye-tracking equipment. ViewSer operates by automatically modifying the appearance of a search engine result page, to clearly show one search result at a time as if through a "viewport", while partially blurring the rest and allowing the participant to move the viewport naturally with a computer mouse or trackpad. Remarkably, the resulting result viewing and clickthrough patterns agree closely with unrestricted viewing of results, as measured by eye-tracking equipment, validated by a study with over 100 participants. We also explore applications of ViewSer to practical search tasks, such as analyzing the search result summary (snip- pet) attractiveness, result re-ranking, and evaluating snippet quality. These experiments could have only be done previously by tracking the eye movements for a small number of subjects in the lab. In contrast, our study was performed with over 100 participants, allowing us to reproduce and extend previous findings, establishing ViewSer as a valuable tool for large-scale search behavior experiments.</p>
<p>【Keywords】:
remote user studies; web search behavior; web search evaluation</p>
<h2 id="Query analysis II    4">Query analysis II    4</h2>
<h3 id="40. CrowdLogging: distributed, private, and anonymous search logging.">40. CrowdLogging: distributed, private, and anonymous search logging.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009969">Paper Link</a>】    【Pages】:375-384</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Feild:Henry_Allen">Henry Allen Feild</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Allan:James">James Allan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Glatt:Joshua">Joshua Glatt</a></p>
<p>【Abstract】:
We describe CrowdLogging, an approach for distributed search log collection, storage, and mining, with the dual goals of preserving privacy and making the mined information broadly available. Most search log mining approaches and most privacy enhancing schemes have focused on centralized search logs and methods for disseminating them to third parties. In our approach, a user's search log is encrypted and shared in such a way that (a) the source of a search behavior artifact, such as a query, is unknown and (b) extremely rare artifacts---that is, artifacts more likely to contain private information---are not revealed. The approach works with any search behavior artifact that can be extracted from a search log, including queries, query reformulations, and query-click pairs. In this work, we: (1) present a distributed search log collection, storage, and mining framework; (2) compare several privacy policies, including differential privacy, showing the trade-offs between strong guarantees and the utility of the released data; (3) demonstrate the impact of our approach using two existing research query logs; and (4) describe a pilot study for which we implemented a version of the framework.</p>
<p>【Keywords】:
distributed query logs; private query log analysis</p>
<h3 id="41. Out of sight, not out of mind: on the effect of social and physical detachment on information need.">41. Out of sight, not out of mind: on the effect of social and physical detachment on information need.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009970">Paper Link</a>】    【Pages】:385-394</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yom=Tov:Elad">Elad Yom-Tov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Diaz:Fernando">Fernando Diaz</a></p>
<p>【Abstract】:
The information needs of users and the documents which answer it are frequently contingent on the different characteristics of users. This is especially evident during natural disasters, such as earthquakes and violent weather incidents, which create a strong transient information need. In this paper we investigate how the information need of users is affected by their physical detachment, as estimated by their physical location in relation to that of the event, and by their social detachment, as quantified by the number of their acquaintances who may be affected by the event. Drawing on large-scale data from three major events, we show that social and physical detachment levels of users are a major influence on their information needs, as manifested by their search engine queries. We demonstrate how knowing social and physical detachment levels can assist in improving retrieval for two applications: identifying search queries related to events and ranking results in response to event-related queries. We find that the average precision in identifying relevant search queries improves by approximately 18%, and that the average precision of ranking that uses detachment information improves by 10%.</p>
<p>【Keywords】:
distance; information; need; physical; social</p>
<h3 id="42. Scalable multi-dimensional user intent identification using tree structured distributions.">42. Scalable multi-dimensional user intent identification using tree structured distributions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009971">Paper Link</a>】    【Pages】:395-404</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jethava:Vinay">Vinay Jethava</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Calder=oacute=n=Benavides:Liliana">Liliana Calderón-Benavides</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Baeza=Yates:Ricardo_A=">Ricardo A. Baeza-Yates</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bhattacharyya:Chiranjib">Chiranjib Bhattacharyya</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dubhashi:Devdatt_P=">Devdatt P. Dubhashi</a></p>
<p>【Abstract】:
The problem of identifying user intent has received considerable attention in recent years, particularly in the context of improving the search experience via query contextualization. Intent can be characterized by multiple dimensions, which are often not observed from query words alone. Accurate identification of Intent from query words remains a challenging problem primarily because it is extremely difficult to discover these dimensions. The problem is often significantly compounded due to lack of representative training sample. We present a generic, extensible framework for learning the multi-dimensional representation of user intent from the query words. The approach models the latent relationships between facets using tree structured distribution which leads to an efficient and convergent algorithm, FastQ, for identifying the multi-faceted intent of users based on just the query words. We also incorporated WordNet to extend the system capabilities to queries which contain words that do not appear in the training data. Empirical results show that FastQ yields accurate identification of intent when compared to a gold standard.</p>
<p>【Keywords】:
chow-liu; facets; fastq; query intent; web search; wordnet</p>
<h3 id="43. Social annotation in query expansion: a machine learning approach.">43. Social annotation in query expansion: a machine learning approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009972">Paper Link</a>】    【Pages】:405-414</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Yuan">Yuan Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Hongfei">Hongfei Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Song">Song Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Zheng">Zheng Ye</a></p>
<p>【Abstract】:
Automatic query expansion technologies have been proven to be effective in many information retrieval tasks. Most existing approaches are based on the assumption that the most informative terms in top-retrieved documents can be viewed as context of the query and thus can be used for query expansion. One problem with these approaches is that some of the expansion terms extracted from feedback documents are irrelevant to the query, and thus may hurt the retrieval performance. In social annotations, users provide different keywords describing the respective Web pages from various aspects. These features may be used to boost IR performance. However, to date, the potential of social annotation for this task has been largely unexplored. In this paper, we explore the possibility and potential of social annotation as a new resource for extracting useful expansion terms. In particular, we propose a term ranking approach based on social annotation resource. The proposed approach consists of two phases: (1) in the first phase, we propose a term-dependency method to choose the most likely expansion terms; (2) in the second phase, we develop a machine learning method for term ranking, which is learnt from the statistics of the candidate expansion terms, using ListNet. Experimental results on three TREC test collections show that the retrieval performance can be improved when the term ranking method is used. In addition, we also demonstrate that terms selected by the term-dependency method from social annotation resources are beneficial to improve the retrieval performance.</p>
<p>【Keywords】:
learning to rank; query expansion; social annotation</p>
<h2 id="Communities    4">Communities    4</h2>
<h3 id="44. Predicting web searcher satisfaction with existing community-based answers.">44. Predicting web searcher satisfaction with existing community-based answers.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009974">Paper Link</a>】    【Pages】:415-424</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Qiaoling">Qiaoling Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dror:Gideon">Gideon Dror</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gabrilovich:Evgeniy">Evgeniy Gabrilovich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maarek:Yoelle">Yoelle Maarek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pelleg:Dan">Dan Pelleg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szpektor:Idan">Idan Szpektor</a></p>
<p>【Abstract】:
Community-based Question Answering (CQA) sites, such as Yahoo! Answers, Baidu Knows, Naver, and Quora, have been rapidly growing in popularity. The resulting archives of posted answers to questions, in Yahoo! Answers alone, already exceed in size 1 billion, and are aggressively indexed by web search engines. In fact, a large number of search engine users benefit from these archives, by finding existing answers that address their own queries. This scenario poses new challenges and opportunities for both search engines and CQA sites. To this end, we formulate a new problem of predicting the satisfaction of web searchers with CQA answers. We analyze a large number of web searches that result in a visit to a popular CQA site, and identify unique characteristics of searcher satisfaction in this setting, namely, the effects of query clarity, query-to-question match, and answer quality. We then propose and evaluate several approaches to predicting searcher satisfaction that exploit these characteristics. To the best of our knowledge, this is the first attempt to predict and validate the usefulness of CQA archives for external searchers, rather than for the original askers. Our results suggest promising directions for improving and exploiting community question answering services in pursuit of satisfying even more Web search queries.</p>
<p>【Keywords】:
answer quality; community question answering; query clarity; query-question match; searcher satisfaction</p>
<h3 id="45. Competition-based user expertise score estimation.">45. Competition-based user expertise score estimation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009975">Paper Link</a>】    【Pages】:425-434</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Jing">Jing Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Young=In">Young-In Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Chin=Yew">Chin-Yew Lin</a></p>
<p>【Abstract】:
In this paper, we consider the problem of estimating the relative expertise score of users in community question and answering services (CQA). Previous approaches typically only utilize the explicit question answering relationship between askers and an-swerers and apply link analysis to address this problem. The im-plicit pairwise comparison between two users that is implied in the best answer selection is ignored. Given a question and answering thread, it's likely that the expertise score of the best answerer is higher than the asker's and all other non-best answerers'. The goal of this paper is to explore such pairwise comparisons inferred from best answer selections to estimate the relative expertise scores of users. Formally, we treat each pairwise comparison between two users as a two-player competition with one winner and one loser. Two competition models are proposed to estimate user expertise from pairwise comparisons. Using the NTCIR-8 CQA task data with 3 million questions and introducing answer quality prediction based evaluation metrics, the experimental results show that the pairwise comparison based competition model significantly outperforms link analysis based approaches (PageRank and HITS) and pointwise approaches (number of best answers and best answer ratio) for estimating the expertise of active users. Furthermore, it's shown that pairwise comparison based competi-tion models have better discriminative power than other methods. It's also found that answer quality (best answer) is an important factor to estimate user expertise.</p>
<p>【Keywords】:
community question answering; competition model; expertise estimation; pairwise comparison</p>
<h3 id="46. Learning online discussion structures by conditional random fields.">46. Learning online discussion structures by conditional random fields.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009976">Paper Link</a>】    【Pages】:435-444</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Hongning">Hongning Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chi">Chi Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
Online forum discussions are emerging as valuable information repository, where knowledge is accumulated by the interaction among users, leading to multiple threads with structures. Such replying structure in each thread conveys important information about the discussion content. Unfortunately, not all the online forum sites would explicitly record such replying relationship, making it hard to for both users and computers to digest the information buried in a thread discussion. In this paper, we propose a probabilistic model in the Conditional Random Fields framework to predict the replying structure for a threaded online discussion. Different from previous thread reconstruction methods, most of which fail to consider dependency between the posts, we cast the problem as a supervised structure learning problem to incorporate the features describing the structural dependency among the discussion content and learn their relationship. Experiment results on three different online forums show that the proposed method can well capture the replying structures in online discussion threads, and multiple tasks such as forum search and question answering can benefit from the reconstructed replying structures.</p>
<p>【Keywords】:
replying relation reconstruction; structure learning; threaded discussion</p>
<h3 id="47. Mining topics on participations for community discovery.">47. Mining topics on participations for community discovery.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009977">Paper Link</a>】    【Pages】:445-454</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zheng:Guoqing">Guoqing Zheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jinwen">Jinwen Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Lichun">Lichun Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Shengliang">Shengliang Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bao:Shenghua">Shenghua Bao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Zhong">Zhong Su</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Dingyi">Dingyi Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:
Community discovery on large-scale linked document corpora has been a hot research topic for decades. There are two types of links. The first one, which we call d2d-link, indicates connectiveness among different documents, such as blog references and research paper citations. The other one, which we call u2u-link, represents co-occurrences or simultaneous participations of different users in one document and typically each document from u2u-link corpus has more than one user/author. Examples of u2u-link data covers email archives and research paper co-authorship networks. Community discovery in d2d-link data has achieved much success, while methods for that in u2u-link data either make no use of the textual content of the documents or make oversimplified assumptions about the users and the textual content. In this paper we propose a general approach of community discovery for u2u-link data, i.e., multiple user data, by placing topical variables on multiple authors' participations in documents. Experiments on a research proceeding co-authorship corpus and a New York Times news corpus show the effectiveness of our model.</p>
<p>【Keywords】:
community discovery; hierarchical dirichlet process; nonparametric statistical model; topics on participations</p>
<h2 id="Classification    3">Classification    3</h2>
<h3 id="48. Authorship classification: a discriminative syntactic tree mining approach.">48. Authorship classification: a discriminative syntactic tree mining approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009979">Paper Link</a>】    【Pages】:455-464</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Sangkyum">Sangkyum Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Hyungsul">Hyungsul Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weninger:Tim">Tim Weninger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Hyun_Duk">Hyun Duk Kim</a></p>
<p>【Abstract】:
In the past, there have been dozens of studies on automatic authorship classification, and many of these studies concluded that the writing style is one of the best indicators for original authorship. From among the hundreds of features which were developed, syntactic features were best able to reflect an author's writing style. However, due to the high computational complexity for extracting and computing syntactic features, only simple variations of basic syntactic features such as function words, POS(Part of Speech) tags, and rewrite rules were considered. In this paper, we propose a new feature set of k-embedded-edge subtree patterns that holds more syntactic information than previous feature sets. We also propose a novel approach to directly mining them from a given set of syntactic trees. We show that this approach reduces the computational burden of using complex syntactic structures as the feature set. Comprehensive experiments on real-world datasets demonstrate that our approach is reliable and more accurate than previous studies.</p>
<p>【Keywords】:
authorship attribution; authorship classification; authorship discrimination; text categorization; text mining</p>
<h3 id="49. On theme location discovery for travelogue services.">49. On theme location discovery for travelogue services.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009980">Paper Link</a>】    【Pages】:465-474</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Mao">Mao Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiao:Rong">Rong Xiao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wang=Chien">Wang-Chien Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Xing">Xing Xie</a></p>
<p>【Abstract】:
In this paper, we aim to develop a travelogue service that discovers and conveys various travelogue digests, in form of theme locations, geographical scope, traveling trajectory and location snippet, to users. In this service, theme locations in a travelogue are the core information to discover. Thus we aim to address the problem of theme location discovery to enable the above travelogue services. Due to the inherent ambiguity of location relevance, we perform location relevance mining (LRM) in two complementary angles, relevance classification and relevance ranking, to provide comprehensive understanding of locations. Furthermore, we explore the textual (e.g., surrounding words) and geographical (e.g., geographical relationship among locations) features of locations to develop a co-training model for enhancement of classification performance. Built upon the mining result of LRM, we develop a series of techniques for provisioning of the aforementioned travelogue digests in our travelogue system. Finally, we conduct comprehensive experiments on collected travelogues to evaluate the performance of our location relevance mining techniques and demonstrate the effectiveness of the travelogue service.</p>
<p>【Keywords】:
classification; co-training; ranking; travelogue services</p>
<h3 id="50. Effective sentiment stream analysis with self-augmenting training and demand-driven projection.">50. Effective sentiment stream analysis with self-augmenting training and demand-driven projection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009981">Paper Link</a>】    【Pages】:475-484</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Silva:Ismael_Santana">Ismael Santana Silva</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gomide:Jana=iacute=na">Janaína Gomide</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Veloso:Adriano">Adriano Veloso</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meira_Jr=:Wagner">Wagner Meira Jr.</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Ferreira:Renato">Renato Ferreira</a></p>
<p>【Abstract】:
How do we analyze sentiments over a set of opinionated Twitter messages? This issue has been widely studied in recent years, with a prominent approach being based on the application of classification techniques. Basically, messages are classified according to the implicit attitude of the writer with respect to a query term. A major concern, however, is that Twitter (and other media channels) follows the data stream model, and thus the classifier must operate with limited resources, including labeled data for training classification models. This imposes serious challenges for current classification techniques, since they need to be constantly fed with fresh training messages, in order to track sentiment drift and to provide up-to-date sentiment analysis. We propose solutions to this problem. The heart of our approach is a training augmentation procedure which takes as input a small training seed, and then it automatically incorporates new relevant messages to the training data. Classification models are produced on-the-fly using association rules, which are kept up-to-date in an incremental fashion, so that at any given time the model properly reflects the sentiments in the event being analyzed. In order to track sentiment drift, training messages are projected on a demand driven basis, according to the content of the message being classified. Projecting the training data offers a series of advantages, including the ability to quickly detect trending information emerging in the stream. We performed the analysis of major events in 2010, and we show that the prediction performance remains about the same, or even increases, as the stream passes and new training messages are acquired. This result holds for different languages, even in cases where sentiment distribution changes over time, or in cases where the initial training seed is rather small. We derive lower-bounds for prediction performance, and we show that our approach is extremely effective under diverse learning scenarios, providing gains that range from 7% to 58%.</p>
<p>【Keywords】:
sentiment analysis; sentiment drift; streams; twitter</p>
<h2 id="Retrieval models II    3">Retrieval models II    3</h2>
<h3 id="51. Hypergeometric language models for republished article finding.">51. Hypergeometric language models for republished article finding.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009983">Paper Link</a>】    【Pages】:485-494</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tsagkias:Manos">Manos Tsagkias</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weerkamp:Wouter">Wouter Weerkamp</a></p>
<p>【Abstract】:
Republished article finding is the task of identifying instances of articles that have been published in one source and republished more or less verbatim in another source, which is often a social media source. We address this task as an ad hoc retrieval problem, using the source article as a query. Our approach is based on language modeling. We revisit the assumptions underlying the unigram language model taking into account the fact that in our setup queries are as long as complete news articles. We argue that in this case, the underlying generative assumption of sampling words from a document with replacement, i.e., the multinomial modeling of documents, produces less accurate query likelihood estimates. To make up for this discrepancy, we consider distributions that emerge from sampling without replacement: the central and non-central hypergeometric distributions. We present two retrieval models that build on top of these distributions: a log odds model and a bayesian model where document parameters are estimated using the Dirichlet compound multinomial distribution. We analyse the behavior of our new models using a corpus of news articles and blog posts and find that for the task of republished article finding, where we deal with queries whose length approaches the length of the documents to be retrieved, models based on distributions associated with sampling without replacement outperform traditional models based on multinomial distributions.</p>
<p>【Keywords】:
hypergeometric; language models; linking; multinomial; online news; social media</p>
<h3 id="52. Estimation methods for ranking recent information.">52. Estimation methods for ranking recent information.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009984">Paper Link</a>】    【Pages】:495-504</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/e/Efron:Miles">Miles Efron</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Golovchinsky:Gene">Gene Golovchinsky</a></p>
<p>【Abstract】:
Temporal aspects of documents can impact relevance for certain kinds of queries. In this paper, we build on earlier work of modeling temporal information. We propose an extension to the Query Likelihood Model that incorporates query-specific information to estimate rate parameters, and we introduce a temporal factor into language model smoothing and query expansion using pseudo-relevance feedback. We evaluate these extensions using a Twitter corpus and two newspaper article collections. Results suggest that, compared to prior approaches, our models are more effective at capturing the temporal variability of relevance associated with some topics.</p>
<p>【Keywords】:
information retrieval; microblogs; ranking algorithms; time</p>
<h3 id="53. Query by document via a decomposition-based two-level retrieval approach.">53. Query by document via a decomposition-based two-level retrieval approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009985">Paper Link</a>】    【Pages】:505-514</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weng:Linkai">Linkai Weng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Zhiwei">Zhiwei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Rui">Rui Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yaoxue">Yaoxue Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Yuezhi">Yuezhi Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Laurence_Tianruo">Laurence Tianruo Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Lei">Lei Zhang</a></p>
<p>【Abstract】:
Retrieving similar documents from a large-scale text corpus according to a given document is a fundamental technique for many applications. However, most of existing indexing techniques have difficulties to address this problem due to special properties of a document query, e.g. high dimensionality, sparse representation and semantic concern. Towards addressing this problem, we propose a two-level retrieval solution based on a document decomposition idea. A document is decomposed to a compact vector and a few document specific keywords by a dimension reduction approach. The compact vector embodies the major semantics of a document, and the document specific keywords complement the discriminative power lost in dimension reduction process. We adopt locality sensitive hashing (LSH) to index the compact vectors, which guarantees to quickly find a set of related documents according to the vector of a query document. Then we re-rank documents in this set by their document specific keywords. In experiments, we obtained promising results on various datasets in terms of both accuracy and performance. We demonstrated that this solution is able to index large-scale corpus for efficient similarity-based document retrieval.</p>
<p>【Keywords】:
document decomposition; indexing; similarity search</p>
<h2 id="Image search    3">Image search    3</h2>
<h3 id="54. Integrating hierarchical feature selection and classifier training for multi-label image annotation.">54. Integrating hierarchical feature selection and classifier training for multi-label image annotation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009987">Paper Link</a>】    【Pages】:515-524</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Cheng">Cheng Jin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Chunlei">Chunlei Yang</a></p>
<p>【Abstract】:
It is well accepted that using high-dimensional multi-modal visual features for image content representation and classifier training may achieve more sufficient characterization of the diverse visual properties of the images and further result in higher discrimination power of the classifiers. However, training the classifiers in a high-dimensional multi-modal feature space requires a large number of labeled training images, which will further result in the problem of curse of dimensionality. To tackle this problem, a hierarchical feature subset selection algorithm is proposed to enable more accurate image classification, where the processes for feature selection and classifier training are seamlessly integrated in a single framework. First, a feature hierarchy (i.e., concept tree for automatic feature space partition and organization) is used to automatically partition high-dimensional heterogeneous multi-modal visual features into multiple low-dimensional homogeneous single-modal feature subsets according to their certain physical meanings and each of them is used to characterize one certain type of the diverse visual properties of the images. Second, principal component analysis (PCA) is performed on each homogeneous singlemodal feature subset to select the most representative feature dimensions and a weak classifier is learned simultaneously. After the weak classifiers and their representative feature dimensions are available for all these homogeneous single-modal feature subsets, they are combined to generate an ensemble image classifier and achieve hierarchical feature subset selection. Our experiments on a specific domain of natural images have also obtained very positive results.</p>
<p>【Keywords】:
boosting; feature hierarchy; hierarchical feature selection; svm image classifier</p>
<h3 id="55. Efficient manifold ranking for image retrieval.">55. Efficient manifold ranking for image retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009988">Paper Link</a>】    【Pages】:525-534</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Bin">Bin Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bu:Jiajun">Jiajun Bu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Chun">Chun Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Deng">Deng Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Xiaofei">Xiaofei He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Wei">Wei Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Luo:Jiebo">Jiebo Luo</a></p>
<p>【Abstract】:
Manifold Ranking (MR), a graph-based ranking algorithm, has been widely applied in information retrieval and shown to have excellent performance and feasibility on a variety of data types. Particularly, it has been successfully applied to content-based image retrieval, because of its outstanding ability to discover underlying geometrical structure of the given image database. However, manifold ranking is computationally very expensive, both in graph construction and ranking computation stages, which significantly limits its applicability to very large data sets. In this paper, we extend the original manifold ranking algorithm and propose a new framework named Efficient Manifold Ranking (EMR). We aim to address the shortcomings of MR from two perspectives: scalable graph construction and efficient computation. Specifically, we build an anchor graph on the data set instead of the traditional k-nearest neighbor graph, and design a new form of adjacency matrix utilized to speed up the ranking computation. The experimental results on a real world image database demonstrate the effectiveness and efficiency of our proposed method. With a comparable performance to the original manifold ranking, our method significantly reduces the computational time, makes it a promising method to large scale real world retrieval problems.</p>
<p>【Keywords】:
efficientmanifold ranking; graph-based algorithms; image retrieval; out-of-sample</p>
<h3 id="56. Mining weakly labeled web facial images for search-based face annotation.">56. Mining weakly labeled web facial images for search-based face annotation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009989">Paper Link</a>】    【Pages】:535-544</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Dayong">Dayong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hoi:Steven_C=_H=">Steven C. H. Hoi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He_0001:Ying">Ying He</a></p>
<p>【Abstract】:
In this paper, we investigate a search-based face annotation framework by mining weakly labeled facial images that are freely available on the internet. A key component of such a search-based annotation paradigm is to build a database of facial images with accurate labels. This is however challenging since facial images on the WWW are often noisy and incomplete. To improve the label quality of raw web facial images, we propose an effective Unsupervised Label Refinement (ULR) approach for refining the labels of web facial images by exploring machine learning techniques. We develop effective optimization algorithms to solve the large-scale learning tasks efficiently, and conduct an extensive empirical study on a web facial image database with 400 persons and 40,000 web facial images. Encouraging results showed that the proposed ULR technique can significantly boost the performance of the promising search-based face annotation scheme.</p>
<p>【Keywords】:
auto face annotation; unsupervised learning; web facial images</p>
<h2 id="Indexing    4">Indexing    4</h2>
<h3 id="57. Temporal index sharding for space-time efficiency in archive search.">57. Temporal index sharding for space-time efficiency in archive search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009991">Paper Link</a>】    【Pages】:545-554</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Anand:Avishek">Avishek Anand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bedathur:Srikanta_J=">Srikanta J. Bedathur</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Berberich:Klaus">Klaus Berberich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schenkel:Ralf">Ralf Schenkel</a></p>
<p>【Abstract】:
Time-travel queries that couple temporal constraints with keyword queries are useful in searching large-scale archives of time-evolving content such as the web archives or wikis. Typical approaches for efficient evaluation of these queries involve slicing either the entire collection [20] or individual index lists [10] along the time-axis. Both these methods are not satisfactory since they sacrifice compactness of index for processing efficiency making them either too big or, otherwise, too slow. We present a novel index organization scheme that shards each index list with almost zero increase in index size but still minimizes the cost of reading index entries during query processing. Based on the optimal sharding thus btained, we develop a practically efficient sharding that takes into account the different costs of random and sequential accesses. Our algorithm merges shards from the optimal solution to allow for a few extra sequential accesses while gaining significantly by reducing the number of random accesses. We empirically establish the effectiveness of our sharding scheme with experiments over the revision history of the English Wikipedia between 2001-2005 (approx 700 GB) and an archive of U.K. governmental web sites (approx 400 GB). Our results demonstrate the feasibility of faster time-travel query processing with no space overhead.</p>
<p>【Keywords】:
inverted index; sharding; slicing; time-travel text search; web archives</p>
<h3 id="58. Inverted indexes for phrases and strings.">58. Inverted indexes for phrases and strings.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009992">Paper Link</a>】    【Pages】:555-564</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Patil:Manish">Manish Patil</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thankachan:Sharma_V=">Sharma V. Thankachan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shah:Rahul">Rahul Shah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hon:Wing=Kai">Wing-Kai Hon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vitter:Jeffrey_Scott">Jeffrey Scott Vitter</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chandrasekaran:Sabrina">Sabrina Chandrasekaran</a></p>
<p>【Abstract】:
Inverted indexes are the most fundamental and widely used data structures in information retrieval. For each unique word occurring in a document collection, the inverted index stores a list of the documents in which this word occurs. Compression techniques are often applied to further reduce the space requirement of these lists. However, the index has a shortcoming, in that only predefined pattern queries can be supported efficiently. In terms of string documents where word boundaries are undefined, if we have to index all the substrings of a given document, then the storage quickly becomes quadratic in the data size. Also, if we want to apply the same type of indexes for querying phrases or sequence of words, then the inverted index will end up storing redundant information. In this paper, we show the first set of inverted indexes which work naturally for strings as well as phrase searching. The central idea is to exclude document d in the inverted list of a string P if every occurrence of P in d is subsumed by another string of which P is a prefix. With this we show that our space utilization is close to the optimal. Techniques from succinct data structures are deployed to achieve compression while allowing fast access in terms of frequency and document id based retrieval. Compression and speed trade-offs are evaluated for different variants of the proposed index. For phrase searching, we show that our indexes compare favorably against a typical inverted index deploying position-wise intersections. We also show efficient top-k based retrieval under relevance metrics like frequency and tf-idf.</p>
<p>【Keywords】:
compressed data structures; inverted indexes; phrase searching; top-k queries</p>
<h3 id="59. Faster temporal range queries over versioned text.">59. Faster temporal range queries over versioned text.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009993">Paper Link</a>】    【Pages】:565-574</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/He:Jinru">Jinru He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suel:Torsten">Torsten Suel</a></p>
<p>【Abstract】:
Versioned textual collections are collections that retain multiple versions of a document as it evolves over time. Important large-scale examples are Wikipedia and the web collection of the Internet Archive. Search queries over such collections often use keywords as well as temporal constraints, most commonly a time range of interest. In this paper, we study how to support such temporal range queries over versioned text. Our goal is to process these queries faster than the corresponding keyword-only queries, by exploiting the additional constraint. A simple approach might partition the index into different time ranges, and then access only the relevant parts. However, specialized inverted index compression techniques are crucial for large versioned collections, and a naive partitioning can negatively affect index size and query throughput. We show how to achieve high query throughput by using smart index partitioning techniques that take index compression into account. Experiments on over 85 million versions of Wikipedia articles show that queries can be executed in a few milliseconds on memory-based index structures, and only slightly more time on disk-based structures. We also show how to efficiently support the recently proposed stable top-k search primitive on top of our schemes.</p>
<p>【Keywords】:
inverted index; query processing; range queries; temporal search; versioned documents</p>
<h3 id="60. Indexing strategies for graceful degradation of search quality.">60. Indexing strategies for graceful degradation of search quality.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009994">Paper Link</a>】    【Pages】:575-584</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Shuai">Shuai Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gollapudi:Sreenivas">Sreenivas Gollapudi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Ieong:Samuel">Samuel Ieong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kenthapadi:Krishnaram">Krishnaram Kenthapadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ntoulas:Alexandros">Alexandros Ntoulas</a></p>
<p>【Abstract】:
Large web search engines process billions of queries each day over tens of billions of documents with often very stringent requirements for a user's search experience, in particular, low latency and highly relevant search results. Index generation and serving are key to satisfying both these requirements. For example, the load to search engines can vary drastically when popular events happen around the world. In the case when the load is exceeding what the search engine can serve, queries will get dropped. This results in an un- graceful degradation in search quality. Another example that could increase the query load and affect the user's search experience are ambiguous queries which often result in the execution of multiple query alterations in the back end. In this paper, we look into the problem of designing robust indexing strategies, i.e. strategies that allow for a graceful degradation of search quality in both the above scenarios. We study the problems of index generation and serving using the notions of document allocation, server selection, and document replication. We explore the space of efficient algorithms for these problems and empirically corroborate with existing theory that it is hard to optimally solve the alocation and selection problems without any replication. We propose a greedy replication algorithm and study its performance under different choices of allocation and selection. Further, we show hat under random selection and allocation, our algorithm is optimal.</p>
<p>【Keywords】:
graceful degradation; indexing stratagies; search quality</p>
<h2 id="Web queries    4">Web queries    4</h2>
<h3 id="61. Incremental diversification for very large sets: a streaming-based approach.">61. Incremental diversification for very large sets: a streaming-based approach.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009996">Paper Link</a>】    【Pages】:585-594</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Minack:Enrico">Enrico Minack</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Siberski:Wolf">Wolf Siberski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nejdl:Wolfgang">Wolfgang Nejdl</a></p>
<p>【Abstract】:
Result diversification is an effective method to reduce the risk that none of the returned results satisfies a user's query intention. It has been shown to decrease query abandonment substantially. On the other hand, computing an optimally diverse set is NP-hard for the usual objectives. Existing greedy diversification algorithms require random access to the input set, rendering them impractical in the context of large result sets or continuous data. To solve this issue, we present a novel diversification approach which treats the input as a stream and processes each element in an incremental fashion, maintaining a near-optimal diverse set at any point in the stream. Our approach exhibits a linear computation and constant memory complexity with respect to input size, without significant loss of diversification quality. In an extensive evaluation on several real-world data sets, we show the applicability and efficiency of our algorithm for large result sets as well as for continuous query scenarios such as news stream subscriptions.</p>
<p>【Keywords】:
approximation; diversification; large sets; streams</p>
<h3 id="62. Intent-aware search result diversification.">62. Intent-aware search result diversification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009997">Paper Link</a>】    【Pages】:595-604</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Santos:Rodrygo_L=_T=">Rodrygo L. T. Santos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
Search result diversification has gained momentum as a way to tackle ambiguous queries. An effective approach to this problem is to explicitly model the possible aspects underlying a query, in order to maximise the estimated relevance of the retrieved documents with respect to the different aspects. However, such aspects themselves may represent information needs with rather distinct intents (e.g., informational or navigational). Hence, a diverse ranking could benefit from applying intent-aware retrieval models when estimating the relevance of documents to different aspects. In this paper, we propose to diversify the results retrieved for a given query, by learning the appropriateness of different retrieval models for each of the aspects underlying this query. Thorough experiments within the evaluation framework provided by the diversity task of the TREC 2009 and 2010 Web tracks show that the proposed approach can significantly improve state-of-the-art diversification approaches.</p>
<p>【Keywords】:
diversity; relevance; web search</p>
<h3 id="63. Parameterized concept weighting in verbose queries.">63. Parameterized concept weighting in verbose queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009998">Paper Link</a>】    【Pages】:605-614</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bendersky:Michael">Michael Bendersky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metzler:Donald">Donald Metzler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
The majority of the current information retrieval models weight the query concepts (e.g., terms or phrases) in an unsupervised manner, based solely on the collection statistics. In this paper, we go beyond the unsupervised estimation of concept weights, and propose a parameterized concept weighting model. In our model, the weight of each query concept is determined using a parameterized combination of diverse importance features. Unlike the existing supervised ranking methods, our model learns importance weights not only for the explicit query concepts, but also for the latent concepts that are associated with the query through pseudo-relevance feedback. The experimental results on both newswire and web TREC corpora show that our model consistently and significantly outperforms a wide range of state-of-the-art retrieval models. In addition, our model significantly reduces the number of latent concepts used for query expansion compared to the non-parameterized pseudo-relevance feedback based models.</p>
<p>【Keywords】:
parameterized concept weighting; query expansion</p>
<h3 id="64. UPS: efficient privacy protection in personalized web search.">64. UPS: efficient privacy protection in personalized web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2009999">Paper Link</a>】    【Pages】:615-624</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0001:Gang">Gang Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bai:He">He Bai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shou:Lidan">Lidan Shou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0005:Ke">Ke Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Yunjun">Yunjun Gao</a></p>
<p>【Abstract】:
In recent years, personalized web search (PWS) has demonstrated effectiveness in improving the quality of search service on the Internet. Unfortunately, the need for collecting private information in PWS has become a major barrier for its wide proliferation. We study privacy protection in PWS engines which capture personalities in user profiles. We propose a PWS framework called UPS that can generalize profiles in for each query according to user-specified privacy requirements. Two predictive metrics are proposed to evaluate the privacy breach risk and the query utility for hierarchical user profile. We develop two simple but effective generalization algorithms for user profiles allowing for query-level customization using our proposed metrics. We also provide an online prediction mechanism based on query utility for deciding whether to personalize a query in UPS. Extensive experiments demonstrate the efficiency and effectiveness of our framework.</p>
<p>【Keywords】:
metric; personalized; privacy; trade-off; utility</p>
<h2 id="Collaborative filtering II    4">Collaborative filtering II    4</h2>
<h3 id="65. Handling data sparsity in collaborative filtering using emotion and semantic based features.">65. Handling data sparsity in collaborative filtering using emotion and semantic based features.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010001">Paper Link</a>】    【Pages】:625-634</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moshfeghi:Yashar">Yashar Moshfeghi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Piwowarski:Benjamin">Benjamin Piwowarski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
Collaborative filtering (CF) aims to recommend items based on prior user interaction. Despite their success, CF techniques do not handle data sparsity well, especially in the case of the cold start problem where there is no past rating for an item. In this paper, we provide a framework, which is able to tackle such issues by considering item-related emotions and semantic data. In order to predict the rating of an item for a given user, this framework relies on an extension of Latent Dirichlet Allocation, and on gradient boosted trees for the final prediction. We apply this framework to movie recommendation and consider two emotion spaces extracted from the movie plot summary and the reviews, and three semantic spaces: actor, director, and genre. Experiments with the 100K and 1M MovieLens datasets show that including emotion and semantic information significantly improves the accuracy of prediction and improves upon the state-of-the-art CF techniques. We also analyse the importance of each feature space and describe some uncovered latent groups.</p>
<p>【Keywords】:
collaborative filtering; collaborative recommendation; emotion; semantic</p>
<h3 id="66. Fast context-aware recommendations with factorization machines.">66. Fast context-aware recommendations with factorization machines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010002">Paper Link</a>】    【Pages】:635-644</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rendle:Steffen">Steffen Rendle</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gantner:Zeno">Zeno Gantner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Freudenthaler:Christoph">Christoph Freudenthaler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schmidt=Thieme:Lars">Lars Schmidt-Thieme</a></p>
<p>【Abstract】:
The situation in which a choice is made is an important information for recommender systems. Context-aware recommenders take this information into account to make predictions. So far, the best performing method for context-aware rating prediction in terms of predictive accuracy is Multiverse Recommendation based on the Tucker tensor factorization model. However this method has two drawbacks: (1) its model complexity is exponential in the number of context variables and polynomial in the size of the factorization and (2) it only works for categorical context variables. On the other hand there is a large variety of fast but specialized recommender methods which lack the generality of context-aware methods. We propose to apply Factorization Machines (FMs) to model contextual information and to provide context-aware rating predictions. This approach results in fast context-aware recommendations because the model equation of FMs can be computed in linear time both in the number of context variables and the factorization size. For learning FMs, we develop an iterative optimization method that analytically finds the least-square solution for one parameter given the other ones. Finally, we show empirically that our approach outperforms Multiverse Recommendation in prediction quality and runtime.</p>
<p>【Keywords】:
context-aware recommender system; factorization machine; rating prediction; tensor factorization</p>
<h3 id="67. Filtering semi-structured documents based on faceted feedback.">67. Filtering semi-structured documents based on faceted feedback.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010003">Paper Link</a>】    【Pages】:645-654</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Lanbo">Lanbo Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yi">Yi Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Qianli">Qianli Xing</a></p>
<p>【Abstract】:
Existing adaptive filtering systems learn user profiles based on users' relevance judgments on documents. In some cases, users have some prior knowledge about what features are important for a document to be relevant. For example, a Spanish speaker may only want news written in Spanish, and thus a relevant document should contain the feature "Language: Spanish"; a researcher working on HIV knows an article with the medical subject "Subject: AIDS" is very likely to be interesting to him/her. Semi-structured documents with rich faceted metadata are increasingly prevalent over the Internet. Motivated by the commonly used faceted search interface in e-commerce, we study whether users' prior knowledge about faceted features could be exploited for filtering semi-structured documents. We envision two faceted feedback solicitation mechanisms, and propose a novel user profile learning algorithm that can incorporate user feedback on features. To evaluate the proposed work, we use two data sets from the TREC filtering track, and conduct a user study on Amazon Mechanical Turk. Our experimental results show that user feedback on faceted features is useful for filtering. The new user profile learning algorithm can effectively learn from user feedback on faceted features and performs better than several other methods adapted from the feature-based feedback techniques proposed for retrieval and text classification tasks in previous work.</p>
<p>【Keywords】:
adaptive filtering; content-based filtering; document facets; faceted feedback; labeled features; semi-structured documents; user feedback</p>
<h3 id="68. Learning relevance from heterogeneous social network and its application in online targeting.">68. Learning relevance from heterogeneous social network and its application in online targeting.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010004">Paper Link</a>】    【Pages】:655-664</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Chi">Chi Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Raina:Rajat">Rajat Raina</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fong:David">David Fong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ding">Ding Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Badros:Greg_J=">Greg J. Badros</a></p>
<p>【Abstract】:
The rise of social networking services in recent years presents new research challenges for matching users with interesting content. While the content-rich nature of these social networks offers many cues on "interests" of a user such as text in user-generated content, the links in the network, and user demographic information, there is a lack of successful methods for combining such heterogeneous data to model interest and relevance. This paper proposes a new method for modeling user interest from heterogeneous data sources with distinct but unknown importance. The model leverages links in the social graph by integrating the conceptual representation of a user's linked objects. The proposed method seeks a scalable relevance model of user interest, that can be discriminatively optimized for various relevance-centric problems, such as Internet advertisement selection, recommendation, and web search personalization. We apply our algorithm to the task of selecting relevant ads for users on Facebook's social network. We demonstrate that our algorithm can be scaled to work with historical data for all users, and learns interesting associations between concept classes automatically. We also show that using the learnt user model to predict the relevance of an ad is the single most important signal in our ranking system for new ads (with no historical clickthrough data), and overall leads to an improvement in the accuracy of the clickthrough rate prediction, a key problem in online advertising.</p>
<p>【Keywords】:
clickthrough prediction; cognitive relevance; heterogeneous social networks; online advertising</p>
<h2 id="Latent semantic analysis    3">Latent semantic analysis    3</h2>
<h3 id="69. ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews.">69. ILDA: interdependent LDA model for learning latent aspects and their ratings from online product reviews.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010006">Paper Link</a>】    【Pages】:665-674</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Moghaddam:Samaneh">Samaneh Moghaddam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ester:Martin">Martin Ester</a></p>
<p>【Abstract】:
Today, more and more product reviews become available on the Internet, e.g., product review forums, discussion groups, and Blogs. However, it is almost impossible for a customer to read all of the different and possibly even contradictory opinions and make an informed decision. Therefore, mining online reviews (opinion mining) has emerged as an interesting new research direction. Extracting aspects and the corresponding ratings is an important challenge in opinion mining. An aspect is an attribute or component of a product, e.g. 'screen' for a digital camera. It is common that reviewers use different words to describe an aspect (e.g. 'LCD', 'display', 'screen'). A rating is an intended interpretation of the user satisfaction in terms of numerical values. Reviewers usually express the rating of an aspect by a set of sentiments, e.g. 'blurry screen'. In this paper we present three probabilistic graphical models which aim to extract aspects and corresponding ratings of products from online reviews. The first two models extend standard PLSI and LDA to generate a rated aspect summary of product reviews. As our main contribution, we introduce Interdependent Latent Dirichlet Allocation (ILDA) model. This model is more natural for our task since the underlying probabilistic assumptions (interdependency between aspects and ratings) are appropriate for our problem domain. We conduct experiments on a real life dataset, Epinions.com, demonstrating the improved effectiveness of the ILDA model in terms of the likelihood of a held-out test set, and the accuracy of aspects and aspect ratings.</p>
<p>【Keywords】:
aspect identification; opinion mining; probabilistic graphical models; rating prediction; variational methods</p>
<h3 id="70. Clickthrough-based latent semantic models for web search.">70. Clickthrough-based latent semantic models for web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010007">Paper Link</a>】    【Pages】:675-684</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Jianfeng">Jianfeng Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Toutanova:Kristina">Kristina Toutanova</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yih:Wen=tau">Wen-tau Yih</a></p>
<p>【Abstract】:
This paper presents two new document ranking models for Web search based upon the methods of semantic representation and the statistical translation-based approach to information retrieval (IR). Assuming that a query is parallel to the titles of the documents clicked on for that query, large amounts of query-title pairs are constructed from clickthrough data; two latent semantic models are learned from this data. One is a bilingual topic model within the language modeling framework. It ranks documents for a query by the likelihood of the query being a semantics-based translation of the documents. The semantic representation is language independent and learned from query-title pairs, with the assumption that a query and its paired titles share the same distribution over semantic topics. The other is a discriminative projection model within the vector space modeling framework. Unlike Latent Semantic Analysis and its variants, the projection matrix in our model, which is used to map from term vectors into sematic space, is learned discriminatively such that the distance between a query and its paired title, both represented as vectors in the projected semantic space, is smaller than that between the query and the titles of other documents which have no clicks for that query. These models are evaluated on the Web search task using a real world data set. Results show that they significantly outperform their corresponding baseline models, which are state-of-the-art.</p>
<p>【Keywords】:
clickthrough data; latent semantic analysis; linear projection; topic model; translation model; web search</p>
<h3 id="71. Regularized latent semantic indexing.">71. Regularized latent semantic indexing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010008">Paper Link</a>】    【Pages】:685-694</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Quan">Quan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu_0001:Jun">Jun Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Craswell:Nick">Nick Craswell</a></p>
<p>【Abstract】:
Topic modeling can boost the performance of information retrieval, but its real-world application is limited due to scalability issues. Scaling to larger document collections via parallelization is an active area of research, but most solutions require drastic steps such as vastly reducing input vocabulary. We introduce Regularized Latent Semantic Indexing (RLSI), a new method which is designed for parallelization. It is as effective as existing topic models, and scales to larger datasets without reducing input vocabulary. RLSI formalizes topic modeling as a problem of minimizing a quadratic loss function regularized by l₂ and/or l₁ norm. This formulation allows the learning process to be decomposed into multiple sub-optimization problems which can be optimized in parallel, for example via MapReduce. We particularly propose adopting l₂ norm on topics and l₁ norm on document representations, to create a model with compact and readable topics and useful for retrieval. Relevance ranking experiments on three TREC datasets show that RLSI performs better than LSI, PLSI, and LDA, and the improvements are sometimes statistically significant. Experiments on a web dataset, containing about 1.6 million documents and 7 million terms, demonstrate a similar boost in performance on a larger corpus and vocabulary than in previous studies.</p>
<p>【Keywords】:
regularization; sparse methods; topic modeling</p>
<h2 id="Multimedia IR    3">Multimedia IR    3</h2>
<h3 id="72. Multimedia answering: enriching text QA with media information.">72. Multimedia answering: enriching text QA with media information.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010010">Paper Link</a>】    【Pages】:695-704</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nie:Liqiang">Liqiang Nie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Meng">Meng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Zheng=Jun">Zheng-Jun Zha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Guangda">Guangda Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a></p>
<p>【Abstract】:
Existing community question-answering forums usually provide only textual answers. However, for many questions, pure texts cannot provide intuitive information, while image or video contents are more appropriate. In this paper, we introduce a scheme that is able to enrich text answers with image and video information. Our scheme investigates a rich set of techniques including question/answer classification, query generation, image and video search reranking, etc. Given a question and the community-contributed answer, our approach is able to determine which type of media information should be added, and then automatically collects data from Internet to enrich the textual answer. Different from some efforts that attempt to directly answer questions with image and video data, our approach is built based on the community-contributed textual answers and thus it is more feasible and able to deal with more complex questions. We have conducted empirical study on more than 3,000 QA pairs and the results demonstrate the effectiveness of our approach.</p>
<p>【Keywords】:
cqa; medium selection; question answering; reranking</p>
<h3 id="73. Enhancing multi-label music genre classification through ensemble techniques.">73. Enhancing multi-label music genre classification through ensemble techniques.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010011">Paper Link</a>】    【Pages】:705-714</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sanden:Chris">Chris Sanden</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:John_Z=">John Z. Zhang</a></p>
<p>【Abstract】:
In the field of Music Information Retrieval (MIR), multi-label genre classification is the problem of assigning one or more genre labels to a music piece. In this work, we propose a set of ensemble techniques, which are specific to the task of multi-label genre classification. Our goal is to enhance classification performance by combining multiple classifiers. In addition, we also investigate some existing ensemble techniques from machine learning. The effectiveness of these techniques is demonstrated through a set of empirical experiments and various related issues are discussed. To the best of our knowledge, there has been limited work on applying ensemble techniques to multi-label genre classification in the literature and we consider the results in this work as our initial efforts toward this end. The significance of our work has two folds: (1) proposing a set of ensemble techniques specific to music genre classification and (2) shedding light on further research along this direction.</p>
<p>【Keywords】:
ensemble techniques; genre classification; multi-label classification; music information retrieval (mir)</p>
<h3 id="74. Picasso - to sing, you must close your eyes and draw.">74. Picasso - to sing, you must close your eyes and draw.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010012">Paper Link</a>】    【Pages】:715-724</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Stupar:Aleksandar">Aleksandar Stupar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Michel:Sebastian">Sebastian Michel</a></p>
<p>【Abstract】:
We study the problem of automatically assigning appropriate music pieces to a picture or, in general, series of pictures. This task, commonly referred to as soundtrack suggestion, is non-trivial as it requires a lot of human attention and a good deal of experience, with master pieces distinguished, e.g., with the Academy Award for Best Original Score. We put forward PICASSO to solve this task in a fully automated way. PICASSO makes use of genuine samples obtained from first-class contemporary movies. Hence, the training set can be arbitrarily large and is also inexpensive to obtain but still provides an excellent source of information. At query time, PICASSO employs a three-level algorithm. First, it selects for a given query image a ranking of the most similar screenshots taken, and subsequently, selects for each screenshot the most similar songs to the music played in the movie when the screenshot was taken. Last, it issues a top-K aggregation algorithm to find the overall best suitable songs available. We have created a large training set consisting of over 40,000 image/soundtrack samples obtained from 28 movies and evaluated the suitability of PICASSO by means of a user study.</p>
<p>【Keywords】:
automatic music selection; background music; slide show; soundtrack recommendation</p>
<h2 id="Summarization    3">Summarization    3</h2>
<h3 id="75. Enhanced results for web search.">75. Enhanced results for web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010014">Paper Link</a>】    【Pages】:725-734</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Haas:Kevin">Kevin Haas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mika:Peter">Peter Mika</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tarjan:Paul">Paul Tarjan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a></p>
<p>【Abstract】:
"Ten blue links" have defined web search results for the last fifteen years -- snippets of text combined with document titles and URLs. In this paper, we establish the notion of enhanced search results that extend web search results to include multimedia objects such as images and video, intent-specific key value pairs, and elements that allow the user to interact with the contents of a web page directly from the search results page. We show that users express a preference for enhanced results both explicitly, and when observed in their search behavior. We also demonstrate the effectiveness of enhanced results in helping users to assess the relevance of search results. Lastly, we show that we can efficiently generate enhanced results to cover a significant fraction of search result pages.</p>
<p>【Keywords】:
search results; semantic web; user interfaces; web search</p>
<h3 id="76. Summarizing the differences in multilingual news.">76. Summarizing the differences in multilingual news.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010015">Paper Link</a>】    【Pages】:735-744</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wan:Xiaojun">Xiaojun Wan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jia:Houping">Houping Jia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Shanshan">Shanshan Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiao:Jianguo">Jianguo Xiao</a></p>
<p>【Abstract】:
There usually exist many news articles written in different languages about a hot news event. The news articles in different languages are written in different ways to reflect different standpoints. For example, the Chinese news agencies and the Western news agencies have published many articles to report the same news of "Liu Xiaobo's Nobel Prize" in Chinese and English languages, respectively. The Chinese news articles and the English news articles share something about the news fact in common, but they focus on different aspects in order to reflect different standpoints about the event. In this paper, we investigate the task of multilingual news summarization for the purpose of finding and summarizing the major differences between the news articles about the same event in the Chinese and English languages. We propose a novel constrained co-ranking (C-CoRank) method for addressing this special task. The C-CoRank method adds the constraints between the difference score and the common score of each sentence to the co-ranking process. Evaluation results on the manually labeled test set with 15 news topics show the effectiveness of our proposed method, and the constrained co-ranking method can outperform a few baselines and the typical co-ranking method.</p>
<p>【Keywords】:
constrained co-ranking; multi-document summarization; multilingual summarization</p>
<h3 id="77. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.">77. Evolutionary timeline summarization: a balanced optimization framework via iterative substitution.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010016">Paper Link</a>】    【Pages】:745-754</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yan:Rui">Rui Yan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wan:Xiaojun">Xiaojun Wan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Otterbacher:Jahna">Jahna Otterbacher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kong:Liang">Liang Kong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaoming">Xiaoming Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0004:Yan">Yan Zhang</a></p>
<p>【Abstract】:
Classic news summarization plays an important role with the exponential document growth on the Web. Many approaches are proposed to generate summaries but seldom simultaneously consider evolutionary characteristics of news plus to traditional summary elements. Therefore, we present a novel framework for the web mining problem named Evolutionary Timeline Summarization (ETS). Given the massive collection of time-stamped web documents related to a general news query, ETS aims to return the evolution trajectory along the timeline, consisting of individual but correlated summaries of each date, emphasizing relevance, coverage, coherence and cross-date diversity. ETS greatly facilitates fast news browsing and knowledge comprehension and hence is a necessity. We formally formulate the task as an optimization problem via iterative substitution from a set of sentences to a subset of sentences that satisfies the above requirements, balancing coherence/diversity measurement and local/global summary quality. The optimized substitution is iteratively conducted by incorporating several constraints until convergence. We develop experimental systems to evaluate on 6 instinctively different datasets which amount to 10251 documents. Performance comparisons between different system-generated timelines and manually created ones by human editors demonstrate the effectiveness of our proposed framework in terms of ROUGE metrics.</p>
<p>【Keywords】:
evolutionary summarization; optimization; timeline</p>
<h2 id="Vertical & entity search    4">Vertical &amp; entity search    4</h2>
<h3 id="78. Ranking related news predictions.">78. Ranking related news predictions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010018">Paper Link</a>】    【Pages】:755-764</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Matthews:Michael">Michael Matthews</a></p>
<p>【Abstract】:
We estimate that nearly one third of news articles contain references to future events. While this information can prove crucial to understanding news stories and how events will develop for a given topic, there is currently no easy way to access this information. We propose a new task to address the problem of retrieving and ranking sentences that contain mentions to future events, which we call ranking related news predictions. In this paper, we formally define this task and propose a learning to rank approach based on 4 classes of features: term similarity, entity-based similarity, topic similarity, and temporal similarity. Through extensive evaluations using a corpus consisting of 1.8 millions news articles and 6,000 manually judged relevance pairs, we show that our approach is able to retrieve a significant number of relevant predictions related to a given topic.</p>
<p>【Keywords】:
future events; news predictions; sentence retrieval and ranking</p>
<h3 id="79. Collective entity linking in web text: a graph-based method.">79. Collective entity linking in web text: a graph-based method.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010019">Paper Link</a>】    【Pages】:765-774</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Han:Xianpei">Xianpei Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Le">Le Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao_0001:Jun">Jun Zhao</a></p>
<p>【Abstract】:
Entity Linking (EL) is the task of linking name mentions in Web text with their referent entities in a knowledge base. Traditional EL methods usually link name mentions in a document by assuming them to be independent. However, there is often additional interdependence between different EL decisions, i.e., the entities in the same document should be semantically related to each other. In these cases, Collective Entity Linking, in which the name mentions in the same document are linked jointly by exploiting the interdependence between them, can improve the entity linking accuracy. This paper proposes a graph-based collective EL method, which can model and exploit the global interdependence between different EL decisions. Specifically, we first propose a graph-based representation, called Referent Graph, which can model the global interdependence between different EL decisions. Then we propose a collective inference algorithm, which can jointly infer the referent entities of all name mentions by exploiting the interdependence captured in Referent Graph. The key benefit of our method comes from: 1) The global interdependence model of EL decisions; 2) The purely collective nature of the inference algorithm, in which evidence for related EL decisions can be reinforced into high-probability decisions. Experimental results show that our method can achieve significant performance improvement over the traditional EL methods.</p>
<p>【Keywords】:
collective entity disambiguation; collective entity linkin; entity disambiguation; entity linking; graph-based entity linking</p>
<h3 id="80. From one tree to a forest: a unified solution for structured web data extraction.">80. From one tree to a forest: a unified solution for structured web data extraction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010020">Paper Link</a>】    【Pages】:775-784</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hao:Qiang">Qiang Hao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cai:Rui">Rui Cai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pang:Yanwei">Yanwei Pang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Lei">Lei Zhang</a></p>
<p>【Abstract】:
Structured data, in the form of entities and associated attributes, has been a rich web resource for search engines and knowledge databases. To efficiently extract structured data from enormous websites in various verticals (e.g., books, restaurants), much research effort has been attracted, but most existing approaches either require considerable human effort or rely on strong features that lack of flexibility. We consider an ambitious scenario -- can we build a system that (1) is general enough to handle any vertical without re-implementation and (2) requires only one labeled example site from each vertical for training to automatically deal with other sites in the same vertical? In this paper, we propose a unified solution to demonstrate the feasibility of this scenario. Specifically, we design a set of weak but general features to characterize vertical knowledge (including attribute-specific semantics and inter-attribute layout relationships). Such features can be adopted in various verticals without redesign; meanwhile, they are weak enough to avoid overfitting of the learnt knowledge to seed sites. Given a new unseen site, the learnt knowledge is first applied to identify page-level candidate attribute values, while inevitably involve false positives. To remove noise, site-level information of the new site is then exploited to boost up the true values. The site-level information is derived in an unsupervised manner, without harm to the applicability of the solution. Promising experimental performance on 80 websites in 8 distinct verticals demonstrated the feasibility and flexibility of the proposed solution.</p>
<p>【Keywords】:
information extraction; site-level information; structured data; vertical knowledge</p>
<h3 id="81. Improving local search ranking through external logs.">81. Improving local search ranking through external logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010021">Paper Link</a>】    【Pages】:785-794</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Berberich:Klaus">Klaus Berberich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/K=ouml=nig:Arnd_Christian">Arnd Christian König</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lymberopoulos:Dimitrios">Dimitrios Lymberopoulos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Peixiang">Peixiang Zhao</a></p>
<p>【Abstract】:
The signals used for ranking in local search are very different from web search: in addition to (textual) relevance, measures of (geographic) distance between the user and the search result, as well as measures of popularity of the result are important for effective ranking. Depending on the query and search result, different ways to quantify these factors exist -- for example, it is possible to use customer ratings to quantify the popularity of restaurants, whereas different measures are more appropriate for other types of businesses. Hence, our approach is to capture the different notions of distance/popularity relevant via a number of external data sources (e.g., logs of customer ratings, driving-direction requests, or site accesses). In this paper we will describe the relevant signal contained in a number of such data sources in detail and present methods to integrate these external data sources into the feature generation for local search ranking. In particular, we propose novel backoff methods to alleviate the impact of skew, noise or incomplete data in these logs in a systematic manner. We evaluate our techniques on both human-judged relevance data as well as click-through data from a commercial local search engine.</p>
<p>【Keywords】:
backoff techniques; click prediction; distance; driving direction logs; local search; mart; mobile search; preference; ranking; ranking model</p>
<h2 id="Query suggestions    4">Query suggestions    4</h2>
<h3 id="82. Query suggestions in the absence of query logs.">82. Query suggestions in the absence of query logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010023">Paper Link</a>】    【Pages】:795-804</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bhatia:Sumit">Sumit Bhatia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Majumdar:Debapriyo">Debapriyo Majumdar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mitra:Prasenjit">Prasenjit Mitra</a></p>
<p>【Abstract】:
After an end-user has partially input a query, intelligent search engines can suggest possible completions of the partial query to help end-users quickly express their information needs. All major web-search engines and most proposed methods that suggest queries rely on search engine query logs to determine possible query suggestions. However, for customized search systems in the enterprise domain, intranet search, or personalized search such as email or desktop search or for infrequent queries, query logs are either not available or the user base and the number of past user queries is too small to learn appropriate models. We propose a probabilistic mechanism for generating query suggestions from the corpus without using query logs. We utilize the document corpus to extract a set of candidate phrases. As soon as a user starts typing a query, phrases that are highly correlated with the partial user query are selected as completions of the partial query and are offered as query suggestions. Our proposed approach is tested on a variety of datasets and is compared with state-of-the-art approaches. The experimental results clearly demonstrate the effectiveness of our approach in suggesting queries with higher quality.</p>
<p>【Keywords】:
enterprise search; query completion; query formulation; query log analysis; query suggestion</p>
<h3 id="83. Synthesizing high utility suggestions for rare web search queries.">83. Synthesizing high utility suggestions for rare web search queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010024">Paper Link</a>】    【Pages】:805-814</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jain:Alpa">Alpa Jain</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ozertem:Umut">Umut Ozertem</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Velipasaoglu:Emre">Emre Velipasaoglu</a></p>
<p>【Abstract】:
Search engines are continuously looking into methods to alleviate users' effort in finding desired information. For this, all major search engines employ query suggestions methods to facilitate effective query formulation and reformulation. Providing high quality query suggestions is a critical task for search engines and so far most research efforts have focused on tapping various information available in search query logs to identify potential suggestions. By relying on this single source of information, suggestion providing systems often restrict themselves to only previously observed query sessions. Therefore, a critical challenge faced by query suggestions provision mechanism is that of coverage, i.e., the number of unique queries for which users are provided with suggestions, while keeping the suggestion quality high. To address this problem, we propose a novel way of generating suggestions for user search queries by moving beyond the dependency on search query logs and providing synthetic suggestions for web search queries. The key challenges in providing synthetic suggestions include identifying important concepts in a query and systematically exploring related concepts while ensuring that the resulting suggestions are relevant to the user query and of high utility. We present an end-to-end system to generate synthetic suggestions that builds upon novel query-level operations and combines information available from various textual sources. We evaluate our suggestion system over a large-scale real-world dataset of query logs and show that our methods increase the coverage of query-suggestion pairs by up to 39% without compromising the quality or the utility of the suggestions.</p>
<p>【Keywords】:
query reformulation; rare queries; search query logs</p>
<h3 id="84. Post-ranking query suggestion by diversifying search results.">84. Post-ranking query suggestion by diversifying search results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010025">Paper Link</a>】    【Pages】:815-824</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Song:Yang">Yang Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Dengyong">Dengyong Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Li=wei">Li-wei He</a></p>
<p>【Abstract】:
Query suggestion refers to the process of suggesting related queries to search engine users. Most existing researches have focused on improving the relevance of suggested queries. In this paper, we introduce the concept of diversifying the content of the search results from suggested queries while keeping the suggestion relevant. Our framework first retrieves a set of query candidates from search engine logs using random walk and other techniques. We then re-rank the suggested queries by ranking them in the order which maximizes the diversification function that measures the difference between the original search results and the results from suggested queries. The diversification function we proposed includes features like ODP category, URL and domain similarity and so on. One important outcome from our research which contradicts with most existing researches is that, with the increase of suggestion relevance, the similarity between the queries actually decreases. Experiments are conducted on a large set of human-labeled data, which is randomly sampled from a commercial search engine's log. Results indicate that the post-ranking framework significantly improves the relevance of suggested queries by comparing to existing models.</p>
<p>【Keywords】:
learning-to-rank; post-ranking re-ordering; query suggestion; random walk</p>
<h3 id="85. Automatic boolean query suggestion for professional search.">85. Automatic boolean query suggestion for professional search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010026">Paper Link</a>】    【Pages】:825-834</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Youngho">Youngho Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seo:Jangwon">Jangwon Seo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
In professional search environments, such as patent search or legal search, search tasks have unique characteristics: 1) users interactively issue several queries for a topic, and 2) users are willing to examine many retrieval results, i.e., there is typically an emphasis on recall. Recent surveys have also verified that professional searchers continue to have a strong preference for Boolean queries because they provide a record of what documents were searched. To support this type of professional search, we propose a novel Boolean query suggestion technique. Specifically, we generate Boolean queries by exploiting decision trees learned from pseudo-labeled documents and rank the suggested queries using query quality predictors. We evaluate our algorithm in simulated patent and medical search environments. Compared with a recent effective query generation system, we demonstrate that our technique is effective and general.</p>
<p>【Keywords】:
boolean query suggestion; patentability search; prior-art search</p>
<h2 id="Linguistic analysis    4">Linguistic analysis    4</h2>
<h3 id="86. Improved video categorization from text metadata and user comments.">86. Improved video categorization from text metadata and user comments.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010028">Paper Link</a>】    【Pages】:835-842</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Filippova:Katja">Katja Filippova</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hall:Keith_B=">Keith B. Hall</a></p>
<p>【Abstract】:
We consider the task of assigning categories (e.g., howto/cooking, sports/basketball, pet/dogs) to YouTube videos from video and text signals. We show that two complementary views on the data -- from the video and text perspectives -- complement each other and refine predictions. The contributions of the paper are threefold: (1) we show that a text-based classifier trained on imperfect predictions of the weakly supervised video content-based classifier is not redundant; (2) we demonstrate that a simple model which combines the predictions made by the two classifiers outperforms each of them taken independently; (3) we analyse such sources of text information as video title, description, user tags and viewers' comments and show that each of them provides valuable clues to the topic of the video.</p>
<p>【Keywords】:
natural language processing; video classification</p>
<h3 id="87. Multifaceted toponym recognition for streaming news.">87. Multifaceted toponym recognition for streaming news.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010029">Paper Link</a>】    【Pages】:843-852</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lieberman:Michael_D=">Michael D. Lieberman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Samet:Hanan">Hanan Samet</a></p>
<p>【Abstract】:
News sources on the Web generate constant streams of information, describing many aspects of the events that shape our world. In particular, geography plays a key role in the news, and enabling geographic retrieval of news articles involves recognizing the textual references to geographic locations (called toponyms) present in the articles, which can be difficult due to ambiguity in natural language. Toponym recognition in news is often accomplished with algorithms designed and tested around small corpora of news articles, but these static collections do not reflect the streaming nature of online news, as evidenced by poor performance in tests. In contrast, a method for toponym recognition is presented that is tuned for streaming news by leveraging a wide variety of recognition components, both rule-based and statistical. An evaluation of this method shows that it outperforms two prominent toponym recognition systems when tested on large datasets of streaming news, indicating its suitability for this domain.</p>
<p>【Keywords】:
geotagging; streaming news; toponym recognition</p>
<h3 id="88. Enriching document representation via translation for improved monolingual information retrieval.">88. Enriching document representation via translation for improved monolingual information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010030">Paper Link</a>】    【Pages】:853-862</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Na:Seung=Hoon">Seung-Hoon Na</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Ng:Hwee_Tou">Hwee Tou Ng</a></p>
<p>【Abstract】:
Word ambiguity and vocabulary mismatch are critical problems in information retrieval. To deal with these problems, this paper proposes the use of translated words to enrich document representation, going beyond the words in the original source language to represent a document. In our approach, each original document is automatically translated into an auxiliary language, and the resulting translated document serves as a semantically enhanced representation for supplementing the original bag of words. The core of our translation representation is the expected term frequency of a word in a translated document, which is calculated by averaging the term frequencies over all possible translations, rather than focusing on the 1-best translation only. To achieve better efficiency of translation, we do not rely on full-fledged machine translation, but instead use monotonic translation by removing the time-consuming reordering component. Experiments carried out on standard TREC test collections show that our proposed translation representation leads to statistically significant improvements over using only the original language of the document collection.</p>
<p>【Keywords】:
document representation; machine translation</p>
<h3 id="89. A novel corpus-based stemming algorithm using co-occurrence statistics.">89. A novel corpus-based stemming algorithm using co-occurrence statistics.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010031">Paper Link</a>】    【Pages】:863-872</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Paik:Jiaul_H=">Jiaul H. Paik</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pal:Dipasree">Dipasree Pal</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Parui:Swapan_K=">Swapan K. Parui</a></p>
<p>【Abstract】:
We present a stemming algorithm for text retrieval. The algorithm uses the statistics collected on the basis of certain corpus analysis based on the co-occurrence between two word variants. We use a very simple co-occurrence measure that reflects how often a pair of word variants occurs in a document as well as in the whole corpus. A graph is formed where the word variants are the nodes and two word variants form an edge if they co-occur. On the basis of the co-occurrence measure, a certain edge strength is defined for each of the edges. Finally, on the basis of the edge strengths, we propose a partition algorithm that groups the word variants based on their strongest neighbors, that is, the neighbors with largest strengths. Our stemming algorithm has two static parameters and does not use any other information except the co-occurrence statistics from the corpus. The experiments on TREC, CLEF and FIRE data consisting of four European and two Asian languages show a significant improvement over no-stem strategy on all the languages. Also, the proposed algorithm significantly outperforms a number of strong stemmers including the rule-based ones on a number of languages. For highly inflectional languages, a relative improvement of about 50% is obtained compared to un-normalized words and a relative improvement ranging from 5% to 16% is obtained compared to the rule based stemmer for the concerned language.</p>
<p>【Keywords】:
corpus analysis; stemming</p>
<h2 id="Clustering    3">Clustering    3</h2>
<h3 id="90. Document clustering with universum.">90. Document clustering with universum.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010033">Paper Link</a>】    【Pages】:873-882</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0007:Dan">Dan Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jingdong">Jingdong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Document clustering is a popular research topic, which aims to partition documents into groups of similar objects (i.e., clusters), and has been widely used in many applications such as automatic topic extraction, document organization and filtering. As a recently proposed concept, Universum is a collection of "non-examples" that do not belong to any concept/cluster of interest. This paper proposes a novel document clustering technique -- Document Clustering with Universum, which utilizes the Universum examples to improve the clustering performance. The intuition is that the Universum examples can serve as supervised information and help improve the performance of clustering, since they are known not belonging to any meaningful concepts/clusters in the target domain. In particular, a maximum margin clustering method is proposed to model both target examples and Universum examples for clustering. An extensive set of experiments is conducted to demonstrate the effectiveness and efficiency of the proposed algorithm.</p>
<p>【Keywords】:
clustering; constrained concave-convex procedure (cccp); maximum margin clustering; universum</p>
<h3 id="91. Identifying points of interest by self-tuning clustering.">91. Identifying points of interest by self-tuning clustering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010034">Paper Link</a>】    【Pages】:883-892</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Yiyang">Yiyang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gong:Zhiguo">Zhiguo Gong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/U:Leong_Hou">Leong Hou U</a></p>
<p>【Abstract】:
Deducing trip related information from web-scale datasets has received very large amounts of attention recently. Identifying points of interest (POIs) in geo-tagged photos is one of these problems. The problem can be viewed as a standard clustering problem of partitioning two dimensional objects. In this work, we study spectral clustering which is the first attempt for the POIs identification. However, there is no unified approach to assign the clustering parameters; especially the features of POIs are immensely varying in different metropolitans and locations. To address this, we are intent to study a self-tuning technique which can properly assign the parameters for the clustering needed. Besides geographical information, web photos inherently store rich information. These information are mutually influenced each others and should be taken into trip related mining tasks. To address this, we study reinforcement which constructs the relationship over multiple sources by iterative learning. At last, we thoroughly demonstrate our findings by web scale datasets collected from Flickr.</p>
<p>【Keywords】:
spectral clustering; web images</p>
<h3 id="92. Cluster-based fusion of retrieved lists.">92. Cluster-based fusion of retrieved lists.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010035">Paper Link</a>】    【Pages】:893-902</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kozorovitzky:Anna_Khudyak">Anna Khudyak Kozorovitzky</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a></p>
<p>【Abstract】:
Methods for fusing document lists that were retrieved in response to a query often use retrieval scores (or ranks) of documents in the lists. We present a novel probabilistic fusion approach that utilizes an additional source of rich information, namely, inter-document similarities. Specifically, our model integrates information induced from clusters of similar documents created across the lists with that produced by some fusion method that relies on retrieval scores (ranks). Empirical evaluation shows that our approach is highly effective for fusion. For example, the performance of our model is consistently better than that of the standard (effective) fusion method that it integrates. The performance also transcends that of standard fusion of re-ranked lists, where list re-ranking is based on clusters created from documents in the list.</p>
<p>【Keywords】:
ad hoc retrieval; cluster-based fusion; fusion</p>
<h2 id="Effectiveness    3">Effectiveness    3</h2>
<h3 id="93. System effectiveness, user models, and user utility: a conceptual framework for investigation.">93. System effectiveness, user models, and user utility: a conceptual framework for investigation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010037">Paper Link</a>】    【Pages】:903-912</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Carterette:Ben">Ben Carterette</a></p>
<p>【Abstract】:
There is great interest in producing effectiveness measures that model user behavior in order to better model the utility of a system to its users. These measures are often formulated as a sum over the product of a discount function of ranks and a gain function mapping relevance assessments to numeric utility values. We develop a conceptual framework for analyzing such effectiveness measures based on classifying members of this broad family of measures into four distinct families, each of which reflects a different notion of system utility. Within this framework we can hypothesize about the properties that such a measure should have and test those hypotheses against user and system data. Along the way we present a collection of novel results about specific measures and relationships between them.</p>
<p>【Keywords】:
evaluation; information retrieval; user models</p>
<h3 id="94. Evaluating the synergic effect of collaboration in information seeking.">94. Evaluating the synergic effect of collaboration in information seeking.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010038">Paper Link</a>】    【Pages】:913-922</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shah:Chirag">Chirag Shah</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gonz=aacute=lez=Ib=aacute==ntilde=ez:Roberto_I=">Roberto I. González-Ibáñez</a></p>
<p>【Abstract】:
It is typically expected that when people work together, they can often accomplish goals that are difficult or even impossible for individuals. We consider this notion of the group achieving more than the sum of all individuals' achievements to be the synergic effect in collaboration. Similar expectation exists for people working in collaboration for information seeking tasks. We, however, lack a methodology and appropriate evaluation metrics for studying and measuring the synergic effect. In this paper we demonstrate how to evaluate this effect and discuss what it means to various collaborative information seeking (CIS) situations. We present a user study with four different conditions: single user, pair of users at the same computer, pair of users at different computers and co-located, and pair of users remotely located. Each of these individuals or pairs was given the same task of information seeking and usage for the same amount of time. We then combined the outputs of single independent users to form artificial pairs, and compared against the real pairs. Not surprisingly, participants using different computers (co-located or remotely located) were able to cover more information sources than those using a single computer (single user or a pair). But more interestingly, we found that real pairs with their own computers (co-located or remotely located) were able to cover more unique and useful information than that of the artificially created pairs. This indicates that those working in collaboration achieved something greater and better than what could be achieved by adding independent users, thus, demonstrating the synergic effect. Remotely located real teams were also able to formulate a wider range of queries than those pairs that were co-located or artificially created. This shows that the collaborators working remotely were able to achieve synergy while still being able to think and work independently. Through the experiments and measurements presented here, we have also contributed a unique methodology and an evaluation metric for CIS.</p>
<p>【Keywords】:
collaborative information seeking; evaluation; synergic effect</p>
<h3 id="95. Repeatable and reliable search system evaluation using crowdsourcing.">95. Repeatable and reliable search system evaluation using crowdsourcing.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010039">Paper Link</a>】    【Pages】:923-932</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Halpin:Harry">Harry Halpin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Herzig:Daniel_M=">Daniel M. Herzig</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mika:Peter">Peter Mika</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pound:Jeffrey">Jeffrey Pound</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Thompson:Henry_S=">Henry S. Thompson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tran:Duc_Thanh">Duc Thanh Tran</a></p>
<p>【Abstract】:
The primary problem confronting any new kind of search task is how to boot-strap a reliable and repeatable evaluation campaign, and a crowd-sourcing approach provides many advantages. However, can these crowd-sourced evaluations be repeated over long periods of time in a reliable manner? To demonstrate, we investigate creating an evaluation campaign for the semantic search task of keyword-based ad-hoc object retrieval. In contrast to traditional search over web-pages, object search aims at the retrieval of information from factual assertions about real-world objects rather than searching over web-pages with textual descriptions. Using the first large-scale evaluation campaign that specifically targets the task of ad-hoc Web object retrieval over a number of deployed systems, we demonstrate that crowd-sourced evaluation campaigns can be repeated over time and still maintain reliable results. Furthermore, we show how these results are comparable to expert judges when ranking systems and that the results hold over different evaluation and relevance metrics. This work provides empirical support for scalable, reliable, and repeatable search system evaluation using crowdsourcing.</p>
<p>【Keywords】:
crowdsourcing; evaluation; retrieval; search engines</p>
<h2 id="Multilingual IR    3">Multilingual IR    3</h2>
<h3 id="96. Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization.">96. Cross-language web page classification via dual knowledge transfer using nonnegative matrix tri-factorization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010041">Paper Link</a>】    【Pages】:933-942</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Hua">Hua Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Heng">Heng Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nie:Feiping">Feiping Nie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Chris_H=_Q=">Chris H. Q. Ding</a></p>
<p>【Abstract】:
The lack of sufficient labeled Web pages in many languages, especially for those uncommonly used ones, presents a great challenge to traditional supervised classification methods to achieve satisfactory Web page classification performance. To address this, we propose a novel Nonnegative Matrix Tri-factorization (NMTF) based Dual Knowledge Transfer (DKT) approach for cross-language Web page classification, which is based on the following two important observations. First, we observe that Web pages for a same topic from different languages usually share some common semantic patterns, though in different representation forms. Second, we also observe that the associations between word clusters and Web page classes are a more reliable carrier than raw words to transfer knowledge across languages. With these recognitions, we attempt to transfer knowledge from the auxiliary language, in which abundant labeled Web pages are available, to target languages, in which we want classify Web pages, through two different paths: word cluster approximations and the associations between word clusters and Web page classes. Due to the reinforcement between these two different knowledge transfer paths, our approach can achieve better classification accuracy. We evaluate the proposed approach in extensive experiments using a real world cross-language Web page data set. Promising results demonstrate the effectiveness of our approach that is consistent with our theoretical analyses.</p>
<p>【Keywords】:
cross-language classification; knowledge transfer; nonnegative matrix factorization</p>
<h3 id="97. No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity.">97. No free lunch: brute force vs. locality-sensitive hashing for cross-lingual pairwise similarity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010042">Paper Link</a>】    【Pages】:943-952</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/T=uuml=re:Ferhan">Ferhan Türe</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elsayed:Tamer">Tamer Elsayed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy_J=">Jimmy J. Lin</a></p>
<p>【Abstract】:
This work explores the problem of cross-lingual pairwise similarity, where the task is to extract similar pairs of documents across two different languages. Solutions to this problem are of general interest for text mining in the multi-lingual context and have specific applications in statistical machine translation. Our approach takes advantage of cross-language information retrieval (CLIR) techniques to project feature vectors from one language into another, and then uses locality-sensitive hashing (LSH) to extract similar pairs. We show that effective cross-lingual pairwise similarity requires working with similarity thresholds that are much lower than in typical monolingual applications, making the problem quite challenging. We present a parallel, scalable MapReduce implementation of the sort-based sliding window algorithm, which is compared to a brute-force approach on German and English Wikipedia collections. Our central finding can be summarized as "no free lunch": there is no single optimal solution. Instead, we characterize effectiveness-efficiency tradeoffs in the solution space, which can guide the developer to locate a desirable operating point based on application- and resource-specific constraints.</p>
<p>【Keywords】:
lsh; machine translation; wikipedia</p>
<h3 id="98. An event-centric model for multilingual document similarity.">98. An event-centric model for multilingual document similarity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010043">Paper Link</a>】    【Pages】:953-962</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Str=ouml=tgen:Jannik">Jannik Strötgen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gertz:Michael">Michael Gertz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Junghans:Conny">Conny Junghans</a></p>
<p>【Abstract】:
Document similarity measures play an important role in many document retrieval and exploration tasks. Over the past decades, several models and techniques have been developed to determine a ranked list of documents similar to a given query document. Interestingly, the proposed approaches typically rely on extensions to the vector space model and are rarely suited for multilingual corpora. In this paper, we present a novel document similarity measure that is based on events extracted from documents. An event is solely described by nearby occurrences of temporal and geographic expressions in a document's text. Thus, a document is modeled as a set of events that can be compared and ranked using temporal and geographic hierarchies. A key feature of our model is that it is term- and language-independent as temporal and geographic expressions mentioned in texts are normalized to a standard format. This also allows to determine similar documents across languages, an important feature in the context of document exploration. Our approach proves to be quite effective, including the discovery of new similarities, as our experiments using different (multilingual) corpora demonstrate.</p>
<p>【Keywords】:
document similarity; event extraction; geographic information; temporal information</p>
<h2 id="Efficiency    4">Efficiency    4</h2>
<h3 id="99. Posting list intersection on multicore architectures.">99. Posting list intersection on multicore architectures.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010045">Paper Link</a>】    【Pages】:963-972</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tatikonda:Shirish">Shirish Tatikonda</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Junqueira:Flavio_Paiva">Flavio Paiva Junqueira</a></p>
<p>【Abstract】:
In current commercial Web search engines, queries are processed in the conjunctive mode, which requires the search engine to compute the intersection of a number of posting lists to determine the documents matching all query terms. In practice, the intersection operation takes a significant fraction of the query processing time, for some queries dominating the total query latency. Hence, efficient posting list intersection is critical for achieving short query latencies. In this work, we focus on improving the performance of posting list intersection by leveraging the compute capabilities of recent multicore systems. To this end, we consider various coarse-grained and fine-grained parallelization models for list intersection. Specifically, we present an algorithm that partitions the work associated with a given query into a number of small and independent tasks that are subsequently processed in parallel. Through a detailed empirical analysis of these alternative models, we demonstrate that exploiting parallelism at the finest-level of granularity is critical to achieve the best performance on multicore systems. On an eight-core system, the fine-grained parallelization method is able to achieve more than five times reduction in average query processing time while still exploiting the parallelism for high query throughput.</p>
<p>【Keywords】:
intra-query parallelism; multicore architectures; posting list intersection; query processing; web search engines</p>
<h3 id="100. Timestamp-based result cache invalidation for web search engines.">100. Timestamp-based result cache invalidation for web search engines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010046">Paper Link</a>】    【Pages】:973-982</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Alici:Sadiye">Sadiye Alici</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Alting=ouml=vde:Ismail_Seng=ouml=r">Ismail Sengör Altingövde</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ozcan:Rifat">Rifat Ozcan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ulusoy:=Ouml=zg=uuml=r">Özgür Ulusoy</a></p>
<p>【Abstract】:
The result cache is a vital component for efficiency of large-scale web search engines, and maintaining the freshness of cached query results is the current research challenge. As a remedy to this problem, our work proposes a new mechanism to identify queries whose cached results are stale. The basic idea behind our mechanism is to maintain and compare generation time of query results with update times of posting lists and documents to decide on staleness of query results. The proposed technique is evaluated using a Wikipedia document collection with real update information and a real-life query log. We show that our technique has good prediction accuracy, relative to a baseline based on the time-to-live mechanism. Moreover, it is easy to implement and incurs less processing overhead on the system relative to a recently proposed, more sophisticated invalidation mechanism.</p>
<p>【Keywords】:
cache invalidation; freshness; result cache; web search</p>
<h3 id="101. Energy-price-driven query processing in multi-center web search engines.">101. Energy-price-driven query processing in multi-center web search engines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010047">Paper Link</a>】    【Pages】:983-992</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kayaaslan:Enver">Enver Kayaaslan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cambazoglu:Berkant_Barla">Berkant Barla Cambazoglu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Blanco:Roi">Roi Blanco</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Junqueira:Flavio_Paiva">Flavio Paiva Junqueira</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aykanat:Cevdet">Cevdet Aykanat</a></p>
<p>【Abstract】:
Concurrently processing thousands of web queries, each with a response time under a fraction of a second, necessitates maintaining and operating massive data centers. For large-scale web search engines, this translates into high energy consumption and a huge electric bill. This work takes the challenge to reduce the electric bill of commercial web search engines operating on data centers that are geographically far apart. Based on the observation that energy prices and query workloads show high spatio-temporal variation, we propose a technique that dynamically shifts the query workload of a search engine between its data centers to reduce the electric bill. Experiments on real-life query workloads obtained from a commercial search engine show that significant financial savings can be achieved by this technique.</p>
<p>【Keywords】:
data center; energy; query processing; web search engine</p>
<h3 id="102. Faster top-k document retrieval using block-max indexes.">102. Faster top-k document retrieval using block-max indexes.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010048">Paper Link</a>】    【Pages】:993-1002</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Shuai">Shuai Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Suel:Torsten">Torsten Suel</a></p>
<p>【Abstract】:
Large search engines process thousands of queries per second over billions of documents, making query processing a major performance bottleneck. An important class of optimization techniques called early termination achieves faster query processing by avoiding the scoring of documents that are unlikely to be in the top results. We study new algorithms for early termination that outperform previous methods. In particular, we focus on safe techniques for disjunctive queries, which return the same result as an exhaustive evaluation over the disjunction of the query terms. The current state-of-the-art methods for this case, the WAND algorithm by Broder et al. [11] and the approach of Strohman and Croft [30], achieve great benefits but still leave a large performance gap between disjunctive and (even non-early terminated) conjunctive queries. We propose a new set of algorithms by introducing a simple augmented inverted index structure called a block-max index. Essentially, this is a structure that stores the maximum impact score for each block of a compressed inverted list in uncompressed form, thus enabling us to skip large parts of the lists. We show how to integrate this structure into the WAND approach, leading to considerable performance gains. We then describe extensions to a layered index organization, and to indexes with reassigned document IDs, that achieve additional gains that narrow the gap between disjunctive and conjunctive top-k query processing.</p>
<p>【Keywords】:
block-max index; early termination; inverted index; ir query processing; top-k query processing</p>
<h2 id="Recommender systems    4">Recommender systems    4</h2>
<h3 id="103. Utilizing marginal net utility for recommendation in e-commerce.">103. Utilizing marginal net utility for recommendation in e-commerce.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010050">Paper Link</a>】    【Pages】:1003-1012</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jian">Jian Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Yi">Yi Zhang</a></p>
<p>【Abstract】:
Traditional recommendation algorithms often select products with the highest predicted ratings to recommend. However, earlier research in economics and marketing indicates that a consumer usually makes purchase decision(s) based on the product's marginal net utility (i.e., the marginal utility minus the product price). Utility is defined as the satisfaction or pleasure user u gets when purchasing the corresponding product. A rational consumer chooses the product to purchase in order to maximize the total net utility. In contrast to the predicted rating, the marginal utility of a product depends on the user's purchase history and changes over time. According to the Law of Diminishing Marginal Utility, many products have the decreasing marginal utility with the increase of purchase count, such as cell phones, computers, and so on. Users are not likely to purchase the same or similar product again in a short time if they already purchased it before. On the other hand, some products, such as pet food, baby diapers, would be purchased again and again. To better match users' purchase decisions in the real world, this paper explores how to recommend products with the highest marginal net utility in e-commerce sites. Inspired by the Cobb-Douglas utility function in consumer behavior theory, we propose a novel utility-based recommendation framework. The framework can be utilized to revamp a family of existing recommendation algorithms. To demonstrate the idea, we use Singular Value Decomposition (SVD) as an example and revamp it with the framework. We evaluate the proposed algorithm on an e-commerce (shop.com) data set. The new algorithm significantly improves the base algorithm, largely due to its ability to recommend both products that are new to the user and products that the user is likely to re-purchase.</p>
<p>【Keywords】:
consumer utility; e-commerce; recommender systems</p>
<h3 id="104. Recommending ephemeral items at web scale.">104. Recommending ephemeral items at web scale.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010051">Paper Link</a>】    【Pages】:1013-1022</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ye">Ye Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Canny:John_F=">John F. Canny</a></p>
<p>【Abstract】:
We describe an innovative and scalable recommendation system successfully deployed at eBay. To build recommenders for long-tail marketplaces requires projection of volatile items into a persistent space of latent products. We first present a generative clustering model for collections of unstructured, heterogeneous, and ephemeral item data, under the assumption that items are generated from latent products. An item is represented as a vector of independently and distinctly distributed variables, while a latent product is characterized as a vector of probability distributions, respectively. The probability distributions are chosen as natural stochastic models for different types of data. The learning objective is to maximize the total intra-cluster coherence measured by the sum of log likelihoods of items under such a generative process. In the space of latent products, robust recommendations can then be derived using naive Bayes for ranking, from historical transactional data. Item-based recommendations are achieved by inferring latent products from unseen items. In particular, we develop a probabilistic scoring function of recommended items, which takes into account item-product membership, product purchase probability, and the important auction-end-time factor. With the holistic probabilistic measure of a prospective item purchase, one can further maximize the expected revenue and the more subjective user satisfaction as well. We evaluated the latent product clustering and recommendation ranking models using real-world e-commerce data from eBay, in both forms of offline simulation and online A/B testing. In the recent production launch, our system yielded 3-5 folds improvement over the existing production system in click-through, purchase-through and gross merchandising value; thus now driving 100% related recommendation traffic with billions of items at eBay. We believe that this work provides a practical yet principled framework for recommendation in the domains with affluent user self-input data.</p>
<p>【Keywords】:
bayesian methods; clustering; collaborative filtering; evaluation; generative models; recommender systems</p>
<h3 id="105. A unified framework for recommendations based on quaternary semantic analysis.">105. A unified framework for recommendations based on quaternary semantic analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010052">Paper Link</a>】    【Pages】:1023-1032</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0025:Wei">Wei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Wynne">Wynne Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Mong=Li">Mong-Li Lee</a></p>
<p>【Abstract】:
Social network systems such as FaceBook and YouTube have played a significant role in capturing both explicit and implicit user preferences for different items in the form of ratings and tags. This forms a quaternary relationship among users, items, tags and ratings. Existing systems have utilized only ternary relationships such as users-items-ratings, or users-items-tags to derive their recommendations. In this paper, we show that ternary relationships are insufficient to provide accurate recommendations. Instead, we model the quaternary relationship among users, items, tags and ratings as a 4-order tensor and cast the recommendation problem as a multi-way latent semantic analysis problem. A unified framework for user recommendation, item recommendation, tag recommendation and item rating prediction is proposed. The results of extensive experiments performed on a real world dataset demonstrate that our unified framework outperforms the state-of-the-art techniques in all the four recommendation tasks.</p>
<p>【Keywords】:
collaborative tagging; personalization; recommendation; social tags; tensor factorization</p>
<h3 id="106. Associative tag recommendation exploiting multiple textual features.">106. Associative tag recommendation exploiting multiple textual features.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010053">Paper Link</a>】    【Pages】:1033-1042</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bel=eacute=m:Fabiano">Fabiano Belém</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Martins:Eder_Ferreira">Eder Ferreira Martins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pontes:Tatiana">Tatiana Pontes</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Almeida:Jussara_M=">Jussara M. Almeida</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gon=ccedil=alves:Marcos_Andr=eacute=">Marcos André Gonçalves</a></p>
<p>【Abstract】:
This work addresses the task of recommending relevant tags to a target object by jointly exploiting three dimensions of the problem: (i) term co-occurrence with tags pre-assigned to the target object, (ii) terms extracted from multiple textual features, and (iii) several metrics of tag relevance. In particular, we propose several new heuristic methods, which extend state-of-the-art strategies by including new metrics that try to capture how accurately a candidate term describes the object's content. We also exploit two learning-to-rank (L2R) techniques, namely RankSVM and Genetic Programming, for the task of generating ranking functions that combine multiple metrics to accurately estimate the relevance of a tag to a given object. We evaluate all proposed methods in various scenarios for three popular Web 2.0 applications, namely, LastFM, YouTube and YahooVideo. We found that our new heuristics greatly outperform the methods on which they are based, producing gains in precision of up to 181%, as well as another state-of-the-art technique, with improvements in precision of up to 40% over the best baseline in any scenario. Further improvements can also be achieved with the new L2R strategies, which have the additional advantage of being quite flexible and extensible to exploit other aspects of the tag recommendation problem.</p>
<p>【Keywords】:
relevance metrics; tag recommendation</p>
<h2 id="Test collections    4">Test collections    4</h2>
<h3 id="107. Evaluating diversified search results using per-intent graded relevance.">107. Evaluating diversified search results using per-intent graded relevance.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010055">Paper Link</a>】    【Pages】:1043-1052</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sakai:Tetsuya">Tetsuya Sakai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Ruihua">Ruihua Song</a></p>
<p>【Abstract】:
Search queries are often ambiguous and/or underspecified. To accomodate different user needs, search result diversification has received attention in the past few years. Accordingly, several new metrics for evaluating diversification have been proposed, but their properties are little understood. We compare the properties of existing metrics given the premises that (1) queries may have multiple intents; (2) the likelihood of each intent given a query is available; and (3) graded relevance assessments are available for each intent. We compare a wide range of traditional and diversified IR metrics after adding graded relevance assessments to the TREC 2009 Web track diversity task test collection which originally had binary relevance assessments. Our primary criterion is discriminative power, which represents the reliability of a metric in an experiment. Our results show that diversified IR experiments with a given number of topics can be as reliable as traditional IR experiments with the same number of topics, provided that the right metrics are used. Moreover, we compare the intuitiveness of diversified IR metrics by closely examining the actual ranked lists from TREC. We show that a family of metrics called D#-measures have several advantages over other metrics such as α-nDCG and Intent-Aware metrics.</p>
<p>【Keywords】:
ambiguity; diversity; evaluation; graded relevance; test collection</p>
<h3 id="108. Evaluating multi-query sessions.">108. Evaluating multi-query sessions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010056">Paper Link</a>】    【Pages】:1053-1062</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanoulas:Evangelos">Evangelos Kanoulas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carterette:Ben">Ben Carterette</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clough:Paul_D=">Paul D. Clough</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanderson:Mark">Mark Sanderson</a></p>
<p>【Abstract】:
The standard system-based evaluation paradigm has focused on assessing the performance of retrieval systems in serving the best results for a single query. Real users, however, often begin an interaction with a search engine with a sufficiently under-specified query that they will need to reformulate before they find what they are looking for. In this work we consider the problem of evaluating retrieval systems over test collections of multi-query sessions. We propose two families of measures: a model-free family that makes no assumption about the user's behavior over a session, and a model-based family with a simple model of user interactions over the session. In both cases we generalize traditional evaluation metrics such as average precision to multi-query session evaluation. We demonstrate the behavior of the proposed metrics by using the new TREC 2010 Session track collection and simulations over the TREC-9 Query track collection.</p>
<p>【Keywords】:
evaluation; information retrieval; sessions; test collections</p>
<h3 id="109. Quantifying test collection quality based on the consistency of relevance judgements.">109. Quantifying test collection quality based on the consistency of relevance judgements.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010057">Paper Link</a>】    【Pages】:1063-1072</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Scholer:Falk">Falk Scholer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Turpin:Andrew">Andrew Turpin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sanderson:Mark">Mark Sanderson</a></p>
<p>【Abstract】:
Relevance assessments are a key component for test collection-based evaluation of information retrieval systems. This paper reports on a feature of such collections that is used as a form of ground truth data to allow analysis of human assessment error. A wide range of test collections are retrospectively examined to determine how accurately assessors judge the relevance of documents. Our results demonstrate a high level of inconsistency across the collections studied. The level of irregularity is shown to vary across topics, with some showing a very high level of assessment error. We investigate possible influences on the error, and demonstrate that inconsistency in judging increases with time. While the level of detail in a topic specification does not appear to influence the errors that assessors make, judgements are significantly affected by the decisions made on previously seen similar documents. Assessors also display an assessment inertia. Alternate approaches to generating relevance judgements appear to reduce errors. A further investigation of the way that retrieval systems are ranked using sets of relevance judgements produced early and late in the judgement process reveals a consistent influence measured across the majority of examined test collections. We conclude that there is a clear value in examining, even inserting, ground truth data in test collections, and propose ways to help minimise the sources of inconsistency when creating future test collections.</p>
<p>【Keywords】:
information retrieval evaluation; search engines</p>
<h3 id="110. Pseudo test collections for learning web search ranking functions.">110. Pseudo test collections for learning web search ranking functions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010058">Paper Link</a>】    【Pages】:1073-1082</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Asadi_0001:Nima">Nima Asadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metzler:Donald">Donald Metzler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elsayed:Tamer">Tamer Elsayed</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy_J=">Jimmy J. Lin</a></p>
<p>【Abstract】:
Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on TREC web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data "for free".</p>
<p>【Keywords】:
anchor text; evaluation; hyperlinks; web</p>
<h2 id="Posters presentations    90">Posters presentations    90</h2>
<h3 id="111. Parallel learning to rank for information retrieval.">111. Parallel learning to rank for information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010060">Paper Link</a>】    【Pages】:1083-1084</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shuaiqiang">Shuaiqiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Ke">Ke Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lauw:Hady_Wirawan">Hady Wirawan Lauw</a></p>
<p>【Abstract】:
Learning to rank represents a category of effective ranking methods for information retrieval. While the primary concern of existing research has been accuracy, learning efficiency is becoming an important issue due to the unprecedented availability of large-scale training data and the need for continuous update of ranking functions. In this paper, we investigate parallel learning to rank, targeting simultaneous improvement in accuracy and efficiency.</p>
<p>【Keywords】:
cooperative coevolution; information retrieval; learning to rank; mapreduce; parallel algorithms</p>
<h3 id="112. Learning features through feedback for blog distillation.">112. Learning features through feedback for blog distillation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010061">Paper Link</a>】    【Pages】:1085-1086</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Dehong">Dehong Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Renxian">Renxian Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lau:Raymond_Yiu=Keung">Raymond Yiu-Keung Lau</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wong:Kam=Fai">Kam-Fai Wong</a></p>
<p>【Abstract】:
The paper is focused on blogosphere research based on the TREC blog distillation task, and aims to explore unbiased and significant features automatically and efficiently. Feedback from faceted feeds is introduced to harvest relevant features and information gain is used to select discriminative features. The evaluation result shows that the selected feedback features can greatly improve the performance and adapt well to the terabyte data.</p>
<p>【Keywords】:
blog distillation; faceted distillation; feedback</p>
<h3 id="113. Time-based relevance models.">113. Time-based relevance models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010062">Paper Link</a>】    【Pages】:1087-1088</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Keikha:Mostafa">Mostafa Keikha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gerani:Shima">Shima Gerani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crestani:Fabio">Fabio Crestani</a></p>
<p>【Abstract】:
This paper addresses blog feed retrieval where the goal is to retrieve the most relevant blog feeds for a given user query. Since the retrieval unit is a blog, as a collection of posts, performing relevance feedback techniques and selecting the most appropriate documents for query expansion becomes challenging. By assuming time as an effective parameter on the blog posts content, we propose a time-based query expansion method. In this method, we select terms for expansion using most relevant days for the query, as opposed to most relevant documents. This provide us with more trustable terms for expansion. Our preliminary experiments on Blog08 collection shows that this method can outperform state of the art relevance feedback methods in blog retrieval.</p>
<p>【Keywords】:
blog retrieval; pseudo relevance feedback</p>
<h3 id="114. Improved query performance prediction using standard deviation.">114. Improved query performance prediction using standard deviation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010063">Paper Link</a>】    【Pages】:1089-1090</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cummins:Ronan">Ronan Cummins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/O=Riordan:Colm">Colm O'Riordan</a></p>
<p>【Abstract】:
Query performance prediction (QPP) is an important task in information retrieval (IR). In this paper, we (1) develop a new predictor based on the standard deviation of scores in a variable length ranked list, and (2) we show that this new predictor outperforms state-of-the-art approaches without the need for tuning.</p>
<p>【Keywords】:
information retrieval; query performance prediction</p>
<h3 id="115. Learning to rank using query-level regression.">115. Learning to rank using query-level regression.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010064">Paper Link</a>】    【Pages】:1091-1092</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Jiajin">Jiajin Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Zhihao">Zhihao Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Yuan">Yuan Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Hongfei">Hongfei Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Zheng">Zheng Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Kan">Kan Xu</a></p>
<p>【Abstract】:
In this paper, we use query-level regression as the loss function. The regression loss function has been used in pointwise methods, however pointwise methods ignore the query boundaries and treat the data equally across queries, and thus the effectiveness is limited. We show that regression is an effective loss function for learning to rank when used in query-level. We use neural network to model the ranking function and gradient descent for optimization and refer our method as ListReg. Experimental results show that ListReg significantly outperforms pointwise Regression and the state-of-the-art listwise method in most cases.</p>
<p>【Keywords】:
learning to rank; loss function; query-level; regression</p>
<h3 id="116. Diversifying product search results.">116. Diversifying product search results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010065">Paper Link</a>】    【Pages】:1093-1094</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Xiangru">Xiangru Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Haofen">Haofen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Xinruo">Xinruo Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pan:Junfeng">Junfeng Pan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:
In recent years, online shopping is becoming more and more popular. Users type keyword queries on product search systems to find relevant products, accessories, and even related products. However, existing product search systems always return very similar products on the first several pages instead of taking diversity into consideration. In this paper, we propose a novel approach to address the diversity issue in the context of product search. We transform search result diversification into a combination of diversifying product categories and diversifying product attribute values within each category. The two sub-problems are optimization problems which can be reduced into well-known NP-hard problems respectively. We further leverage greedy-based approximation algorithms for efficient product search results re-ranking.</p>
<p>【Keywords】:
product search; result diversification</p>
<h3 id="117. Ad hoc IR: not much room for improvement.">117. Ad hoc IR: not much room for improvement.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010066">Paper Link</a>】    【Pages】:1095-1096</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Trotman:Andrew">Andrew Trotman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keeler:David">David Keeler</a></p>
<p>【Abstract】:
Ranking function performance reached a plateau in 1994. The reason for this is investigated. First the performance of BM25 is measured as the proportion of queries satisfied on the first page of 10 results -- it performs well. The performance is then compared to human performance. They perform comparably. The conclusion is there isn't much room for ranking function improvement.</p>
<p>【Keywords】:
document retrieval; relevance ranking</p>
<h3 id="118. Image annotation based on recommendation model.">118. Image annotation based on recommendation model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010067">Paper Link</a>】    【Pages】:1097-1098</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Zijia">Zijia Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Guiguang">Guiguang Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0001:Jianmin">Jianmin Wang</a></p>
<p>【Abstract】:
In this paper, a novel approach based on recommendation model is proposed for automatic image annotation. For any to-be-annotated image, we first select some related images with tags from training dataset according to their visual similarity. And then we estimate the initial ratings for tags of the training images based on tag ranking method and construct a rating matrix. We also construct a trust matrix based on visual similarity with a k-NN strategy. Then a recommendation model is built on both matrices to rank candidate tags for the target image. The proposed approach is evaluated using two benchmark image datasets, and experimental results have indicated its effectiveness.</p>
<p>【Keywords】:
image annotation; recommendation model; retrieval; trustwalker</p>
<h3 id="119. Utilizing minimal relevance feedback for ad hoc retrieval.">119. Utilizing minimal relevance feedback for ad hoc retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010068">Paper Link</a>】    【Pages】:1099-1100</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Krikon:Eyal">Eyal Krikon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kurland:Oren">Oren Kurland</a></p>
<p>【Abstract】:
Using relevance feedback can significantly improve (ad hoc) retrieval effectiveness. Yet, if little feedback is available, effectively exploiting it is a challenge. To that end, we present a novel approach that utilizes document passages. Empirical evaluation demonstrates the merits of the approach.</p>
<p>【Keywords】:
language models; passages; relevance feedback</p>
<h3 id="120. Sense discrimination for physics retrieval.">120. Sense discrimination for physics retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010069">Paper Link</a>】    【Pages】:1101-1102</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lioma:Christina">Christina Lioma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kothari:Alok">Alok Kothari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=uuml=tze:Hinrich">Hinrich Schütze</a></p>
<p>【Abstract】:
Information Retrieval in technical domains like physics is characterised by long and precise queries, whose meaning is strongly influenced by term context and domain. We treat this as a disambiguation problem, and present initial findings of a retrieval model that posits a higher probability of relevance for documents matching disambiguated query terms. Preliminary evaluation on a real-life physics test collection shows promising performance improvement.</p>
<p>【Keywords】:
information retrieval; sense discrimination</p>
<h3 id="121. When documents are very long, BM25 fails!">121. When documents are very long, BM25 fails!</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010070">Paper Link</a>】    【Pages】:1103-1104</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lv:Yuanhua">Yuanhua Lv</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:ChengXiang">ChengXiang Zhai</a></p>
<p>【Abstract】:
We reveal that the Okapi BM25 retrieval function tends to overly penalize very long documents. To address this problem, we present a simple yet effective extension of BM25, namely BM25L, which "shifts" the term frequency normalization formula to boost scores of very long documents. Our experiments show that BM25L, with the same computation cost, is more effective and robust than the standard BM25.</p>
<p>【Keywords】:
bm25; bm25l; term frequency; very long documents</p>
<h3 id="122. Location and timeliness of information sources during news events.">122. Location and timeliness of information sources during news events.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010071">Paper Link</a>】    【Pages】:1105-1106</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yom=Tov:Elad">Elad Yom-Tov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Diaz:Fernando">Fernando Diaz</a></p>
<p>【Abstract】:
People nowadays can obtain information on current news events through media outlets, social media, and by actively seeking information using search engines. In this paper we investigate the temporal relationship between news coverage by media outlets, social media, and query logs and show that social media frequently precedes other information sources. Additionally, we demonstrate that there is strong negative correlation between the probability for reporting of an event and the distance of the information source from the event.</p>
<p>【Keywords】:
distance; information; need; physical; social</p>
<h3 id="123. What deliberately degrading search quality tells us about discount functions.">123. What deliberately degrading search quality tells us about discount functions.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010072">Paper Link</a>】    【Pages】:1107-1108</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Thomas:Paul">Paul Thomas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jones_0001:Timothy">Timothy Jones</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hawking:David">David Hawking</a></p>
<p>【Abstract】:
Deliberate degradation of search results is a common tool in user experiments. We degrade high-quality search results by inserting non-relevant documents at different ranks. The effect of these manipulations, on a number of commonly-used metrics, is counter-intuitive: the discount functions implicit in P@k, MRR, NDCG, and others do not account for the true relationship between rank and value to the user. We propose an alternative, based on visibility data.</p>
<p>【Keywords】:
metrics; result set manipulation</p>
<h3 id="124. Collective topic modeling for heterogeneous networks.">124. Collective topic modeling for heterogeneous networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010073">Paper Link</a>】    【Pages】:1109-1110</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Deng:Hongbo">Hongbo Deng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhao:Bo">Bo Zhao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Han:Jiawei">Jiawei Han</a></p>
<p>【Abstract】:
In this paper, we propose a joint probabilistic topic model for simultaneously modeling the contents of multi-typed objects of a heterogeneous information network. The intuition behind our model is that different objects of the heterogeneous network share a common set of latent topics so as to adjust the multinomial distributions over topics for different objects collectively. Experimental results demonstrate the effectiveness of our approach for the tasks of topic modeling and object clustering.</p>
<p>【Keywords】:
heterogeneous network; topic modeling</p>
<h3 id="125. Graph-cut based tag enrichment.">125. Graph-cut based tag enrichment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010074">Paper Link</a>】    【Pages】:1111-1112</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/q/Qian:Xueming">Xueming Qian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hua:Xian=Sheng">Xian-Sheng Hua</a></p>
<p>【Abstract】:
In this paper, a graph cut based tag enrichment approach is proposed. We build a graph for each image with its initial tags. The graph is with two terminals. Nodes of the graph are full connected with each other. Min-cut/max-flow algorithm is utilized to find the relevant tags for the image. Experiments on Flickr dataset demonstrate the effectiveness of the proposed graph-cut based tag enrichment approach.</p>
<p>【Keywords】:
flickr; graph cut; min-cut/max-flow; tag enrichment; tag ranking</p>
<h3 id="126. Personalized social query expansion using social bookmarking systems.">126. Personalized social query expansion using social bookmarking systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010075">Paper Link</a>】    【Pages】:1113-1114</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bouadjenek:Mohamed_Reda">Mohamed Reda Bouadjenek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hacid:Hakim">Hakim Hacid</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bouzeghoub:Mokrane">Mokrane Bouzeghoub</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Daigremont:Johann">Johann Daigremont</a></p>
<p>【Abstract】:
We propose a new approach for social and personalized query expansion using social structures in the Web 2.0. While focusing on social tagging systems, the proposed approach considers (i) the semantic similarity between tags composing a query, (ii) a social proximity between the query and the user profile, and (iii) on the fly, a strategy for expanding user queries. The proposed approach has been evaluated using a large dataset crawled from del.icio.us.</p>
<p>【Keywords】:
personalization; social information retrieval; social networks</p>
<h3 id="127. What are the real differences of children's and adults' web search.">127. What are the real differences of children's and adults' web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010076">Paper Link</a>】    【Pages】:1115-1116</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gossen:Tatiana">Tatiana Gossen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Low:Thomas">Thomas Low</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=uuml=rnberger:Andreas">Andreas Nürnberger</a></p>
<p>【Abstract】:
We present first results of a logfile analysis on web search engines for children. The aim of this research is to analyse fundamental facts about how children's web search behaviour differs from that of adults. We show differences to previous results, which are often based on small lab experiments. Our large-scale analysis suggests that children search queries are more information-oriented and shorter on average. Children indeed make a lot of spelling errors and often repeat searches and revisit web pages.</p>
<p>【Keywords】:
children information retrieval; web log analysis; web search</p>
<h3 id="128. Cognitive coordinating behaviors in multitasking web search.">128. Cognitive coordinating behaviors in multitasking web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010077">Paper Link</a>】    【Pages】:1117-1118</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Du:Jia_Tina">Jia Tina Du</a></p>
<p>【Abstract】:
This paper investigates how users cognitively coordinate multitasking Web search across different information search problems. The analysis suggests that (1) multitasking is a prevalent Web search behavior including both sequential multitasking (31%) and parallel multitasking (69%); (2) multitasking is performed through a task switching process; and (3) such a process is supported and underpinned by cognitive coordination mechanisms and strategy coordination.</p>
<p>【Keywords】:
cognitive coordination; information problem; multitasking; task switching; web search</p>
<h3 id="129. Optimizing multimodal reranking for web image search.">129. Optimizing multimodal reranking for web image search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010078">Paper Link</a>】    【Pages】:1119-1120</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hao">Hao Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Meng">Meng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Zhisheng">Zhisheng Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Zheng=Jun">Zheng-Jun Zha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Jialie">Jialie Shen</a></p>
<p>【Abstract】:
In this poster, we introduce a web image search reranking approach with exploring multiple modalities. Diff erent from the conventional methods that build graph with one feature set for reranking, our approach integrates multiple feature sets that describe visual content from different aspects. We simultaneously integrate the learning of relevance scores, the weighting of different feature sets, the distance metric and the scaling for each feature set into a unified scheme. Experimental results on a large data set that contains more than 1,100 queries and 1 million images demonstrate the effectiveness of our approach.</p>
<p>【Keywords】:
graph-based learning; image search; reranking</p>
<h3 id="130. Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce.">130. Multi-layer graph-based semi-supervised learning for large-scale image datasets using mapreduce.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010079">Paper Link</a>】    【Pages】:1121-1122</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Wen=Yu">Wen-Yu Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsieh:Liang=Chi">Liang-Chi Hsieh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Guan=Long">Guan-Long Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Winston_H=">Winston H. Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Su:Ya=Fan">Ya-Fan Su</a></p>
<p>【Abstract】:
Semi-supervised learning is to exploit the vast amount of unlabeled data in the world. This paper proposes a scalable graph-based technique leveraging the distributed computing power of the MapReduce programming model. For a higher quality of learning, the paper also presents a multi-layer learning structure to unify both visual and textual information of image data during the learning process. Experimental results show the effectiveness of the proposed methods.</p>
<p>【Keywords】:
image retrieval; mapreduce; semi-supervised learning</p>
<h3 id="131. Tackling class imbalance and data scarcity in literature-based gene function annotation.">131. Tackling class imbalance and data scarcity in literature-based gene function annotation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010080">Paper Link</a>】    【Pages】:1123-1124</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Blondel:Mathieu">Mathieu Blondel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Seki:Kazuhiro">Kazuhiro Seki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Uehara:Kuniaki">Kuniaki Uehara</a></p>
<p>【Abstract】:
In recent years, a number of machine learning approaches to literature-based gene function annotation have been proposed. However, due to issues such as lack of labeled data, class imbalance and computational cost, they have usually been unable to surpass simpler approaches based on string-matching. In this paper, we propose a principled machine learning approach based on kernel classifiers. We show that kernels can address the task's inherent data scarcity by embedding additional knowledge and we propose a simple yet effective solution to deal with class imbalance. From experiments on the TREC Genomics Track data, our approach achieves better F1-score than two state-of-the-art approaches based on string-matching and cross-species information.</p>
<p>【Keywords】:
kernel methods; text classification</p>
<h3 id="132. Bootstrapping subjectivity detection.">132. Bootstrapping subjectivity detection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010081">Paper Link</a>】    【Pages】:1125-1126</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jijkoun:Valentin">Valentin Jijkoun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijke:Maarten_de">Maarten de Rijke</a></p>
<p>【Abstract】:
We describe a method for automatically generating subjectivity clues for a specific topic and a set of (relevant) document, evaluating it on the task of classifying sentences w.r.t. subjectivity, with improvements over previous work.</p>
<p>【Keywords】:
sentiment retrieval; subjectivity</p>
<h3 id="133. The effects of choice in routing relevance judgments.">133. The effects of choice in routing relevance judgments.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010082">Paper Link</a>】    【Pages】:1127-1128</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Law:Edith">Edith Law</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bennett:Paul_N=">Paul N. Bennett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Horvitz:Eric">Eric Horvitz</a></p>
<p>【Abstract】:
The emergence of human computation systems, including Mechanical Turk and games with a purpose, has made it feasible to distribute relevance judgment tasks to workers over the Web. Most human computation systems assign tasks to individuals randomly, and such assignments may match workers with tasks that they may be unqualified or unmotivated to perform. We compare two groups of workers, those given a choice of queries to judge versus those who are not, in terms of their self-rated competence and their actual performance. Results show that when given a choice of task, workers choose ones for which they have greater expertise, interests, confidence, and understanding.</p>
<p>【Keywords】:
human computation; relevance judgments; task routing</p>
<h3 id="134. Statistical feature extraction for cross-language web content quality assessment.">134. Statistical feature extraction for cross-language web content quality assessment.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010083">Paper Link</a>】    【Pages】:1129-1130</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Geng:Guanggang">Guanggang Geng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xiaodong">Xiaodong Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Li=Ming">Li-Ming Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0083:Wei">Wei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Shuo">Shuo Shen</a></p>
<p>【Abstract】:
Web content quality assessment is a typical static ranking problem. Heuristic content and TFIDF features based statistical systems have proven effective for Web content quality assessment. But they are all language dependent features, which are not suitable for cross-language ranking. In this paper, we fuse a series of language-independent features including hostname features, domain registration features, two-layer hyperlink analysis features and third-party Web service features to assess the Web content quality. The experiments on ECML/PKDD 2010 Discovery Challenge cross-language datasets show that the assessment is effective.</p>
<p>【Keywords】:
feature extraction; machine learning; quality assessment</p>
<h3 id="135. Exploiting endorsement information and social influence for item recommendation.">135. Exploiting endorsement information and social influence for item recommendation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010084">Paper Link</a>】    【Pages】:1131-1132</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Cheng=Te">Cheng-Te Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Shou=De">Shou-De Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shan:Man=Kwan">Man-Kwan Shan</a></p>
<p>【Abstract】:
Social networking services possess two features: (1) capturing the social relationships among people, represented by the social network, and (2) allowing users to express their preferences on different kinds of items (e.g. photo, celebrity, pages) through endorsing buttons, represented by a kind of endorsement bipartite graph. In this work, using such information, we propose a novel recommendation method, which leverages the viral marketing in the social network and the wisdom of crowds from endorsement network. Our recommendation consists of two parts. First, given some query terms describing user's preference, we find a set of targeted influencers who have the maximum activation probability on those nodes related to the query terms in the social network. Second, based on the derived targeted influencers as key experts, we recommend items via the endorsement network. We conduct the experiments on DBLP co-authorship social network with author-reference data as the endorsement network. The results show our method can achieve effective recommendations.</p>
<p>【Keywords】:
endorsement network; recommendation; social influence; social network</p>
<h3 id="136. Modeling subset distributions for verbose queries.">136. Modeling subset distributions for verbose queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010085">Paper Link</a>】    【Pages】:1133-1134</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xue:Xiaobing">Xiaobing Xue</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Croft:W=_Bruce">W. Bruce Croft</a></p>
<p>【Abstract】:
Improving verbose (or long) queries poses a new challenge for search systems. Previous techniques mainly focused on two aspects, weighting the important words or phrases and selecting the best subset query. The former does not consider how words and phrases are used in actual subset queries, while the latter ignores alternative subset queries. Recently, a novel reformulation framework has been proposed to transform the original query as a distribution of reformulated queries, which overcomes the disadvantages of previous techniques. In this paper, we apply this framework to verbose queries, where a reformulated query is specified as a subset query. Experiments on TREC collections show that the query distribution based framework outperforms the state-of-the-art techniques.</p>
<p>【Keywords】:
query distribution; query reformulation; verbose query</p>
<h3 id="137. Domain expert topic familiarity and search behavior.">137. Domain expert topic familiarity and search behavior.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010086">Paper Link</a>】    【Pages】:1135-1136</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Karimi:Sarvnaz">Sarvnaz Karimi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scholer:Falk">Falk Scholer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Clark:Adam">Adam Clark</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kharazmi:Sadegh">Sadegh Kharazmi</a></p>
<p>【Abstract】:
Users of information retrieval systems employ a variety of strategies when searching for information. One factor that can directly influence how searchers go about their information finding task is the level of familiarity with a search topic. We investigate how the search behavior of domain experts changes based on their previous level of familiarity with a search topic, reporting on a user study of biomedical experts searching for a range of domain-specific material. The results of our study show that topic familiarity can influence the number of queries that are employed to complete a task, the types of queries that are entered, and the overall number of query terms. Our findings suggest that biomedical search systems should enable searching through a variety of querying modes, to support the different search strategies that users were found to employ depending on their familiarity with the information that they are searching for.</p>
<p>【Keywords】:
biomedical search; evaluation.; search engines</p>
<h3 id="138. Sample selection for dictionary-based corpus compression.">138. Sample selection for dictionary-based corpus compression.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010087">Paper Link</a>】    【Pages】:1137-1138</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hoobin:Christopher">Christopher Hoobin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Puglisi:Simon_J=">Simon J. Puglisi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zobel:Justin">Justin Zobel</a></p>
<p>【Abstract】:
Compression of large text corpora has the potential to drastically reduce both storage requirements and per-document access costs. Adaptive methods used for general-purpose compression are ineffective for this application, and historically the most successful methods have been based on word-based dictionaries, which allow use of global properties of the text. However, these are dependent on the text complying with assumptions about content and lead to dictionaries of unpredictable size. In recent work we have described an LZ-like approach in which sampled blocks of a corpus are used as a dictionary against which the complete corpus is compressed, giving compression twice as effective than that of zlib. Here we explore how pre-processing can be used to eliminate redundancy in our sampled dictionary. Our experiments show that dictionary size can be reduced by 50% or more (less than 0.1% of the collection size) with no significant effect on compression or access speed.</p>
<p>【Keywords】:
dictionary compression; document retrieval; random access; sampling</p>
<h3 id="139. Evaluating medical information retrieval.">139. Evaluating medical information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010088">Paper Link</a>】    【Pages】:1139-1140</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Koopman:Bevan">Bevan Koopman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bruza:Peter">Peter Bruza</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sitbon:Laurianne">Laurianne Sitbon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lawley:Michael">Michael Lawley</a></p>
<p>【Abstract】:
This paper presents a framework for evaluating information retrieval of medical records. We use the BLULab corpus, a large collection of real-world de-identified medical records. The collection has been hand coded by clinical terminologists using the ICD-9 medical classification system. The ICD codes are used to devise queries and relevance judgements for this collection. Results of initial test runs using a baseline IR system show that there is room for improvement in medical information retrieval. Queries and relevance judgements are made available at <a href="http://aehrc.com/med_eval">http://aehrc.com/med_eval</a></p>
<p>【Keywords】:
evaluation; medical information retrieval</p>
<h3 id="140. Region-based landmark discovery by crowdsourcing geo-referenced photos.">140. Region-based landmark discovery by crowdsourcing geo-referenced photos.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010089">Paper Link</a>】    【Pages】:1141-1142</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Yen=Ta">Yen-Ta Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:An=Jung">An-Jung Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsieh:Liang=Chi">Liang-Chi Hsieh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Winston_H=">Winston H. Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Kuo=Wei">Kuo-Wei Chang</a></p>
<p>【Abstract】:
We propose a novel model for landmark discovery that locates region-based landmarks on map in contrast to the traditional point-based landmarks. The proposed method preserves more information and automatically identifies candidate regions on map by crowdsourcing geo-referenced photos. Gaussian kernel convolution is applied to remove noises and generate detected region. We adopt F1 measure to evaluate discovered landmarks and manually check the association between tags and regions. The experiment results show that more than 90% of attractions in the selected city can be correctly located by this method.</p>
<p>【Keywords】:
crowdsourcing; geo-referenced photo.; region-based</p>
<h3 id="141. Towards effective short text deep classification.">141. Towards effective short text deep classification.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010090">Paper Link</a>】    【Pages】:1143-1144</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Xinruo">Xinruo Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Haofen">Haofen Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:
Recently, more and more short texts (e.g., ads, tweets) appear on the Web. Classifying short texts into a large taxonomy like ODP or Wikipedia category system has become an important mining task to improve the performance of many applications such as contextual advertising and topic detection for micro-blogging. In this paper, we propose a novel multi-stage classification approach to solve the problem. First, explicit semantic analysis is used to add more features for both short texts and categories. Second, we leverage information retrieval technologies to fetch the most relevant categories for an input short text from thousands of candidates. Finally, a SVM classifier is applied on only a few selected categories to return the final answer. Our experimental results show that the proposed method achieved significant improvements on classification accuracy compared with several existing state of art approaches.</p>
<p>【Keywords】:
classification; large scale hierarchy; short text</p>
<h3 id="142. Temporal latent semantic analysis for collaboratively generated content: preliminary results.">142. Temporal latent semantic analysis for collaboratively generated content: preliminary results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010091">Paper Link</a>】    【Pages】:1145-1146</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0022:Yu">Yu Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a></p>
<p>【Abstract】:
Latent semantic analysis (LSA) has been intensively studied because of its wide application to Information Retrieval and Natural Language Processing. Yet, traditional models such as LSA only examine one (current) version of the document. However, due to the recent proliferation of collaboratively generated content such as threads in online forums, Collaborative Question Answering archives, Wikipedia, and other versioned content, the document generation process is now directly observable. In this study, we explore how this additional temporal information about the document evolution could be used to enhance the identification of latent document topics. Specifically, we propose a novel hidden-topic modeling algorithm, temporal Latent Semantic Analysis (tLSA), which elegantly extends LSA to modeling document revision history using tensor decomposition. Our experiments show that tLSA outperforms LSA on word relatedness estimation using benchmark data, and explore applications of tLSA for other tasks.</p>
<p>【Keywords】:
temporal semantics; word relatedness</p>
<h3 id="143. Self-adjusting hybrid recommenders based on social network analysis.">143. Self-adjusting hybrid recommenders based on social network analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010092">Paper Link</a>】    【Pages】:1147-1148</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bellog=iacute=n:Alejandro">Alejandro Bellogín</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Castells:Pablo">Pablo Castells</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cantador:Iv=aacute=n">Iván Cantador</a></p>
<p>【Abstract】:
Ensemble recommender systems successfully enhance recom-mendation accuracy by exploiting different sources of user prefe-rences, such as ratings and social contacts. In linear ensembles, the optimal weight of each recommender strategy is commonly tuned empirically, with limited guarantee that such weights are optimal afterwards. We propose a self-adjusting hybrid recommendation approach that alleviates the social cold start situation by weighting the recommender combination dynamically at recommendation time, based on social network analysis algorithms. We show empirical results where our approach outperforms the best static combination for different hybrid recommenders.</p>
<p>【Keywords】:
graph theory; hybrid recommender systems; link analysis; social networks</p>
<h3 id="144. BlogCast effect on information diffusion in a blogosphere.">144. BlogCast effect on information diffusion in a blogosphere.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010093">Paper Link</a>】    【Pages】:1149-1150</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Sang=Wook">Sang-Wook Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Faloutsos:Christos">Christos Faloutsos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Ha:Jiwoon">Jiwoon Ha</a></p>
<p>【Abstract】:
A blog service company provides a function named BlogCast that exposes quality posts on the blog main page to vitalize a blogosphere. This paper analyzes a new type of information diffusion via BlogCast. We show that there exists a strong halo effect in a blogosphere via thorough investigation on a huge volume of blog data.</p>
<p>【Keywords】:
blogosphere; information diffusion; social networks</p>
<h3 id="145. Product comparison using comparative relations.">145. Product comparison using comparative relations.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010094">Paper Link</a>】    【Pages】:1151-1152</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li_0001:Si">Si Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Zheng=Jun">Zheng-Jun Zha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ming:Zhaoyan">Zhaoyan Ming</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Meng">Meng Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chua:Tat=Seng">Tat-Seng Chua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jun">Jun Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Weiran">Weiran Xu</a></p>
<p>【Abstract】:
This paper proposes a novel Product Comparison approach. The comparative relations between products are first mined from both user reviews on multiple review websites and community-based question answering pairs containing product comparison information. A unified graph model is then developed to integrate the resultant comparative relations for product comparison. Experiments on popular electronic products show that the proposed approach outperforms the state-of-the-art methods.</p>
<p>【Keywords】:
comparative relations graph; product comparison</p>
<h3 id="146. Collaborative cyberporn filtering with collective intelligence.">146. Collaborative cyberporn filtering with collective intelligence.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010095">Paper Link</a>】    【Pages】:1153-1154</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Lung=Hao">Lung-Hao Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Hsin=Hsi">Hsin-Hsi Chen</a></p>
<p>【Abstract】:
This paper presents a user intent method to generate blacklists for collaborative cyberporn filtering. A novel porn detection framework that finds new pornographic web pages by mining user search behaviors is proposed. It employs users' clicks in search query logs to select the suspected web pages without extra human efforts to label data for training, and determines their categories with the help of URL host name and path information, but without web page content. We adopt an MSN porn data set to explore the effectiveness of our method. This user intent approach achieves high precision, while maintaining favorably low false positive rate. In addition, real-life filtering simulation reveals that our user intent method with its accumulative update strategy achieves 43.36% of blocking rate, while maintaining a steadily less than 7% of over-blocking rate.</p>
<p>【Keywords】:
pornographic blacklists; query log analysis; searches-and-clicks</p>
<h3 id="147. Do IR models satisfy the TDC retrieval constraint.">147. Do IR models satisfy the TDC retrieval constraint.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010096">Paper Link</a>】    【Pages】:1155-1156</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Clinchant:St=eacute=phane">Stéphane Clinchant</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gaussier:=Eacute=ric">Éric Gaussier</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
axiomatic constraint; retrieval models; tdc constraint</p>
<h3 id="148. On diversifying and personalizing web search.">148. On diversifying and personalizing web search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010097">Paper Link</a>】    【Pages】:1157-1158</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vallet:David">David Vallet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Castells:Pablo">Pablo Castells</a></p>
<p>【Abstract】:
Diversification and personalization methods are common ap-proaches to deal with the one-size-fits-all paradigm of Web search engines. We performed a user study with 190 subjects where we analyzed the effects of diversification and personalization methods in a Web search engine. The obtained results suggest that our proposed combination of diversification and personalization factors may be a way to overcome the notion of intrusiveness in personalized approaches.</p>
<p>【Keywords】:
diversity; personalization; web search</p>
<h3 id="149. Semantic tag recommendation using concept model.">149. Semantic tag recommendation using concept model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010098">Paper Link</a>】    【Pages】:1159-1160</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chenliang">Chenliang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Datta:Anwitaman">Anwitaman Datta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Aixin">Aixin Sun</a></p>
<p>【Abstract】:
The common tags given by multiple users to a particular document are often semantically relevant to the document and each tag represents a specific topic. In this paper, we attempt to emulate human tagging behavior to recommend tags by considering the concepts contained in documents. Specifically, we represent each document using a few most relevant concepts contained in the document, where the concept space is derived from Wikipedia. Tags are then recommended based on the tag concept model derived from the annotated documents of each tag. Evaluated on a Delicious dataset of more than 53K documents, the proposed technique achieved comparable tag recommendation accuracy as the state-of-the-art, while yielding an order of magnitude speed-up.</p>
<p>【Keywords】:
concept model; semantic tag; tag recommendation; wikipedia</p>
<h3 id="150. Recommending interesting activity-related local entities.">150. Recommending interesting activity-related local entities.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010099">Paper Link</a>】    【Pages】:1161-1162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jie">Jie Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/White:Ryen_W=">Ryen W. White</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bailey:Peter">Peter Bailey</a></p>
<p>【Abstract】:
When searching for entities with a strong local character (e.g., a museum), people may also be interested in discovering proximal activity-related entities (e.g., a café). Geographical proximity is a necessary, but not sufficient, qualifier for recommending other entities such that they are related in a useful manner (e.g., interest in a fish market does not imply interest in nearby bookshops, but interest in other produce stores is more likely). We describe and evaluate methods to identify such activity-related local entities.</p>
<p>【Keywords】:
entity resolution; entity-entity matching; local entities</p>
<h3 id="151. Cross-corpus relevance projection.">151. Cross-corpus relevance projection.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010100">Paper Link</a>】    【Pages】:1163-1164</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Asadi_0001:Nima">Nima Asadi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metzler:Donald">Donald Metzler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Jimmy_J=">Jimmy J. Lin</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
learning to rank; test collections; web search</p>
<h3 id="152. Location disambiguation for geo-tagged images.">152. Location disambiguation for geo-tagged images.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010101">Paper Link</a>】    【Pages】:1165-1166</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Zhu">Zhu Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shou:Lidan">Lidan Shou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Kuang">Kuang Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0001:Gang">Gang Chen</a></p>
<p>【Abstract】:
In this poster, we address the problem of location disambiguation for geotagged Web photo resources. We propose an approach for analyzing and partitioning large geotagged photo collections using geographic and semantic information. By organizing the dataset in a structural scheme, we resolve the location ambiguity and clutter problem yield by massive volume of geotagged photos.</p>
<p>【Keywords】:
geotagged images; location disambiguation</p>
<h3 id="153. Towards an indexing method to speed-up music retrieval.">153. Towards an indexing method to speed-up music retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010102">Paper Link</a>】    【Pages】:1167-1168</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Martin:Benjamin">Benjamin Martin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hanna:Pierre">Pierre Hanna</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Robine:Matthias">Matthias Robine</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Ferraro:Pascal">Pascal Ferraro</a></p>
<p>【Abstract】:
Computations in most music retrieval systems strongly depend on the size of data compared. We propose to enhance performances of a music retrieval system, namely a harmonic similarity evaluation method, by first indexing relevant parts of music pieces. The indexing algorithm represents each audio piece exclusively by its major repetition, using harmonic descriptions and string matching techniques. Evaluations are performed in the context of a state-of-the-art retrieval method, namely cover songs identification, and results highlight the success of our indexing system in keeping similar results while yielding a substantial gain in computation time.</p>
<p>【Keywords】:
cover songs identification; music information retrieval</p>
<h3 id="154. An investigation of decompounding for cross-language patent search.">154. An investigation of decompounding for cross-language patent search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010103">Paper Link</a>】    【Pages】:1169-1170</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Leveling:Johannes">Johannes Leveling</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Magdy:Walid">Walid Magdy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jones:Gareth_J=_F=">Gareth J. F. Jones</a></p>
<p>【Abstract】:
Decompounding has been found to improve information retrieval (IR) effectiveness in general domains for languages such as German or Dutch. We investigate if cross-language patent retrieval can profit from decompounding. This poses two challenges: i) There may be few resources such as parallel corpora available for training an machine translation system for a compounding language. ii) Patents have a specific writing style and vocabulary ("patentese"), which may affect the performance of decompounding and translation methods. Experiments on data from the CLEF-IP 2010 task show that decompounding patents for translation can overcome out-of-vocabulary problems (OOV) and that decompounding improves IR performance significantly for small training corpora.</p>
<p>【Keywords】:
decompounding; patent retrieval</p>
<h3 id="155. Detecting seasonal queries by time-series analysis.">155. Detecting seasonal queries by time-series analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010104">Paper Link</a>】    【Pages】:1171-1172</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shokouhi:Milad">Milad Shokouhi</a></p>
<p>【Abstract】:
Seasonal events such as Halloween and Christmas repeat every year and initiate several temporal information needs. The impact of such events on users is often reflected in search logs in form of seasonal spikes in the frequency of related queries (e.g. "halloween costumes", "where is santa"). Many seasonal queries such as "sigir conference" mainly target fresh pages (e.g. sigir2011.org) that have less usage data such as clicks and anchor-text compared to older alternatives (e.g.sigir2009.org). Thus, it is important for search engines to correctly identify seasonal queries and make sure that their results are temporally reordered if necessary. In this poster, we focus on detecting seasonal queries using time-series analysis. We demonstrate that the seasonality of a query can be determined with high accuracy according to its historical frequency distribution.</p>
<p>【Keywords】:
seasonal query classification; temporal queries</p>
<h3 id="156. Learning to rank under tight budget constraints.">156. Learning to rank under tight budget constraints.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010105">Paper Link</a>】    【Pages】:1173-1174</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/P=ouml=litz:Christian">Christian Pölitz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schenkel:Ralf">Ralf Schenkel</a></p>
<p>【Abstract】:
This paper investigates the influence of pruning feature lists to keep a given budget for the evaluation of ranking methods. We learn from a given training set how important the individual prefixes are for the ranking quality. Based on there importance we choose the best prefixes to calculate the ranking while keeping the budget.</p>
<p>【Keywords】:
efficiency; learning to rank</p>
<h3 id="157. A novel hybrid index structure for efficient text retrieval.">157. A novel hybrid index structure for efficient text retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010106">Paper Link</a>】    【Pages】:1175-1176</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Broschart:Andreas">Andreas Broschart</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schenkel:Ralf">Ralf Schenkel</a></p>
<p>【Abstract】:
Query processing with precomputed term pair lists can improve efficiency for some queries, but suffers from the quadratic number of index lists that need to be read. We present a novel hybrid index structure that aims at decreasing the number of index lists retrieved at query processing time, trading off a reduced number of index lists for an increased number of bytes to read. Our experiments demonstrate significant cold-cache performance gains of almost 25% on standard benchmark queries.</p>
<p>【Keywords】:
efficiency; proximity scoring</p>
<h3 id="158. A weighted curve fitting method for result merging in federated search.">158. A weighted curve fitting method for result merging in federated search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010107">Paper Link</a>】    【Pages】:1177-1178</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/He:Chuan">Chuan He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hong:Dzung">Dzung Hong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a></p>
<p>【Abstract】:
Result merging is an important step in federated search to merge the documents returned from multiple source-specific ranked lists for a user query. Previous result merging methods such as Semi-Supervised Learning (SSL) and Sample- Agglomerate Fitting Estimate (SAFE) use regression methods to estimate global document scores from document ranks in individual ranked lists. SSL relies on overlapping documents that exist in both individual ranked lists and a centralized sample database. SAFE goes a step further by using both overlapping documents with accurate rank information and documents with estimated rank information for regression. However, existing methods do not distinguish the accurate rank information from the estimated information. Furthermore, all documents are assigned equal weights in regression while intuitively, documents in the top should carry higher weights. This paper proposes a weighted curve fitting method for result merging in federated search. The new method explicitly models the importance of information from overlapping documents over non-overlapping ones. It also weights documents at different positions differently. Empirically results on two datasets clearly demonstrate the advantage of the proposed algorithm.</p>
<p>【Keywords】:
curve fitting; federated search; result merging</p>
<h3 id="159. Effect of different docid orderings on dynamic pruning retrieval strategies.">159. Effect of different docid orderings on dynamic pruning retrieval strategies.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010108">Paper Link</a>】    【Pages】:1179-1180</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tonellotto:Nicola">Nicola Tonellotto</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
Document-at-a-time (DAAT) dynamic pruning strategies for information retrieval systems such as MaxScore and Wand can increase querying efficiency without decreasing effectiveness. Both work on posting lists sorted by ascending document identifier (docid). The order in which docids are assigned -- and hence the order of postings in the posting lists -- is known to have a noticeable impact on posting list compression. However, the resulting impact on dynamic pruning strategies is not well understood. In this poster, we examine the impact on the efficiency of these strategies across different docid orderings, by experimenting using the TREC ClueWeb09 corpus. We find that while the number of postings scored by dynamic pruning strategies do not markedly vary for different docid orderings, the ordering still has a marked impact on mean query response time. Moreover, when docids are assigned by lexicographical URL ordering, the benefit to response time for is more pronounced for Wand than for MaxScore.</p>
<p>【Keywords】:
docid ordering; dynamic pruning</p>
<h3 id="160. Time-based query performance predictors.">160. Time-based query performance predictors.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010109">Paper Link</a>】    【Pages】:1181-1182</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=oslash=rv=aring=g:Kjetil">Kjetil Nørvåg</a></p>
<p>【Abstract】:
Query performance prediction is aimed at predicting the retrieval effectiveness that a query will achieve with respect to a particular ranking model. In this paper, we study query performance prediction for a ranking model that explicitly incorporates the time dimension into ranking. Different time-based predictors are proposed as analogous to existing keyword-based predictors. In order to improve predicting performance, we combine different predictors using linear regression and neural networks. Extensive experiments are conducted using queries and relevance judgments obtained by crowdsourcing.</p>
<p>【Keywords】:
query performance prediction; time-aware ranking</p>
<h3 id="161. Search task difficulty: the expected vs. the reflected.">161. Search task difficulty: the expected vs. the reflected.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010110">Paper Link</a>】    【Pages】:1183-1184</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Jingjing">Jingjing Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Belkin:Nicholas_J=">Nicholas J. Belkin</a></p>
<p>【Abstract】:
We report findings on how the user's perception of task difficulty changes before and after searching for information to solve tasks. We found that while in one type of task, the dependent task, this did not change, in another, the parallel task, it did. The findings have implications on designing systems that can provide assistance to users with their search and task solving strategies.</p>
<p>【Keywords】:
expected difficulty; reflected difficulty; task difficulty; task type</p>
<h3 id="162. On the suitability of diversity metrics for learning-to-rank for diversity.">162. On the suitability of diversity metrics for learning-to-rank for diversity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010111">Paper Link</a>】    【Pages】:1185-1186</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Santos:Rodrygo_L=_T=">Rodrygo L. T. Santos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
An optimally diverse ranking should achieve the maximum coverage of the aspects underlying an ambiguous or underspecified query, with minimum redundancy with respect to the covered aspects. Although evaluation metrics that reward coverage and penalise redundancy provide intuitive objective functions for learning a diverse ranking, it is unclear whether they are the most effective. In this paper, we contrast the suitability of relevance and diversity metrics as objective functions for learning a diverse ranking. Our results in the context of the diversity task of the TREC 2009 and 2010 Web tracks show that diversity metrics are not necessarily better suited for guiding a learning approach. Moreover, the suitability of these metrics is compromised as they try to penalise redundancy during the learning process.</p>
<p>【Keywords】:
diversity; learning-to-rank; web search</p>
<h3 id="163. How diverse are web search results?">163. How diverse are web search results?</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010112">Paper Link</a>】    【Pages】:1187-1188</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Santos:Rodrygo_L=_T=">Rodrygo L. T. Santos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
Search result diversification has recently gained attention as a means to tackle ambiguous queries. While query ambiguity is of particular concern for the short queries commonly observed in a Web search scenario, it is unclear how much diversity is actually promoted by Web search engines (WSEs). In this paper, we assess the diversification performance of two leading WSEs in the context of the diversity task of the TREC 2009 and 2010 Web tracks. Our results show that these WSEs perform effectively for queries with multiple interpretations, but not for those open to multiple aspects related to a single interpretation. Moreover, by deploying a state-of-the-art diversification approach based on query suggestions from these WSEs themselves, we show that their diversification performance can be further improved.</p>
<p>【Keywords】:
diversity; search engines; web search</p>
<h3 id="164. Analysis of an expert search query log.">164. Analysis of an expert search query log.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010113">Paper Link</a>】    【Pages】:1189-1190</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/f/Fang:Yi">Yi Fang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Somasundaram:Naveen">Naveen Somasundaram</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Ko:Jeongwoo">Jeongwoo Ko</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mathur:Aditya_P=">Aditya P. Mathur</a></p>
<p>【Abstract】:
Expert search has made rapid progress in modeling, algorithms and evaluations in the recent years. However, there is very few work on analyzing how users interact with expert search systems. In this paper, we conduct analysis of an expert search query log. The aim is to understand the special characteristics of expert search usage. To the best of our knowledge, this is one of the earliest work on expert search query log analysis. We find that expert search users generally issue shorter queries, more common queries, and use more advanced search features, with fewer queries in a session, than general Web search users do. This study explores a new research direction in expert search by analyzing and exploiting query logs.</p>
<p>【Keywords】:
expert search; query log analysis</p>
<h3 id="165. A model for expert finding in social networks.">165. A model for expert finding in social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010114">Paper Link</a>】    【Pages】:1191-1192</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Smirnova:Elena">Elena Smirnova</a></p>
<p>【Abstract】:
Expert finding is a task of finding knowledgeable people on a given topic. State-of-the-art expertise retrieval algorithms identify matching experts based on analysis of textual content of documents experts are associated with. While powerful, these models ignore social structure that might be available. In this paper, we develop a Bayesian hierarchical model for expert finding that accounts for both social relationships and content. The model assumes that social links are determined by expertise similarity between candidates. We demonstrate the improved retrieval performance of our model over the baseline on a realistic data set.</p>
<p>【Keywords】:
expert finding; social network; topic model</p>
<h3 id="166. Transductive learning over automatically detected themes for multi-document summarization.">166. Transductive learning over automatically detected themes for multi-document summarization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010115">Paper Link</a>】    【Pages】:1193-1194</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Amini:Massih=Reza">Massih-Reza Amini</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Usunier:Nicolas">Nicolas Usunier</a></p>
<p>【Abstract】:
We propose a new method for query-biased multi-document summarization, based on sentence extraction. The summary of multiple documents is created in two steps. Sentences are first clustered; where each cluster corresponds to one of the main themes present in the collection. Inside each theme, sentences are then ranked using a transductive learning-to-rank algorithm based on RankNet, in order to better identify those which are relevant to the query. The final summary contains the top-ranked sentences of each theme. Our approach is validated on DUC 2006 and DUC 2007 datasets.</p>
<p>【Keywords】:
learning to rank; multi-document summarization</p>
<h3 id="167. Rating-based collaborative filtering combined with additional regularization.">167. Rating-based collaborative filtering combined with additional regularization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010116">Paper Link</a>】    【Pages】:1195-1196</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Shu">Shu Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Shengrui">Shengrui Wang</a></p>
<p>【Abstract】:
The collaborative filtering (CF) approach to recommender system has received much attention recently. However, previous work mainly focuses on improving the formula of rating prediction, e.g. by adding user and item biases, implicit feedback and time-aware factors, etc, to reach a better prediction by minimizing an objective function. However, little effort has been made on improving CF by incorporating additional regularization to the objective function. Regularization can further bound the searching range of predicted ratings. In this paper, we improve the conventional rating-based objective function by using ranking constraints as the supplementary regularization to restrict the searching of predicted ratings in smaller and more likely ranges, and develop a novel method, called RankSVD++, based on the SVD++ model. Experimental results show that RankSVD++ achieves better performance than existing main-streaming methods due to the addition of informative ranking-based regularization. The idea proposed here can also be easily incorporated to the other CF models.</p>
<p>【Keywords】:
collaborative filtering; matrix factorization</p>
<h3 id="168. Words-of-interest selection based on temporal motion coherence for video retrieval.">168. Words-of-interest selection based on temporal motion coherence for video retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010117">Paper Link</a>】    【Pages】:1197-1198</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Lei">Lei Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Dawei">Dawei Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Elyan:Eyad">Eyad Elyan</a></p>
<p>【Abstract】:
The "Bag of Visual Words" (BoW) framework has been widely used in query-by-example video retrieval to model the visual content by a set of quantized local feature descriptors. In this paper, we propose a novel technique to enhance BoW by the selection of Word-of-Interest (WoI) that utilizes the quantified temporal motion coherence of the visual words between the adjacent frames in the query example. Experiments carried out using TRECVID datasets show that our technique improves the retrieval performance of the classical BoW-based approach.</p>
<p>【Keywords】:
bag of visual words; temporal motion coherence; video retrieval; words-of-interest</p>
<h3 id="169. Aggregating multiple opinion evidence in proximity-based opinion retrieval.">169. Aggregating multiple opinion evidence in proximity-based opinion retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010118">Paper Link</a>】    【Pages】:1199-1200</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gerani:Shima">Shima Gerani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Keikha:Mostafa">Mostafa Keikha</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Crestani:Fabio">Fabio Crestani</a></p>
<p>【Abstract】:
Blog post opinion retrieval is the problem of ranking blog posts according to the likelihood that the post is relevant to the query and that the author was expressing an opinion about the topic (of the query). A recent study has proposed a method for finding the opinion density at query term positions in a document which uses the proximity of query term and opinion term as an indicator of their relatedness. The maximum opinion density between different query positions was used as an opinion score of the whole document. In this paper we investigate the effect of exploiting multiple opinion evidence of a document. We propose using the ordered weighted averaging (OWA) operator in order to combine the opinion score of different query positions for a final score of a document, in the proximity-based opinion retrieval system.</p>
<p>【Keywords】:
blog; opinion retrieval; owa; proximity</p>
<h3 id="170. Enhancing mobile search using web search log data.">170. Enhancing mobile search using web search log data.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010119">Paper Link</a>】    【Pages】:1201-1202</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/i/Inagaki:Yoshiyuki">Yoshiyuki Inagaki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bian:Jiang">Jiang Bian</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Yi">Yi Chang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maki:Motoko">Motoko Maki</a></p>
<p>【Abstract】:
Mobile search is still in infancy compared with general purpose web search. With limited training data and weak relevance features, the ranking performance in mobile search is far from satisfactory. To address this problem, we propose to leverage the knowledge of Web search to enhance the ranking of mobile search. In this paper, we first develop an equivalent page conversion between web search and mobile search, then we design a few novel ranking features, generated from the click-through data in web search, for estimating the relevance of mobile search. Large scale evaluations demonstrate that the knowledge from web search is quite effective for boosting the relevance of ranking on mobile search.</p>
<p>【Keywords】:
mobile search; ranking; ranking features</p>
<h3 id="171. Award prediction with temporal citation network analysis.">171. Award prediction with temporal citation network analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010120">Paper Link</a>】    【Pages】:1203-1204</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Zaihan">Zaihan Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Dawei">Dawei Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davison_0001:Brian_D=">Brian D. Davison</a></p>
<p>【Abstract】:
Each year many ACM SIG communities will recognize an outstanding researcher through an award in honor of his or her profound impact and numerous research contributions. This work is the first to investigate an automated mechanism to help in selecting future award winners. We approach the problem as a researchers' expertise ranking problem, and propose a temporal probabilistic ranking model which combines content with citation network analysis. Experimental results based on real-world citation data and historical awardees indicate that some kinds of SIG awards are well-modeled by this approach.</p>
<p>【Keywords】:
citation network; link analysis; temporal correlation</p>
<h3 id="172. Rating prediction using feature words extracted from customer reviews.">172. Rating prediction using feature words extracted from customer reviews.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010121">Paper Link</a>】    【Pages】:1205-1206</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Ochi:Masanao">Masanao Ochi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Okabe:Makoto">Makoto Okabe</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Onai:Rikio">Rikio Onai</a></p>
<p>【Abstract】:
We developed a simple method of improving the accuracy of rating prediction using feature words extracted from customer reviews. Many rating predictors work well for a small and dense dataset of customer reviews. However, a practical dataset tends to be large and sparse, because it often includes too many products for each customer to buy and evaluate. Data sparseness reduces prediction accuracy. To improve accuracy, we reduced the dimension of the feature vector using feature words extracted by analyzing the relationship between ratings and accompanying review comments instead of using ratings. We applied our method to the Pranking algorithm and evaluated it on a corpus of golf course reviews supplied by a Japanese e-commerce company. We found that by successfully reducing data sparseness, our method improves prediction accuracy as measured using RankLoss.</p>
<p>【Keywords】:
rating prediction; review mining; sentiment analysis</p>
<h3 id="173. Ranking tags in resource collections.">173. Ranking tags in resource collections.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010122">Paper Link</a>】    【Pages】:1207-1208</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Skoutas:Dimitrios">Dimitrios Skoutas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Alrifai:Mohammad">Mohammad Alrifai</a></p>
<p>【Abstract】:
We examine different tag ranking strategies for constructing tag clouds to represent collections of tagged objects. The proposed methods are based on random walk on graphs, diversification, and rank aggregation, and they are empirically evaluated on a data set of tagged images from Flickr.</p>
<p>【Keywords】:
tag clouds; tag ranking</p>
<h3 id="174. Identifying similar people in professional social networks with discriminative probabilistic models.">174. Identifying similar people in professional social networks with discriminative probabilistic models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010123">Paper Link</a>】    【Pages】:1209-1210</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cetintas:Suleyman">Suleyman Cetintas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rogati:Monica">Monica Rogati</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fang:Yi">Yi Fang</a></p>
<p>【Abstract】:
Identifying similar professionals is an important task for many core services in professional social networks. Information about users can be obtained from heterogeneous information sources, and different sources provide different insights on user similarity. This paper proposes a discriminative probabilistic model that identifies latent content and graph classes for people with similar profile content and social graph similarity patterns, and learns a specialized similarity model for each latent class. To the best of our knowledge, this is the first work on identifying similar professionals in professional social networks, and the first work that identifies latent classes to learn a separate similarity model for each latent class. Experiments on a real-world dataset demonstrate the effectiveness of the proposed discriminative learning model.</p>
<p>【Keywords】:
discriminative learning; similar people; social networks</p>
<h3 id="175. Intent-oriented diversity in recommender systems.">175. Intent-oriented diversity in recommender systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010124">Paper Link</a>】    【Pages】:1211-1212</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Vargas:Saul">Saul Vargas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Castells:Pablo">Pablo Castells</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vallet:David">David Vallet</a></p>
<p>【Abstract】:
Diversity as a relevant dimension of retrieval quality is receiving increasing attention in the Information Retrieval and Recommender Systems (RS) fields. The problem has nonetheless been approached under different views and formulations in IR and RS respectively, giving rise to different models, methodologies, and metrics, with little convergence between both fields. In this poster we explore the adaptation of diversity metrics, techniques, and principles from ad-hoc IR to the recommendation task, by introducing the notion of user profile aspect as an analogue of query intent. As a particular approach, user aspects are automatically extracted from latent item features. Empirical results support the proposed approach and provide further insights.</p>
<p>【Keywords】:
diversity; diversity metrics; profile aspects; query intent; recommender systems; user profiles</p>
<h3 id="176. Disambiguating biomedical acronyms using EMIM.">176. Disambiguating biomedical acronyms using EMIM.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010125">Paper Link</a>】    【Pages】:1213-1214</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Limsopatham:Nut">Nut Limsopatham</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Santos:Rodrygo_L=_T=">Rodrygo L. T. Santos</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Macdonald:Craig">Craig Macdonald</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ounis:Iadh">Iadh Ounis</a></p>
<p>【Abstract】:
Expanding a query with acronyms or their corresponding 'long-forms' has not been shown to provide consistent improvements in the biomedical IR literature. The major open issue with expanding acronyms in a query is their inherent ambiguity, as an acronym can refer to multiple long-forms. At the same time, a long-form identified in a query can be expanded with its acronym(s); however, some of these may be also ambiguous and lead to poor retrieval performance. In this work, we propose the use of the EMIM (Expected Mutual Information Measure) between a long-form and its abbreviated acronym to measure ambiguity. We experiment with expanding both acronyms and long-forms identified in the queries from the adhoc task of the TREC 2004 Genomics track. Our preliminary analysis shows the potential of both acronym and long-form expansions for biomedical IR.</p>
<p>【Keywords】:
acronym expansion; biomedical retrieval</p>
<h3 id="177. Best document selection based on approximate utility optimization.">177. Best document selection based on approximate utility optimization.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010126">Paper Link</a>】    【Pages】:1215-1216</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Hungyu_Henry">Hungyu Henry Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yi">Yi Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davis:James_E=">James E. Davis</a></p>
<p>【Abstract】:
This poster describes an alternative approach to handling the best document selection problem. Best document selection is a common problem with many real world applications, but is not a well studied problem by itself; a simple solution would be to treat it as a ranking problem and to use existing ranking algorithms to rank all documents. We could then select only the first element of the sorted list. However, because ranking models optimize for all ranks, the model may sacrifice accuracy of the top rank for the sake of overall accuracy. This is an unnecessary trade-off. We begin by first defining an appropriate objective function for the domain, then create a boosting algorithm that explicitly targets this function. Based on experiments on a benchmark retrieval data set and Digg.com news commenting data set, we find that even a simple algorithm built for this specific problem gives better results than baseline algorithms that were designed for the more complicated ranking tasks.</p>
<p>【Keywords】:
best document selection; boosting; learning to rank</p>
<h3 id="178. Forecasting counts of user visits for online display advertising with probabilistic latent class models.">178. Forecasting counts of user visits for online display advertising with probabilistic latent class models.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010127">Paper Link</a>】    【Pages】:1217-1218</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cetintas:Suleyman">Suleyman Cetintas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Datong">Datong Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Bin">Bin Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Datbayev:Zhanibek">Zhanibek Datbayev</a></p>
<p>【Abstract】:
Display advertising is a multi-billion dollar industry where advertisers promote their products to users by having publishers display their advertisements on popular Web pages. An important problem in online advertising is how to forecast the number of user visits for a Web page during a particular period of time. Prior research addressed the problem by using traditional time-series forecasting techniques on historical data of user visits; (e.g., via a single regression model built for forecasting based on historical data for all Web pages) and did not fully explore the fact that different types of Web pages have different patterns of user visits. In this paper we propose a probabilistic latent class model to automatically learn the underlying user visit patterns among multiple Web pages. Experiments carried out on real-world data demonstrate the advantage of using latent classes in forecasting online user visits.</p>
<p>【Keywords】:
display advertising; forecasting; user visits</p>
<h3 id="179. Knowledge effects on document selection in search results pages.">179. Knowledge effects on document selection in search results pages.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010128">Paper Link</a>】    【Pages】:1219-1220</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cole:Michael_J=">Michael J. Cole</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiangmin">Xiangmin Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Chang">Chang Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Belkin:Nicholas_J=">Nicholas J. Belkin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gwizdka:Jacek">Jacek Gwizdka</a></p>
<p>【Abstract】:
Click through events in search results pages (SERPs) are not reliable implicit indicators of document relevance. A user's task and domain knowledge are key factors in recognition and link selection and the most useful SERP document links may be those that best match the user's domain knowledge. User study participants rated their knowledge of genomics MeSH terms before conducting 2004 TREC Genomics Track tasks. Each participant's document knowledge was represented by their knowledge of the indexing MeSH terms. Results show high, intermediate, and low domain knowledge groups had similar document selection SERP rank distributions. SERP link selection distribution varied when participant knowledge of the available documents was analyzed. High domain knowledge participants usually selected a document with the highest personal knowledge rating. Low domain knowledge participants were reasonably successful at selecting available documents of which they had the most knowledge, while intermediate knowledge participants often failed to do so. This evidence for knowledge effects on SERP link selection may contribute to understanding the potential for personalization of search results ranking based on user domain knowledge.</p>
<p>【Keywords】:
information search behavior; knowledge effects; personalization; user study</p>
<h3 id="180. Learning to rank from a noisy crowd.">180. Learning to rank from a noisy crowd.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010129">Paper Link</a>】    【Pages】:1221-1222</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:Abhimanu">Abhimanu Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lease:Matthew">Matthew Lease</a></p>
<p>【Abstract】:
We study how to best use crowdsourced relevance judgments learning to rank [1, 7]. We integrate two lines of prior work: unreliable crowd-based binary annotation for binary classification [5, 3], and aggregating graded relevance judgments from reliable experts for ranking [7]. To model varying performance of the crowd, we simulate annotation noise with varying magnitude and distributional properties. Evaluation on three LETOR test collections reveals a striking trend contrary to prior studies: single labeling outperforms consensus methods in maximizing learner accuracy relative to annotator eýort. We also see surprising consistency of the learning curve across noise distributions, as well as greater challenge with the adversarial case for multi-class labeling.</p>
<p>【Keywords】:
active learning; crowdsourcing; learning to rank</p>
<h3 id="181. How to count thumb-ups and thumb-downs?: an information retrieval approach to user-rating based ranking of items.">181. How to count thumb-ups and thumb-downs?: an information retrieval approach to user-rating based ranking of items.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010130">Paper Link</a>】    【Pages】:1223-1224</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Dell">Dell Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Robert">Robert Mao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Haitao">Haitao Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mao:Joanne">Joanne Mao</a></p>
<p>【Abstract】:
It is a common practice among Web 2.0 services to allow users to rate items on their sites. In this paper, we first point out the flaws of the popular methods for user-rating based ranking of items, and then argue that two well-known Information Retrieval (IR) techniques, namely the Probability Ranking Principle and Statistical Language Modelling, provide a simple but effective solution to this problem.</p>
<p>【Keywords】:
information retrieval; probability ranking principle; smoothing; statistical language modelling; web 2.0</p>
<h3 id="182. Predicting users' domain knowledge from search behaviors.">182. Predicting users' domain knowledge from search behaviors.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010131">Paper Link</a>】    【Pages】:1225-1226</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiangmin">Xiangmin Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cole:Michael_J=">Michael J. Cole</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Belkin:Nicholas_J=">Nicholas J. Belkin</a></p>
<p>【Abstract】:
This study uses regression modeling to predict a user's domain knowledge level (DK) from implicit evidence provided by certain search behaviors. A user study (n=35) with recall-oriented search tasks in the genomic domain was conducted. A number of regression models of a person's DK, were generated using different behavior variable selection methods. The best model highlights three behavior variables as DK predictors: the number of documents saved, the average query length, and the average ranking position of documents opened. The model is validated using the split sampling method. Limitations and future research directions are discussed.</p>
<p>【Keywords】:
domain knowledge modeling; domain knowledge prediction; information retrieval; user modeling; user studies</p>
<h3 id="183. The interactive PRP for diversifying document rankings.">183. The interactive PRP for diversifying document rankings.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010132">Paper Link</a>】    【Pages】:1227-1228</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zuccon:Guido">Guido Zuccon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Azzopardi:Leif">Leif Azzopardi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rijsbergen:C=_J=_van">C. J. van Rijsbergen</a></p>
<p>【Abstract】:
The assumptions underlying the Probability Ranking Principle (PRP) have led to a number of alternative approaches that cater or compensate for the PRP's limitations. In this poster we focus on the Interactive PRP (iPRP), which rejects the assumption of independence between documents made by the PRP. Although the theoretical framework of the iPRP is appealing, no instantiation has been proposed and investigated. In this poster, we propose a possible instantiation of the principle, performing the first empirical comparison of the iPRP against the PRP. For document diversification, our results show that the iPRP is significantly better than the PRP, and comparable to or better than other methods such as Modern Portfolio Theory.</p>
<p>【Keywords】:
diversity; interactive prp; interdependent document relevance</p>
<h3 id="184. Detecting success in mobile search from interaction.">184. Detecting success in mobile search from interaction.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010133">Paper Link</a>】    【Pages】:1229-1230</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Qi">Qi Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Shuai">Shuai Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a></p>
<p>【Abstract】:
Predicting searcher success and satisfaction is a key problem in Web search, which is essential for automatic evaluating and improving search engine performance. This problem has been studied actively in the desktop search setting, but not specifically for mobile search, despite many known differences between the two modalities. As mobile devices become increasingly popular for searching the Web, improving the searcher experience on such devices is becoming crucially important. In this paper, we explore the possibility of predicting searcher success and satisfaction in mobile search with a smart phone. Specifically, we investigate client-side interaction signals, including the number of browsed pages, and touch screen-specific actions such as zooming and sliding. Exploiting this information with machine learning techniques results in nearly 80% accuracy for predicting searcher success -- significantly outperforming the previous models.</p>
<p>【Keywords】:
detecting success and satisfaction; mobile search behavior</p>
<h3 id="185. Measuring assessor accuracy: a comparison of nist assessors and user study participants.">185. Measuring assessor accuracy: a comparison of nist assessors and user study participants.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010134">Paper Link</a>】    【Pages】:1231-1232</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Smucker:Mark_D=">Mark D. Smucker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jethani:Chandra_Prakash">Chandra Prakash Jethani</a></p>
<p>【Abstract】:
In many situations, humans judging document relevance are forced to trade-off accuracy for speed. The development of better interactive retrieval systems and relevance assessing platforms requires the measurement of assessor accuracy, but to date the subjective nature of relevance has prevented such measurement. To quantify assessor performance, we define relevance to be a group's majority opinion, and demonstrate the value of this approach by comparing the performance of NIST assessors to a group of assessors representative of participants in many information retrieval user studies. Using data collected as part of a user study with 48 participants, we found that NIST assessors discriminate between relevant and non-relevant documents better than the average participant in our study, but that NIST assessors' true positive rate is no better than that of the study participants. In addition, we found NIST assessors to be conservative in their judgment of relevance compared to the average participant.</p>
<p>【Keywords】:
assessors; relevance; signal detection theory</p>
<h3 id="186. A bipartite graph based social network splicing method for person name disambiguation.">186. A bipartite graph based social network splicing method for person name disambiguation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010135">Paper Link</a>】    【Pages】:1233-1234</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tang:Jintao">Jintao Tang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Qin">Qin Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Ting">Ting Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Ji">Ji Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Wenjie">Wenjie Li</a></p>
<p>【Abstract】:
The key issue of person name disambiguation is to discover different namesakes in massive web documents rather than simply cluster documents by using textual features. In this paper, we describe a novel person name disambiguation method based on social networks to effectively identify namesakes. The social network snippets in each document are extracted. Then, the namesakes are identified via splicing the social networks of each namesake by using the snippets as a bipartite graph. Experimental results show that our method achieves better result than the top performance of WePS-2 in identifying different namesakes.</p>
<p>【Keywords】:
bipartite graph; person name disambiguation; social network</p>
<h3 id="187. Link formation analysis in microblogs.">187. Link formation analysis in microblogs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010136">Paper Link</a>】    【Pages】:1235-1236</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yin:Dawei">Dawei Yin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hong:Liangjie">Liangjie Hong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xiong:Xiong">Xiong Xiong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davison_0001:Brian_D=">Brian D. Davison</a></p>
<p>【Abstract】:
Unlike a traditional social network service, a microblogging network like Twitter is a hybrid network, combining aspects of both social networks and information networks. Understanding the structure of such hybrid networks and to predict new links are important for many tasks such as friend recommendation, community detection, and network growth models. In this paper, by analyzing data collected over time, we find that 90% of new links are to people just two hops away and dynamics of friend acquisition are also related to users' account age. Finally, we compare two popular sampling methods which are widely used for network analysis and find that ForestFire does not preserve properties required for the link prediction task.</p>
<p>【Keywords】:
link analysis; link formation; link prediction; microblogs</p>
<h3 id="188. Evolution of web search results within years.">188. Evolution of web search results within years.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010137">Paper Link</a>】    【Pages】:1237-1238</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Alting=ouml=vde:Ismail_Seng=ouml=r">Ismail Sengör Altingövde</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ozcan:Rifat">Rifat Ozcan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Ulusoy:=Ouml=zg=uuml=r">Özgür Ulusoy</a></p>
<p>【Abstract】:
We provide a first large-scale analysis of the evolution of query results obtained from a real search engine at two distant points in time, namely, in 2007 and 2010, for a set of 630,000 real queries.</p>
<p>【Keywords】:
longitudinal; result update; temporal; web search</p>
<h3 id="189. Decayed DivRank: capturing relevance, diversity and prestige in information networks.">189. Decayed DivRank: capturing relevance, diversity and prestige in information networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010138">Paper Link</a>】    【Pages】:1239-1240</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Du:Pan">Pan Du</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Jiafeng">Jiafeng Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Xueqi">Xueqi Cheng</a></p>
<p>【Abstract】:
Many network-based ranking approaches have been proposed to rank objects according to different criteria, including relevance, prestige and diversity. However, existing approaches either only aim at one or two of the criteria, or handle them with additional heuristics in multiple steps. Inspired by DivRank, we propose a unified ranking model, Decayed DivRank (DDRank), to meet the three criteria simultaneously. Empirical experiments on paper citation network show that DDRank can outperform existing algorithms in capturing relevance, diversity and prestige simultaneously in ranking.</p>
<p>【Keywords】:
diversity; multi-objective ranking; prestige; relevance</p>
<h3 id="190. Multi-objective optimization in learning to rank.">190. Multi-objective optimization in learning to rank.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010139">Paper Link</a>】    【Pages】:1241-1242</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Dai:Na">Na Dai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shokouhi:Milad">Milad Shokouhi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Davison_0001:Brian_D=">Brian D. Davison</a></p>
<p>【Abstract】:
Supervised learning to rank algorithms typically optimize for high relevance and ignore other facets of search quality, such as freshness and diversity. Prior work on multi-objective ranking trained rankers focused on using hybrid labels that combine overall quality of documents, and implicitly incorporate multiple criteria into quantifying ranking risks. However, these hybrid scores are usually generated based on heuristics without considering potential correlations between individual facets (e.g., freshness versus relevance). In this poster, we empirically demonstrate that the correlation between objective facets in multi-criteria ranking optimization may significantly influence the effectiveness of trained rankers with respect to each objective.</p>
<p>【Keywords】:
learning to rank; multi-objective optimization</p>
<h3 id="191. A large-scale study of the effect of training set characteristics over learning-to-rank algorithms.">191. A large-scale study of the effect of training set characteristics over learning-to-rank algorithms.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010140">Paper Link</a>】    【Pages】:1243-1244</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanoulas:Evangelos">Evangelos Kanoulas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Savev:Stefan">Stefan Savev</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Metrikov:Pavel">Pavel Metrikov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pavlu:Virgiliu">Virgiliu Pavlu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Aslam:Javed_A=">Javed A. Aslam</a></p>
<p>【Abstract】:
In this work we describe the results of a large-scale study on the effect of the distribution of labels across the different grades of relevance in the training set on the performance of trained ranking functions. In a controlled experiment we generate a large number of training datasets wih different label distributions and employ three learning to rank algo- rithms over these datasets. We investigate the effect of these distributions on the accuracy of obtained ranking functions to give an insight into the manner training sets should be constructed.</p>
<p>【Keywords】:
document selection methodologies; learning-to-rank</p>
<h3 id="192. Exploring term temporality for pseudo-relevance feedback.">192. Exploring term temporality for pseudo-relevance feedback.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010141">Paper Link</a>】    【Pages】:1245-1246</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Whiting:Stewart">Stewart Whiting</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Moshfeghi:Yashar">Yashar Moshfeghi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jose:Joemon_M=">Joemon M. Jose</a></p>
<p>【Abstract】:
As digital collections expand, the importance of the temporal aspect of information has become increasingly apparent. The aim of this paper is to investigate the effect of using long-term temporal profiles of terms in information retrieval by enhancing the term selection process of pseudo-relevance feedback (PRF). For this purpose, two temporal PRF approaches were introduced considering only temporal aspect and temporal along with textual aspect. Experiments used the AP88-89 and WSJ87-92 test collections with TREC Ad-Hoc Topics 51-100. Term temporal profiles are extracted from the Google Books n-grams dataset. The results show that the long-term temporal aspects of terms are capable of enhancing retrieval effectiveness.</p>
<p>【Keywords】:
relevance feedback; retrieval; temporal; topic</p>
<h3 id="193. MSSF: a multi-document summarization framework based on submodularity.">193. MSSF: a multi-document summarization framework based on submodularity.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010142">Paper Link</a>】    【Pages】:1247-1248</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Jingxuan">Jingxuan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Lei">Lei Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Tao">Tao Li</a></p>
<p>【Abstract】:
Multi-document summarization aims to distill the most representative information from a set of documents to generate a summary. Given a set of documents as input, most of existing multi-document summarization approaches utilize different sentence selection techniques to extract a set of sentences from the document set as the summary. The submodularity hidden in textual-unit similarity motivates us to incorporate this property into our solution to multi-document summarization tasks. In this poster, we propose a new principled and versatile framework for different multi-document summarization tasks using the submodular function [8].</p>
<p>【Keywords】:
framework; submodularity; summarization</p>
<h3 id="194. SEJoin: an optimized algorithm towards efficient approximate string searches.">194. SEJoin: an optimized algorithm towards efficient approximate string searches.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010143">Paper Link</a>】    【Pages】:1249-1250</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Junfeng">Junfeng Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ziyang">Ziyang Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Jingrong">Jingrong Zhang</a></p>
<p>【Abstract】:
We investigated the problem of finding from a collection of strings those similar to a given query string based on edit distance, for which the critical operation is merging inverted lists of grams generated from the collection of strings. We present an efficient algorithm to accelerate the merging operation.</p>
<p>【Keywords】:
approximate string search</p>
<h3 id="195. Bag-of-visual-words vs global image descriptors on two-stage multimodal retrieval.">195. Bag-of-visual-words vs global image descriptors on two-stage multimodal retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010144">Paper Link</a>】    【Pages】:1251-1252</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chatzichristofis:Savvas_A=">Savvas A. Chatzichristofis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zagoris:Konstantinos">Konstantinos Zagoris</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Arampatzis:Avi">Avi Arampatzis</a></p>
<p>【Abstract】:
The Bag-Of-Visual-Words (BOVW) paradigm is fast becoming a popular image representation for Content-Based Image Retrieval (CBIR), mainly because of its better retrieval effectiveness over global feature representations on collections with images being near-duplicate to queries. In this experimental study we demonstrate that this advantage of BOVW is diminished when visual diversity is enhanced by using a secondary modality, such as text, to pre-filter images. The TOP-SURF descriptor is evaluated against Compact Composite Descriptors on a two-stage image retrieval setup, which first uses a text modality to rank the collection and then perform CBIR only on the top-K items.</p>
<p>【Keywords】:
bag-of-visual-words; image retrieval; two-stage retrieval</p>
<h3 id="196. Query term ranking based on search results overlap.">196. Query term ranking based on search results overlap.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010145">Paper Link</a>】    【Pages】:1253-1254</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Song:Wei">Wei Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Yu">Yu Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xie:Yubin">Yubin Xie</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Ting">Ting Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Sheng">Sheng Li</a></p>
<p>【Abstract】:
In this paper, we propose a method to rank and assign weights to query terms according to their impact on the topic of the query. We use Search Result Overlap Ratio (SROR) to quantify the overlap of the search results of the full query and a shorten query after removing one term. Intuitively, if the overlap is small, it indicates a big topic shift and the removed term should be discriminative and important. The SROR could be used for measuring query term importance with a search engine automatically. By this way, learning based models could be trained based on a large number of automatically labeled instances and make predictions for future queries efficiently.</p>
<p>【Keywords】:
query reformulation; query term ranking; search results overlap</p>
<h3 id="197. Tossing coins to trim long queries.">197. Tossing coins to trim long queries.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010146">Paper Link</a>】    【Pages】:1255-1256</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Datta:Sudip">Sudip Datta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Varma:Vasudeva">Vasudeva Varma</a></p>
<p>【Abstract】:
Verbose web queries are often descriptive in nature where a term based search engine is unable to distinguish between the essential and noisy words, which can result in a drift from the user intent. We present a randomized query reduction technique that builds on an earlier learning to rank based approach. The proposed technique randomly picks only a small set of samples, instead of the exponentially many sub-queries, thus being fast enough to be useful for web search engines, while still covering wide sub-query space.</p>
<p>【Keywords】:
randomized algorithm; verbose queries</p>
<h3 id="198. A comparison of time-aware ranking methods.">198. A comparison of time-aware ranking methods.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010147">Paper Link</a>】    【Pages】:1257-1258</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kanhabua:Nattiya">Nattiya Kanhabua</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/N=oslash=rv=aring=g:Kjetil">Kjetil Nørvåg</a></p>
<p>【Abstract】:
When searching a temporal document collection, e.g., news archives or blogs, the time dimension must be explicitly incorporated into a retrieval model in order to improve relevance ranking. Previous work has followed one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, or 2) a probabilistic model generating a query from the textual and temporal part of a document independently. In this paper, we compare the effectiveness of different time-aware ranking methods by using a mixture model applied to all methods. Extensive evaluation is conducted using the New York Times Annotated Corpus, queries and relevance judgments obtained using the Amazon Mechanical Turk.</p>
<p>【Keywords】:
temporal similarity; time-aware ranking</p>
<h3 id="199. Learning for graphs with annotated edges.">199. Learning for graphs with annotated edges.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010148">Paper Link</a>】    【Pages】:1259-1260</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Fan">Fan Li</a></p>
<p>【Abstract】:
Automatic classification with graphs containing annotated edges is an interesting problem and has many potential applications. We present a risk minimization formulation that exploits the annotated edges for classification tasks. One major advantage of our approach compared to other methods is that the weight of each edge in the graph structures in our model, including both positive and negative weights, can be learned automatically from training data based on edge features. The empirical results show that our approach can lead to significantly improved classification performance compared to several baseline approaches.</p>
<p>【Keywords】:
graph regularization; webpage categorization</p>
<h3 id="200. Formulating effective questions for community-based question answering.">200. Formulating effective questions for community-based question answering.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010149">Paper Link</a>】    【Pages】:1261-1262</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Suzuki:Saori">Saori Suzuki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nakayama:Shin=ichi">Shin-ichi Nakayama</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Joho:Hideo">Hideo Joho</a></p>
<p>【Abstract】:
Community-based Question Answering (CQA) services have become a major venue for people's information seeking on the Web. However, many studies on CQA have focused on the prediction of the best answers for a given question. This paper looks into the formulation of effective questions in the context of CQA. In particular, we looked at effect of contextual factors appended to a basic question on the performance of submitted answers. This study analysed a total of 930 answers returned in response to 266 questions that were formulated by 46 participants. The results show that adding a questionnaire's personal and social attribute to the question helped improve the perceptions of answers both in information seeking questions and opinion seeking questions.</p>
<p>【Keywords】:
context; cqa; question formulation</p>
<h2 id="Demonstrations    15">Demonstrations    15</h2>
<h3 id="201. ClusteringWiki: personalized and collaborative clustering of search results.">201. ClusteringWiki: personalized and collaborative clustering of search results.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010151">Paper Link</a>】    【Pages】:1263-1264</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Anastasiu:Dragos_C=">Dragos C. Anastasiu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gao:Byron_J=">Byron J. Gao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Buttler:David">David Buttler</a></p>
<p>【Abstract】:
How to organize and present search results plays a critical role in the utility of search engines. Due to the unprecedented scale of the Web and diversity of search results, the common strategy of ranked lists has become increasingly inadequate, and clustering has been considered as a promising alternative. Clustering divides a long list of disparate search results into a few topic-coherent clusters, allowing the user to quickly locate relevant results by topic navigation. While many clustering algorithms have been proposed that innovate on the automatic clustering procedure, we introduce ClusteringWiki, the first prototype and framework for personalized clustering that allows direct user editing of clustering results. Through a Wiki interface, the user can edit and annotate the membership, structure and labels of clusters for a personalized presentation. In addition, the edits and annotations can be shared among users as a mass collaborative way of improving search result organization and search engine utility.</p>
<p>【Keywords】:
document clustering; information retrieval; mass collaboration; personalized clustering; search result clustering; search result organization; social tagging; wiki</p>
<h3 id="202. OrientSTS: spatio-temporal sequence searching in flickr.">202. OrientSTS: spatio-temporal sequence searching in flickr.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010152">Paper Link</a>】    【Pages】:1265-1266</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Chunjie">Chunjie Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Dongqi">Dongqi Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Meng:Xiaofeng">Xiaofeng Meng</a></p>
<p>【Abstract】:
Nowadays, due to the increasing user requirements of efficient and personalized services, a perfect travel plan is urgently needed. However, at present it is hard for people to make a personalized traveling plan. Most of them follow other people's general travel trajectory. So only after finishing their travel, do they know which scene is their favorite, which is not, and what is the perfect order of visits. In this research we propose a novel spatio-temporal sequence (STS) searching, which mainly includes two steps. Firstly, we propose a novel method to detect tourist features of every scene, and its difference in different seasons. Secondly, combined with personal profile and scene features, a set of interesting scenes will be chosen and each scene has a specific weight for each user. The goal of our research is to provide the traveler with the STS, which passes through as many chosen scenes as possible with the maximum weight and the minimum distance within his travel time. We propose a method based on topic model to detect scene features, and provide two approximate algorithms to mine STS: a local optimization algorithm and a global optimization algorithm. System evaluations have been conducted and the performance results show the efficiency.</p>
<p>【Keywords】:
profile; sequence; spatio-temporal</p>
<h3 id="203. A toolkit for knowledge base population.">203. A toolkit for knowledge base population.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010153">Paper Link</a>】    【Pages】:1267-1268</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zheng">Zheng Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tamang:Suzanne">Suzanne Tamang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Adam">Adam Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Ji:Heng">Heng Ji</a></p>
<p>【Abstract】:
The main goal of knowledge base population (KBP) is to distill entity information (e.g., facts of a person) from multiple unstructured and semi-structured data sources, and incorporate the information into a knowledge base (KB). In this work, we intend to release an open source KBP toolkit that is publicly available for research purposes.</p>
<p>【Keywords】:
knowledge base population; toolkit</p>
<h3 id="204. iMecho: a context-aware desktop search system.">204. iMecho: a context-aware desktop search system.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010154">Paper Link</a>】    【Pages】:1269-1270</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Jidong">Jidong Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Hang">Hang Guo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wu:Wentao">Wentao Wu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang_0009:Wei">Wei Wang</a></p>
<p>【Abstract】:
In this demo, we present iMecho, a context-aware desktop search system to help users get more relevant results. Different from other desktop search engines, iMecho ranks results not only by the content of the query, but also the context of the query. It employs an Hidden Markov Model (HMM)-based user model, which is learned from user's activity logs, to estimate the query context when he submits the query. The results from keyword search are re-ranked by their relevances to the context with acceptable overhead.</p>
<p>【Keywords】:
context-aware search; desktop search; user model</p>
<h3 id="205. Visualizing and querying semantic social networks.">205. Visualizing and querying semantic social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010155">Paper Link</a>】    【Pages】:1271-1272</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sun:Aixin">Aixin Sun</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Datta:Anwitaman">Anwitaman Datta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lim:Ee=Peng">Ee-Peng Lim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Kuiyu">Kuiyu Chang</a></p>
<p>【Abstract】:
We demonstrate SSNetViz that is developed for integrating, visualizing and querying heterogeneous semantic social networks obtained from multiple information sources. A semantic social network refers to a social network graph with multi-typed nodes and links. We demonstrate various innovative features of SSNetViz with social networks from three information sources covering a similar set of entities and relationships in terrorism domain.</p>
<p>【Keywords】:
integration; search; social network; visualization</p>
<h3 id="206. What-you-retrieve-is-what-you-see: a preliminary cyber-physical search engine.">206. What-you-retrieve-is-what-you-see: a preliminary cyber-physical search engine.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010156">Paper Link</a>】    【Pages】:1273-1274</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shou:Lidan">Lidan Shou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0005:Ke">Ke Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0001:Gang">Gang Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Chao">Chao Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Ma:Yi">Yi Ma</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xian">Xian Zhang</a></p>
<p>【Abstract】:
The cyber-physical systems (CPS) are envisioned as a class of real-time systems integrating the computing, communication and storage facilities with monitoring and control of the physical world. One interesting CPS application in the mobile Internet is to provide Web search "on the spot" regarding the physical world that a user sees, or literally WYRIWYS (What-You-Retrieve-Is-What-You-See). The objective of our work is to develop server/browser software for supporting WYRIWYS search in our prototype cyber-physical search engine. A WYRIWYS search retrieves visible Web objects and ranks them by their cyber-physical relevances (term, visual, spatial, temporal etc.). This work is distinguished from previous LWS as it provides quality Web search geared with the physical world. Therefore it suggests a very promising solution to cyber-physical Web search.</p>
<p>【Keywords】:
cyber-physical system; visibility; web search</p>
<h3 id="207. QuickView: advanced search of tweets.">207. QuickView: advanced search of tweets.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010157">Paper Link</a>】    【Pages】:1275-1276</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Xiaohua">Xiaohua Liu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Long">Long Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wei:Furu">Furu Wei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a></p>
<p>【Abstract】:
Tweets have become a comprehensive repository for real-time information. However, it is often hard for users to quickly get information they are interested in from tweets, owing to the sheer volume of tweets as well as their noisy and informal nature. We present QuickView, an NLP-based tweet search platform to tackle this issue. Specifically, it exploits a series of natural language processing technologies, such as tweet normalization, named entity recognition, semantic role labeling, sentiment analysis, tweet classification, to extract useful information, i.e., named entities, events, opinions, etc., from a large volume of tweets. Then, non-noisy tweets, together with the mined information, are indexed, on top of which two brand new scenarios are enabled, i.e., categorized browsing and advanced search, allowing users to effectively access either the tweets or fine-grained information they are interested in.</p>
<p>【Keywords】:
information extraction; tweet search</p>
<h3 id="208. Personalized video: leanback online video consumption.">208. Personalized video: leanback online video consumption.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010158">Paper Link</a>】    【Pages】:1277-1278</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ramanathan:Krishnan">Krishnan Ramanathan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sankarasubramaniam:Yogesh">Yogesh Sankarasubramaniam</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Govindaraju:Vidhya">Vidhya Govindaraju</a></p>
<p>【Abstract】:
Current user interfaces for online video consumption are mostly browser based, lean forward, require constant interaction and provide a fragmented view of the total content available. For easier consumption, the user interface and interactions need to be redesigned for less interruptive and lean back experience. In this paper, we describe Personalized Video, an application that converts the online video experience into a personalized lean back experience. It has been implemented on the Windows platform and integrated with intuitive user interactions like gesture and face recognition. It also supports group personalization for concurrent users.</p>
<p>【Keywords】:
group recommendation; personalization; ranking; similar content</p>
<h3 id="209. GreenMeter: a tool for assessing the quality and recommending tags for web 2.0 applications.">209. GreenMeter: a tool for assessing the quality and recommending tags for web 2.0 applications.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010159">Paper Link</a>】    【Pages】:1279-1280</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ricci:Saulo_M=_R=">Saulo M. R. Ricci</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guimar=atilde=es:Dilson_A=">Dilson A. Guimarães</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bel=eacute=m:Fabiano_Muniz">Fabiano Muniz Belém</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Almeida:Jussara_M=">Jussara M. Almeida</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gon=ccedil=alves:Marcos_Andr=eacute=">Marcos André Gonçalves</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Prates:Raquel_Oliveira">Raquel Oliveira Prates</a></p>
<p>【Abstract】:
We present GreenMeter, a tool for assessing the quality and recommending tags for Web 2.0 content. Its goal is to improve tag quality and the effectiveness of various information services (e.g., search, content recommendation) that rely on tags as data sources. We demonstrate an implementation of GreenMeter for the popular Last.fm application.</p>
<p>【Keywords】:
information quality; tag recommendation</p>
<h3 id="210. JuSe: a picture dictionary query system for children.">210. JuSe: a picture dictionary query system for children.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010160">Paper Link</a>】    【Pages】:1281-1282</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Polajnar:Tamara">Tamara Polajnar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Glassey:Richard">Richard Glassey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Azzopardi:Leif">Leif Azzopardi</a></p>
<p>【Abstract】:
As adults we take for granted our capacity to express our information needs verbally and textually. However, young children also have preferences and information needs, but are just learning to be able to express themselves effectively. Consequently they encounter many barriers when trying to spell, type, and communicate their needs to a 'faceless' search engine text box. Junior Search (JuSe) is an interface that enables preschoolers and young children to search and find consumable online content (such as games for kids, videos, etc.) through adaptable picture dictionaries. Inspired by educational children's toys, rather than search engines designed for adults, JuSe incorporates a learning element by combining audio-visual and textual cues to improve written word recognition and vocabulary skills. JuSe provides an interactive learning environment that allows parents to introduce new words and concepts into the child's lexicon, as well as controlling the content and search queries.</p>
<p>【Keywords】:
children; information retrieval</p>
<h3 id="211. CrowdTracker: enabling community-based real-time web monitoring.">211. CrowdTracker: enabling community-based real-time web monitoring.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010161">Paper Link</a>】    【Pages】:1283-1284</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Caverlee:James">James Caverlee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Zhiyuan">Zhiyuan Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eoff:Brian">Brian Eoff</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Chiao=Fang">Chiao-Fang Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kamath:Krishna_Yeswanth">Krishna Yeswanth Kamath</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/McGee:Jeffrey">Jeffrey McGee</a></p>
<p>【Abstract】:
CrowdTracker is a community-based web monitoring system optimized for real-time web streams like Twitter, Facebook, and Google Buzz. In this demo summary, we provide an overview of the system and architecture, and outline the demonstration plan.</p>
<p>【Keywords】:
community; crowd; monitoring; real-time web</p>
<h3 id="212. The Meta-Dex Suite: generating and analyzing indexes and meta-indexes.">212. The Meta-Dex Suite: generating and analyzing indexes and meta-indexes.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010162">Paper Link</a>】    【Pages】:1285-1286</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Huggett:Michael">Michael Huggett</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rasmussen:Edie">Edie Rasmussen</a></p>
<p>【Abstract】:
Our Meta-dex software suite extracts content and index text from a corpus of PDF files, and generates a meta-index that references entries across an entire domain. We provide tools to analyze the individual and integrated indexes, and visualize entries and books within the meta-index. The suite is scalable to very large data sets.</p>
<p>【Keywords】:
digital books; digital collections; indexes; meta-indexes; user interfaces; visualization</p>
<h3 id="213. Tulsa: web search for writing assistance.">213. Tulsa: web search for writing assistance.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010163">Paper Link</a>】    【Pages】:1287-1288</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Ding:Duo">Duo Ding</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Xingping">Xingping Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Scott:Matthew_R=">Matthew R. Scott</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhou:Ming">Ming Zhou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
natural language processing; web search; writing assistance</p>
<h3 id="214. The TREC files: the (ground) truth is out there.">214. The TREC files: the (ground) truth is out there.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010164">Paper Link</a>】    【Pages】:1289-1290</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chatzichristofis:Savvas_A=">Savvas A. Chatzichristofis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zagoris:Konstantinos">Konstantinos Zagoris</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Arampatzis:Avi">Avi Arampatzis</a></p>
<p>【Abstract】:
Traditional tools for information retrieval (IR) evaluation, such as TREC's trec_eval, have outdated command-line interfaces with many unused features, or 'switches', accumulated over the years. They are usually seen as cumbersome applications by new IR researchers, steepening the learning curve. We introduce a platform-independent application for IR evaluation with a graphical easy-to-use interface: the TREC_Files Evaluator. The application supports most of the standard measures used for evaluation in TREC, CLEF, and elsewhere, such as MAP, P10, P20, and bpref, as well as the Averaged Normalized Modified Retrieval Rank (ANMRR) proposed by MPEG for image retrieval evaluation. Additional features include a batch mode and statistical significance testing of the results against a pre-selected baseline.</p>
<p>【Keywords】:
evaluation; trec files</p>
<h3 id="215. A tool for comparative IR evaluation on component level.">215. A tool for comparative IR evaluation on component level.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010165">Paper Link</a>】    【Pages】:1291-1292</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wilhelm:Thomas">Thomas Wilhelm</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/K=uuml=rsten:Jens">Jens Kürsten</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eibl:Maximilian">Maximilian Eibl</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
component-level evaluation</p>
<h2 id="Tutorials    7">Tutorials    7</h2>
<h3 id="216. Machine learning for information retrieval.">216. Machine learning for information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010167">Paper Link</a>】    【Pages】:1293-1294</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Si:Luo">Luo Si</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jin:Rong">Rong Jin</a></p>
<p>【Abstract】:
In recent years, we have witnessed successful application of machine learning techniques to a wide range of information retrieval problems, including Web search engines, recommendation systems, online advertising, etc. It is thus critical for researchers in the information retrieval community to understand the core machine learning techniques. In order to accommodate audiences with different levels of understanding of machine learning, we divide this tutorial into two sessions: the first session will focus on basic machine learning concepts and tools; in the second session, we will introduce more advanced topics in machine learning, and will present recent developments in machine learning and its application to information retrieval. Each season is self-contained. Session 1: Core Learning Technologies for Information Retrieval. This session of the tutorial will cover the core machine learning methods, basic optimization techniques and key information retrieval applications. In particular, it includes: 1). Core concepts in machine learning, such as supervised learning/unsupervised learning, bias and variance trade off, and probabilistic models; 2). Useful concepts and algorithms in optimization including the first and second order gradient methods, and Expectation and Maximization; 3). The application of machine learning methods to key information retrieval problems including text classification, collaborative filtering, clustering and learning to rank; Session 2: Emerging Learning Technologies for Information Retrieval. This session will cover more advanced machine learning techniques that have started to be utilized in information retrieval applications. In particular, it will cover: 1). Advanced Optimization Techniques including stochastic optimization and smooth minimization; 2). Emerging Learning Techniques such as Multiple-Instance Learning, Active Learning and Semi-supervised Learning. The tutorial will benefit a large body of audience in the information retrieval community, ranging from students who are new to machine learning to the seasoned researchers who would like to understand the recent advance in machine learning for information retrieval research. This tutorial will also benefit the practitioners who apply learning techniques to real-world information retrieval systems.</p>
<p>【Keywords】:
information retrieval application; machine learning; optimization; probabilistic methods</p>
<h3 id="217. Enhancing web search by mining search and browse logs.">217. Enhancing web search by mining search and browse logs.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010168">Paper Link</a>】    【Pages】:1295-1296</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Daxin">Daxin Jiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pei:Jian">Jian Pei</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a></p>
<p>【Abstract】:
Huge amounts of search log data have been accumulated in various search engines. Currently, a commercial search engine receives billions of queries and collects tera-bytes of log data on any single day. Other than search log data, browse logs can be collected by client-side browser plug-ins, which record the browse information if users' permissions are granted. Such massive amounts of search/browse log data, on the one hand, provide great opportunities to mine the wisdom of crowds and improve web search results. On the other hand, designing effective and efficient methods to clean, model, and process large scale log data also presents great challenges. In this tutorial, we will focus on mining search and browse log data for search engines. We will start with an introduction of search and browse log data and an overview of frequently-used data summarization in log mining. We will then elaborate how log mining applications enhance the five major components of a search engine, namely, query understanding, document understanding, query-document matching, user understanding, and monitoring and feedbacks. For each aspect, we will survey the major tasks, fundamental principles, and state-of-the-art methods. Finally, we will discuss the challenges and future trends of log data mining. The goal of this tutorial is to provide a systematic survey on large-scale search/browse log mining to the IR community. It may help IR researchers to get familiar with the core challenges and promising directions in log mining. At the same time, this tutorial may also serve the developers of web information retrieval systems as a comprehensive and in-depth reference to the advanced log mining techniques.</p>
<p>【Keywords】:
log mining; search and browse logs; search applications</p>
<h3 id="218. A new look at old tricks: the fertile roots of current research.">218. A new look at old tricks: the fertile roots of current research.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010169">Paper Link</a>】    【Pages】:1297-1298</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kantor:Paul_B=">Paul B. Kantor</a></p>
<p>【Abstract】:
As we face an explosion of potential new applications for the fundamental concepts and technologies of information retrieval, ranging from ad ranking to social media, from collaborative recommending to question answering systems, many researchers are spending unnecessary time reinventing ideas and relationships that are buried in the prehistory of information retrieval (which, for many researchers, means anything published before they entered graduate school). A lot of the ideas that surface as "new" in today's super-heated research environment have very firm roots in earlier developments in fields as diverse as citation analysis and pattern recognition. The purpose of this tutorial is to survey those roots, and their relation to the contemporary fruits on the tree of information retrieval, and to separate, as much as is possible in an era of increasing secrecy about methods, the problems to be solved, the algorithms for solving them, and the heuristics that are the bread and butter of a working operation. Participants will become familiar with roots in Pattern Analysis, Statistics, Information Science and other sources of key ideas that reappear in the current development of Information Retrieval as it applies to Search Engines, Social Media, and Collaborative Systems. They will be able to separate problems from algorithms, and algorithms from heuristics, in the application of these ideas to their own research and/or development activities. Course materials will be made available on a Web site two weeks prior to the tutorial. They will include links to relevant software; links to publications that will be discussed; and mechanisms for chat among the tutorial participants, before, during and after the tutorial.</p>
<p>【Keywords】:
foundations of information retrieval</p>
<h3 id="219. Crowdsourcing for information retrieval: principles, methods, and applications.">219. Crowdsourcing for information retrieval: principles, methods, and applications.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010170">Paper Link</a>】    【Pages】:1299-1300</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Alonso:Omar">Omar Alonso</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lease:Matthew">Matthew Lease</a></p>
<p>【Abstract】:
Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today's digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained particular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. We will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide a basic foundation to begin crowdsourcing in the context of one's own particular tasks</p>
<p>【Keywords】:
crowdsourcing; human computation</p>
<h3 id="220. Practical online retrieval evaluation.">220. Practical online retrieval evaluation.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010171">Paper Link</a>】    【Pages】:1301-1302</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Radlinski:Filip">Filip Radlinski</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yue:Yisong">Yisong Yue</a></p>
<p>【Abstract】:
Online evaluation is amongst the few evaluation techniques available to the information retrieval community that is guaranteed to reflect how users actually respond to improvements developed by the community. Broadly speaking, online evaluation refers to any evaluation of retrieval quality conducted while observing user behavior in a natural context. However, it is rarely employed outside of large commercial search engines due primarily to a perception that it is impractical at small scales. The goal of this tutorial is to familiarize information retrieval researchers with state-of-the-art techniques in evaluating information retrieval systems based on natural user clicking behavior, as well as to show how such methods can be practically deployed. In particular, our focus will be on demonstrating how the Interleaving approach and other click based techniques contrast with traditional offline evaluation, and how these online methods can be effectively used in academic-scale research. In addition to lecture notes, we will also provide sample software and code walk-throughs to showcase the ease with which Interleaving and other click-based methods can be employed by students, academics and other researchers.</p>
<p>【Keywords】:
clickthrough data; interleaving; online evaluation; preference judgments; web search</p>
<h3 id="221. Web retrieval: the role of users.">221. Web retrieval: the role of users.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010172">Paper Link</a>】    【Pages】:1303-1304</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Baeza=Yates:Ricardo_A=">Ricardo A. Baeza-Yates</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Maarek:Yoelle">Yoelle Maarek</a></p>
<p>【Abstract】:
Web retrieval methods have evolved through three major steps in the last decade or so. They started from standard document-centric IR in the early days of the Web, then made a major step forward by leveraging the structure of the Web, using link analysis techniques in both crawling and ranking challenges. A more recent, no less important but maybe more discrete step forward, has been to enter the user in this equation in two ways: (1) Implicitly, through the analysis of usage data captured by query logs, and session and click information in general; the goal here being to improve ranking as well as to measure user's happiness and engagement; (2) Explicitly, by offering novel interactive features; the goal here being to better answer users' needs. This half day tutorial covers the user-related challenges associated with the implicit and explicit role of users in Web retrieval. More specifically, we review and discuss challenges associated with: (1) Usage data analysis and metrics - It is critical to monitor how users take advantage and interact with Web retrieval systems, as this implicit relevant feedback aggregated at a large scale, can provide insights on users' underlying intent as well as approximate quite accurately the level of success of a given feature. Here we have to consider not only clicks statistics, the sequences of queries, the time spent in a page, the number of actions per session, etc. This is the focus of the first part of the tutorial. (2) User interaction - Given the intrinsic problems posed by the Web, the key challenge for the user is to conceive a good query to be submitted to the search system, one that leads to a manageable and relevant answer. The retrieval system must assist users during two key stages of interaction: efore the query is fully expressed and after the results are returned. After quite some stagnation on the front-end of Web retrieval, we have seen numerous novel interactive features appear in the last 3 to 4 years, as the leading commercial search engines seem to compete for users' attention. The second part of the tutorial will be dedicated to explicit user interaction. We will introduce novel material (as compared to previous versions of this tutorial that were given at SIGIR'2010, WSDM'2011 and ECIR'2011) in order to reflect recent Web search features such as Google Instant or Yahoo! Direct Search. The goal of this tutorial is to teach the key principles and technologies behind the activities and challenges briefly outlined above, bring new understanding and insights to the attendees, and hopefully foster future research. A previous version of this tutorial was offered at the ACM SIGIR'2010, WSDM'2011 and ECIR'2011.</p>
<p>【Keywords】:
user interaction; web retrieval</p>
<h3 id="222. Information organization and retrieval with collaboratively generated content.">222. Information organization and retrieval with collaboratively generated content.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010173">Paper Link</a>】    【Pages】:1307-1308</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Agichtein:Eugene">Eugene Agichtein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gabrilovich:Evgeniy">Evgeniy Gabrilovich</a></p>
<p>【Abstract】:
Proliferation of ubiquitous access to the Internet enables millions of Web users to collaborate online on a variety of activities. Many of these activities result in the construction of large repositories of knowledge, either as their primary aim (e.g., Wikipedia) or as a by-product (e.g., Yahoo! Answers). In this tutorial, we will discuss organizing and exploiting Collaboratively Generated Content (CGC) for information organization and retrieval. Specifically, we intend to cover two complementary areas of the problem: (1) using such content as a powerful enabling resource for knowledge-enriched, intelligent representations and new information retrieval algorithms, and (2) development of supporting technologies for extracting, filtering, and organizing collaboratively created content. The unprecedented amounts of information in CGC enable new, knowledge-rich approaches to information access, which are significantly more powerful than the conventional word-based methods. Considerable progress has been made in this direction over the last few years. Examples include explicit manipulation of human-defined concepts and their use to augment the bag of words (cf. Explicit Semantic Analysis), using large-scale taxonomies of topics from Wikipedia or the Open Directory Project to construct additional class-based features, or using Wikipedia for better word sense disambiguation. However, the quality and comprehensiveness of collaboratively created content vary widely, and in order for this resource to be useful, a significant amount of preprocessing, filtering, and organization is necessary. Consequently, new methods for analyzing CGC and corresponding user interactions are required to effectively harness the resulting knowledge. Thus, not only the content repositories can be used to improve IR methods, but the reverse pollination is also possible, as better information extraction methods can be used for automatically collecting more knowledge, or verifying the contributed content. This natural connection between modeling the generation process of CGC and effectively using the accumulated knowledge suggests covering both areas together in a single tutorial. The intended audience of the tutorial includes IR researchers and graduate students, who would like to learn about the recent advances and research opportunities in working with collaboratively generated content. The emphasis of the tutorial is on comparing the existing approaches and presenting practical techniques that IR practitioners can use in their research. We also cover open research challenges, as well as survey available resources (software tools and data) for getting started in this research field.</p>
<p>【Keywords】:
collaboratively generated content</p>
<h2 id="Doctoral consortium    11">Doctoral consortium    11</h2>
<h3 id="223. Persistence in the ephemeral: utilizing repeat behaviors for multi-session personalized search.">223. Persistence in the ephemeral: utilizing repeat behaviors for multi-session personalized search.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010175">Paper Link</a>】    【Pages】:1311-1312</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tyler:Sarah_K=">Sarah K. Tyler</a></p>
<p>【Abstract】:
As the abundance of information on the Internet grows, an increasing burden is placed on the user to specify his or her query precisely in order to avoid extraneous results that may be relevant, but not useful. At the same time, users have a tendency to repeat their search behaviors, seeking the same URL (re-finding) as well as issuing the same query (re-searching). These repeated actions reveal a form of user preference that the search engine can utilize to personalize the results. In our approach, we personalize search results related to ongoing tasks, allowing for a different degree of strength of interest, and diversity of interest per task. We focus on high valued queries; queries that are both related to past queries and will be related to future queries given the ongoing nature of the task.</p>
<p>【Keywords】:
query log analysis; re-finding; re-search; task oriented search</p>
<h3 id="224. Search engines that learn online.">224. Search engines that learn online.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010176">Paper Link</a>】    【Pages】:1313-1314</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hofmann:Katja">Katja Hofmann</a></p>
<p>【Abstract】:
The goal of my research is to develop self-learning search engines, that can learn online, i.e., directly from interactions with actual users. Such systems can continuously adapt to user preferences throughout their lifetime, leading to better search performance in settings where expensive manual tuning is infeasible. Challenges that are addressed in my work include the development of effective online learning to rank algorithms for IR, user aspects, and evaluation.</p>
<p>【Keywords】:
evaluation; implicit feedback; online learning to rank</p>
<h3 id="225. Query expansion based on a semantic graph model.">225. Query expansion based on a semantic graph model.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010177">Paper Link</a>】    【Pages】:1315-1316</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jiang:Xue">Xue Jiang</a></p>
<p>【Abstract】:
Query expansion is a classical topic in the field of information retrieval, which is proposed to bridge the gap between searchers' information intents and their queries. Previous researches usually expand queries based on document collections, or some external resources such as WordNet and Wikipedia [1, 2, 3, 4, 5]. However, it seems that independently using one of these resources has some defects, document collections lack semantic information of words, while WordNet and Wikipedia may not include domain-specific knowledge in certain document collection. Our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific information but also semantic information. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query. The first layer of the word graph contains terms in related documents, while the second layer contains semantic senses corresponding to these terms. These terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships, such as mutual information and semantic similarities. We utilized mutual information, semantic similarity and uniform distribution as the weight of term-term relation, sense-sense relation and word-sense relation respectively. Though these experiments show that our expansion outperform original queries, we are troubled with some difficult problems. Given the framework of semantic graph model, we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses. We utilized a two-layer graph model in our preliminary research, where terms from different documents are treated equally. Maybe we can introduce the document as a third layer in future work, where we can differ the same terms in different documents according to document relevance and context. Then we need appropriately represent initial weights of this words, senses and relationships. Various measures for weights of terms and term relations have been proved effective in other information retrieval tasks, such as TFIDF, mutual information (MI), but there is little research on weights for semantic senses and their relations. For polysemous words, we add all of their semantic senses to the graph and assume that these senses are uniformly distributed. Actually, it is not precise for a word in a special document and query. As we know, a polysemous word may have only one or two senses in a document, and they are not uniformly distributed. Give a word, what we should do is to determine its word senses in a relevant document and estimate the distribution of these senses. Word sense disambiguation may help us in this problem. Then, there are many methods to compute word similarity according to WordNet, which we use to represent the weights of relationships between word senses. Varelas et al implemented some popular methods to compute semantic similarity by mapping terms to an ontology and examining their relationships in that ontology [4]. We also need to know which algorithm for semantic similarity is most suitable for our model. Additional, WordNet is suitable to calculate word similarity but not suitable to measure word relevance. The inner hyperlinks of Wikipedia could help us to calculate word relevance. We wish to find an effective way to combine the similarity measure from WordNet and relevance measure from Wikipedia, which may completely reflect word relationships.</p>
<p>【Keywords】:
query expansion; semantic network; word graph</p>
<h3 id="226. Descriptive modelling of text classification and its integration with other IR tasks.">226. Descriptive modelling of text classification and its integration with other IR tasks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010178">Paper Link</a>】    【Pages】:1317-1318</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Martinez=Alvarez:Miguel">Miguel Martinez-Alvarez</a></p>
<p>【Abstract】:
Nowadays, Information Retrieval (IR) systems have to deal with multiple sources of data available in different formats. Datasets can consist of complex and heterogeneous objects with relationships between them. In addition, information needs can vary wildly and they can include different tasks. As a result, the importance of flexibility in IR systems is rapidly growing. This fact is specially important in environments where the information required at different moments is very different and its utility may be contingent on timely implementation. In these cases, how quickly a new problem is solved is as important as how well you solve it. Current systems are usually developed for specific cases. It implies that too much engineering effort is needed to adapt them when new knowledge appears or there are changes in the requirements. Furthermore, heterogeneous and linked data present greater challenges, as well as the simultaneous application of different tasks. This research proposes the usage of descriptive approaches for three different purposes: the modelling of the specific task of Text Classification (TC), focusing on knowledge and complex data exploitation; the flexible application of models to different tasks; and the simultaneously application of different IR-tasks. This investigation will contribute to the long-term goal of achieving a descriptive and composable IR technology that provides a modular framework that knowledge engineers can compose into a task-specific solution. The ultimate goal is to develop a flexible framework that offers classifiers, retrieval models, information extractors, and other functions. In addition, those functional blocks could be customised to satisfy user needs. Descriptive approaches allow a high-level definition of algorithms which are, in some cases, as compact as mathematical formulations. One of the expected benefits is to make the implementation clearer and the knowledge transfer easier. They allow models from different tasks to be defined as modules that can be "concatenated", processing the information as a pipeline where some of the outputs of one module are the input of the following one. This combination involves minimum engineering effort due to the paradigm's "Plug &amp; Play" capabilities offered by its functional syntax. This solution provides the flexibility needed to customise and quickly combine different IR-tasks and/or models. Classification is a desired candidate for being part of a flexible IR framework because it can be required in several situations for different purposes. In particular, descriptive approaches will improve its modelling with complex and heterogeneous objects. Furthermore, we aim to show how this approach allows to apply TC models for ad-hoc retrieval (and vice versa) and their simultaneous application for complex information needs. The main hypothesis of this research is that a seamless approach for modelling TC and its integration with other IR-tasks will provide a general framework for rapid prototyping and modelling of solutions for specific users. In addition, it will allow new complex models that take into account relationships and inference from large ontologies. The importance of flexibility for information systems and the exploitation of complex information and knowledge from heterogeneous sources are the main points for discussion. The main challenges are expressiveness and scalability. Abstraction improves flexibility and maintainability. However, it limits the modelling power. A balance between abstraction and expressiveness has to be reached. On the other hand, scalability has been traditionally a challenge for descriptive modelling. Our goal is to prove the feasibility of our approach for real-scale environments.</p>
<p>【Keywords】:
descriptive approach; modelling; text classification</p>
<h3 id="227. Efficient and effective solutions for search engines.">227. Efficient and effective solutions for search engines.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010179">Paper Link</a>】    【Pages】:1319-1320</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jia:Xiang=Fei">Xiang-Fei Jia</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
efficiency; impact-ordering; pruning</p>
<h3 id="228. Modeling document scores for distributed information retrieval.">228. Modeling document scores for distributed information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010180">Paper Link</a>】    【Pages】:1321-1322</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Markov:Ilya">Ilya Markov</a></p>
<p>【Abstract】:
Distributed Information Retrieval (DIR), also known as Federated Search, integrates multiple searchable collections and provides direct access to them through a unified interface [3]. This is done by a centralized broker, that receives user queries, forwards them to appropriate collections and returns merged results to users. In practice, most of federated resources do not cooperate with a broker and do not provide neither their content nor the statistics used for retrieval. This is known as uncooperative DIR. In this case a broker creates a resource representation by sending sample queries to a collection and analyzing retrieved documents. This process is called query-based sampling. The key issue here is the following: 1.1 How many documents have to be retrieved from a resource in order to obtain a representative sample? Although there have been a number of attempts to address this issue it is still not solved appropriately. For a given user query resources are ranked according to their similarity to the query or based on the number of relevant documents they contain. Since resource representations are usually incomplete, the similarity or the number of relevant documents cannot be calculated precisely. Resource selection algorithms proposed in the literature estimate these numbers based on incomplete samples. However these estimates are subjects to error. In practice, inaccurate estimates that have high error should be trusted less then the more accurate estimates with low error. Unfortunately none of the existing algorithms can make the calculation of the estimation errors possible. Therefore the following questions arise: 2.1 How to estimate resource scores so that the estimation errors can be calculated? 2.2 How to use these errors in order to improve the resource selection performance? Existing results merging algorithms estimate normalized document scores based on scores of documents that appear both in a sample and in a result list. The problem similar to the resource selection one arises. The normalized document scores are only the estimates and are subjects to error. Inaccurate estimates should be trusted less then the more accurate ones. Again none of the existing algorithms provide a way for calculating these errors. Thus the two question to be address on the results merging phase are similar to the resource selection ones: 3.1 How to estimate normalized document scores so that the estimation errors can be calculated? 3.2 How to use these errors in order to improve the results merging performance? In this work we address the above issues by applying score distribution models (SDM) to different phases of DIR [2]. In particular, we discuss the SDM-based resource selection technique that allows the calculation of resource score estimation errors and can be extended in order to calculate the number of documents to be sampled from each resource for a given query. We have performed initial experiments comparing the SDM-based resource selection technique to the state-of-the-art algorithms and we are currently experimenting with the SDM-based results merging method. We plan to apply the existing score normalization techniques from meta-search to the DIR results merging problem [1]. However, the SDM-based results merging approaches require the relevance scores to be returned together with retrieved documents. It is not yet clear how to relax this strong assumption that does not always hold in practice.</p>
<p>【Keywords】:
distributed information retrieval; federated search; score distribution models</p>
<h3 id="229. Improving query and result list adaptation in personalized multilingual information retrieval.">229. Improving query and result list adaptation in personalized multilingual information retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010181">Paper Link</a>】    【Pages】:1323-1324</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ghorab:M=_Rami">M. Rami Ghorab</a></p>
<p>【Abstract】:
A general characteristic of Information Retrieval (IR) and Multilingual IR (MIR) [5] systems is that if the same query was submitted by different users, the system would yield the same results, regardless of the user. On the other hand, Adaptive Hypermedia (AH) systems operate in a personalized manner where the services are adapted to the user [1]. Personalized IR (PIR) is motivated by the success in both areas, IR and AH [4]. IR systems have the advantage of scalability and AH systems have the advantage of satisfying individual user needs. The majority of studies in PIR literature have focused on monolingual IR, and relatively little work has been done concerning multilingual IR. This PhD research study aims to improve personalization in MIR systems, by improving the relevance of multilingual search results with respect to the user and not just the query. The study investigates how to model different aspects of a multilingual search user. Information about users can be demographic information, such as language and country, or information about the user's search interests. This information can be gathered explicitly by asking the user to supply the required information or implicitly by inferring the information from the user's search history. The study will then investigate how to exploit the modeled user information to personalize the user's multilingual search by performing query and result list adaptation. The main research questions that are addressed in this study are: how to improve the relevance of search results with respect to individual users in PMIR and how to construct profiles that represent aspects and interests of a multilingual search user. So far, the work carried out for this study included: (1) a proposed framework for the delivery and evaluation of PMIR [3]; and (2) exploratory experiments with search history and collection (result) re-ranking on a dataset of multilingual search logs [2]. The next stage of experimentation will involve the investigation and development of algorithms for: (1) constructing multilingual user profiles; (2) pre-translation and post-translation query expansion based on terms from the user profile; and (3) result list re-ranking based on the user's interests, and preferred language. Two types of experiments will be conducted in an in-lab setting, with a group of users from different linguistic backgrounds. In the first set of experiments, users will be asked to use a baseline web search system for their daily search activities over a period of time. The baseline system will be wrapped around one of the major search engines. Interactions with the system will be logged, and part of this information will be used for training the system (constructing user profiles from text of queries and clicked documents); the other part (remaining queries) will be used for testing the effectiveness of the query adaptation and result list adaptation algorithms, where the users will be asked to provide some personal relevance judgements. In the second set of experiments, the users will be asked to use the PMIR system to fulfill a number of defined search tasks. Quantitative and qualitative techniques will be used to evaluate different aspects of the experiments, including: (1) retrieval effectiveness, which can be measured using standard IR metrics; (2) user's performance on search tasks, which can be measured in terms of time and number of actions needed to fulfill the tasks; (3) user profile accuracy, which can be assessed by questionnaires that indicate how well the user profile depicted the users' search interests; and (4) usability and user satisfaction, which can be assessed using standard system usability questionnaires.</p>
<p>【Keywords】:
personalized multilingual information retrieval; user modeling</p>
<h3 id="230. Using k-Top retrieved web snippets to date temporalimplicit queries based on web content analysis.">230. Using k-Top retrieved web snippets to date temporalimplicit queries based on web content analysis.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010182">Paper Link</a>】    【Pages】:1325-1326</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Campos:Ricardo_Nuno_Taborda">Ricardo Nuno Taborda Campos</a></p>
<p>【Abstract】:
The World Wide Web (WWW) is a huge information network from which retrieving and organizing quality relevant content remains an open question for mostly all ambiguous queries. As an example, many queries have temporal implicit intents associated with them but they are not inferred by search engines. Inferring the user intentions and the period he has in mind, may therefore play an extremely important role in the improvement of the results. Our work goes in this direction. We aim to introduce a temporal analysis framework for analyzing documents in a temporal dimension in order to identify and understand the temporal nature of any given query, namely implicit ones. Our analysis is not based on metadata, but on the exploitation of temporal information from the content itself, particularly within web snippets, which are interesting pieces of concentrated information, where time clues, especially years, often appear. Our intention is to develop a language-independent solution and to model the degree of relationship between the terms and dates identified. This is the core part of the framework and the basis for both temporal query understanding and search results exploration, such as temporal clustering. We believe that inferring this knowledge is a very important step in the process of adding a temporal dimension to IR systems, thus disambiguating a large class of queries for which search engines continue to fail.</p>
<p>【Keywords】:
implicit temporal queries; temporal information retrieval</p>
<h3 id="231. Domain-specific information retrieval using rcommenders.">231. Domain-specific information retrieval using rcommenders.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010183">Paper Link</a>】    【Pages】:1327-1328</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Wei">Wei Li</a></p>
<p>【Abstract】:
The continuing increase in the volume of information available in our daily lives is creating ever greater challenges for people to find personally useful information. One approach used to addressing this problem is Personalized Information Retrieval (PIR). PIR systems collect a user's personal information from both implicit and explicit sources to build a user profile with the objective of giving retrieval results which better meet their individual user information needs than a standard Information Retrieval (IR) system. However, in many situations there may be no opportunity to learn about the specific interests of a user and build a personal model when this user is querying on a new topic, e.g. when a user visits a museum or exhibition which is unrelated to their normal search interests. Under this condition, the experiences and behaviours of other previous users, who have made similar queries, could be used to build a model of user behavior in this domain. My PhD proposes to focus on the development of new and innovative methods of domain-specific IR. My work seeks to combine recommender algorithms trained using previous search behaviours from different searchers with a standard ranked IR method to form a domain-specific IR model to improve the search effectiveness for a user entering a query without personal prior search history on this topic. The challenges for my work are: how to provide users better results; how to train and evaluate the methods proposed in my work.</p>
<p>【Keywords】:
domain-specific information retrieval; recommenders</p>
<h3 id="232. Understanding and using contextual information in recommender systems.">232. Understanding and using contextual information in recommender systems.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010184">Paper Link</a>】    【Pages】:1329-1330</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Licai">Licai Wang</a></p>
<p>【Abstract】:</p>
<p>【Keywords】:
context; hosvd; mood; recommender systems; social network.; user preferences</p>
<h3 id="233. Multidimensional search result diversification: diverse search results for diverse users.">233. Multidimensional search result diversification: diverse search results for diverse users.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010185">Paper Link</a>】    【Pages】:1331-1332</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bhatia:Sumit">Sumit Bhatia</a></p>
<p>【Abstract】:
Hundreds of millions of people today rely on Web based Search Engines to satisfy their information needs. In order to meet the expectations of this vast and diverse user population, the search engine should present a list of results such that the probability of satisfying the average user is maximized. This leads us to the problem of Search Result Diversification. Given a user submitted query, the search engine should include results that are relevant to the user query and at the same time, diverse enough to meet the expectations of diverse user populations. However, it is not clear in what respect the results should be diversified. Much of the current work in diversity focuses on ambiguous and underspecified queries and tries to include results corresponding to diverse interpretations of the ambiguous query. This is not always sufficient. My analysis of a commercial web search engine's logs reveals that even for well-specified informational queries, click entropy is very high indicating that different users prefer different types of documents. Very recently, a diversification algorithm fine-tuned for such informational queries has been proposed. Further, high click entropies were also observed for a large fraction of transactional queries. One major goal of my PhD thesis will then be to identify the various possible dimensions along which the search results can be diversified. Having such an information will enhance our understanding about the expectations of an average user from the search engine. By utilizing aggregate statistics about queries, users and their interaction with the search engine for different queries, more concrete evidences about diverse user preferences as well as relative importance of different diversity dimensions can be derived. Once we know different diversity dimensions, the next natural question is: given a query, how can we determine the diversification requirement best suited for the query? For some queries sub-topic coverage may be more important while for others diversification with respect to document source or stylistics might be important. This problem is related to the problem of selective diversification where the goal is to identify queries for which diversification techniques should be used. However, in addition, we are also interested in identifying different diversity classes a given query belongs to. Further, for some queries it may be required to diversify along multiple diversity dimensions. In such cases, it is also important to determine the relative importance of different diversity dimensions for the given query. By utilizing past user interaction data, query level features (like query clarity, entropy, lexical features etc.) and document level features (e.g. popularity, content quality, previous click history etc.), classifiers for diversification requirements can be developed. Given a user query, once we know the type of diversity requirements for the user, an appropriate diversification technique is required. I would like to study the problem of simultaneously diversifying search results along multiple dimensions, as discussed above. One possible way here could be to build upon the nugget based framework introduced by Clarke et al. where we represent each document as a set of nuggets, each nugget corresponding to a diversity dimension.</p>
<p>【Keywords】:
diversity; query log analysis.; search result diversification</p>
<h2 id="Industrial track    5">Industrial track    5</h2>
<h3 id="234. Sensor-aided mobile information management and retrieval.">234. Sensor-aided mobile information management and retrieval.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010187">Paper Link</a>】    【Pages】:1333-1334</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chang:Edward_Y=">Edward Y. Chang</a></p>
<p>【Abstract】:
The number of "smart" mobile devices such as wireless phones and tablet computers has been rapidly growing. These mobile devices are equipped with a variety of sensors such as camera, gyroscope, accelerometer, compass, NFC, WiFi, GPS, etc. These sensors can be used to capture images and voice, detect motion patterns, and predict locations, to name just a few. This keynote depicts techniques in configuration, calibration, computation, and fusion for improving sensor performance and conserving power consumption. We also present novel mobile information management and retrieval applications that can benefit a great deal from enhanced sensor technologies.</p>
<p>【Keywords】:
inertial navigation system; location; mobile; sensor.</p>
<h3 id="235. Predicting eBay listing conversion.">235. Predicting eBay listing conversion.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010188">Paper Link</a>】    【Pages】:1335-1336</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Ted_Tao">Ted Tao Yuan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Zhaohui">Zhaohui Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mathieson:Mike">Mike Mathieson</a></p>
<p>【Abstract】:
At eBay Market Place, listing conversion rate can be measured by number of items sold divided by number of items in a sample set. For a given item, conversion rate can also be treated as the probability of sale. By investigating eBay listings' transactional patterns, as well as item attributes and user click-through data, we developed conversion models that allow us to predict a live listing's probability of sale. In this paper, we discuss the design and implementation of such conversion models. These models are highly valuable in analysis of inventory quality and ranking. Our work reveals the uniqueness of sales-oriented search at eBay and its similarity to general web search problems.</p>
<p>【Keywords】:
ecommerce; online auction; predictive model; ranking.</p>
<h3 id="236. A large scale machine learning system for recommending heterogeneous content in social networks.">236. A large scale machine learning system for recommending heterogeneous content in social networks.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010189">Paper Link</a>】    【Pages】:1337-1338</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shi:Yanxin">Yanxin Shi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:David">David Ye</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Goder:Andrey">Andrey Goder</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Narayanan:Srinivas">Srinivas Narayanan</a></p>
<p>【Abstract】:
The goal of the Facebook recommendation engine is to compare and rank heterogeneous types of content in order to find the most relevant recommendations based on user preference and page context. The challenges for such a recommendation engine include several aspects: 1) the online queries being processed are at very large scale; 2) with new content types and new user-generated content constantly added to the system, the candidate object set and underlying data distribution change rapidly; 3) different types of content usually have very distinct characteristics, which makes generic feature engineering difficult; and 4) unlike a search engine that can capture intention of users based on their search queries, our recommendation engine needs to focus more on users' profile and interests, past behaviors and current actions in order to infer their cognitive states. In this presentation, we would like to introduce an effective, scalable, online machine learning framework we developed in order to address the aforementioned challenges. We also want to discuss the insights, approaches and experiences we have accumulated during our research and development process.</p>
<p>【Keywords】:
ctr prediction; large scale system; online learning</p>
<h3 id="237. Review of MSR-Bing web scale speller challenge.">237. Review of MSR-Bing web scale speller challenge.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010190">Paper Link</a>】    【Pages】:1339-1340</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Kuansan">Kuansan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pedersen:Jan">Jan Pedersen</a></p>
<p>【Abstract】:
In this paper, we provide an overview of the MSR-Bing Web Scale Speller Challenge of 2011. We describe the motivation and outline the algorithmic and engineering challenges posed by this activity. The design and the evaluation methods are also reviewed, and the online resources that will remain publicly available to the community are also described. The Challenge will culminate in a workshop after the time of the writing where the top prize winners will publish their approaches. The main findings and the lessons learned will be summarized and shared in the Industry Track presentation accompanying this paper.</p>
<p>【Keywords】:
language model; msr-bing speller challenge; spelling alteration; web search</p>
<h3 id="238. Elsevier SIGIR 2011 application challenge abstract.">238. Elsevier SIGIR 2011 application challenge abstract.</h3>
<p>【<a href="http://doi.acm.org/10.1145/2009916.2010191">Paper Link</a>】    【Pages】:1341-1342</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/V=auml=lim=auml=ki:Jukka">Jukka Välimäki</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caprio:Remko">Remko Caprio</a></p>
<p>【Abstract】:
Elsevier SIGIR 2011 Application Challenge is an international competition that encourages software developers to create applications that run on Elsevier's SciVerse platform. The Challenge is open to all SIGIR 2011 Conference participants.</p>
<p>【Keywords】:
api; elsevier; sciverse; search of scientific articles</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>


<div class="theme" >
<i title='主题' onclick="setStyle()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<!-- 更换theme -->
<script type="text/javascript">
function setStyle(){ 
    if(document.getElementById("dark").disabled){
        document.getElementById("light").disabled = true; 
        document.getElementById("dark").disabled = false; 
        }
    else{
        document.getElementById("dark").disabled = true; 
        document.getElementById("light").disabled = false; 
        }
    }
</script>


<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
