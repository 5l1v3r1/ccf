 
<head>
<meta name="HunterCmd" charset="utf-8">

<link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link id="cssfile" rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/huntercmd/blog/master/config/css/light.css">
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/skin.js"></script>
<script src="https://cdn.rawgit.com/huntercmd/blog/master/config/css/classie.js"></script>

<!-- This is for Mathjax -->

<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [ ['$','$'], ["$","$"] ],
			displayMath: [ ['$$','$$'], ["$$","$$"] ],
			processEscapes: true
			},
		TeX: {equationNumbers: {autoNumber: ["AMS"], useLabelIds: true}},
		"HTML-CSS": {linebreaks: {automatic: true}},
		SVG: {linebreaks: {automatic: true}}
	});
</script>

<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<title>HunterCmd</title>
</head>

<body class="cbp-spmenu-push">

<nav class="cbp-spmenu cbp-spmenu-vertical cbp-spmenu-left" id="menu-s1" style="width: 320px;overflow: auto;
">

<h1>Table of contents</h1>
<ul>
<li><a href="#30. ICML 2013:Atlanta, GA, USA">30. ICML 2013:Atlanta, GA, USA</a><ul>
<li><a href="#Paper Num: 283 || Session Num: 3">Paper Num: 283 || Session Num: 3</a></li>
<li><a href="#Cycle 1 Papers    74">Cycle 1 Papers    74</a><ul>
<li><a href="#1. An Optimal Policy for Target Localization with Application to Electron Microscopy.">1. An Optimal Policy for Target Localization with Application to Electron Microscopy.</a></li>
<li><a href="#2. Domain Generalization via Invariant Feature Representation.">2. Domain Generalization via Invariant Feature Representation.</a></li>
<li><a href="#3. A Spectral Learning Approach to Range-Only SLAM.">3. A Spectral Learning Approach to Range-Only SLAM.</a></li>
<li><a href="#4. Near-Optimal Bounds for Cross-Validation via Loss Stability.">4. Near-Optimal Bounds for Cross-Validation via Loss Stability.</a></li>
<li><a href="#5. Sparsity-Based Generalization Bounds for Predictive Sparse Coding.">5. Sparsity-Based Generalization Bounds for Predictive Sparse Coding.</a></li>
<li><a href="#6. Sparse Uncorrelated Linear Discriminant Analysis.">6. Sparse Uncorrelated Linear Discriminant Analysis.</a></li>
<li><a href="#7. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.">7. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.</a></li>
<li><a href="#8. Fast Probabilistic Optimization from Noisy Gradients.">8. Fast Probabilistic Optimization from Noisy Gradients.</a></li>
<li><a href="#9. Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.">9. Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.</a></li>
<li><a href="#10. Stochastic Alternating Direction Method of Multipliers.">10. Stochastic Alternating Direction Method of Multipliers.</a></li>
<li><a href="#11. Noisy Sparse Subspace Clustering.">11. Noisy Sparse Subspace Clustering.</a></li>
<li><a href="#12. Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models.">12. Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models.</a></li>
<li><a href="#13. Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction.">13. Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction.</a></li>
<li><a href="#14. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.">14. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.</a></li>
<li><a href="#15. Gibbs Max-Margin Topic Models with Fast Sampling Algorithms.">15. Gibbs Max-Margin Topic Models with Fast Sampling Algorithms.</a></li>
<li><a href="#16. Cost-Sensitive Tree of Classifiers.">16. Cost-Sensitive Tree of Classifiers.</a></li>
<li><a href="#17. Learning Hash Functions Using Column Generation.">17. Learning Hash Functions Using Column Generation.</a></li>
<li><a href="#18. Combinatorial Multi-Armed Bandit: General Framework and Applications.">18. Combinatorial Multi-Armed Bandit: General Framework and Applications.</a></li>
<li><a href="#19. Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization.">19. Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization.</a></li>
<li><a href="#20. Convex formulations of radius-margin based Support Vector Machines.">20. Convex formulations of radius-margin based Support Vector Machines.</a></li>
<li><a href="#21. Modelling Sparse Dynamical Systems with Compressed Predictive State Representations.">21. Modelling Sparse Dynamical Systems with Compressed Predictive State Representations.</a></li>
<li><a href="#22. A Machine Learning Framework for Programming by Example.">22. A Machine Learning Framework for Programming by Example.</a></li>
<li><a href="#23. Discriminatively Activated Sparselets.">23. Discriminatively Activated Sparselets.</a></li>
<li><a href="#24. The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification.">24. The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification.</a></li>
<li><a href="#25. Fixed-Point Model For Structured Labeling.">25. Fixed-Point Model For Structured Labeling.</a></li>
<li><a href="#26. Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation.">26. Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation.</a></li>
<li><a href="#27. Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization.">27. Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization.</a></li>
<li><a href="#28. Principal Component Analysis on non-Gaussian Dependent Data.">28. Principal Component Analysis on non-Gaussian Dependent Data.</a></li>
<li><a href="#29. Learning Linear Bayesian Networks with Latent Variables.">29. Learning Linear Bayesian Networks with Latent Variables.</a></li>
<li><a href="#30. Multiple Identifications in Multi-Armed Bandits.">30. Multiple Identifications in Multi-Armed Bandits.</a></li>
<li><a href="#31. Learning Optimally Sparse Support Vector Machines.">31. Learning Optimally Sparse Support Vector Machines.</a></li>
<li><a href="#32. Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks.">32. Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks.</a></li>
<li><a href="#33. Efficient Sparse Group Feature Selection via Nonconvex Optimization.">33. Efficient Sparse Group Feature Selection via Nonconvex Optimization.</a></li>
<li><a href="#34. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.">34. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.</a></li>
<li><a href="#35. Maximum Variance Correction with Application to A* Search.">35. Maximum Variance Correction with Application to A* Search.</a></li>
<li><a href="#36. Adaptive Sparsity in Gaussian Graphical Models.">36. Adaptive Sparsity in Gaussian Graphical Models.</a></li>
<li><a href="#37. Average Reward Optimization Objective In Partially Observable Domains.">37. Average Reward Optimization Objective In Partially Observable Domains.</a></li>
<li><a href="#38. Feature Selection in High-Dimensional Classification.">38. Feature Selection in High-Dimensional Classification.</a></li>
<li><a href="#39. Human Boosting.">39. Human Boosting.</a></li>
<li><a href="#40. Efficient Dimensionality Reduction for Canonical Correlation Analysis.">40. Efficient Dimensionality Reduction for Canonical Correlation Analysis.</a></li>
<li><a href="#41. Parsing epileptic events using a Markov switching process model for correlated time series.">41. Parsing epileptic events using a Markov switching process model for correlated time series.</a></li>
<li><a href="#42. Optimal rates for stochastic convex optimization under Tsybakov noise condition.">42. Optimal rates for stochastic convex optimization under Tsybakov noise condition.</a></li>
<li><a href="#43. A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning.">43. A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning.</a></li>
<li><a href="#44. Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery.">44. Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery.</a></li>
<li><a href="#45. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.">45. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.</a></li>
<li><a href="#46. A New Frontier of Kernel Design for Structured Data.">46. A New Frontier of Kernel Design for Structured Data.</a></li>
<li><a href="#47. Learning with Marginalized Corrupted Features.">47. Learning with Marginalized Corrupted Features.</a></li>
<li><a href="#48. Approximation properties of DBNs with binary hidden units and real-valued visible units.">48. Approximation properties of DBNs with binary hidden units and real-valued visible units.</a></li>
<li><a href="#49. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.">49. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.</a></li>
<li><a href="#50. General Functional Matrix Factorization Using Gradient Boosting.">50. General Functional Matrix Factorization Using Gradient Boosting.</a></li>
<li><a href="#51. Iterative Learning and Denoising in Convolutional Neural Associative Memories.">51. Iterative Learning and Denoising in Convolutional Neural Associative Memories.</a></li>
<li><a href="#52. Scaling Multidimensional Gaussian Processes using Projected Additive Approximations.">52. Scaling Multidimensional Gaussian Processes using Projected Additive Approximations.</a></li>
<li><a href="#53. Active Learning for Multi-Objective Optimization.">53. Active Learning for Multi-Objective Optimization.</a></li>
<li><a href="#54. A Generalized Kernel Approach to Structured Output Learning.">54. A Generalized Kernel Approach to Structured Output Learning.</a></li>
<li><a href="#55. Efficient Active Learning of Halfspaces: an Aggressive Approach.">55. Efficient Active Learning of Halfspaces: an Aggressive Approach.</a></li>
<li><a href="#56. Enhanced statistical rankings via targeted data collection.">56. Enhanced statistical rankings via targeted data collection.</a></li>
<li><a href="#57. Online Feature Selection for Model-based Reinforcement Learning.">57. Online Feature Selection for Model-based Reinforcement Learning.</a></li>
<li><a href="#58. ELLA: An Efficient Lifelong Learning Algorithm.">58. ELLA: An Efficient Lifelong Learning Algorithm.</a></li>
<li><a href="#59. A Structural SVM Based Approach for Optimizing Partial AUC.">59. A Structural SVM Based Approach for Optimizing Partial AUC.</a></li>
<li><a href="#60. Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs.">60. Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs.</a></li>
<li><a href="#61. Adaptive Task Assignment for Crowdsourced Classification.">61. Adaptive Task Assignment for Crowdsourced Classification.</a></li>
<li><a href="#62. Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning.">62. Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning.</a></li>
<li><a href="#63. Better Mixing via Deep Representations.">63. Better Mixing via Deep Representations.</a></li>
<li><a href="#64. Online Latent Dirichlet Allocation with Infinite Vocabulary.">64. Online Latent Dirichlet Allocation with Infinite Vocabulary.</a></li>
<li><a href="#65. Characterizing the Representer Theorem.">65. Characterizing the Representer Theorem.</a></li>
<li><a href="#66. Dynamical Models and tracking regret in online convex programming.">66. Dynamical Models and tracking regret in online convex programming.</a></li>
<li><a href="#67. Large-Scale Bandit Problems and KWIK Learning.">67. Large-Scale Bandit Problems and KWIK Learning.</a></li>
<li><a href="#68. Vanishing Component Analysis.">68. Vanishing Component Analysis.</a></li>
<li><a href="#69. Learning an Internal Dynamics Model from Control Demonstration.">69. Learning an Internal Dynamics Model from Control Demonstration.</a></li>
<li><a href="#70. Robust Structural Metric Learning.">70. Robust Structural Metric Learning.</a></li>
<li><a href="#71. Constrained fractional set programs and their application in local clustering and community detection.">71. Constrained fractional set programs and their application in local clustering and community detection.</a></li>
<li><a href="#72. Efficient Semi-supervised and Active Learning of Disjunctions.">72. Efficient Semi-supervised and Active Learning of Disjunctions.</a></li>
<li><a href="#73. Convex Adversarial Collective Classification.">73. Convex Adversarial Collective Classification.</a></li>
<li><a href="#74. Rounding Methods for Discrete Linear Classification.">74. Rounding Methods for Discrete Linear Classification.</a></li>
</ul>
</li>
<li><a href="#Cycle 2 Papers    42">Cycle 2 Papers    42</a><ul>
<li><a href="#75. Mixture of Mutually Exciting Processes for Viral Diffusion.">75. Mixture of Mutually Exciting Processes for Viral Diffusion.</a></li>
<li><a href="#76. Gaussian Process Vine Copulas for Multivariate Dependence.">76. Gaussian Process Vine Copulas for Multivariate Dependence.</a></li>
<li><a href="#77. Stochastic Simultaneous Optimistic Optimization.">77. Stochastic Simultaneous Optimistic Optimization.</a></li>
<li><a href="#78. Toward Optimal Stratification for Stratified Monte-Carlo Integration.">78. Toward Optimal Stratification for Stratified Monte-Carlo Integration.</a></li>
<li><a href="#79. A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems.">79. A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems.</a></li>
<li><a href="#80. Thurstonian Boltzmann Machines: Learning from Multiple Inequalities.">80. Thurstonian Boltzmann Machines: Learning from Multiple Inequalities.</a></li>
<li><a href="#81. A Variational Approximation for Topic Modeling of Hierarchical Corpora.">81. A Variational Approximation for Topic Modeling of Hierarchical Corpora.</a></li>
<li><a href="#82. Forecastable Component Analysis.">82. Forecastable Component Analysis.</a></li>
<li><a href="#83. Ellipsoidal Multiple Instance Learning.">83. Ellipsoidal Multiple Instance Learning.</a></li>
<li><a href="#84. Local Low-Rank Matrix Approximation.">84. Local Low-Rank Matrix Approximation.</a></li>
<li><a href="#85. Generic Exploration and K-armed Voting Bandits.">85. Generic Exploration and K-armed Voting Bandits.</a></li>
<li><a href="#86. A unifying framework for vector-valued manifold regularization and multi-view learning.">86. A unifying framework for vector-valued manifold regularization and multi-view learning.</a></li>
<li><a href="#87. Learning Connections in Financial Time Series.">87. Learning Connections in Financial Time Series.</a></li>
<li><a href="#88. Fast dropout training.">88. Fast dropout training.</a></li>
<li><a href="#89. Scalable Optimization of Neighbor Embedding for Visualization.">89. Scalable Optimization of Neighbor Embedding for Visualization.</a></li>
<li><a href="#90. Precision-recall space to correct external indices for biclustering.">90. Precision-recall space to correct external indices for biclustering.</a></li>
<li><a href="#91. Monochromatic Bi-Clustering.">91. Monochromatic Bi-Clustering.</a></li>
<li><a href="#92. Gated Autoencoders with Tied Input Weights.">92. Gated Autoencoders with Tied Input Weights.</a></li>
<li><a href="#93. Strict Monotonicity of Sum of Squares Error and Normalized Cut in the Lattice of Clusterings.">93. Strict Monotonicity of Sum of Squares Error and Normalized Cut in the Lattice of Clusterings.</a></li>
<li><a href="#94. Transition Matrix Estimation in High Dimensional Time Series.">94. Transition Matrix Estimation in High Dimensional Time Series.</a></li>
<li><a href="#95. Label Partitioning For Sublinear Ranking.">95. Label Partitioning For Sublinear Ranking.</a></li>
<li><a href="#96. Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing.">96. Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing.</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav><h1 id="30. ICML 2013:Atlanta, GA, USA">30. ICML 2013:Atlanta, GA, USA</h1>
<p><a href="http://jmlr.org/proceedings/papers/v28/">Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013.</a> JMLR.org
【<a href="http://dblp.uni-trier.de/db/conf/icml/icml2013.html">DBLP Link</a>】</p>
<h2 id="Paper Num: 283 || Session Num: 3">Paper Num: 283 || Session Num: 3</h2>
<ul>
<li><a href="#Cycle 1 Papers    74">Cycle 1 Papers    74</a></li>
<li><a href="#Cycle 2 Papers    42">Cycle 2 Papers    42</a></li>
</ul>
<h2 id="Cycle 1 Papers    74">Cycle 1 Papers    74</h2>
<h3 id="1. An Optimal Policy for Target Localization with Application to Electron Microscopy.">1. An Optimal Policy for Target Localization with Application to Electron Microscopy.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/sznitman13.html">Paper Link</a>】    【Pages】:1-9</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Sznitman:Raphael">Raphael Sznitman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lucchi:Aur=eacute=lien">Aurélien Lucchi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Frazier:Peter_I=">Peter I. Frazier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jedynak:Bruno">Bruno Jedynak</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fua:Pascal">Pascal Fua</a></p>
<p>【Abstract】:
This paper considers the task of finding a target location by making a limited number of sequential observations. Each observation results from evaluating an imperfect classifier of a chosen cost and accuracy on an interval of chosen length and position. Within a Bayesian framework, we study the problem of minimizing an objective that combines the entropy of the posterior distribution with the cost of the questions asked. In this problem, we show that the one-step lookahead policy is Bayes-optimal for any arbitrary time horizon. Moreover, this one-step lookahead policy is easy to compute and implement. We then use this policy in the context of localizing mitochondria in electron microscope images, and experimentally show that significant speed ups in acquisition can be gained, while maintaining near equal image quality at target locations, when compared to current policies.</p>
<p>【Keywords】:</p>
<h3 id="2. Domain Generalization via Invariant Feature Representation.">2. Domain Generalization via Invariant Feature Representation.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/muandet13.html">Paper Link</a>】    【Pages】:10-18</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Muandet:Krikamol">Krikamol Muandet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Balduzzi:David">David Balduzzi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sch=ouml=lkopf:Bernhard">Bernhard Schölkopf</a></p>
<p>【Abstract】:
This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.</p>
<p>【Keywords】:</p>
<h3 id="3. A Spectral Learning Approach to Range-Only SLAM.">3. A Spectral Learning Approach to Range-Only SLAM.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/boots13.html">Paper Link</a>】    【Pages】:19-26</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Boots:Byron">Byron Boots</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gordon:Geoffrey_J=">Geoffrey J. Gordon</a></p>
<p>【Abstract】:
We present a novel spectral learning algorithm for simultaneous localization and mapping (SLAM) from range data with known correspondences. This algorithm is an instance of a general spectral system identification framework, from which it inherits several desirable properties, including statistical consistency and no local optima. Compared with popular batch optimization or multiple-hypothesis tracking (MHT) methods for range-only SLAM, our spectral approach offers guaranteed low computational requirements and good tracking performance. Compared with MHT and with popular extended Kalman filter (EKF) or extended information filter (EIF) approaches, our approach does not need to linearize a transition or measurement model. We provide a theoretical analysis of our method, including finite-sample error bounds. Finally, we demonstrate on a real-world robotic SLAM problem that our algorithm is not only theoretically justified, but works well in practice: in a comparison of multiple methods, the lowest errors come from a combination of our algorithm with batch optimization, but our method alone produces nearly as good a result at far lower computational cost.</p>
<p>【Keywords】:</p>
<h3 id="4. Near-Optimal Bounds for Cross-Validation via Loss Stability.">4. Near-Optimal Bounds for Cross-Validation via Loss Stability.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kumar13a.html">Paper Link</a>】    【Pages】:27-35</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:Ravi">Ravi Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lokshtanov:Daniel">Daniel Lokshtanov</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vassilvitskii:Sergei">Sergei Vassilvitskii</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vattani:Andrea">Andrea Vattani</a></p>
<p>【Abstract】:
Multi-fold cross-validation is an established practice to estimate the error rate of a learning algorithm. Quantifying the variance reduction gains due to cross-validation has been challenging due to the inherent correlations introduced by the folds. In this work we introduce a new and weak measure of stability called loss stability and relate the cross-validation performance to loss stability; we also establish that this relationship is near-optimal. Our work thus quantitatively improves the current best bounds on cross-validation.</p>
<p>【Keywords】:</p>
<h3 id="5. Sparsity-Based Generalization Bounds for Predictive Sparse Coding.">5. Sparsity-Based Generalization Bounds for Predictive Sparse Coding.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/mehta13.html">Paper Link</a>】    【Pages】:36-44</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Mehta:Nishant_Ajay">Nishant Ajay Mehta</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gray:Alexander_G=">Alexander G. Gray</a></p>
<p>【Abstract】:
The goal of predictive sparse coding is to learn a representation of examples as sparse linear combinations of elements from a dictionary, such that a learned hypothesis linear in the new representation performs well on a predictive task. Predictive sparse coding has demonstrated impressive performance on a variety of supervised tasks, but its generalization properties have not been studied. We establish the first generalization error bounds for predictive sparse coding, in the overcomplete setting, where the number of features k exceeds the original dimensionality d. The learning bound decays as (sqrt(d k/m)) with respect to d, k, and the size m of the training sample. It depends intimately on stability properties of the learned sparse encoder, as measured on the training sample. Consequently, we also present a fundamental stability result for the LASSO, a result that characterizes the stability of the sparse codes with respect to dictionary perturbations.</p>
<p>【Keywords】:</p>
<h3 id="6. Sparse Uncorrelated Linear Discriminant Analysis.">6. Sparse Uncorrelated Linear Discriminant Analysis.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/zhang13.html">Paper Link</a>】    【Pages】:45-52</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Xiaowei">Xiaowei Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chu:Delin">Delin Chu</a></p>
<p>【Abstract】:
In this paper, we develop a novel approach for sparse uncorrelated linear discriminant analysis (ULDA). Our proposal is based on characterization of all solutions of the generalized ULDA. We incorporate sparsity into the ULDA transformation by seeking the solution with minimum (\ell<em>1)-norm from all minimum dimension solutions of the generalized ULDA. The problem is then formulated as a (\ell</em>{1})-minimization problem and is solved by accelerated linearized Bregman method. Experiments on high-dimensional gene expression data demonstrate that our approach not only computes extremely sparse solutions but also performs well in classification. Experimental results also show that our approach can help for data visualization in low-dimensional space.</p>
<p>【Keywords】:</p>
<h3 id="7. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.">7. Block-Coordinate Frank-Wolfe Optimization for Structural SVMs.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/lacoste-julien13.html">Paper Link</a>】    【Pages】:53-61</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lacoste=Julien:Simon">Simon Lacoste-Julien</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jaggi:Martin">Martin Jaggi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schmidt:Mark_W=">Mark W. Schmidt</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pletscher:Patrick">Patrick Pletscher</a></p>
<p>【Abstract】:
We propose a randomized block-coordinate variant of the classic Frank-Wolfe algorithm for convex optimization with block-separable constraints. Despite its lower iteration cost, we show that it achieves a similar convergence rate in duality gap as the full Frank-Wolfe algorithm. We also show that, when applied to the dual structural support vector machine (SVM) objective, this yields an online algorithm that has the same low iteration complexity as primal stochastic subgradient methods. However, unlike stochastic subgradient methods, the block-coordinate Frank-Wolfe algorithm allows us to compute the optimal step-size and yields a computable duality gap guarantee. Our experiments indicate that this simple algorithm outperforms competing structural SVM solvers.</p>
<p>【Keywords】:</p>
<h3 id="8. Fast Probabilistic Optimization from Noisy Gradients.">8. Fast Probabilistic Optimization from Noisy Gradients.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/hennig13.html">Paper Link</a>】    【Pages】:62-70</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hennig:Philipp">Philipp Hennig</a></p>
<p>【Abstract】:
Stochastic gradient descent remains popular in large-scale machine learning, on account of its very low computational cost and robustness to noise. However, gradient descent is only linearly efficient and not transformation invariant. Scaling by a local measure can substantially improve its performance. One natural choice of such a scale is the Hessian of the objective function: Were it available, it would turn linearly efficient gradient descent into the quadratically efficient Newton-Raphson optimization. Existing covariant methods, though, are either super-linearly expensive or do not address noise. Generalising recent results, this paper constructs a nonparametric Bayesian quasi-Newton algorithm that learns gradient and Hessian from noisy evaluations of the gradient. Importantly, the resulting algorithm, like stochastic gradient descent, has cost linear in the number of input dimensions.</p>
<p>【Keywords】:</p>
<h3 id="9. Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.">9. Stochastic Gradient Descent for Non-smooth Optimization: Convergence Results and Optimal Averaging Schemes.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/shamir13.html">Paper Link</a>】    【Pages】:71-79</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shamir:Ohad">Ohad Shamir</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang_0001:Tong">Tong Zhang</a></p>
<p>【Abstract】:
Stochastic Gradient Descent (SGD) is one of the simplest and most popular stochastic optimization methods. While it has already been theoretically studied for decades, the classical analysis usually required non-trivial smoothness assumptions, which do not apply to many modern applications of SGD with non-smooth objective functions such as support vector machines. In this paper, we investigate the performance of SGD without such smoothness assumptions, as well as a running average scheme to convert the SGD iterates to a solution with optimal optimization accuracy. In this framework, we prove that after (T) rounds, the suboptimality of the last SGD iterate scales as (O(\log(T)/\sqrt{T})) for non-smooth convex objective functions, and (O(\log(T)/T)) in the non-smooth strongly convex case. To the best of our knowledge, these are the first bounds of this kind, and almost match the minimax-optimal rates obtainable by appropriate averaging schemes. We also propose a new and simple averaging scheme, which not only attains optimal rates, but can also be easily computed on-the-fly (in contrast, the suffix averaging scheme proposed in  is not as simple to implement). Finally, we provide some experimental illustrations.</p>
<p>【Keywords】:</p>
<h3 id="10. Stochastic Alternating Direction Method of Multipliers.">10. Stochastic Alternating Direction Method of Multipliers.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/ouyang13.html">Paper Link</a>】    【Pages】:80-88</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Ouyang:Hua">Hua Ouyang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/He:Niao">Niao He</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tran:Long">Long Tran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gray:Alexander_G=">Alexander G. Gray</a></p>
<p>【Abstract】:
The Alternating Direction Method of Multipliers (ADMM) has received lots of attention recently due to the tremendous demand from large-scale and data-distributed machine learning applications. In this paper, we present a stochastic setting for optimization problems with non-smooth composite objective functions. To solve this problem, we propose a stochastic ADMM algorithm. Our algorithm applies to a more general class of convex and nonsmooth objective functions, beyond the smooth and separable least squares loss used in lasso. We also demonstrate the rates of convergence for our algorithm under various structural assumptions of the stochastic function: (O(1/\sqrt{t})) for convex functions and (O(\log t/t)) for strongly convex functions. Compared to previous literature, we establish the convergence rate of ADMM for convex problems in terms of both the objective value and the feasibility violation. A novel application named Graph-Guided SVM is proposed to demonstrate the usefulness of our algorithm.</p>
<p>【Keywords】:</p>
<h3 id="11. Noisy Sparse Subspace Clustering.">11. Noisy Sparse Subspace Clustering.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wang13.html">Paper Link</a>】    【Pages】:89-97</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yu=Xiang">Yu-Xiang Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Huan">Huan Xu</a></p>
<p>【Abstract】:
This paper considers the problem of subspace clustering under noise. Specifically, we study the behavior of Sparse Subspace Clustering (SSC) when either adversarial or random noise is added to the unlabelled input data points, which are assumed to lie in a union of low-dimensional subspaces. We show that a modified version of SSC is provably effective in correctly identifying the underlying subspaces, even with noisy data. This extends theoretical guarantee of this algorithm to the practical setting and provides justification to the success of SSC in a class of real applications.</p>
<p>【Keywords】:</p>
<h3 id="12. Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models.">12. Parallel Markov Chain Monte Carlo for Nonparametric Mixture Models.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/williamson13.html">Paper Link</a>】    【Pages】:98-106</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Williamson:Sinead">Sinead Williamson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dubey:Avinava">Avinava Dubey</a> ; <a href="http://dblp.uni-trier.de/pers/hd/x/Xing:Eric_P=">Eric P. Xing</a></p>
<p>【Abstract】:
Nonparametric mixture models based on the Dirichlet process are an elegant alternative to finite models when the number of underlying components is unknown, but inference in such models can be slow. Existing attempts to parallelize inference in such models have relied on introducing approximations, which can lead to inaccuracies in the posterior estimate. In this paper, we describe auxiliary variable representations for the Dirichlet process and the hierarchical Dirichlet process that allow us to perform MCMC using the correct equilibrium distribution, in a distributed manner. We show that our approach allows scalable inference without the deterioration in estimate quality that accompanies existing methods.</p>
<p>【Keywords】:</p>
<h3 id="13. Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction.">13. Risk Bounds and Learning Algorithms for the Regression Approach to Structured Output Prediction.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/giguere13.html">Paper Link</a>】    【Pages】:107-114</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gigu=egrave=re:S=eacute=bastien">Sébastien Giguère</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Laviolette:Fran=ccedil=ois">François Laviolette</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Marchand:Mario">Mario Marchand</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sylla:Khadidja">Khadidja Sylla</a></p>
<p>【Abstract】:
We provide rigorous guarantees for the regression approach to structured output prediction. We show that the quadratic regression loss is a convex surrogate of the prediction loss when the output kernel satisfies some condition with respect to the prediction loss. We provide two upper bounds of the prediction risk that depend on the empirical quadratic risk of the predictor. The minimizer of the first bound is the predictor proposed by Cortes et al. (2007) while the minimizer of the second bound is a predictor that has never been proposed so far. Both predictors are compared on practical tasks.</p>
<p>【Keywords】:</p>
<h3 id="14. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.">14. Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/bergstra13.html">Paper Link</a>】    【Pages】:115-123</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bergstra:James">James Bergstra</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yamins:Daniel">Daniel Yamins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cox:David_D=">David D. Cox</a></p>
<p>【Abstract】:
Many computer vision algorithms depend on configuration settings that are typically hand-tuned in the course of evaluating the algorithm for a particular data set. While such parameter tuning is often presented as being incidental to the algorithm, correctly setting these parameter choices is frequently critical to realizing a method’s full potential. Compounding matters, these parameters often must be re-tuned when the algorithm is applied to a new problem domain, and the tuning process itself often depends on personal experience and intuition in ways that are hard to quantify or describe. Since the performance of a given technique depends on both the fundamental quality of the algorithm and the details of its tuning, it is sometimes difficult to know whether a given technique is genuinely better, or simply better tuned. In this work, we propose a meta-modeling approach to support automated hyperparameter optimization, with the goal of providing practical tools that replace hand-tuning with a reproducible and unbiased optimization process. Our approach is to expose the underlying expression graph of how a performance metric (e.g. classification accuracy on validation examples) is computed from hyperparameters that govern not only how individual processing steps are applied, but even which processing steps are included. A hyperparameter optimization algorithm transforms this graph into a program for optimizing that performance metric. Our approach yields state of the art results on three disparate computer vision problems: a face-matching verification task (LFW), a face identification task (PubFig83) and an object recognition task (CIFAR-10), using a single broad class of feed-forward vision architectures.</p>
<p>【Keywords】:</p>
<h3 id="15. Gibbs Max-Margin Topic Models with Fast Sampling Algorithms.">15. Gibbs Max-Margin Topic Models with Fast Sampling Algorithms.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/zhu13.html">Paper Link</a>】    【Pages】:124-132</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhu:Jun">Jun Zhu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Ning">Ning Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Perkins:Hugh">Hugh Perkins</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Bo">Bo Zhang</a></p>
<p>【Abstract】:
Existing max-margin supervised topic models rely on an iterative procedure to solve multiple latent SVM subproblems with additional mean-field assumptions on the desired posterior distributions. This paper presents Gibbs max-margin supervised topic models by minimizing an expected margin loss, an upper bound of the existing margin loss derived from an expected prediction rule. By introducing augmented variables, we develop simple and fast Gibbs sampling algorithms with no restricting assumptions and no need to solve SVM subproblems for both classification and regression. Empirical results demonstrate significant improvements on time efficiency. The classification performance is also significantly improved over competitors.</p>
<p>【Keywords】:</p>
<h3 id="16. Cost-Sensitive Tree of Classifiers.">16. Cost-Sensitive Tree of Classifiers.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/xu13.html">Paper Link</a>】    【Pages】:133-141</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xu:Zhixiang_Eddie">Zhixiang Eddie Xu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kusner:Matt_J=">Matt J. Kusner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Minmin">Minmin Chen</a></p>
<p>【Abstract】:
Recently, machine learning algorithms have successfully entered large-scale real-world industrial applications (e.g. search engines and email spam filters). Here, the CPU cost during test-time must be budgeted and accounted for. In this paper, we address the challenge of balancing test-time cost and the classifier accuracy in a principled fashion. The test-time cost of a classifier is often dominated by the computation required for feature extraction-which can vary drastically across features. We incorporate this extraction time by constructing a tree of classifiers, through which test inputs traverse along individual paths. Each path extracts different features and is optimized for a specific sub-partition of the input space. By only computing features for inputs that benefit from them the most, our cost-sensitive tree of classifiers can match the high accuracies of the current state-of-the-art at a small fraction of the computational cost.</p>
<p>【Keywords】:</p>
<h3 id="17. Learning Hash Functions Using Column Generation.">17. Learning Hash Functions Using Column Generation.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/li13a.html">Paper Link</a>】    【Pages】:142-150</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Xi">Xi Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lin:Guosheng">Guosheng Lin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shen:Chunhua">Chunhua Shen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hengel:Anton_van_den">Anton van den Hengel</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dick:Anthony_R=">Anthony R. Dick</a></p>
<p>【Abstract】:
Fast nearest neighbor searching is becoming an increasingly important tool in solving many large-scale problems. Recently a number of approaches to learning data-dependent hash functions have been developed. In this work, we propose a column generation based method for learning data-dependent hash functions on the basis of proximity comparison information. Given a set of triplets that encode the pairwise proximity comparison information, our method learns hash functions that preserve the relative comparison relationships in the data as well as possible within the large-margin learning framework. The learning procedure is implemented using column generation and hence is named CGHash. At each iteration of the column generation procedure, the best hash function is selected. Unlike most other hashing methods, our method generalizes to new data points naturally; and has a training objective which is convex, thus ensuring that the global optimum can be identified. Experiments demonstrate that the proposed method learns compact binary codes and that its retrieval performance compares favorably with state-of-the-art methods when tested on a few benchmark datasets.</p>
<p>【Keywords】:</p>
<h3 id="18. Combinatorial Multi-Armed Bandit: General Framework and Applications.">18. Combinatorial Multi-Armed Bandit: General Framework and Applications.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chen13a.html">Paper Link</a>】    【Pages】:151-159</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen_0013:Wei">Wei Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Yajun">Yajun Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yuan:Yang">Yang Yuan</a></p>
<p>【Abstract】:
We define a general framework for a large class of combinatorial multi-armed bandit (CMAB) problems, where simple arms with unknown istributions form super arms. In each round, a super arm is played and the outcomes of its related simple arms are observed, which helps the selection of super arms in future rounds. The reward of the super arm depends on the outcomes of played arms, and it only needs to satisfy two mild assumptions, which allow a large class of nonlinear reward instances. We assume the availability of an ((\alpha,\beta))-approximation oracle that takes the means of the distributions of arms and outputs a super arm that with probability (\beta) generates an (\alpha) fraction of the optimal expected reward. The objective of a CMAB algorithm is to minimize ((\alpha,\beta))-approximation regret, which is the difference in total expected reward between the (\alpha\beta) fraction of expected reward when always playing the optimal super arm, and the expected reward of playing super arms according to the algorithm. We provide CUCB algorithm that achieves (O(\log n)) regret, where (n) is the number of rounds played, and we further provide distribution-independent bounds for a large class of reward functions. Our regret analysis is tight in that it matches the bound for classical MAB problem up to a constant factor, and it significantly improves the regret bound in a recent paper on combinatorial bandits with linear rewards. We apply our CMAB framework to two new applications, probabilistic maximum coverage (PMC) for online advertising and social influence maximization for viral marketing, both having nonlinear reward structures.</p>
<p>【Keywords】:</p>
<h3 id="19. Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization.">19. Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chen13b.html">Paper Link</a>】    【Pages】:160-168</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yuxin">Yuxin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krause:Andreas">Andreas Krause</a></p>
<p>【Abstract】:
Active learning can lead to a dramatic reduction in labeling effort. However, in many practical implementations (such as crowdsourcing, surveys, high-throughput experimental design), it is preferable to query labels for batches of examples to be labelled in parallel. While several heuristics have been proposed for batch-mode active learning, little is known about their theoretical performance. We consider batch mode active learning and more general information-parallel stochastic optimization problems that exhibit adaptive submodularity, a natural diminishing returns condition. We prove that for such problems, a simple greedy strategy is competitive with the optimal batch-mode policy. In some cases, surprisingly, the use of batches incurs competitively low cost, even when compared to a fully sequential strategy. We demonstrate the effectiveness of our approach on batch-mode active learning tasks, where it outperforms the state of the art, as well as the novel problem of multi-stage influence maximization in social networks.</p>
<p>【Keywords】:</p>
<h3 id="20. Convex formulations of radius-margin based Support Vector Machines.">20. Convex formulations of radius-margin based Support Vector Machines.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/do13.html">Paper Link</a>】    【Pages】:169-177</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Do:Huyen">Huyen Do</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kalousis:Alexandros">Alexandros Kalousis</a></p>
<p>【Abstract】:
We consider Support Vector Machines (SVMs) learned together with linear transformations of the feature spaces on which they are applied. Under this scenario the radius of the smallest data enclosing sphere is no longer fixed. Therefore optimizing the SVM error bound by considering both the radius and the margin has the potential to deliver a tighter error bound. In this paper we present two novel algorithms: (R-SVM<em>{\mu}^+)—a SVM radius-margin based feature selection algorithm, and (R-SVM^+) — a metric learning-based SVM. We derive our algorithms by exploiting a new tighter approximation of the radius and a metric learning interpretation of SVM. Both optimize directly the radius-margin error bound using linear transformations. Unlike almost all existing radius-margin based SVM algorithms which are either non-convex or combinatorial, our algorithms are standard quadratic convex optimization problems with linear or quadratic constraints. We perform a number of experiments on benchmark datasets. (R-SVM</em>{\mu}^+) exhibits excellent feature selection performance compared to the state-of-the-art feature selection methods, such as (L_1)-norm and elastic-net based methods. (R-SVM^+) achieves a significantly better classification performance compared to SVM and its other state-of-the-art variants. From the results it is clear that the incorporation of the radius, as a means to control the data spread, in the cost function has strong beneficial effects.</p>
<p>【Keywords】:</p>
<h3 id="21. Modelling Sparse Dynamical Systems with Compressed Predictive State Representations.">21. Modelling Sparse Dynamical Systems with Compressed Predictive State Representations.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/hamilton13.html">Paper Link</a>】    【Pages】:178-186</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hamilton:William_L=">William L. Hamilton</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fard:Mahdi_Milani">Mahdi Milani Fard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Pineau:Joelle">Joelle Pineau</a></p>
<p>【Abstract】:
Efficiently learning accurate models of dynamical systems is of central importance for developing rational agents that can succeed in a wide range of challenging domains. The difficulty of this learning problem is particularly acute in settings with large observation spaces and partial observability. We present a new algorithm, called Compressed Predictive State Representation (CPSR), for learning models of high-dimensional partially observable uncontrolled dynamical systems from small sample sets. The algorithm, which extends previous work on Predictive State Representations, exploits a particular sparse structure present in many domains. This sparse structure is used to compress information during learning, allowing for an increase in both the efficiency and predictive power. The compression technique also relieves the burden of domain specific feature selection and allows for domains with extremely large discrete observation spaces to be efficiently modelled. We present empirical results showing that the algorithm is able to build accurate models more efficiently than its uncompressed counterparts, and provide theoretical results on the accuracy of the learned compressed model.</p>
<p>【Keywords】:</p>
<h3 id="22. A Machine Learning Framework for Programming by Example.">22. A Machine Learning Framework for Programming by Example.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/menon13.html">Paper Link</a>】    【Pages】:187-195</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Menon:Aditya_Krishna">Aditya Krishna Menon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tamuz:Omer">Omer Tamuz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gulwani:Sumit">Sumit Gulwani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lampson:Butler_W=">Butler W. Lampson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kalai:Adam">Adam Kalai</a></p>
<p>【Abstract】:
Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.</p>
<p>【Keywords】:</p>
<h3 id="23. Discriminatively Activated Sparselets.">23. Discriminatively Activated Sparselets.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/girshick13.html">Paper Link</a>】    【Pages】:196-204</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Girshick:Ross_B=">Ross B. Girshick</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Song:Hyun_Oh">Hyun Oh Song</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Darrell:Trevor">Trevor Darrell</a></p>
<p>【Abstract】:
Shared representations are highly appealing due to their potential for gains in computational and statistical efficiency. Compressing a shared representation leads to greater computational savings, but at the same time can severely decrease performance on a target task. Recently, sparselets (Song et al., 2012) were introduced as a new shared intermediate representation for multiclass object detection with deformable part models (Felzenszwalb et al., 2010a), showing significant speedup factors, but with a large decrease in task performance. In this paper we describe a new training framework that learns which sparselets to activate in order to optimize a discriminative objective, leading to larger speedup factors with no decrease in task performance. We first reformulate sparselets in a general structured output prediction framework, then analyze when sparselets lead to computational efficiency gains, and lastly show experimental results on object detection and image classification tasks. Our experimental results demonstrate that discriminative activation substantially outperforms the previous reconstructive approach which, together with our structured output prediction formulation, make sparselets broadly applicable and significantly more effective.</p>
<p>【Keywords】:</p>
<h3 id="24. The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification.">24. The Pairwise Piecewise-Linear Embedding for Efficient Non-Linear Classification.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/pele13.html">Paper Link</a>】    【Pages】:205-213</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pele:Ofir">Ofir Pele</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Taskar:Ben">Ben Taskar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Globerson:Amir">Amir Globerson</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Werman:Michael">Michael Werman</a></p>
<p>【Abstract】:
Linear classiffers are much faster to learn and test than non-linear ones. On the other hand, non-linear kernels offer improved performance, albeit at the increased cost of training kernel classiffers. To use non-linear mappings with efficient linear learning algorithms, explicit embeddings that approximate popular kernels have recently been proposed. However, the embedding process itself is often costly and the results are usually less accurate than kernel methods. In this work we propose a non-linear feature map that is both very efficient, but at the same time highly expressive. The method is based on discretization and interpolation of individual features values and feature pairs. The discretization allows us to model different regions of the feature space separately, while the interpolation preserves the original continuous values. Using this embedding is strictly more general than a linear model and as efficient as the second-order polynomial explicit feature map. An extensive empirical evaluation shows that our method consistently signiffcantly outperforms other methods, including a wide range of kernels. This is in contrast to other proposed embeddings that were faster than kernel methods, but with lower accuracy.</p>
<p>【Keywords】:</p>
<h3 id="25. Fixed-Point Model For Structured Labeling.">25. Fixed-Point Model For Structured Labeling.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/li13b.html">Paper Link</a>】    【Pages】:214-221</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Quannan">Quannan Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Jingdong">Jingdong Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wipf:David_P=">David P. Wipf</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tu:Zhuowen">Zhuowen Tu</a></p>
<p>【Abstract】:
In this paper, we propose a simple but effective solution to the structured labeling problem: a fixed-point model. Recently, layered models with sequential classifiers/regressors have gained an increasing amount of interests for structural prediction. Here, we design an algorithm with a new perspective on layered models; we aim to find a fixed-point function with the structured labels being both the output and the input. Our approach alleviates the burden in learning multiple/different classifiers in different layers. We devise a training strategy for our method and provide justifications for the fixed-point function to be a contraction mapping. The learned function captures rich contextual information and is easy to train and test. On several widely used benchmark datasets, the proposed method observes significant improvement in both performance and efficiency over many state-of-the-art algorithms.</p>
<p>【Keywords】:</p>
<h3 id="26. Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation.">26. Connecting the Dots with Landmarks: Discriminatively Learning Domain-Invariant Features for Unsupervised Domain Adaptation.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/gong13.html">Paper Link</a>】    【Pages】:222-230</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gong:Boqing">Boqing Gong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Grauman:Kristen">Kristen Grauman</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sha:Fei">Fei Sha</a></p>
<p>【Abstract】:
Learning domain-invariant features is of vital importance to unsupervised domain adaptation, where classifiers trained on the source domain need to be adapted to a different target domain for which no labeled examples are available. In this paper, we propose a novel approach for learning such features. The central idea is to exploit the existence of landmarks, which are a subset of labeled data instances in the source domain that are distributed most similarly to the target domain. Our approach automatically discovers the landmarks and use them to bridge the source to the target by constructing provably easier auxiliary domain adaptation tasks. The solutions of those auxiliary tasks form the basis to compose invariant features for the original task. We show how this composition can be optimized discriminatively without requiring labels from the target domain. We validate the method on standard benchmark datasets for visual object recognition and sentiment analysis of text. Empirical results show the proposed method outperforms the state-of-the-art significantly.</p>
<p>【Keywords】:</p>
<h3 id="27. Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization.">27. Fast Conical Hull Algorithms for Near-separable Non-negative Matrix Factorization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kumar13b.html">Paper Link</a>】    【Pages】:231-239</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar_0001:Abhishek">Abhishek Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sindhwani:Vikas">Vikas Sindhwani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kambadur:Prabhanjan">Prabhanjan Kambadur</a></p>
<p>【Abstract】:
The separability assumption (Arora et al., 2012; Donoho &amp; Stodden, 2003) turns non-negative matrix factorization (NMF) into a tractable problem. Recently, a new class of provably-correct NMF algorithms have emerged under this assumption. In this paper, we reformulate the separable NMF problem as that of finding the extreme rays of the conical hull of a finite set of vectors. From this geometric perspective, we derive new separable NMF algorithms that are highly scalable and empirically noise robust, and have several favorable properties in relation to existing methods. A parallel implementation of our algorithm scales excellently on shared and distributed-memory machines.</p>
<p>【Keywords】:</p>
<h3 id="28. Principal Component Analysis on non-Gaussian Dependent Data.">28. Principal Component Analysis on non-Gaussian Dependent Data.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/han13.html">Paper Link</a>】    【Pages】:240-248</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Han:Fang">Fang Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a></p>
<p>【Abstract】:
In this paper, we analyze the performance of a semiparametric principal component analysis named Copula Component Analysis (COCA) (Han &amp; Liu, 2012) when the data are dependent. The semiparametric model assumes that, after unspecified marginally monotone transformations, the distributions are multivariate Gaussian. We study the scenario where the observations are drawn from non-i.i.d. processes (m-dependency or a more general phi-mixing case). We show that COCA can allow weak dependence. In particular, we provide the generalization bounds of convergence for both support recovery and parameter estimation of COCA for the dependent data. We provide explicit sufficient conditions on the degree of dependence, under which the parametric rate can be maintained. To our knowledge, this is the first work analyzing the theoretical performance of PCA for the dependent data in high dimensional settings. Our results strictly generalize the analysis in Han &amp; Liu (2012) and the techniques we used have the separate interest for analyzing a variety of other multivariate statistical methods.</p>
<p>【Keywords】:</p>
<h3 id="29. Learning Linear Bayesian Networks with Latent Variables.">29. Learning Linear Bayesian Networks with Latent Variables.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/anandkumar13.html">Paper Link</a>】    【Pages】:249-257</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Anandkumar:Animashree">Animashree Anandkumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hsu:Daniel_J=">Daniel J. Hsu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Javanmard:Adel">Adel Javanmard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kakade:Sham">Sham Kakade</a></p>
<p>【Abstract】:
This work considers the problem of learning linear Bayesian networks when some of the variables are unobserved. Identifiability and efficient recovery from low-order observable moments are established under a novel graphical constraint. The constraint concerns the expansion properties of the underlying directed acyclic graph (DAG) between observed and unobserved variables in the network, and it is satisfied by many natural families of DAGs that include multi-level DAGs, DAGs with effective depth one, as well as certain families of polytrees.</p>
<p>【Keywords】:</p>
<h3 id="30. Multiple Identifications in Multi-Armed Bandits.">30. Multiple Identifications in Multi-Armed Bandits.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/bubeck13.html">Paper Link</a>】    【Pages】:258-265</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bubeck:S=eacute=bastien">Sébastien Bubeck</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Tengyao">Tengyao Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Viswanathan:Nitin">Nitin Viswanathan</a></p>
<p>【Abstract】:
We study the problem of identifying the top m arms in a multi-armed bandit game. Our proposed solution relies on a new algorithm based on successive rejects of the seemingly bad arms, and successive accepts of the good ones. This algorithmic contribution allows to tackle other multiple identifications settings that were previously out of reach. In particular we show that this idea of successive accepts and rejects applies to the multi-bandit best arm identification problem.</p>
<p>【Keywords】:</p>
<h3 id="31. Learning Optimally Sparse Support Vector Machines.">31. Learning Optimally Sparse Support Vector Machines.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/cotter13.html">Paper Link</a>】    【Pages】:266-274</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Cotter:Andrew">Andrew Cotter</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shalev=Shwartz:Shai">Shai Shalev-Shwartz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Srebro:Nati">Nati Srebro</a></p>
<p>【Abstract】:
We show how to train SVMs with an optimal guarantee on the number of support vectors (up to constants), and with sample complexity and training runtime bounds matching the best known for kernel SVM optimization (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice.</p>
<p>【Keywords】:</p>
<h3 id="32. Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks.">32. Dynamic Probabilistic Models for Latent Feature Propagation in Social Networks.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/heaukulani13.html">Paper Link</a>】    【Pages】:275-283</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Heaukulani:Creighton">Creighton Heaukulani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghahramani:Zoubin">Zoubin Ghahramani</a></p>
<p>【Abstract】:
Current Bayesian models for dynamic social network data have focused on modelling the influence of evolving unobserved structure on observed social interactions. However, an understanding of how observed social relationships from the past affect future unobserved structure in the network has been neglected. In this paper, we introduce a new probabilistic model for capturing this phenomenon, which we call latent feature propagation, in social networks. We demonstrate our model’s capability for inferring such latent structure in varying types of social network datasets, and experimental studies show this structure achieves higher predictive performance on link prediction and forecasting tasks.</p>
<p>【Keywords】:</p>
<h3 id="33. Efficient Sparse Group Feature Selection via Nonconvex Optimization.">33. Efficient Sparse Group Feature Selection via Nonconvex Optimization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/xiang13.html">Paper Link</a>】    【Pages】:284-292</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xiang:Shuo">Shuo Xiang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tong:Xiaoshen">Xiaoshen Tong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Jieping">Jieping Ye</a></p>
<p>【Abstract】:
Sparse feature selection has been demonstrated to be effective in handling high-dimensional data. While promising, most of the existing works use convex methods, which may be suboptimal in terms of the accuracy of feature selection and parameter estimation. In this paper, we expand a nonconvex paradigm to sparse group feature selection, which is motivated by applications that require identifying the underlying group structure and performing feature selection simultaneously. The main contributions of this article are twofold: (1) computationally, we introduce a nonconvex sparse group feature selection model and present an efficient optimization algorithm, of which the key step is a projection with two coupled constraints; (2) statistically, we show that the proposed model can reconstruct the oracle estimator. Therefore, consistent feature selection and parameter estimation can be achieved. Numerical results on synthetic and real-world data suggest that the proposed nonconvex method compares favorably against its competitors, thus achieving desired goal of delivering high performance.</p>
<p>【Keywords】:</p>
<h3 id="34. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.">34. Domain Adaptation for Sequence Labeling Tasks with a Probabilistic Language Adaptation Model.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/xiao13.html">Paper Link</a>】    【Pages】:293-301</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/x/Xiao:Min">Min Xiao</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guo:Yuhong">Yuhong Guo</a></p>
<p>【Abstract】:
In this paper, we propose to address the problem of domain adaptation for sequence labeling tasks via distributed representation learning by using a log-bilinear language adaptation model. The proposed neural probabilistic language model simultaneously models two different but related data distributions in the source and target domains based on induced distributed representations, which encode both generalizable and domain-specific latent features. We then use the learned dense real-valued representation as augmenting features for natural language processing systems. We empirically evaluate the proposed learning technique on WSJ and MEDLINE domains with POS tagging systems, and on WSJ and Brown corpora with syntactic chunking and name entity recognition systems. Our primary results show that the proposed domain adaptation method outperforms a number comparison methods for cross domain sequence labeling tasks.</p>
<p>【Keywords】:</p>
<h3 id="35. Maximum Variance Correction with Application to A* Search.">35. Maximum Variance Correction with Application to A* Search.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chen13c.html">Paper Link</a>】    【Pages】:302-310</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Wenlin">Wenlin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yixin">Yixin Chen</a></p>
<p>【Abstract】:
In this paper we introduce Maximum Variance Correction (MVC), which finds large-scale feasible solutions to Maximum Variance Unfolding (MVU) by post-processing embeddings from any manifold learning algorithm. It increases the scale of MVU embeddings by several orders of magnitude and is naturally parallel. This unprecedented scalability opens up new avenues of applications for manifold learning, in particular the use of MVU embeddings as effective heuristics to speed-up A<em> search (Rayner et al. 2011). We demonstrate that MVC embeddings lead to un-matched reductions in search time across several non-trivial A</em> benchmark search problems and bridge the gap between the manifold learning literature and one of its most promising high impact applications.</p>
<p>【Keywords】:</p>
<h3 id="36. Adaptive Sparsity in Gaussian Graphical Models.">36. Adaptive Sparsity in Gaussian Graphical Models.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wong13.html">Paper Link</a>】    【Pages】:311-319</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wong:Eleanor">Eleanor Wong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Awate:Suyash_P=">Suyash P. Awate</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fletcher:P=_Thomas">P. Thomas Fletcher</a></p>
<p>【Abstract】:
An effective approach to structure learning and parameter estimation for Gaussian graphical models is to impose a sparsity prior, such as a Laplace prior, on the entries of the precision matrix. Such an approach involves a hyperparameter that must be tuned to control the amount of sparsity. In this paper, we introduce a parameter-free method for estimating a precision matrix with sparsity that adapts to the data automatically. We achieve this by formulating a hierarchical Bayesian model of the precision matrix with a non-informative Jeffreys’ hyperprior. We also naturally enforce the symmetry and positive-definiteness constraints on the precision matrix by parameterizing it with the Cholesky decomposition. Experiments on simulated and real (cell signaling) data demonstrate that the proposed approach not only automatically adapts the sparsity of the model, but it also results in improved estimates of the precision matrix compared to the Laplace prior model with sparsity parameter chosen by cross-validation.</p>
<p>【Keywords】:</p>
<h3 id="37. Average Reward Optimization Objective In Partially Observable Domains.">37. Average Reward Optimization Objective In Partially Observable Domains.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/grinberg13.html">Paper Link</a>】    【Pages】:320-328</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Grinberg:Yuri">Yuri Grinberg</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Precup:Doina">Doina Precup</a></p>
<p>【Abstract】:
We consider the problem of average reward optimization in domains with partial observability, within the modeling framework of linear predictive state representations (PSRs). The key to average-reward computation is to have a well-defined stationary behavior of a system, so the required averages can be computed. If, additionally, the stationary behavior varies smoothly with changes in policy parameters, average-reward control through policy search also becomes a possibility. In this paper, we show that PSRs have a well-behaved stationary distribution, which is a rational function of policy parameters. Based on this result, we define a related reward process particularly suitable for average reward optimization, and analyze its properties. We show that in such a predictive state reward process, the average reward is a rational function of the policy parameters, whose complexity depends on the dimension of the underlying linear PSR. This result suggests that average reward-based policy search methods can be effective when the dimension of the system is small, even when the system representation in the POMDP framework requires many hidden states. We provide illustrative examples of this type.</p>
<p>【Keywords】:</p>
<h3 id="38. Feature Selection in High-Dimensional Classification.">38. Feature Selection in High-Dimensional Classification.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kolar13.html">Paper Link</a>】    【Pages】:329-337</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kolar:Mladen">Mladen Kolar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a></p>
<p>【Abstract】:
High-dimensional discriminant analysis is of fundamental importance in multivariate statistics. Existing theoretical results sharply characterize different procedures, providing sharp convergence results for the classification risk, as well as the l2 convergence results to the discriminative rule. However, sharp theoretical results for the problem of variable selection have not been established, even though model interpretation is of importance in many scientific domains. In this paper, we bridge this gap by providing sharp sufficient conditions for consistent variable selection using the ROAD estimator (Fan et al., 2010). Our results provide novel theoretical insights for the ROAD estimator. Sufficient conditions are complemented by the necessary information theoretic limits on variable selection in high-dimensional discriminant analysis. This complementary result also establishes optimality of the ROAD estimator for a certain family of problems.</p>
<p>【Keywords】:</p>
<h3 id="39. Human Boosting.">39. Human Boosting.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/pareek13.html">Paper Link</a>】    【Pages】:338-346</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/p/Pareek:Harsh_H=">Harsh H. Pareek</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ravikumar:Pradeep">Pradeep Ravikumar</a></p>
<p>【Abstract】:
Humans may be exceptional learners but they have biological limitations and moreover, inductive biases similar to machine learning algorithms. This puts limits on human learning ability and on the kinds of learning tasks humans can easily handle. In this paper, we consider the problem of “boosting” human learners to extend the learning ability of human learners and achieve improved performance on tasks which individual humans find difficult. We consider classification (category learning) tasks, propose a boosting algorithm for human learners and give theoretical justifications. We conduct experiments using Amazon’s Mechanical Turk on two synthetic datasets – a crosshair task with a nonlinear decision boundary and a gabor patch task with a linear boundary but which is inaccessible to human learners – and one real world dataset – the Opinion Spam detection task introduced in (Ott et al). Our results show that boosting human learners produces gains in accuracy and can overcome some fundamental limitations of human learners.</p>
<p>【Keywords】:</p>
<h3 id="40. Efficient Dimensionality Reduction for Canonical Correlation Analysis.">40. Efficient Dimensionality Reduction for Canonical Correlation Analysis.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/avron13.html">Paper Link</a>】    【Pages】:347-355</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Avron:Haim">Haim Avron</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boutsidis:Christos">Christos Boutsidis</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Toledo:Sivan">Sivan Toledo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zouzias:Anastasios">Anastasios Zouzias</a></p>
<p>【Abstract】:
We present a fast algorithm for approximate Canonical Correlation Analysis (CCA). Given a pair of tall-and-thin matrices, the proposed algorithm first employs a randomized dimensionality reduction transform to reduce the size of the input matrices, and then applies any standard CCA algorithm to the new pair of matrices. The algorithm computes an approximate CCA to the original pair of matrices with provable guarantees, while requiring asymptotically less operations than the state-of-the-art exact algorithms.</p>
<p>【Keywords】:</p>
<h3 id="41. Parsing epileptic events using a Markov switching process model for correlated time series.">41. Parsing epileptic events using a Markov switching process model for correlated time series.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wulsin13.html">Paper Link</a>】    【Pages】:356-364</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wulsin:Drausin">Drausin Wulsin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fox:Emily_B=">Emily B. Fox</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Litt:Brian">Brian Litt</a></p>
<p>【Abstract】:
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in addition to full-blown clinical seizures. We believe the relationship between these two classes of events—something not previously studied quantitatively—could yield important insights into the nature and intrinsic dynamics of seizures. A goal of our work is to parse these complex epileptic events into distinct dynamic regimes. A challenge posed by the intracranial EEG (iEEG) data we study is the fact that the number and placement of electrodes can vary between patients. We develop a Bayesian nonparametric Markov switching process that allows for (i) shared dynamic regimes between a variable numbers of channels, (ii) asynchronous regime-switching, and (iii) an unknown dictionary of dynamic regimes. We encode a sparse and changing set of dependencies between the channels using a Markov-switching Gaussian graphical model for the innovations process driving the channel dynamics. We demonstrate the importance of this model in parsing and out-of-sample predictions of iEEG data. We show that our model produces intuitive state assignments that can help automate clinical analysis of seizures and enable the comparison of sub-clinical bursts and full clinical seizures.</p>
<p>【Keywords】:</p>
<h3 id="42. Optimal rates for stochastic convex optimization under Tsybakov noise condition.">42. Optimal rates for stochastic convex optimization under Tsybakov noise condition.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/ramdas13.html">Paper Link</a>】    【Pages】:365-373</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ramdas:Aaditya">Aaditya Ramdas</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singh:Aarti">Aarti Singh</a></p>
<p>【Abstract】:
We focus on the problem of minimizing a convex function (f) over a convex set (S) given (T) queries to a stochastic first order oracle. We argue that the complexity of convex minimization is only determined by the rate of growth of the function around its minimum (x^<em>_{f,S}), as quantified by a Tsybakov-like noise condition. Specifically, we prove that if (f) grows at least as fast as (|x-x^</em><em>{f,S}|^\kappa) around its minimum, for some (\kappa &gt; 1), then the optimal rate of learning (f(x^*</em>{f,S})) is (\Theta(T^{-\frac{\kappa}{2\kappa-2}})). The classic rate (\Theta(1/\sqrt T)) for convex functions and (\Theta(1/T)) for strongly convex functions are special cases of our result for (\kappa \rightarrow \infty) and (\kappa=2), and even faster rates are attained for (1 &lt; \kappa &lt; 2). We also derive tight bounds for the complexity of learning (x_{f,S}^*), where the optimal rate is (\Theta(T^{-\frac{1}{2\kappa-2}})). Interestingly, these precise rates also characterize the complexity of active learning and our results further strengthen the connections between the fields of active learning and convex optimization, both of which rely on feedback-driven queries.</p>
<p>【Keywords】:</p>
<h3 id="43. A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning.">43. A Randomized Mirror Descent Algorithm for Large Scale Multiple Kernel Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/afkanpour13.html">Paper Link</a>】    【Pages】:374-382</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Afkanpour:Arash">Arash Afkanpour</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gy=ouml=rgy:Andr=aacute=s">András György</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bowling:Michael">Michael Bowling</a></p>
<p>【Abstract】:
We consider the problem of simultaneously learning to linearly combine a very large number of kernels and learn a good predictor based on the learnt kernel. When the number of kernels d to be combined is very large, multiple kernel learning methods whose computational cost scales linearly in d are intractable. We propose a randomized version of the mirror descent algorithm to overcome this issue, under the objective of minimizing the group p-norm penalized empirical risk. The key to achieve the required exponential speed-up is the computationally efficient construction of low-variance estimates of the gradient. We propose importance sampling based estimates, and find that the ideal distribution samples a coordinate with a probability proportional to the magnitude of the corresponding gradient. We show that in the case of learning the coefficients of a polynomial kernel, the combinatorial structure of the base kernels to be combined allows sampling from this distribution in (O(\log(d))) time, making the total computational cost of the method to achieve an epsilon-optimal solution to be (O(\log(d)/epsilon^2)), thereby allowing our method to operate for very large values of d. Experiments with simulated and real data confirm that the new algorithm is computationally more efficient than its state-of-the-art alternatives.</p>
<p>【Keywords】:</p>
<h3 id="44. Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery.">44. Noisy and Missing Data Regression: Distribution-Oblivious Support Recovery.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chen13d.html">Paper Link</a>】    【Pages】:383-391</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Yudong">Yudong Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Caramanis:Constantine">Constantine Caramanis</a></p>
<p>【Abstract】:
Many models for sparse regression typically assume that the covariates are known completely, and without noise. Particularly in high-dimensional applications, this is often not the case. Worse yet, even estimating statistics of the noise (the noise covariance) can be a central challenge. In this paper we develop a simple variant of orthogonal matching pursuit (OMP) for precisely this setting. We show that without knowledge of the noise covariance, our algorithm recovers the support, and we provide matching lower bounds that show that our algorithm performs at the minimax optimal rate. While simple, this is the first algorithm that (provably) recovers support in a noise-distribution-oblivious manner. When knowledge of the noise-covariance is available, our algorithm matches the best-known (\ell^2)-recovery bounds available. We show that these too are min-max optimal. Along the way, we also obtain improved performance guarantees for OMP for the standard sparse regression problem with Gaussian noise.</p>
<p>【Keywords】:</p>
<h3 id="45. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.">45. Dual Averaging and Proximal Gradient Descent for Online Alternating Direction Multiplier Method.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/suzuki13.html">Paper Link</a>】    【Pages】:392-400</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Suzuki:Taiji">Taiji Suzuki</a></p>
<p>【Abstract】:
We develop new stochastic optimization methods that are applicable to a wide range of structured regularizations. Basically our methods are combinations of basic stochastic optimization techniques and Alternating Direction Multiplier Method (ADMM). ADMM is a general framework for optimizing a composite function, and has a wide range of applications. We propose two types of online variants of ADMM, which correspond to online proximal gradient descent and regularized dual averaging respectively. The proposed algorithms are computationally efficient and easy to implement. Our methods yield (O(1/\sqrt{T})) convergence of the expected risk. Moreover, the online proximal gradient descent type method yields (O(\log(T)/T)) convergence for a strongly convex loss. Numerical experiments show effectiveness of our methods in learning tasks with structured sparsity such as overlapped group lasso.</p>
<p>【Keywords】:</p>
<h3 id="46. A New Frontier of Kernel Design for Structured Data.">46. A New Frontier of Kernel Design for Structured Data.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/shin13.html">Paper Link</a>】    【Pages】:401-409</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/s/Shin:Kilho">Kilho Shin</a></p>
<p>【Abstract】:
Many kernels for discretely structured data in the literature are designed within the framework of the convolution kernel and its generalization, the mapping kernel. The two most important advantages to use this framework is an easy-to-check criteria of positive definiteness and efficient computation based on the dynamic programming methodology of the resulting kernels. On the other hand, the recent theory of partitionable kernels reveals that the known kernels only take advantage of a very small portion of the potential of the framework. In fact, we have good opportunities to find novel and important kernels in the unexplored area. In this paper, we shed light on a novel important class of kernels within the framework: We give a mathematical characterization of the class, show a parametric method to optimize kernels of the class to specific problems, based on this characterization, and present some experimental results, which show the new kernels are promising in both accuracy and efficiency.</p>
<p>【Keywords】:</p>
<h3 id="47. Learning with Marginalized Corrupted Features.">47. Learning with Marginalized Corrupted Features.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/vandermaaten13.html">Paper Link</a>】    【Pages】:410-418</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maaten:Laurens_van_der">Laurens van der Maaten</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Minmin">Minmin Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/t/Tyree:Stephen">Stephen Tyree</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Weinberger:Kilian_Q=">Kilian Q. Weinberger</a></p>
<p>【Abstract】:
The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples – which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution – essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.</p>
<p>【Keywords】:</p>
<h3 id="48. Approximation properties of DBNs with binary hidden units and real-valued visible units.">48. Approximation properties of DBNs with binary hidden units and real-valued visible units.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/krause13.html">Paper Link</a>】    【Pages】:419-426</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Krause:Oswin">Oswin Krause</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Fischer:Asja">Asja Fischer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Glasmachers:Tobias">Tobias Glasmachers</a> ; <a href="http://dblp.uni-trier.de/pers/hd/i/Igel:Christian">Christian Igel</a></p>
<p>【Abstract】:
Deep belief networks (DBNs) can approximate any distribution over fixed-length binary vectors. However, DBNs are frequently applied to model real-valued data, and so far little is known about their representational power in this case. We analyze the approximation properties of DBNs with two layers of binary hidden units and visible units with conditional distributions from the exponential family. It is shown that these DBNs can, under mild assumptions, model any additive mixture of distributions from the exponential family with independent variables. An arbitrarily good approximation in terms of Kullback-Leibler divergence of an m-dimensional mixture distribution with n components can be achieved by a DBN with m visible variables and n and n+1 hidden variables in the first and second hidden layer, respectively. Furthermore, relevant infinite mixtures can be approximated arbitrarily well by a DBN with a finite number of neurons. This includes the important special case of an infinite mixture of Gaussian distributions with fixed variance restricted to a compact domain, which in turn can approximate any strictly positive density over this domain.</p>
<p>【Keywords】:</p>
<h3 id="49. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.">49. Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/jaggi13.html">Paper Link</a>】    【Pages】:427-435</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/j/Jaggi:Martin">Martin Jaggi</a></p>
<p>【Abstract】:
We provide stronger and more general primal-dual convergence results for Frank-Wolfe-type algorithms (a.k.a. conditional gradient) for constrained convex optimization, enabled by a simple framework of duality gap certificates. Our analysis also holds if the linear subproblems are only solved approximately (as well as if the gradients are inexact), and is proven to be worst-case optimal in the sparsity of the obtained solutions. On the application side, this allows us to unify a large variety of existing sparse greedy methods, in particular for optimization over convex hulls of an atomic set, even if those sets can only be approximated, including sparse (or structured sparse) vectors or matrices, low-rank matrices, permutation matrices, or max-norm bounded matrices. We present a new general framework for convex optimization over matrix factorizations, where every Frank-Wolfe iteration will consist of a low-rank update, and discuss the broad application areas of this approach.</p>
<p>【Keywords】:</p>
<h3 id="50. General Functional Matrix Factorization Using Gradient Boosting.">50. General Functional Matrix Factorization Using Gradient Boosting.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chen13e.html">Paper Link</a>】    【Pages】:436-444</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chen:Tianqi">Tianqi Chen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Hang">Hang Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yang_0001:Qiang">Qiang Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yong">Yong Yu</a></p>
<p>【Abstract】:
Matrix factorization is among the most successful techniques for collaborative filtering. One challenge of collaborative filtering is how to utilize available auxiliary information to improve prediction accuracy. In this paper, we study the problem of utilizing auxiliary information as features of factorization and propose formalizing the problem as general functional matrix factorization, whose model includes conventional matrix factorization models as its special cases. Moreover, we propose a gradient boosting based algorithm to efficiently solve the optimization problem. Finally, we give two specific algorithms for efficient feature function construction for two specific tasks. Our method can construct more suitable feature functions by searching in an infinite functional space based on training data and thus can yield better prediction accuracy. The experimental results demonstrate that the proposed method outperforms the baseline methods on three real-world datasets.</p>
<p>【Keywords】:</p>
<h3 id="51. Iterative Learning and Denoising in Convolutional Neural Associative Memories.">51. Iterative Learning and Denoising in Convolutional Neural Associative Memories.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/karbasi13.html">Paper Link</a>】    【Pages】:445-453</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Karbasi:Amin">Amin Karbasi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Salavati:Amir_Hesam">Amir Hesam Salavati</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shokrollahi:Amin">Amin Shokrollahi</a></p>
<p>【Abstract】:
The task of a neural associative memory is to retrieve a set of previously memorized patterns from their noisy versions by using a network of neurons. Hence, an ideal network should be able to 1) gradually learn a set of patterns, 2) retrieve the correct pattern from noisy queries and 3) maximize the number of memorized patterns while maintaining the reliability in responding to queries. We show that by considering the inherent redundancy in the memorized patterns, one can obtain all the mentioned properties at once. This is in sharp contrast with the previous work that could only improve one or two aspects at the expense of the third. More specifically, we devise an iterative algorithm that learns the redundancy among the patterns. The resulting network has a retrieval capacity that is exponential in the size of the network. Lastly, by considering the local structures of the network, the asymptotic error correction performance can be made linear in the size of the network.</p>
<p>【Keywords】:</p>
<h3 id="52. Scaling Multidimensional Gaussian Processes using Projected Additive Approximations.">52. Scaling Multidimensional Gaussian Processes using Projected Additive Approximations.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/gilboa13.html">Paper Link</a>】    【Pages】:454-461</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gilboa:Elad">Elad Gilboa</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saat=ccedil=i:Yunus">Yunus Saatçi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cunningham:John_P=">John P. Cunningham</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Gilboa:Elad">Elad Gilboa</a></p>
<p>【Abstract】:
Exact Gaussian Process (GP) regression has (O(N^3)) runtime for data size N, making it intractable for large N. Advances in GP scaling have not been extended to the multidimensional input setting, despite the preponderance of multidimensional applications. This paper introduces and tests a novel method of projected additive approximation to multidimensional GPs. We thoroughly illustrate the power of this method on several datasets, achieving close performance to the naive Full GP at orders of magnitude less cost.</p>
<p>【Keywords】:</p>
<h3 id="53. Active Learning for Multi-Objective Optimization.">53. Active Learning for Multi-Objective Optimization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/zuluaga13.html">Paper Link</a>】    【Pages】:462-470</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zuluaga:Marcela">Marcela Zuluaga</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sergent:Guillaume">Guillaume Sergent</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Krause:Andreas">Andreas Krause</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/P=uuml=schel:Markus">Markus Püschel</a></p>
<p>【Abstract】:
In many fields one encounters the challenge of identifying, out of a pool of possible designs, those that simultaneously optimize multiple objectives. This means that usually there is not one optimal design but an entire set of Pareto-optimal ones with optimal tradeoffs in the objectives. In many applications, evaluating one design is expensive; thus, an exhaustive search for the Pareto-optimal set is unfeasible. To address this challenge, we propose the Pareto Active Learning (PAL) algorithm, which intelligently samples the design space to predict the Pareto-optimal set. Key features of PAL include (1) modeling the objectives as samples from a Gaussian process distribution to capture structure and accommodate noisy evaluation; (2) a method to carefully choose the next design to evaluate to maximize progress; and (3) the ability to control prediction accuracy and sampling cost. We provide theoretical bounds on PAL’s sampling cost required to achieve a desired accuracy. Further, we show an experimental evaluation on three real-world data sets. The results show PAL’s effectiveness; in particular it improves significantly over a state-of-the-art evolutionary algorithm, saving in many cases about 33%.</p>
<p>【Keywords】:</p>
<h3 id="54. A Generalized Kernel Approach to Structured Output Learning.">54. A Generalized Kernel Approach to Structured Output Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kadri13.html">Paper Link</a>】    【Pages】:471-479</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kadri:Hachem">Hachem Kadri</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghavamzadeh:Mohammad">Mohammad Ghavamzadeh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Preux:Philippe">Philippe Preux</a></p>
<p>【Abstract】:
We study the problem of structured output learning from a regression perspective. We first provide a general formulation of the kernel dependency estimation (KDE) approach to this problem using operator-valued kernels. Our formulation overcomes the two main limitations of the original KDE approach, namely the decoupling between outputs in the image space and the inability to use a joint feature space. We then propose a covariance-based operator-valued kernel that allows us to take into account the structure of the kernel feature space. This kernel operates on the output space and only encodes the interactions between the outputs without any reference to the input space. To address this issue, we introduce a variant of our KDE method based on the conditional covariance operator that in addition to the correlation between the outputs takes into account the effects of the input variables. Finally, we evaluate the performance of our KDE approach using both covariance and conditional covariance kernels on three structured output problems, and compare it to the state-of-the art kernel-based structured output regression methods.</p>
<p>【Keywords】:</p>
<h3 id="55. Efficient Active Learning of Halfspaces: an Aggressive Approach.">55. Efficient Active Learning of Halfspaces: an Aggressive Approach.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/gonen13.html">Paper Link</a>】    【Pages】:480-488</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gonen:Alon">Alon Gonen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sabato:Sivan">Sivan Sabato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shalev=Shwartz:Shai">Shai Shalev-Shwartz</a></p>
<p>【Abstract】:
We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efficient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efficient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in significantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings.</p>
<p>【Keywords】:</p>
<h3 id="56. Enhanced statistical rankings via targeted data collection.">56. Enhanced statistical rankings via targeted data collection.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/osting13.html">Paper Link</a>】    【Pages】:489-497</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/o/Osting:Braxton">Braxton Osting</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Brune:Christoph">Christoph Brune</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Osher:Stanley">Stanley Osher</a></p>
<p>【Abstract】:
Given a graph where vertices represent alternatives and pairwise comparison data, (y<em>{ij}), is given on the edges, the statistical ranking problem is to find a potential function, defined on the vertices, such that the gradient of the potential function agrees with pairwise comparisons. We study the dependence of the statistical ranking problem on the available pairwise data, i.e., pairs (i,j) for which the pairwise comparison data (y</em>{ij}) is known, and propose a framework to identify data which, when augmented with the current dataset, maximally increases the Fisher information of the ranking. Under certain assumptions, the data collection problem decouples, reducing to a problem of finding an edge set on the graph (with a fixed number of edges) such that the second eigenvalue of the graph Laplacian is maximal. This reduction of the data collection problem to a spectral graph-theoretic question is one of the primary contributions of this work. As an application, we study the Yahoo! Movie user rating dataset and demonstrate that the addition of a small number of well-chosen pairwise comparisons can significantly increase the Fisher informativeness of the ranking.</p>
<p>【Keywords】:</p>
<h3 id="57. Online Feature Selection for Model-based Reinforcement Learning.">57. Online Feature Selection for Model-based Reinforcement Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/nguyen13.html">Paper Link</a>】    【Pages】:498-506</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen_0005:Trung_Thanh">Trung Thanh Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Li:Zhuoru">Zhuoru Li</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Silander:Tomi">Tomi Silander</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Leong:Tze=Yun">Tze-Yun Leong</a></p>
<p>【Abstract】:
We propose a new framework for learning the world dynamics of feature-rich environments in model-based reinforcement learning. The main idea is formalized as a new, factored state-transition representation that supports efficient online-learning of the relevant features. We construct the transition models through predicting how the actions change the world. We introduce an online sparse coding learning technique for feature selection in high-dimensional spaces. We derive theoretical guarantees for our framework and empirically demonstrate its practicality in both simulated and real robotics domains.</p>
<p>【Keywords】:</p>
<h3 id="58. ELLA: An Efficient Lifelong Learning Algorithm.">58. ELLA: An Efficient Lifelong Learning Algorithm.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/ruvolo13.html">Paper Link</a>】    【Pages】:507-515</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Ruvolo:Paul">Paul Ruvolo</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Eaton:Eric">Eric Eaton</a></p>
<p>【Abstract】:
The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines. In this paper, we develop a method for online multi-task learning in the lifelong learning setting. The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multi-task learning methods, and provide robust theoretical performance guarantees. We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.</p>
<p>【Keywords】:</p>
<h3 id="59. A Structural SVM Based Approach for Optimizing Partial AUC.">59. A Structural SVM Based Approach for Optimizing Partial AUC.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/narasimhan13.html">Paper Link</a>】    【Pages】:516-524</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/n/Narasimhan:Harikrishna">Harikrishna Narasimhan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Agarwal_0001:Shivani">Shivani Agarwal</a></p>
<p>【Abstract】:
The area under the ROC curve (AUC) is a widely used performance measure in machine learning. Increasingly, however, in several applications, ranging from ranking and biometric screening to medical diagnosis, performance is measured not in terms of the full area under the ROC curve, but instead, in terms of the partial area under the ROC curve between two specified false positive rates. In this paper, we develop a structural SVM framework for directly optimizing the partial AUC between any two false positive rates. Our approach makes use of a cutting plane solver along the lines of the structural SVM based approach for optimizing the full AUC developed by Joachims (2005). Unlike the full AUC, where the combinatorial optimization problem needed to find the most violated constraint in the cutting plane solver can be decomposed easily to yield an efficient algorithm, the corresponding optimization problem in the case of partial AUC is harder to decompose. One of our key technical contributions is an efficient algorithm for solving this combinatorial optimization problem that has the same computational complexity as Joachims’ algorithm for optimizing the usual AUC. This allows us to efficiently optimize the partial AUC in any desired false positive range. We demonstrate the approach on a variety of real-world tasks.</p>
<p>【Keywords】:</p>
<h3 id="60. Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs.">60. Convex Relaxations for Learning Bounded-Treewidth Decomposable Graphs.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kumar13c.html">Paper Link</a>】    【Pages】:525-533</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kumar:K=_S=_Sesh">K. S. Sesh Kumar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bach:Francis_R=">Francis R. Bach</a></p>
<p>【Abstract】:
We consider the problem of learning the structure of undirected graphical models with bounded treewidth, within the maximum likelihood framework. This is an NP-hard problem and most approaches consider local search techniques. In this paper, we pose it as a combinatorial optimization problem, which is then relaxed to a convex optimization problem that involves searching over the forest and hyperforest polytopes with special structures. A supergradient method is used to solve the dual problem, with a run-time complexity of (O(k^3 n^{k+2} \log n)) for each iteration, where (n) is the number of variables and (k) is a bound on the treewidth. We compare our approach to state-of-the-art methods on synthetic datasets and classical benchmarks, showing the gains of the novel convex approach.</p>
<p>【Keywords】:</p>
<h3 id="61. Adaptive Task Assignment for Crowdsourced Classification.">61. Adaptive Task Assignment for Crowdsourced Classification.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/ho13.html">Paper Link</a>】    【Pages】:534-542</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Ho:Chien=Ju">Chien-Ju Ho</a> ; <a href="http://dblp.uni-trier.de/pers/hd/j/Jabbari:Shahin">Shahin Jabbari</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Vaughan:Jennifer_Wortman">Jennifer Wortman Vaughan</a></p>
<p>【Abstract】:
Crowdsourcing markets have gained popularity as a tool for inexpensively collecting data from diverse populations of workers. Classification tasks, in which workers provide labels (such as “offensive” or “not offensive”) for instances (such as websites), are among the most common tasks posted, but due to a mix of human error and the overwhelming prevalence of spam, the labels collected are often noisy. This problem is typically addressed by collecting labels for each instance from multiple workers and combining them in a clever way. However, the question of how to choose which tasks to assign to each worker is often overlooked. We investigate the problem of task assignment and label inference for heterogeneous classification tasks. By applying online primal-dual techniques, we derive a provably near-optimal adaptive assignment algorithm. We show that adaptively assigning workers to tasks can lead to more accurate predictions at a lower cost when the available workers are diverse.</p>
<p>【Keywords】:</p>
<h3 id="62. Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning.">62. Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/maillard13.html">Paper Link</a>】    【Pages】:543-551</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Maillard:Odalric=Ambrym">Odalric-Ambrym Maillard</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nguyen:Phuong">Phuong Nguyen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ortner:Ronald">Ronald Ortner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Ryabko:Daniil">Daniil Ryabko</a></p>
<p>【Abstract】:
We consider an agent interacting with an environment in a single stream of actions, observations, and rewards, with no reset. This process is not assumed to be a Markov Decision Process (MDP). Rather, the agent has several representations (mapping histories of past interactions to a discrete state space) of the environment with unknown dynamics, only some of which result in an MDP. The goal is to minimize the average regret criterion against an agent who knows an MDP representation giving the highest optimal reward, and acts optimally in it. Recent regret bounds for this setting are of order (O(T^{2/3})) with an additive term constant yet exponential in some characteristics of the optimal MDP. We propose an algorithm whose regret after T time steps is (O(\sqrt{T})), with all constants reasonably small. This is optimal in T since (O(\sqrt{T})) is the optimal regret in the setting of learning in a (single discrete) MDP.</p>
<p>【Keywords】:</p>
<h3 id="63. Better Mixing via Deep Representations.">63. Better Mixing via Deep Representations.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/bengio13.html">Paper Link</a>】    【Pages】:552-560</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Bengio:Yoshua">Yoshua Bengio</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Mesnil:Gr=eacute=goire">Grégoire Mesnil</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Dauphin:Yann">Yann Dauphin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rifai:Salah">Salah Rifai</a></p>
<p>【Abstract】:
It has been hypothesized, and supported with experimental evidence, that deeper representations, when well trained, tend to do a better job at disentangling the underlying factors of variation. We study the following related conjecture: better representations, in the sense of better disentangling, can be exploited to produce Markov chains that mix faster between modes. Consequently, mixing between modes would be more efficient at higher levels of representation. To better understand this, we propose a secondary conjecture: the higher-level samples fill more uniformly the space they occupy and the high-density manifolds tend to unfold when represented at higher levels. The paper discusses these hypotheses and tests them experimentally through visualization and measurements of mixing between modes and interpolating between samples.</p>
<p>【Keywords】:</p>
<h3 id="64. Online Latent Dirichlet Allocation with Infinite Vocabulary.">64. Online Latent Dirichlet Allocation with Infinite Vocabulary.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/zhai13.html">Paper Link</a>】    【Pages】:561-569</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/z/Zhai:Ke">Ke Zhai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Boyd=Graber:Jordan_L=">Jordan L. Boyd-Graber</a></p>
<p>【Abstract】:
Topic models based on latent Dirichlet allocation (LDA) assume a predefined vocabulary a priori. This is reasonable in batch settings, but it is not reasonable when data are revealed over time, as is the case with streaming / online algorithms. To address this lacuna, we extend LDA by drawing topics from a Dirichlet process whose base distribution is a distribution over all strings rather than from a finite Dirichlet. We develop inference using online variational inference and because we only can consider a finite number of words for each truncated topic propose heuristics to dynamically organize, expand, and contract the set of words we consider in our vocabulary truncation. We show our model can successfully incorporate new words as it encounters new terms and that it performs better than online LDA in evaluations of topic quality and classification performance.</p>
<p>【Keywords】:</p>
<h3 id="65. Characterizing the Representer Theorem.">65. Characterizing the Representer Theorem.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/yu13.html">Paper Link</a>】    【Pages】:570-578</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Yaoliang">Yaoliang Yu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cheng:Hao">Hao Cheng</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schuurmans:Dale">Dale Schuurmans</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Szepesv=aacute=ri:Csaba">Csaba Szepesvári</a></p>
<p>【Abstract】:
The representer theorem assures that kernel methods retain optimality under penalized empirical risk minimization. While a sufficient condition on the form of the regularizer guaranteeing the representer theorem has been known since the initial development of kernel methods, necessary conditions have only been investigated recently. In this paper we completely characterize the necessary and sufficient conditions on the regularizer that ensure the representer theorem holds. The results are surprisingly simple yet broaden the conditions where the representer theorem is known to hold. Extension to the matrix domain is also addressed.</p>
<p>【Keywords】:</p>
<h3 id="66. Dynamical Models and tracking regret in online convex programming.">66. Dynamical Models and tracking regret in online convex programming.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/hall13.html">Paper Link</a>】    【Pages】:579-587</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hall:Eric_C=">Eric C. Hall</a> ; <a href="http://dblp.uni-trier.de/pers/hd/w/Willett:Rebecca">Rebecca Willett</a></p>
<p>【Abstract】:
This paper describes a new online convex optimization method which incorporates a family of candidate dynamical models and establishes novel tracking regret bounds that scale with comparator’s deviation from the best dynamical model in this family. Previous online optimization methods are designed to have a total accumulated loss comparable to that of the best comparator sequence, and existing tracking or shifting regret bounds scale with the overall variation of the comparator sequence. In many practical scenarios, however, the environment is nonstationary and comparator sequences with small variation are quite weak, resulting in large losses. The proposed dynamic mirror descent method, in contrast, can yield low regret relative to highly variable comparator sequences by both tracking the best dynamical model and forming predictions based on that model. This concept is demonstrated empirically in the context of sequential compressive observations of a dynamic scene and tracking a dynamic social network.</p>
<p>【Keywords】:</p>
<h3 id="67. Large-Scale Bandit Problems and KWIK Learning.">67. Large-Scale Bandit Problems and KWIK Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/abernethy13.html">Paper Link</a>】    【Pages】:588-596</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/a/Abernethy:Jacob">Jacob Abernethy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/a/Amin:Kareem">Kareem Amin</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kearns:Michael">Michael Kearns</a> ; <a href="http://dblp.uni-trier.de/pers/hd/d/Draief:Moez">Moez Draief</a></p>
<p>【Abstract】:
We show that parametric multi-armed bandit (MAB) problems with large state and action spaces can be algorithmically reduced to the supervised learning model known as Knows What It Knows or KWIK learning. We give matching impossibility results showing that the KWIK learnability requirement cannot be replaced by weaker supervised learning assumptions. We provide such results in both the standard parametric MAB setting, as well as for a new model in which the action space is finite but growing with time.</p>
<p>【Keywords】:</p>
<h3 id="68. Vanishing Component Analysis.">68. Vanishing Component Analysis.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/livni13.html">Paper Link</a>】    【Pages】:597-605</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Livni:Roi">Roi Livni</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lehavi:David">David Lehavi</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Schein:Sagi">Sagi Schein</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nachlieli:Hila">Hila Nachlieli</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Shalev=Shwartz:Shai">Shai Shalev-Shwartz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Globerson:Amir">Amir Globerson</a></p>
<p>【Abstract】:
The vanishing ideal of a set of n points S, is the set of all polynomials that attain the value of zero on all the points in S. Such ideals can be compactly represented using a small set of polynomials known as generators of the ideal. Here we describe and analyze an efficient procedure that constructs a set of generators of a vanishing ideal. Our procedure is numerically stable, and can be used to find approximately vanishing polynomials. The resulting polynomials capture nonlinear structure in data, and can for example be used within supervised learning. Empirical comparison with kernel methods show that our method constructs more compact classifiers with comparable accuracy.</p>
<p>【Keywords】:</p>
<h3 id="69. Learning an Internal Dynamics Model from Control Demonstration.">69. Learning an Internal Dynamics Model from Control Demonstration.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/golub13.html">Paper Link</a>】    【Pages】:606-614</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Golub:Matthew">Matthew Golub</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Chase:Steven">Steven Chase</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yu:Byron">Byron Yu</a></p>
<p>【Abstract】:
Much work in optimal control and inverse control has assumed that the controller has perfect knowledge of plant dynamics. However, if the controller is a human or animal subject, the subject’s internal dynamics model may differ from the true plant dynamics. Here, we consider the problem of learning the subject’s internal model from demonstrations of control and knowledge of task goals. Due to sensory feedback delay, the subject uses an internal model to generate an internal prediction of the current plant state, which may differ from the actual plant state. We develop a probabilistic framework and exact EM algorithm to jointly estimate the internal model, internal state trajectories, and feedback delay. We applied this framework to demonstrations by a nonhuman primate of brain-machine interface (BMI) control. We discovered that the subject’s internal model deviated from the true BMI plant dynamics and provided significantly better explanation of the recorded neural control signals than did the true plant dynamics.</p>
<p>【Keywords】:</p>
<h3 id="70. Robust Structural Metric Learning.">70. Robust Structural Metric Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/lim13.html">Paper Link</a>】    【Pages】:615-623</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lim:Daryl">Daryl Lim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lanckriet:Gert_R=_G=">Gert R. G. Lanckriet</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/McFee:Brian">Brian McFee</a></p>
<p>【Abstract】:
Metric learning algorithms produce a linear transformation of data which is optimized for a prediction task, such as nearest-neighbor classification or ranking. However, when the input data contains a large portion of non-informative features, existing methods fail to identify the relevant features, and performance degrades accordingly. In this paper, we present an efficient and robust structural metric learning algorithm which enforces group sparsity on the learned transformation, while optimizing for structured ranking output prediction. Experiments on synthetic and real datasets demonstrate that the proposed method outperforms previous methods in both high- and low-noise settings.</p>
<p>【Keywords】:</p>
<h3 id="71. Constrained fractional set programs and their application in local clustering and community detection.">71. Constrained fractional set programs and their application in local clustering and community detection.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/buhler13.html">Paper Link</a>】    【Pages】:624-632</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/B=uuml=hler:Thomas">Thomas Bühler</a> ; <a href="http://dblp.uni-trier.de/pers/hd/r/Rangapuram:Syama_Sundar">Syama Sundar Rangapuram</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Setzer:Simon">Simon Setzer</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hein_0001:Matthias">Matthias Hein</a></p>
<p>【Abstract】:
The (constrained) minimization of a ratio of set functions is a problem frequently occurring in clustering and community detection. As these optimization problems are typically NP-hard, one uses convex or spectral relaxations in practice. While these relaxations can be solved globally optimally, they are often too loose and thus lead to results far away from the optimum. In this paper we show that every constrained minimization problem of a ratio of non-negative set functions allows a tight relaxation into an unconstrained continuous optimization problem. This result leads to a flexible framework for solving constrained problems in network analysis. While a globally optimal solution for the resulting non-convex problem cannot be guaranteed, we outperform the loose convex or spectral relaxations by a large margin on constrained local clustering problems.</p>
<p>【Keywords】:</p>
<h3 id="72. Efficient Semi-supervised and Active Learning of Disjunctions.">72. Efficient Semi-supervised and Active Learning of Disjunctions.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/balcan13.html">Paper Link</a>】    【Pages】:633-641</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/b/Balcan:Nina">Nina Balcan</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Berlind:Christopher">Christopher Berlind</a> ; <a href="http://dblp.uni-trier.de/pers/hd/e/Ehrlich:Steven">Steven Ehrlich</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liang:Yingyu">Yingyu Liang</a></p>
<p>【Abstract】:
We provide efficient algorithms for learning disjunctions in the semi-supervised setting under a natural regularity assumption introduced by (Balcan &amp; Blum, 2005). We prove bounds on the sample complexity of our algorithms under a mild restriction on the data distribution. We also give an active learning algorithm with improved sample complexity and extend all our algorithms to the random classification noise setting.</p>
<p>【Keywords】:</p>
<h3 id="73. Convex Adversarial Collective Classification.">73. Convex Adversarial Collective Classification.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/torkamani13.html">Paper Link</a>】    【Pages】:642-650</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Torkamani:MohamadAli">MohamadAli Torkamani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lowd:Daniel">Daniel Lowd</a></p>
<p>【Abstract】:
In this paper, we present a novel method for robustly performing collective classification in the presence of a malicious adversary that can modify up to a fixed number of binary-valued attributes. Our method is formulated as a convex quadratic program that guarantees optimal weights against a worst-case adversary in polynomial time. In addition to increased robustness against active adversaries, this kind of adversarial regularization can also lead to improved generalization even when no adversary is present. In experiments on real and simulated data, our method consistently outperforms both non-adversarial and non-relational baselines.</p>
<p>【Keywords】:</p>
<h3 id="74. Rounding Methods for Discrete Linear Classification.">74. Rounding Methods for Discrete Linear Classification.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/chevaleyre13.html">Paper Link</a>】    【Pages】:651-659</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Chevaleyre:Yann">Yann Chevaleyre</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koriche:Fr=eacute=d=eacute=ric">Frédéric Koriche</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zucker:Jean=Daniel">Jean-Daniel Zucker</a></p>
<p>【Abstract】:
Learning discrete linear functions is a notoriously difficult challenge. In this paper, the learning task is cast as combinatorial optimization problem: given a set of positive and negative feature vectors in the Euclidean space, the goal is to find a discrete linear function that minimizes the cumulative hinge loss of this training set. Since this problem is NP-hard, we propose two simple rounding algorithms that discretize the fractional solution of the problem. Generalization bounds are derived for two important classes of binary-weighted linear functions, by establishing the Rademacher complexity of these classes and proving approximation bounds for rounding methods. These methods are compared on both synthetic and real-world data.</p>
<p>【Keywords】:</p>
<h2 id="Cycle 2 Papers    42">Cycle 2 Papers    42</h2>
<h3 id="75. Mixture of Mutually Exciting Processes for Viral Diffusion.">75. Mixture of Mutually Exciting Processes for Viral Diffusion.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/yang13a.html">Paper Link</a>】    【Pages】:1-9</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Shuang=Hong">Shuang-Hong Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zha:Hongyuan">Hongyuan Zha</a></p>
<p>【Abstract】:
Diffusion network inference and meme tracking have been two key challenges in viral diffusion. This paper shows that these two tasks can be addressed simultaneously with a probabilistic model involving a mixture of mutually exciting point processes. A fast learning algorithms is developed based on mean-field variational inference with budgeted diffusion bandwidth. The model is demonstrated with applications to the diffusion of viral texts in (1) online social networks (e.g., Twitter) and (2) the blogosphere on the Web.</p>
<p>【Keywords】:</p>
<h3 id="76. Gaussian Process Vine Copulas for Multivariate Dependence.">76. Gaussian Process Vine Copulas for Multivariate Dependence.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/lopez-paz13.html">Paper Link</a>】    【Pages】:10-18</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lopez=Paz:David">David Lopez-Paz</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Hern=aacute=ndez=Lobato:Jos=eacute=_Miguel">José Miguel Hernández-Lobato</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Ghahramani:Zoubin">Zoubin Ghahramani</a></p>
<p>【Abstract】:
Copulas allow to learn marginal distributions separately from the multivariate dependence structure (copula) that links them together into a density function. Vine factorizations ease the learning of high-dimensional copulas by constructing a hierarchy of conditional bivariate copulas. However, to simplify inference, it is common to assume that each of these conditional bivariate copulas is independent from its conditioning variables. In this paper, we relax this assumption by discovering the latent functions that specify the shape of a conditional copula given its conditioning variables. We learn these functions by following a Bayesian approach based on sparse Gaussian processes with expectation propagation for scalable, approximate inference. Experiments on real-world datasets show that, when modeling all conditional dependencies, we obtain better estimates of the underlying copula of the data.</p>
<p>【Keywords】:</p>
<h3 id="77. Stochastic Simultaneous Optimistic Optimization.">77. Stochastic Simultaneous Optimistic Optimization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/valko13.html">Paper Link</a>】    【Pages】:19-27</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/v/Valko:Michal">Michal Valko</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Carpentier:Alexandra">Alexandra Carpentier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Munos:R=eacute=mi">Rémi Munos</a></p>
<p>【Abstract】:
We study the problem of global maximization of a function f given a finite number of evaluations perturbed by noise. We consider a very weak assumption on the function, namely that it is locally smooth (in some precise sense) with respect to some semi-metric, around one of its global maxima. Compared to previous works on bandits in general spaces (Kleinberg et al., 2008; Bubeck et al., 2011a) our algorithm does not require the knowledge of this semi-metric. Our algorithm, StoSOO, follows an optimistic strategy to iteratively construct upper confidence bounds over the hierarchical partitions of the function domain to decide which point to sample next. A finite-time analysis of StoSOO shows that it performs almost as well as the best specifically-tuned algorithms even though the local smoothness of the function is not known.</p>
<p>【Keywords】:</p>
<h3 id="78. Toward Optimal Stratification for Stratified Monte-Carlo Integration.">78. Toward Optimal Stratification for Stratified Monte-Carlo Integration.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/carpentier13.html">Paper Link</a>】    【Pages】:28-36</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/c/Carpentier:Alexandra">Alexandra Carpentier</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Munos:R=eacute=mi">Rémi Munos</a></p>
<p>【Abstract】:
We consider the problem of adaptive stratified sampling for Monte Carlo integration of a function, given a finite number of function evaluations perturbed by noise. Here we address the problem of adapting simultaneously the number of samples into each stratum and the stratification itself. We show a tradeoff in the size of the partitioning. On the one hand it is important to refine the partition in areas where the observation noise or the function are heterogeneous in order to reduce this variability. But on the other hand, a too refined stratification makes it harder to assign the samples according to a near-optimal (oracle) allocation strategy. In this paper we provide an algorithm Monte-Carlo Upper-Lower Confidence Bound that selects online, among a large class of partitions, the partition that provides a near-optimal trade-off, and allocates the samples almost optimally on this partition.</p>
<p>【Keywords】:</p>
<h3 id="79. A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems.">79. A General Iterative Shrinkage and Thresholding Algorithm for Non-convex Regularized Optimization Problems.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/gong13a.html">Paper Link</a>】    【Pages】:37-45</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Gong:Pinghua">Pinghua Gong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/z/Zhang:Changshui">Changshui Zhang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lu:Zhaosong">Zhaosong Lu</a> ; <a href="http://dblp.uni-trier.de/pers/hd/h/Huang:Jianhua">Jianhua Huang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Ye:Jieping">Jieping Ye</a></p>
<p>【Abstract】:
Non-convex sparsity-inducing penalties have recently received considerable attentions in sparse learning. Recent theoretical investigations have demonstrated their superiority over the convex counterparts in several sparse learning settings. However, solving the non-convex optimization problems associated with non-convex penalties remains a big challenge. A commonly used approach is the Multi-Stage (MS) convex relaxation (or DC programming), which relaxes the original non-convex problem to a sequence of convex problems. This approach is usually not very practical for large-scale problems because its computational cost is a multiple of solving a single convex problem. In this paper, we propose a General Iterative Shrinkage and Thresholding (GIST) algorithm to solve the nonconvex optimization problem for a large class of non-convex penalties. The GIST algorithm iteratively solves a proximal operator problem, which in turn has a closed-form solution for many commonly used penalties. At each outer iteration of the algorithm, we use a line search initialized by the Barzilai-Borwein (BB) rule that allows finding an appropriate step size quickly. The paper also presents a detailed convergence analysis of the GIST algorithm. The efficiency of the proposed algorithm is demonstrated by extensive experiments on large-scale data sets.</p>
<p>【Keywords】:</p>
<h3 id="80. Thurstonian Boltzmann Machines: Learning from Multiple Inequalities.">80. Thurstonian Boltzmann Machines: Learning from Multiple Inequalities.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/tran13.html">Paper Link</a>】    【Pages】:46-54</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/t/Tran:Truyen">Truyen Tran</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Phung:Dinh_Q=">Dinh Q. Phung</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Venkatesh:Svetha">Svetha Venkatesh</a></p>
<p>【Abstract】:
We introduce Thurstonian Boltzmann Machines (TBM), a unified architecture that can naturally incorporate a wide range of data inputs at the same time. Our motivation rests in the Thurstonian view that many discrete data types can be considered as being generated from a subset of underlying latent continuous variables, and in the observation that each realisation of a discrete type imposes certain inequalities on those variables. Thus learning and inference in TBM reduce to making sense of a set of inequalities. Our proposed TBM naturally supports the following types: Gaussian, intervals, censored, binary, categorical, muticategorical, ordinal, (in)-complete rank with and without ties. We demonstrate the versatility and capacity of the proposed model on three applications of very different natures; namely handwritten digit recognition, collaborative filtering and complex social survey analysis.</p>
<p>【Keywords】:</p>
<h3 id="81. A Variational Approximation for Topic Modeling of Hierarchical Corpora.">81. A Variational Approximation for Topic Modeling of Hierarchical Corpora.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/kim13.html">Paper Link</a>】    【Pages】:55-63</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Do=kyum">Do-kyum Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/v/Voelker:Geoffrey_M=">Geoffrey M. Voelker</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Saul:Lawrence_K=">Lawrence K. Saul</a></p>
<p>【Abstract】:
We study the problem of topic modeling in corpora whose documents are organized in a multi-level hierarchy. We explore a parametric approach to this problem, assuming that the number of topics is known or can be estimated by cross-validation. The models we consider can be viewed as special (finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For these models we show that there exists a simple variational approximation for probabilistic inference. The approximation relies on a previously unexploited inequality that handles the conditional dependence between Dirichlet latent variables in adjacent levels of the model’s hierarchy. We compare our approach to existing implementations of nonparametric HDPs. On several benchmarks we find that our approach is faster than Gibbs sampling and able to learn more predictive models than existing variational methods. Finally, we demonstrate the large-scale viability of our approach on two newly available corpora from researchers in computer security–one with 350,000 documents and over 6,000 internal subcategories, the other with a five-level deep hierarchy.</p>
<p>【Keywords】:</p>
<h3 id="82. Forecastable Component Analysis.">82. Forecastable Component Analysis.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/goerg13.html">Paper Link</a>】    【Pages】:64-72</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Goerg:Georg_M=">Georg M. Goerg</a></p>
<p>【Abstract】:
I introduce Forecastable Component Analysis (ForeCA), a novel dimension reduction technique for temporally dependent signals. Based on a new forecastability measure, ForeCA finds an optimal transformation to separate a multivariate time series into a forecastable and an orthogonal white noise space. I present a converging algorithm with a fast eigenvector solution. Applications to financial and macro-economic time series show that ForeCA can successfully discover informative structure, which can be used for forecasting as well as classication. The R package ForeCA accompanies this work and is publicly available on CRAN.</p>
<p>【Keywords】:</p>
<h3 id="83. Ellipsoidal Multiple Instance Learning.">83. Ellipsoidal Multiple Instance Learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/krummenacher13.html">Paper Link</a>】    【Pages】:73-81</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/k/Krummenacher:Gabriel">Gabriel Krummenacher</a> ; <a href="http://dblp.uni-trier.de/pers/hd/o/Ong:Cheng_Soon">Cheng Soon Ong</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Buhmann:Joachim_M=">Joachim M. Buhmann</a></p>
<p>【Abstract】:
We propose a large margin method for asymmetric learning with ellipsoids, called eMIL, suited to multiple instance learning (MIL). We derive the distance between ellipsoids and the hyperplane, generalising the standard support vector machine. Negative bags in MIL contain only negative instances, and we treat them akin to uncertain observations in the robust optimisation framework. However, our method allows positive bags to cross the margin, since it is not known which instances within are positive. We show that representing bags as ellipsoids under the introduced distance is the most robust solution when treating a bag as a random variable with finite mean and covariance. Two algorithms are derived to solve the resulting non-convex optimization problem: a concave-convex procedure and a quasi-Newton method. Our method achieves competitive results on benchmark datasets. We introduce a MIL dataset from a real world application of detecting wheel defects from multiple partial observations, and show that eMIL outperforms competing approaches.</p>
<p>【Keywords】:</p>
<h3 id="84. Local Low-Rank Matrix Approximation.">84. Local Low-Rank Matrix Approximation.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/lee13.html">Paper Link</a>】    【Pages】:82-90</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/l/Lee:Joonseok">Joonseok Lee</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kim:Seungyeon">Seungyeon Kim</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lebanon:Guy">Guy Lebanon</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Singer:Yoram">Yoram Singer</a></p>
<p>【Abstract】:
Matrix approximation is a common tool in recommendation systems, text mining, and computer vision. A prevalent assumption in constructing matrix approximations is that the partially observed matrix is of low-rank. We propose a new matrix approximation model where we assume instead that the matrix is locally of low-rank, leading to a representation of the observed matrix as a weighted sum of low-rank matrices. We analyze the accuracy of the proposed local low-rank modeling. Our experiments show improvements in prediction accuracy over classical approaches for recommendation tasks.</p>
<p>【Keywords】:</p>
<h3 id="85. Generic Exploration and K-armed Voting Bandits.">85. Generic Exploration and K-armed Voting Bandits.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/urvoy13.html">Paper Link</a>】    【Pages】:91-99</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/u/Urvoy:Tanguy">Tanguy Urvoy</a> ; <a href="http://dblp.uni-trier.de/pers/hd/c/Cl=eacute=rot:Fabrice">Fabrice Clérot</a> ; <a href="http://dblp.uni-trier.de/pers/hd/f/Feraud:Rapha=euml=l">Raphaël Feraud</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Naamane:Sami">Sami Naamane</a></p>
<p>【Abstract】:
We study a stochastic online learning scheme with partial feedback where the utility of decisions is only observable through an estimation of the environment parameters. We propose a generic pure-exploration algorithm, able to cope with various utility functions from multi-armed bandits settings to dueling bandits. The primary application of this setting is to offer a natural generalization of dueling bandits for situations where the environment parameters reflect the idiosyncratic preferences of a mixed crowd.</p>
<p>【Keywords】:</p>
<h3 id="86. A unifying framework for vector-valued manifold regularization and multi-view learning.">86. A unifying framework for vector-valued manifold regularization and multi-view learning.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/haquang13.html">Paper Link</a>】    【Pages】:100-108</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/m/Minh:Ha_Quang">Ha Quang Minh</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Bazzani:Loris">Loris Bazzani</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Murino:Vittorio">Vittorio Murino</a></p>
<p>【Abstract】:
This paper presents a general vector-valued reproducing kernel Hilbert spaces (RKHS) formulation for the problem of learning an unknown functional dependency between a structured input space and a structured output space, in the Semi-Supervised Learning setting. Our formulation includes as special cases Vector-valued Manifold Regularization and Multi-view Learning, thus provides in particular a unifying framework linking these two important learning approaches. In the case of least square loss function, we provide a closed form solution with an efficient implementation. Numerical experiments on challenging multi-class categorization problems show that our multi-view learning formulation achieves results which are comparable with state of the art and are significantly better than single-view learning.</p>
<p>【Keywords】:</p>
<h3 id="87. Learning Connections in Financial Time Series.">87. Learning Connections in Financial Time Series.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/ganeshapillai13.html">Paper Link</a>】    【Pages】:109-117</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/g/Ganeshapillai:Gartheeban">Gartheeban Ganeshapillai</a> ; <a href="http://dblp.uni-trier.de/pers/hd/g/Guttag:John_V=">John V. Guttag</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Lo:Andrew">Andrew Lo</a></p>
<p>【Abstract】:
To reduce risk, investors seek assets that have high expected return and are unlikely to move in tandem. Correlation measures are generally used to quantify the connections between equities. The 2008 financial crisis, and its aftermath, demonstrated the need for a better way to quantify these connections. We present a machine learning-based method to build a connectedness matrix to address the shortcomings of correlation in capturing events such as large losses. Our method uses an unconstrained optimization to learn this matrix, while ensuring that the resulting matrix is positive semi-definite. We show that this matrix can be used to build portfolios that not only “beat the market,” but also outperform optimal (i.e., minimum variance) portfolios.</p>
<p>【Keywords】:</p>
<h3 id="88. Fast dropout training.">88. Fast dropout training.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wang13a.html">Paper Link</a>】    【Pages】:118-126</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Sida_I=">Sida I. Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Manning:Christopher_D=">Christopher D. Manning</a></p>
<p>【Abstract】:
Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.</p>
<p>【Keywords】:</p>
<h3 id="89. Scalable Optimization of Neighbor Embedding for Visualization.">89. Scalable Optimization of Neighbor Embedding for Visualization.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/yang13b.html">Paper Link</a>】    【Pages】:127-135</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/y/Yang:Zhirong">Zhirong Yang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/p/Peltonen:Jaakko">Jaakko Peltonen</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Kaski:Samuel">Samuel Kaski</a></p>
<p>【Abstract】:
Neighbor embedding (NE) methods have found their use in data visualization but are limited in big data analysis tasks due to their O(n2) complexity for n data samples. We demonstrate that the obvious approach of subsampling produces inferior results and propose a generic approximated optimization technique that reduces the NE optimization cost to O(n log n). The technique is based on realizing that in visualization the embedding space is necessarily very low-dimensional (2D or 3D), and hence efficient approximations developed for n-body force calculations can be applied. In gradient-based NE algorithms the gradient for an individual point decomposes into “forces” exerted by the other points. The contributions of close-by points need to be computed individually but far-away points can be approximated by their “center of mass”, rapidly computable by applying a recursive decomposition of the visualization space into quadrants. The new algorithm brings a significant speed-up for medium-size data, and brings “big data” within reach of visualization.</p>
<p>【Keywords】:</p>
<h3 id="90. Precision-recall space to correct external indices for biclustering.">90. Precision-recall space to correct external indices for biclustering.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/hanczar13.html">Paper Link</a>】    【Pages】:136-144</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Hanczar:Blaise">Blaise Hanczar</a> ; <a href="http://dblp.uni-trier.de/pers/hd/n/Nadif:Mohamed">Mohamed Nadif</a></p>
<p>【Abstract】:
Biclustering is a major tool of data mining in many domains and many algorithms have emerged in recent years. All these algorithms aim to obtain coherent biclusters and it is crucial to have a reliable procedure for their validation. We point out the problem of size bias in biclustering evaluation and show how it can lead to wrong conclusions in a comparative study. We present the theoretical corrections for all of the most popular measures in order to remove this bias. We introduce the corrected precision-recall space that combines the advantages of corrected measures, the ease of interpretation and visualization of uncorrected measures. Numerical experiments demonstrate the interest of our approach.</p>
<p>【Keywords】:</p>
<h3 id="91. Monochromatic Bi-Clustering.">91. Monochromatic Bi-Clustering.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wulff13.html">Paper Link</a>】    【Pages】:145-153</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wulff:Sharon">Sharon Wulff</a> ; <a href="http://dblp.uni-trier.de/pers/hd/u/Urner:Ruth">Ruth Urner</a> ; <a href="http://dblp.uni-trier.de/pers/hd/b/Ben=David:Shai">Shai Ben-David</a></p>
<p>【Abstract】:
We propose a natural cost function for the bi-clustering task, the monochromatic cost. This cost function is suitable for detecting meaningful homogeneous bi-clusters based on categorical valued input matrices. Such tasks arise in many applications, such as the analysis of social networks and in systems-biology where researchers try to infer functional grouping of biological agents based on their pairwise interactions. We analyze the computational complexity of the resulting optimization problem. We present a polynomial time approximation algorithm for this bi-clustering task and complement this result by showing that finding (exact) optimal solutions is NP-hard. As far as we know, these are the first positive approximation guarantees and formal NP-hardness results for any bi-clustering optimization problem. In addition, we show that our optimization problem can be efficiently solved by deterministic annealing, yielding a promising heuristic for large problem instances.</p>
<p>【Keywords】:</p>
<h3 id="92. Gated Autoencoders with Tied Input Weights.">92. Gated Autoencoders with Tied Input Weights.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/alain13.html">Paper Link</a>】    【Pages】:154-162</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/d/Droniou:Alain">Alain Droniou</a> ; <a href="http://dblp.uni-trier.de/pers/hd/s/Sigaud:Olivier">Olivier Sigaud</a></p>
<p>【Abstract】:
The semantic interpretation of images is one of the core applications of deep learning. Several techniques have been recently proposed to model the relation between two images, with application to pose estimation, action recognition or invariant object recognition. Among these techniques, higher-order Boltzmann machines or relational autoencoders consider projections of the images on different subspaces and intermediate layers act as transformation specific detectors. In this work, we extend the mathematical study of (Memisevic, 2012b) to show that it is possible to use a unique projection for both images in a way that turns intermediate layers as spectrum encoders of transformations. We show that this results in networks that are easier to tune and have greater generalization capabilities.</p>
<p>【Keywords】:</p>
<h3 id="93. Strict Monotonicity of Sum of Squares Error and Normalized Cut in the Lattice of Clusterings.">93. Strict Monotonicity of Sum of Squares Error and Normalized Cut in the Lattice of Clusterings.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/rebagliati13.html">Paper Link</a>】    【Pages】:163-171</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/r/Rebagliati:Nicola">Nicola Rebagliati</a></p>
<p>【Abstract】:
Sum of Squares Error and Normalized Cut are two widely used clustering functional. It is known their minimum values are monotone with respect to the input number of clusters and this monotonicity does not allow for a simple automatic selection of a correct number of clusters. Here we study monotonicity not just on the minimizers but on the entire clustering lattice. We show the value of Sum of Squares Error is strictly monotone under the strict refinement relation of clusterings and we obtain data-dependent bounds on the difference between the value of a clustering and one of its refinements. Using analogous techniques we show the value of Normalized Cut is strictly anti-monotone. These results imply that even if we restrict our solutions to form a chain of clustering, like the one we get from hierarchical algorithms, we cannot rely on the functional values in order to choose the number of clusters. By using these results we get some data-dependent bounds on the difference of the values of any two clusterings.</p>
<p>【Keywords】:</p>
<h3 id="94. Transition Matrix Estimation in High Dimensional Time Series.">94. Transition Matrix Estimation in High Dimensional Time Series.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/han13a.html">Paper Link</a>】    【Pages】:172-180</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/h/Han:Fang">Fang Han</a> ; <a href="http://dblp.uni-trier.de/pers/hd/l/Liu:Han">Han Liu</a></p>
<p>【Abstract】:
In this paper, we propose a new method in estimating transition matrices of high dimensional vector autoregressive (VAR) models. Here the data are assumed to come from a stationary Gaussian VAR time series. By formulating the problem as a linear program, we provide a new approach to conduct inference on such models. In theory, under a doubly asymptotic framework in which both the sample size T and dimensionality d of the time series can increase, we provide explicit rates of convergence between the estimator and the population transition matrix under different matrix norms. Our results show that the spectral norm of the transition matrix plays a pivotal role in determining the final rates of convergence. This is the first work analyzing the estimation of transition matrices under a high dimensional doubly asymptotic framework. Experiments are conducted on both synthetic and real-world stock data to demonstrate the effectiveness of the proposed method compared with the existing methods. The results of this paper have broad impact on different applications, including finance, genomics, and brain imaging.</p>
<p>【Keywords】:</p>
<h3 id="95. Label Partitioning For Sublinear Ranking.">95. Label Partitioning For Sublinear Ranking.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/weston13.html">Paper Link</a>】    【Pages】:181-189</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Weston:Jason">Jason Weston</a> ; <a href="http://dblp.uni-trier.de/pers/hd/m/Makadia:Ameesh">Ameesh Makadia</a> ; <a href="http://dblp.uni-trier.de/pers/hd/y/Yee:Hector">Hector Yee</a></p>
<p>【Abstract】:
We consider the case of ranking a very large set of labels, items, or documents, which is common to information retrieval, recommendation, and large-scale annotation tasks. We present a general approach for converting an algorithm which has linear time in the size of the set to a sublinear one via label partitioning. Our method consists of learning an input partition and a label assignment to each partition of the space such that precision at k is optimized, which is the loss function of interest in this setting. Experiments on large-scale ranking and recommendation tasks show that our method not only makes the original linear time algorithm computationally tractable, but can also improve its performance.</p>
<p>【Keywords】:</p>
<h3 id="96. Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing.">96. Subproblem-Tree Calibration: A Unified Approach to Max-Product Message Passing.</h3>
<p>【<a href="http://jmlr.org/proceedings/papers/v28/wang13b.html">Paper Link</a>】    【Pages】:190-198</p>
<p>【Authors】:
<a href="http://dblp.uni-trier.de/pers/hd/w/Wang:Huayan">Huayan Wang</a> ; <a href="http://dblp.uni-trier.de/pers/hd/k/Koller:Daphne">Daphne Koller</a></p>
<p>【Abstract】:
Max-product (max-sum) message passing algorithms are widely used for MAP inference in MRFs. It has many variants sharing a common flavor of passing</p>
 

<div class="home">
<i title='主页' onclick="location.href='../index.html'"><i class="fa fa-home fa-lg"></i></i>
</div>

<div class="toc">
<i id="showLeftPush" title='目录'><i class="fa fa-list fa-lg"></i></i>
</div>

<!-- Classie - class helper functions by @desandro https://github.com/desandro/classie -->
<script>
	var menuLeft = document.getElementById( 'menu-s1' ),
		showLeftPush = document.getElementById( 'showLeftPush' ),
		body = document.body;

	showLeftPush.onclick = function() {
		classie.toggle( this, 'active' );
		classie.toggle( body, 'cbp-spmenu-push-toright' );
		classie.toggle( menuLeft, 'cbp-spmenu-open' );
		disableOther( 'showLeftPush' );
	};
</script>

<div class="go-top" >
<i title='顶部' onclick="window.scrollTo('0', '0')"><i class="fa fa-angle-double-up fa-2x"></i></i>
</div>

<div class="theme" >
<i title='主题' onclick="change_css()"><i class="fa fa-adjust fa-lg"></i></i>
</div>

<div id="footer">

  <p> <i class="fa fa-envelope-o fa-1x"></i>:&nbsp huntercmd@163.com &nbsp Published under<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/deed.zh"> (CC) BY-NC-SA 3.0</a></p>

  <p>&copy; 2013 HunterCmd &nbsp <a href="http://huntercmd.github.io"><i class="fa fa-github fa-1x"></i>
  </p>
</div>

</body>
